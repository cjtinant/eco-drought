
Title (13-words or less): Relating Precipitation and Hydrologic Drought in a Semi-arid Climate with gSSURO Data  
Subtitle: Pine Ridge Reservation and surrounding Areas in Southwestern South Dakota 


## Introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

Intro p1: drought is a "creeping disaster"
Intro p2: what makes a drought? Intensity & duration 
Intro p3: different kinds of drought 
Intro p4: working with drought - skewed distribution -> how to fix?

## What are the gaps this paper hopes to address? 
Evaluate a multi-component regional drought history for the region
How to define minimum precipip record set & distributional characteristics 
Identify "pure" streamflow realizations that incorporate short streamflow gage records to characterize hydrologic drought
Identify spatial vars in gSSURGO data to relate lags to a physical process

# Specific Aims
1. Refine an approach for relating precipitation drought to hydrological drought in gaged watersheds
2. Develop a method to estimate regional hydrologic group membership for ungaged watersheds using gSSURGO data 

An example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

## Methods


## Results
Regional monthly precipitation fits a Pearson type-III distribution.
The mean and deviation of monthly precipitation depth among stations are similiar (e.g little spatial difference).  
However, the distribution of wet and dry periods is temporally dependant 
Station distance is a substantial predictor of covarience
=> Next STEP use distance from centroid to create a predictor variable to test significance..
Regional streamflow clusters into 7 groups.
Hydrologic export coefficient describes ~95% of varience and flashiness (e.g Q1 -Q3) describing 5% of varience.

TO DO - CHECK SPI FIN3 NAME
# dplyr::select -- this is a potential issue!
# CHECK PACKAGES

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE)   
options(tibble.print_max = 70) # sets tibble output for printing 

# Sets up the library of packages 
library("here") # identifies where to save work 
library("rio") # more robust I/O - to import and clean data 
library("janitor") # tools to clean dirty data 
library("broom") # tidies up objects 

library("SCI") # calculates SPI & RDI
library("forecast") # using the BoxCox function
library("mclust") 

library("factoextra") 
library("ggpubr") 
library("corrplot")

library("lubridate") # fixes dates 
library("tidyverse") 
#library("magrittr") # contains easier ways to say things about lists


#library("DataExplorer")
#library("clValid")
#library("cluster")
# library("munsell") # https://github.com/cwickham/munsell
# library("grateful") - not yet ready for R 3.5.0
#library("lintr")
#library("test_that")
#library("jsonlite") # Convert between JSON data and R objects
#library("curl") # Drop-in replacement for base url
#library("listviewer") # htmlwidget for interactive views of R lists
# library("SPEI") # Calculates SPI-index # having some issues... 
#library("standardize")
#lmomco <- citation("lmomco")
#toBibtex(lmomco)



# variables 
# a_session    list variable of session information
# sta          precipitation station  
# spi          Standardized Precipitation Index vals  
# gage         streamflow gage 

# general modifiers:
# _meta        metadata  
# _raw         original imported data in wide format
# _gath        wide data changed to long data format
# _na          exploratory na values 
# _outlier     split outlier data 
# _typical     split not outlier data 
# _replace     intermediate value used to replace outlier value 

# _9398        first streamflow cluster by waterYear

# intermediate variables 
# abbrev       abbreviation; a look-up table 
# _mean        intermediate variable; mean value
# _sum         summary data 
# _prep        intermediate variable 

# regression variables:
# _lm          linear model 
# _aug         Added prediction info about each dataset observation 
# _fit         Fitted regression model information 
# _tidy        Summarizes information about model components 

# precipitation station identifiers:
# _cot        Cottonwood precipitation station
# _oel        Oelrichs precipitation station 
# _rap        Rapid City precipitation station 
# _int        Interior precipitation station 
# _ora        Oral precipitation station

# SCI input & intermediate variables:
# time_scale   sets the length of the averaging period 
# distrib      sets the distribution type 
# p_zero       sets a function to reduce zero-precip bias
# scale        scales input by subtract mean & divide by sd
# warn_me      sets explicit warning
# first_mon    sets the first month for SCI; based on the data

# .cot        Cottonwood precipitation station
# .oel        Oelrichs precipitation station 
# .rap        Rapid City precipitation station 
# .int        Interior precipitation station 
# .ora        Oral precipitation station
# .l          a variable as a list 
# _M          Correlation matrix 
# _check      Temporary variable used to check data

# SCI output variables
# zero_prob   probability of a zero depth month
# _att        SPI function attributes: scale, location, shape 
# _corr       final correlation dataframe
# _index      SPI index

# Gage PCA input & intermediate variables:
# _depth      streamflow depth; depth = Q/A 
# _lambda     BoxCox lambda values defined by a ML estimator 
#   _q1       one-day average streamflow volume or depth 
#   _q7       seven-day average streamflow volume or depth 
#   _q30      thirty-day average streamflow volume or depth 

# pca variables
# _input      matrix with only observations
# _meta       matrix with info about observations 
# _matrix     result of prcomp analysis as prcomp object 
# _eigen      tidy eigenvecter summary 
# _vars       tidy summary about pc loadings on variables 
# _aug        added prediction info about each dataset observation 
# _sum        summary of results



# Session Info
a_session <- devtools::session_info() 
```

```{r spi_overview eval=FALSE} 
# overview of steps---- 
# 2.1    Standardized Precipitation Index 
# note: SPI package not working; created a test case to send to author
# 2.1.1  Download & munge precip data: complete; _04_prco-data_munging
# 2.1.2  Identify distribution: complete;  _05_L-moment_diagram
# 2.1.3   Calculate SPI: completed spi1, spi6

# 2.2    Stream Drought Index
# 2.2.1  Download & mungeStream data: finished - July 16
# 2.2.2  Learn purrr::map(): ok.finished 
#          http://r4ds.had.co.nz/iteration.html#the-map-functions
# 2.2.3 Cluster time series - in progress
#         see Kassambara, "Practical Guide to Cluster Analysis in R
# 2.2.4   Calculate SDI
# 2.2.5   Delineate watersheds
# 2.2.6   Cluster ungaged stations
# 2.2.7 Learn 'rf' function
# 2.3   Disseminate results
# 2.3.1 Identify journal

## Analysis Steps & progress
# 1.    Identify precipitation records for drought analysis
#         I imported Global Historical Climatology Network (GHCN) 
#         daily precipitation records for candidate "WEATHER STATIONS" 
#         into R-Studio (REF1) using the "rnoaa" package.
# 1.2   Cleaned data (see 04_prcp-data_munging)  
# 1.2.1   I used Theissen polygons and the length and continuity of 
#         precipitation records for initial station selection 
# 1.2.2  'dplyr' to fill daily NA values with data from nearest station
# 1.2.3   create monthly vals from daily vals.
# 1.2.4 I removed short record: Long Valley after checking covariance.
# 1.3   Exploratory EDA
# 1.3.1 Applied sqrt & log10 transform to explore effects on skew 
# 1.3.2 Explored the data with box plots, violin plot.
#        Sqrt trans vs plotting vals look slightly sinusoidal
#        Log-tranformation is mirror of orig depth vs plotting
# 1.3.3 Applied Weibull plotting position & graphed on sqrt plot 
# 1.3.4 Completed exploratory PCA => covarience by zero months?
# 1.4.  Calculated L-moments and L-moment ratios => Pearson-type III
# 1.4.1 Calculated 1 & 6 month SPI 
# 1.5   Clustered streamflows during wet period 
# 1.5.1 Identify streamflow records for hydrology analysis (list) 
# 1.5.2 Exploratory EDA of streamflow to identify membership of 
#       short records 

## Thoughts - the orig depth vs plotting vals look j-shaped. 

# Next steps: 
#2. find drought years & wet years for precip
#3. Examine clusters for dry years
#4. Supervised classification - random forest should cross-validate?

# Someday - Maybe
# 1. Map the variable as a function - might put off, but ugly and long 
#   code below!
# 2. figure out how to reference stuff with - grateful package
```

```{r rules_for_papers, eval=FALSE, include=FALSE} 
<!---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ten simple rules for structuring papers 

Citation: Mensh B, Kording K (2017) Ten simple rules for structuring papers. PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619

Principles (Rules 1–4)
Rule 1: Focus your paper on a central contribution, which you communicate in the title

Your communication efforts are successful if readers can still describe the main contribution of your paper to their colleagues a year after reading it. Although it is clear that a paper often needs to communicate a number of innovations on the way to its final message, it does not pay to be greedy. Focus on a single message; papers that simultaneously focus on multiple contributions tend to be less convincing about each and are therefore less memorable.

The most important element of a paper is the title—think of the ratio of the number of titles you read to the number of papers you read. The title is typically the first element a reader encounters, so its quality [3] determines whether the reader will invest time in reading the abstract.

The title not only transmits the paper’s central contribution but can also serve as a constant reminder (to you) to focus the text on transmitting that idea. Science is, after all, the abstraction of simple principles from complex data. The title is the ultimate refinement of the paper’s contribution. Thinking about the title early—and regularly returning to hone it—can help not only the writing of the paper but also the process of designing experiments or developing theories.

This Rule of One is the most difficult rule to optimally implement because it comes face-to-face with the key challenge of science, which is to make the claim and/or model as simple as the data and logic can support but no simpler. In the end, your struggle to find this balance may appropriately result in “one contribution” that is multifaceted. For example, a technology paper may describe both its new technology and a biological result using it; the bridge that unifies these two facets is a clear description of how the new technology can be used to do new biology.

Rule 2: Write for flesh-and-blood human beings who do not know your work

Because you are the world’s leading expert at exactly what you are doing, you are also the world’s least qualified person to judge your writing from the perspective of the naïve reader. The majority of writing mistakes stem from this predicament. Think like a designer—for each element, determine the impact that you want to have on people and then strive to achieve that objective [4]. Try to think through the paper like a naïve reader who must first be made to care about the problem you are addressing (see Rule 6) and then will want to understand your answer with minimal effort.

Define technical terms clearly because readers can become frustrated when they encounter a word that they don’t understand. Avoid abbreviations and acronyms so that readers do not have to go back to earlier sections to identify them.

The vast knowledge base of human psychology is useful in paper writing. For example, people have working memory constraints in that they can only remember a small number of items and are better at remembering the beginning and the end of a list than the middle [5]. Do your best to minimize the number of loose threads that the reader has to keep in mind at any one time.

Rule 3: Stick to the context-content-conclusion (C-C-C) scheme

The vast majority of popular (i.e., memorable and re-tellable) stories have a structure with a discernible beginning, a well-defined body, and an end. The beginning sets up the context for the story, while the body (content) advances the story towards an ending in which the problems find their conclusions. This structure reduces the chance that the reader will wonder “Why was I told that?” (if the context is missing) or “So what?” (if the conclusion is missing).

There are many ways of telling a story. Mostly, they differ in how well they serve a patient reader versus an impatient one [6]. The impatient reader needs to be engaged quickly; this can be accomplished by presenting the most exciting content first (e.g., as seen in news articles). The C-C-C scheme that we advocate serves a more patient reader who is willing to spend the time to get oriented with the context. A consequent disadvantage of C-C-C is that it may not optimally engage the impatient reader. This disadvantage is mitigated by the fact that the structure of scientific articles, specifically the primacy of the title and abstract, already forces the content to be revealed quickly. Thus, a reader who proceeds to the introduction is likely engaged enough to have the patience to absorb the context. Furthermore, one hazard of excessive “content first” story structures in science is that you may generate skepticism in the reader because they may be missing an important piece of context that makes your claim more credible. For these reasons, we advocate C-C-C as a “default” scientific story structure.

The C-C-C scheme defines the structure of the paper on multiple scales. At the whole-paper scale, the introduction sets the context, the results are the content, and the discussion brings home the conclusion. Applying C-C-C at the paragraph scale, the first sentence defines the topic or context, the body hosts the novel content put forth for the reader’s consideration, and the last sentence provides the conclusion to be remembered.

Deviating from the C-C-C structure often leads to papers that are hard to read, but writers often do so because of their own autobiographical context. During our everyday lives as scientists, we spend a majority of our time producing content and a minority amidst a flurry of other activities. We run experiments, develop the exposition of available literature, and combine thoughts using the magic of human cognition. It is natural to want to record these efforts on paper and structure a paper chronologically. But for our readers, most details of our activities are extraneous. They do not care about the chronological path by which you reached a result; they just care about the ultimate claim and the logic supporting it (see Rule 7). Thus, all our work must be reformatted to provide a context that makes our material meaningful and a conclusion that helps the reader to understand and remember it.

Rule 4: Optimize your logical flow by avoiding zig-zag and using parallelism
Avoiding zig-zag.

Only the central idea of the paper should be touched upon multiple times. Otherwise, each subject should be covered in only one place in order to minimize the number of subject changes. Related sentences or paragraphs should be strung together rather than interrupted by unrelated material. Ideas that are similar, such as two reasons why we should believe something, should come one immediately after the other.
Using parallelism.

Similarly, across consecutive paragraphs or sentences, parallel messages should be communicated with parallel form. Parallelism makes it easier to read the text because the reader is familiar with the structure. For example, if we have three independent reasons why we prefer one interpretation of a result over another, it is helpful to communicate them with the same syntax so that this syntax becomes transparent to the reader, which allows them to focus on the content. There is nothing wrong with using the same word multiple times in a sentence or paragraph. Resist the temptation to use a different word to refer to the same concept—doing so makes readers wonder if the second word has a slightly different meaning.
The components of a paper (Rules 5–8)

The individual parts of a paper—abstract, introduction, results, and discussion—have different objectives, and thus they each apply the C-C-C structure a little differently in order to achieve their objectives. We will discuss these specialized structures in this section and summarize them in Fig 1.
thumbnail

Fig 1. Summary of a paper’s structural elements at three spatial scales: Across sections, across paragraphs, and within paragraphs.

Note that the abstract is special in that it contains all three elements (Context, Content, and Conclusion), thus comprising all three colors.

https://doi.org/10.1371/journal.pcbi.1005619.g001
Rule 5: Tell a complete story in the abstract

The abstract is, for most readers, the only part of the paper that will be read. This means that the abstract must convey the entire message of the paper effectively. To serve this purpose, the abstract’s structure is highly conserved. Each of the C-C-C elements is detailed below.

The context must communicate to the reader what gap the paper will fill. The first sentence orients the reader by introducing the broader field in which the particular research is situated. Then, this context is narrowed until it lands on the open question that the research answered. A successful context section sets the stage for distinguishing the paper’s contributions from the current state of the art by communicating what is missing in the literature (i.e., the specific gap) and why that matters (i.e., the connection between the specific gap and the broader context that the paper opened with).

The content (“Here we”) first describes the novel method or approach that you used to fill the gap or question. Then you present the meat—your executive summary of the results.

Finally, the conclusion interprets the results to answer the question that was posed at the end of the context section. There is often a second part to the conclusion section that highlights how this conclusion moves the broader field forward (i.e., “broader significance”). This is particularly true for more “general” journals with a broad readership.

This structure helps you avoid the most common mistake with the abstract, which is to talk about results before the reader is ready to understand them. Good abstracts usually take many iterations of refinement to make sure the results fill the gap like a key fits its lock. The broad-narrow-broad structure allows you to communicate with a wider readership (through breadth) while maintaining the credibility of your claim (which is always based on a finite or narrow set of results).

Rule 6: Communicate why the paper matters in the introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

As an example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

Rule 7-Results: Deliver the results as a sequence of statements, supported by figures, that connect logically to support the central contribution

The results section needs to convince the reader that the central claim is supported by data and logic. Every scientific argument has its own particular logical structure, which dictates the sequence in which its elements should be presented.

For example, a paper may set up a hypothesis, verify that a method for measurement is valid in the system under study, and then use the measurement to disprove the hypothesis. Alternatively, a paper may set up multiple alternative (and mutually exclusive) hypotheses and then disprove all but one to provide evidence for the remaining interpretation. The fabric of the argument will contain controls and methods where they are needed for the overall logic.

In the outlining phase of paper preparation (see Rule 9), sketch out the logical structure of how your results support your claim and convert this into a sequence of declarative statements that become the headers of subsections within the results section (and/or the titles of figures). Most journals allow this type of formatting, but if your chosen journal does not, these headers are still useful during the writing phase and can either be adapted to serve as introductory sentences to your paragraphs or deleted before submission. Such a clear progression of logical steps makes the paper easy to follow.

Figures, their titles, and legends are particularly important because they show the most objective support (data) of the steps that culminate in the paper’s claim. Moreover, figures are often viewed by readers who skip directly from the abstract in order to save time. Thus, the title of the figure should communicate the conclusion of the analysis, and the legend should explain how it was done. Figure making is an art unto itself; the Edward Tufte books remain the gold standard for learning this craft [7,8].

The first results paragraph is special in that it typically summarizes the overall approach to the problem outlined in the introduction, along with any key innovative methods that were developed. Most readers do not read the methods, so this paragraph gives them the gist of the methods that were used.

Each subsequent paragraph in the results section starts with a sentence or two that set up the question that the paragraph answers, such as the following: “To verify that there are no artifacts…,” “What is the test-retest reliability of our measure?,” or “We next tested whether Ca2+ flux through L-type Ca2+ channels was involved.” The middle of the paragraph presents data and logic that pertain to the question, and the paragraph ends with a sentence that answers the question. For example, it may conclude that none of the potential artifacts were detected. This structure makes it easy for experienced readers to fact-check a paper. Each paragraph convinces the reader of the answer given in its last sentence. This makes it easy to find the paragraph in which a suspicious conclusion is drawn and to check the logic of that paragraph. The result of each paragraph is a logical statement, and paragraphs farther down in the text rely on the logical conclusions of previous paragraphs, much as theorems are built in mathematical literature.
Rule 8: Discuss how the gap was filled, the limitations of the interpretation, and the relevance to the field

The discussion section explains how the results have filled the gap that was identified in the introduction, provides caveats to the interpretation, and describes how the paper advances the field by providing new opportunities. This is typically done by recapitulating the results, discussing the limitations, and then revealing how the central contribution may catalyze future progress. The first discussion paragraph is special in that it generally summarizes the important findings from the results section. Some readers skip over substantial parts of the results, so this paragraph at least gives them the gist of that section.

Each of the following paragraphs in the discussion section starts by describing an area of weakness or strength of the paper. It then evaluates the strength or weakness by linking it to the relevant literature. Discussion paragraphs often conclude by describing a clever, informal way of perceiving the contribution or by discussing future directions that can extend the contribution.

For example, the first paragraph may summarize the results, focusing on their meaning. The second through fourth paragraphs may deal with potential weaknesses and with how the literature alleviates concerns or how future experiments can deal with these weaknesses. The fifth paragraph may then culminate in a description of how the paper moves the field forward. Step by step, the reader thus learns to put the paper’s conclusions into the right context.
Process (Rules 9 and 10)

To produce a good paper, authors can use helpful processes and habits. Some aspects of a paper affect its impact more than others, which suggests that your investment of time should be weighted towards the issues that matter most. Moreover, iteratively using feedback from colleagues allows authors to improve the story at all levels to produce a powerful manuscript. Choosing the right process makes writing papers easier and more effective.
Rule 9: Allocate time where it matters: Title, abstract, figures, and outlining

The central logic that underlies a scientific claim is paramount. It is also the bridge that connects the experimental phase of a research effort with the paper-writing phase. Thus, it is useful to formalize the logic of ongoing experimental efforts (e.g., during lab meetings) into an evolving document of some sort that will ultimately steer the outline of the paper.

You should also allocate your time according to the importance of each section. The title, abstract, and figures are viewed by far more people than the rest of the paper, and the methods section is read least of all. Budget accordingly.

The time that we do spend on each section can be used efficiently by planning text before producing it. Make an outline. We like to write one informal sentence for each planned paragraph. It is often useful to start the process around descriptions of each result—these may become the section headers in the results section. Because the story has an overall arc, each paragraph should have a defined role in advancing this story. This role is best scrutinized at the outline stage in order to reduce wasting time on wordsmithing paragraphs that don’t end up fitting within the overall story.
Rule 10: Get feedback to reduce, reuse, and recycle the story

Writing can be considered an optimization problem in which you simultaneously improve the story, the outline, and all the component sentences. In this context, it is important not to get too attached to one’s writing. In many cases, trashing entire paragraphs and rewriting is a faster way to produce good text than incremental editing.

There are multiple signs that further work is necessary on a manuscript (see Table 1). For example, if you, as the writer, cannot describe the entire outline of a paper to a colleague in a few minutes, then clearly a reader will not be able to. You need to further distill your story. Finding such violations of good writing helps to improve the paper at all levels.
thumbnail

https://doi.org/10.1371/journal.pcbi.1005619.t001

Successfully writing a paper typically requires input from multiple people. Test readers are necessary to make sure that the overall story works. They can also give valuable input on where the story appears to move too quickly or too slowly. They can clarify when it is best to go back to the drawing board and retell the entire story. Reviewers are also extremely useful. Non-specific feedback and unenthusiastic reviews often imply that the reviewers did not “get” the big picture story line. Very specific feedback usually points out places where the logic within a paragraph was not sufficient. It is vital to accept this feedback in a positive way. Because input from others is essential, a network of helpful colleagues is fundamental to making a story memorable. To keep this network working, make sure to pay back your colleagues by reading their manuscripts.
Discussion

This paper focused on the structure, or “anatomy,” of manuscripts. We had to gloss over many finer points of writing, including word choice and grammar, the creative process, and collaboration. A paper about writing can never be complete; as such, there is a large body of literature dealing with issues of scientific writing [9,10,11,12,13,14,15,16,17].

Personal style often leads writers to deviate from a rigid, conserved structure, and it can be a delight to read a paper that creatively bends the rules. However, as with many other things in life, a thorough mastery of the standard rules is necessary to successfully bend them [18]. In following these guidelines, scientists will be able to address a broad audience, bridge disciplines, and more effectively enable integrative science.
Acknowledgments

We took our own advice and sought feedback from a large number of colleagues throughout the process of preparing this paper. We would like to especially thank the following people who gave particularly detailed and useful feedback:

Sandra Aamodt, Misha Ahrens, Vanessa Bender, Erik Bloss, Davi Bock, Shelly Buffington, Xing Chen, Frances Cho, Gabrielle Edgerton, multiple generations of the COSMO summer school, Jason Perry, Jermyn See, Nelson Spruston, David Stern, Alice Ting, Joshua Vogelstein, Ronald Weber.
References

    1. Hirsch JE (2005) An index to quantify an individual's scientific research output. Proc Natl Acad Sci U S A. 102: 16569–16572. pmid:16275915
    2. Acuna DE, Allesina S, Kording KP (2012) Future impact: Predicting scientific success. Nature. 489: 201–202. pmid:22972278
    3. Paiva CE, Lima JPSN, Paiva BSR (2012) Articles with short titles describing the results are cited more often. Clinics. 67: 509–513. pmid:22666797
    4. Carter M (2012) Designing Science Presentations: A Visual Guide to Figures, Papers, Slides, Posters, and More: Academic Press.
    5. Murdock BB Jr (1968) Serial order effects in short-term memory. J Exp Psychol. 76: Suppl:1–15.
    6. Schimel J (2012) Writing science: how to write papers that get cited and proposals that get funded. USA: OUP.
    7. Tufte ER (1990) Envisioning information. Graphics Press.
    8. Tufte ER The Visual Display of Quantitative Information. Graphics Press.
    9. Lisberger SG (2011) From Science to Citation: How to Publish a Successful Scientific Paper. Stephen Lisberger.
    10. Simons D (2012) Dan's writing and revising guide. http://www.dansimons.com/resources/Simons_on_writing.pdf [cited 2017 Sep 9].
    11. Sørensen C (1994) This is Not an Article—Just Some Thoughts on How to Write One. Syöte, Finland: Oulu University, 46–59.
    12. Day R (1988) How to write and publish a scientific paper. Phoenix: Oryx.
    13. Lester JD, Lester J (1967) Writing research papers. Scott, Foresman.
    14. Dumont J-L (2009) Trees, Maps, and Theorems. Principiae. http://www.treesmapsandtheorems.com/ [cited 2017 Sep 9].
    15. Pinker S (2014) The Sense of Style: The Thinking Person’s Guide to Writing in the 21st Century. Viking Adult.
    16. Bern D (1987) Writing the empirical journal. The compleat academic: A practical guide for the beginning social scientist. 171.
    17. George GD, Swan JA (1990) The science of scientific writing. Am Sci. 78: 550–558.
    18. Strunk W (2007) The elements of style. Penguin. 
```

```{r SPI_index_discuss, eval=FALSE, include=FALSE}
# The two SCI functions used below are fitSCI & tranformSCI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   fitSCI estimates the parameters for transforming a meteorological 
#   & environmental time series to a Standardized Climate Index (SCI).  

#       fitSCI(x, first.mon, time.scale, distr, p0, 
#         p0.center.mass = FALSE, scaling = c("no","max","sd"), 
#         mledist.par =  list(), start.fun = dist.start, 
#         start.fun.fix = FALSE, warn = TRUE, ...) 

#   transformSCI applies the transformation 
#       transformSCI(x, first.mon, obj, sci.limit = Inf,
#         warn = TRUE, ...) 

# description of arguments for the SCI functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# x	- numeric vector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A monthly univariate time series for SCI input 

# first.mon 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Value in [1:12] indicating month of the first element of x 

# time.scale 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The time scale is the window length of an backward-looking running 
# mean.  Time scale is an integer value. 

# distr	
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A character string "name" naming a distribution for which the 
# corresponding density function (dname), the corresponding 
# distribution function (pname) and the quantile function (qname) must 
# be defined (see for example GammaDist) 

# dist.para 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A column matrix containing the parameters of distribution distr for 
# each month. Row names correspond to the distribution parameters. 

# p0 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated separately.

# If p0=TRUE an additional row named P0 is introduced, indicating the 
# probability of zero (precipitation) events.

# If TRUE, model Probability of zero (precipitation) months is 
# modeled with a mixed distribution as D(x) = p0 + (1-p0)G(x), 
# where G(x) > 0 is the reference distribution (e.g. Gamma) p0 is the 
# probability of a zero (precipitation) month. 

# Following Stagge et al. (2014) the probability of zero events is 
# then estimated as p0 = (n_p)/(n + 1), where np refers to the number 
# of zero events and n is the sample size. 
# The resulting mixed distribution for SCI transformation is then: 

# g(x) = if(x > 0) p0 + (1 - p0) G(x) 
#   else if(x == 0) (np + 1)/(2(n + 1))
#    where G(x) > 0 is a model (e.g. gamma) distribution.

# p0.center.mass 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated using the "centre of mass" estimator 
# (see Stagge et al. (2014) for details).

# The probability of zero precipitation events (p0) can be estimated 
# using a "center of mass" estimate based on the Weibull plotting 
# position function to reduce biases in the presence of many zero 
# precipitation events by 'p0.center.mass = TRUE' 

# If TRUE, the Probability of zero (precipitation) is 
# estimated using a "center of mass" estimate based on the Weibull 
# plotting position function (see details). Only applies if p0=TRUE.

# scaling 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Indicates whether to do some scaling of x prior to parameter 
# identification. Scaling can stabilize parameter estimation. 
# "no" (the default) indicates no scaling. 
# "max" indicates scaling by the maximum of x, such that 
#      x <- x/max(x,na.rm=TRUE). 
# "sd" stands for scaling by the standard deviation. 
# warn	- Issue warnings if problems in parameter estimation occur. 

# mledist.par 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# named list that can be used to pass parameters to mledist in package 
# fitdistrplus.  # STILL NOT SURE ABOUT THIS!!!

# start.fun 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Function with arguments x and distr estimating initial 
# parameters of the function distr for each month. The function should 
# return a named list corresponding to the parameters of distr. 
# (See also dist.start)

# start.fun.fix 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical argument, indicating if parameter estimates 
# of start.fun should be used if maximum likelihood estimation breaks 
# down. This stabilizes the implementation but can introduce biases in 
# the resulting SCI.  Should look at this for Feb Interior.

# obj 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# an object of class fitSCI, output from fitSCI.

# sci.limit 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Truncate absolute values of SCI that are larger than sci.limit. 
# transformSCI allows for a truncation of the SCI series such that 
#   abs(sci) <= sci.limit. Uncertainty in distribution parameters can 
# cause unrealistically large or small SCI values if values in x 
# exceed the values used for parameter estimation. 
# The truncation can be disabled by setting sci.limit = Inf.

# Output flags 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# fitSCI returns an object of class "fitSCI".  Some flags are:
# dist.para.flag
# an vector indicating possible issues occurring throughout parameter 
# estimation. Possible values are: 
#     0. no problems occurred; 
#     1. starting values could not be estimated; 
#     2. mledist crashed with unknown error; 
#     3. mledist did not converge; 
#     4. all values in this month are NA; 
#     5. all values in this mon are constant, distribution not defined

# scaling
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# numeric value that has been used to scale x (see argument scaling). 
# A value of 1 results from scaling="no", other values are the maximum 
# value or the standard deviation of x, depending on the choice of the 
# parameter scaling. 

# call 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the function call transform SCI returns a numeric vector containing 
# the SCI, having values of the standard normal distribution. 
```

```{r PCA_approach, eval=FALSE, include=FALSE}
# Exploratory PCA--------------------------------------------------- 
# PCA is related to eigenvectors and eigenvalues.  The variance or 
# spread of the observations is measured as the average squared 
# distance from the center of the point cloud to each observation (c).  
# The total reconstruction error is measured as the average squared 
# length of the errors (b), and distance along the principal axis (a) 
# can also be measured.  Therefore the sum of the square of the errors 
# plus the sum of the square distance along the principal axis equals 
# the average squared distance between the center of the point cloud 
# each observation; this is precisely Pythagoras theorem. 

# You can imagine that the PC axis is a solid rod and each error 
# is a spring. The energy of the spring is proportional to its squared 
# length (this is known in physics as the Hooke's law), so the rod 
# will orient itself such as to minimize the sum of these squared 
# distances. 

# Regarding eigenvectors and eigenvalues. A 2×2 matrix given by: 
#   (1.07     0.63)
#   (0.63     0.64)

# The variance of the x variable is 1.07, 
# the variance of the y variable is 0.64, 
# and the covariance between them is 0.63. 

# As it is a square symmetric matrix, it can be diagonalized by 
# choosing a new orthogonal coordinate system, given by its 
# eigenvectors (incidentally, this is called spectral theorem); 
# corresponding eigenvalues will then be located on the diagonal. 
# In this new coordinate system, the covariance matrix is diagonal 
# and looks like this:
#   (1.52     0) 
#   (0     0.19)

# The correlation between points is now zero. It becomes clear that 
# the variance of any projection will be given by a weighted average 
# of the eigenvalues.  Consequently, the maximum possible variance 
# (1.52) will be achieved if we simply take the projection on the 
# first coordinate axis. It follows that the direction of the first 
# principal component is given by the first eigenvector of the 
# covariance matrix. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-->
```

```{r CLUSTERING_INFO, eval=FALSE, INCLUDE=FALSE}




Several R packages available from CRAN or Bioconducto perform cluster validation, including: 

|    Package   |    Function(s)   |      Author       |      Notes    |
|:------------:|:----------------:|:-----------------:|:-------------:|
|   cclust     | clustIndex()     |    Dimitriadou    | No user guide |
|     fpc      | cluster.stats()  |       Hennig      | No user guide |
|              | clusterboot()    |                   |               |
| clusterRepro |                  | Kapp & Tibshirani | not general   |
|  clusterSim  |                  | Walesiak & Dudek  | poor user guide |
|  clusterStab |                  | MacDonald et al.  | narrow vignette |
|     clue     | cl_validity() +  | Hornik, September |     maybe...    |
|     e1071    | fclustIndex() ++ | Dimitriadou et al.| 2006  | unk.

+ validation for both paritioning methods (“dissimilarity accounted for”) and hierarchical methods (“variance accounted for”) 
++ fuzzy cluster validation measures.

pam() in recommended pack- age cluster (Rousseeuw, Struyf, Hubert, and Maechler, 2005; Struyf, Hubert, and Rousseeuw, 1996), and Mclust() in package mclust (Fraley, Raftery, and Wehrens, 2005; Fraley and Raftery, 2003), are available as components named cluster, clustering, and classification, 

RWeka (Hornik, Hothorn, and Karatzoglou, 2006), cba (Buchta and Hahsler, 2005), cclust (Dimitriadou, 2005), cluster, e1071 (Dimitriadou, Hornik, Leisch, Meyer, and Weingessel, 2005), flexclust (Leisch, 2006), flexmix (Leisch, 2004), kernlab (Karatzoglou, Smola, Hornik, and Zeileis, 2004), and mclust (and of course, clue itself).
```

```{r precip-EDA, include=FALSE, eval=FALSE}
# Purpose: EDA of precipitation data.
# Outcome: Differences among stations 1971-2018 are small.
#          less than +/-5 mm on average.

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time?  
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.

# drop rows with missing data - this organizes the stations to same 
# time series length
sta_prep <- sta_raw %>%  
  drop_na %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) 

# create abreviated column for plotting & join
abbrev <- data.frame("sta" = c("cot", "int", "oel", "ora", "rap"),
                     "sta_abrev" = c("C", "I", "E", "O", "R")) 

sta_prep <- full_join(sta_prep, abbrev, by = "sta")

# create a year_month column
sta_prep <- sta_prep %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) 

# find mean values by month & join
sta_mean <- sta_prep %>% 
  group_by(month, year) %>% 
  summarize(mean_depth = mean(depth)) %>% 
  ungroup() %>% 
  mutate(mean_depth = round(mean_depth, digits = 2)) %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) %>% 
  select(-year, -month)
 
sta_prep <- full_join(sta_prep, sta_mean, by = "yr_mon") 
rm(abbrev, sta_mean)

# create a column of deviation from the mean by year & month
sta_prep <- sta_prep %>%
  mutate(deviation = depth - mean_depth) 

# plot the precip data as an overall boxplot 
ggplot(sta_prep, aes(as.factor(sta), depth)) + 
  geom_violin() + 
  geom_boxplot() + 
  scale_y_sqrt() + 
  theme_bw() + 
  ggtitle("Weather stations near Pine Ridge Reservation, SD",  
          subtitle = "1971-2018") + 
  xlab("") +  
  ylab("Monthly depth in mm") + 
  NULL 

ggplot2::ggsave(filename = "prcp_boxplot.png", 
                width = 6, height = 6, units = "in") 

# plot the precip depth as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, depth)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# plot the precip deviations as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, deviation)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# prepare summary of station data & clean up
sta_sum <- sta_prep %>% 
  group_by(sta, month) %>% 
  summarise(mean_depth = mean(depth),
            sd_depth = sd(depth),
            mean_dev = mean(deviation)) 

rm(sta_prep)
```

```{r spi-with-SCI, message=FALSE}   
# The following code calculates SPI using the Standardized  
# Climate Index (SCI) package.  
#   SCI is the Standardized Precipitation Index 
#   SRI is the Standardized Runoff Index (SRI) - used below 
# Another SCI package index is the Standardized Precipitation 
#   Evapotranspiration Index (SPEI) - not used

# import precipitation data----
sta_meta    <- as.tibble(import("data/sta_meta.csv")) %>% 
  mutate(min_date = ymd(mindate)) %>% 
  mutate(max_date = ymd(maxdate)) %>% 
  select(-mindate, -maxdate) %>% 
  mutate(dur_year = max_date - min_date) %>% 
  mutate(dur_year = days(dur_year)) %>% 
  mutate(dur_year = time_length(dur_year, unit = "year")) 
  
sta_raw     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  mutate(date = ymd(date)) %>%  
  arrange(date) 
 
# checks for NA based on length of dataframe
sta_chk <- sta_raw %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) %>% 
  drop_na(depth)  %>% 
  mutate(sta = as.factor(sta)) %>% 
    spread(sta, depth) 

rm(sta_chk) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Initial values for SCI calculations 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
time_scale <- 1  # sets the length of the averaging period 
distrib <- "pe3" # sets the distribution type 
p_zero <- TRUE   # sets a function to reduce zero-precip bias
p_zero_cm <- TRUE # ?????
scale <- "sd"    # scales input by subtract mean & divide by sd
warn_me <- TRUE  # sets explicit warning

# Set first month for each station 
first_mon_cot <- 6 
first_mon_int <- 11 
first_mon_oel <- 6 
first_mon_ora <- 5  
first_mon_rap <- 5 

# prepare station for analysis 
sta_cot <- sta_raw %>% arrange(date) %>% select(date, cot) 
sta_int <- sta_raw %>% arrange(date) %>% select(date, int) 
sta_ora <- sta_raw %>% arrange(date) %>% select(date, ora) 
sta_oel <- sta_raw %>% arrange(date) %>% select(date, oel) 
sta_rap <- sta_raw %>% arrange(date) %>% select(date, rap) 

 # change tibble to a vector as double
cot <- as.double(sta_cot$cot)
int <- as.double(sta_int$int) 
ora <- as.double(sta_ora$ora)
oel <- as.double(sta_oel$oel) 
rap <- as.double(sta_rap$rap) 

# notes: 
# A note on variable naming convention; the SCI package doesn't like 
#   snake_case variables 
# 1 month spi - Int did not close on february

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1mon-spi-with-SCI-1mo----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - throws a month 7 NA error.
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   1mon- combine & gather the SPI variables---- 
spi_gath01 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_01 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 2mon-spi-with-SCI-2mo----

# set SCI state variables 
time_scale <- 2  # sets the length of the averaging period 

# Cottonwoods SPI - 2 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value)

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   2mon- combine & gather the SPI variables---- 
spi_gath02 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_02 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 3mon-spi-with-SCI---- 
time_scale <- 3  # sets the length of the averaging period 

# Cottonwoods SPI - 3
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib,  
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   3mon - combine & gather the SPI variables---- 
spi_gath03 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_03 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 4mon-spi-with-SCI-4mo----
# notes:  ORAL HAS ONE SPI VAL OF -8
time_scale <- 4  # sets the length of the averaging period 

# Cottonwoods SPI - 4
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results

spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   4mon - combine & gather the SPI variables---- 
spi_gath04 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_04 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#   5mon-spi-with-SCI-5mo----
# notes: ora has an extreme low value & really high skew 
time_scale <- 5  # sets the length of the averaging period 

# Cottonwoods SPI - 5 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value



#   5mon - combine & gather the SPI variables---- 
spi_gath05 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_05 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 6mon-spi-with-SCI-6mo---- 
 time_scale <- 6  # sets the length of the averaging period 

# Cottonwoods SPI - 6
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   6mon - combine & gather the SPI variables---- 
spi_gath06 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_06 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#   9mon-spi-with-SCI-9mo----
# Notes: ora has an extreme low value & really high skew ??

# set SCI state variables 
time_scale <- 9  # sets the length of the averaging period 

# Cottonwoods SPI - 9 
#~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   9mon - combine & gather the SPI variables---- 
spi_gath09 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_09 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 12mon- spi-with-SCI-12mo----
time_scale <- 12  # sets the length of the averaging period 

# Cottonwoods SPI - 12
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)  

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results  
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   12mon - combine & gather the SPI variables---- 
spi_gath12 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_12 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 18mon-spi-with-SCI-18mo----
time_scale <- 18  # sets the length of the averaging period 

# Cottonwoods SPI - 18
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   18mon - combine & gather the SPI variables---- 
spi_gath18 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_18 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 24mon- spi-with-SCI-24mo---- 
time_scale <- 24  # sets the length of the averaging period 

# Cottonwoods SPI - 24
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   24mon - combine & gather the SPI variables---- 
spi_gath24 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_24 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 30mon- spi-with-SCI-30mo---- 
time_scale <- 30  # sets the length of the averaging period 

# Cottonwoods SPI - 30 
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   30mon - combine & gather the SPI variables---- 
spi_gath30 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_30 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 36mon- spi-with-SCI-36mo---- 
time_scale <- 36  # sets the length of the averaging period 

# Cottonwoods SPI - 36 
#~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   36mon - combine & gather the SPI variables---- 
spi_gath36 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_36 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 42mon- spi-with-SCI-42mo---- 
time_scale <- 42  # sets the length of the averaging period 

# Cottonwoods SPI - 42
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   42mon - combine & gather the SPI variables---- 
spi_gath42 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_42 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 48mon- spi-with-SCI-48mo---- 
time_scale <- 48  # sets the length of the averaging period 

# Cottonwoods SPI -  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   48mon - combine & gather the SPI variables---- 
spi_gath48 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_48 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# 54mon- spi-with-SCI-54mo---- 
time_scale <- 54  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   54mon - combine & gather the SPI variables---- 
spi_gath54 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_54 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# 60mon- spi-with-SCI-60mo---- 
time_scale <- 60  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   60mon - combine & gather the SPI variables---- 
spi_gath60 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_60 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
```

```{r join_spi_values} 

# clean up some of the unneeded variables 
rm(distrib, p_zero, p_zero_cm, scale, time_scale, warn_me, 
   sta_cot, sta_int, sta_oel, sta_ora, sta_rap, 
   cot, int, oel, ora, rap, 
   first_mon_cot, first_mon_int, first_mon_oel, 
   first_mon_ora, first_mon_rap) 

# Fix month 7 Interior 1-mo-SCI (this is actually Feb!)
# finds dates of record - note that uses Mar rather than Feb
spi_ml_fail <- spi_gath01 %>% 
  mutate(year = year(date)) %>%              # creates ymd
  mutate(mon = month(date)) %>% 
  mutate(day = day(date)) %>% 
  filter(mon == 3) %>%                       # filters dates for 'int'
  filter(sta == "int") %>% 
  mutate(mon = 2) %>% 
  mutate(date = str_c(year, mon, day, sep = "-")) %>% 
  mutate(date = ymd(date)) %>%               # creates updated months
  select(date, sta) %>% 
  group_by(sta) %>% 
  summarise(min_date = min(date))            # summarizes to min month
  
# calculates the average value for each of the dates
spi_na_fix <- spi_gath01 %>% 
  mutate(mon = month(date)) %>% 
  filter(mon == 2) %>% 
  spread(key = sta, value = spi_index) %>% 
  select(-mon) %>% 
  select(date, everything()) %>% 
  mutate(spi_index = rowMeans(.[,-1], na.rm = TRUE)) %>% 
  mutate(sta = "int") 

# filters by the min date for 'int' & prepares to append to spi_gath01
spi_na_fix <- spi_na_fix %>% 
  filter(date >= spi_ml_fail$min_date) %>% 
  select(date, sta, spi_index)

# append the missing values & clean up
spi_gath01 <- bind_rows(spi_gath01, spi_na_fix) 
rm(spi_ml_fail, spi_na_fix)

#create a list, join, rename, and gather the SPI_indices 
spi_index <- list(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
                   spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
                   spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
                   spi_gath42, spi_gath48, spi_gath54, spi_gath60) %>%
  reduce(left_join, by = c("date", "sta")) 

# renames as spi_length variables 
names(spi_index)[3:18] =
  c("1","2", "3", "4", "5", "6", 
    "9", "12", "18", "24", "30", "36", 
                   "42", "48","54", "60") 

# gather the dataframe 
spi_index <- spi_index %>% 
  gather(key = spi_length, val = spi_index, -date, -sta) %>% 
  mutate(spi_length = as.integer(spi_length)) 

# remove the joined dataframes
rm(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
                   spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
                   spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
                   spi_gath42, spi_gath48, spi_gath54, spi_gath60) 

# check for na values - these are caused by rolling averages
spi_index_na <- spi_index %>% 
  filter(is.na(spi_index) | 
           is.na(sta)) %>% 
  group_by(sta, spi_length) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(count_dif = spi_length - count) %>% 
  distinct(count_dif)                     # if ans = 1 then ok

# remove the na values
spi_index <- spi_index %>% 
  na.omit()

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a dataframe of spi attributes
spi_att01 <- pluck(spi_list_01, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 1)

spi_att02 <- pluck(spi_list_02, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 2) 

spi_att03 <- pluck(spi_list_03, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 3) 

spi_att04 <- pluck(spi_list_04, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 4) 

spi_att05 <- pluck(spi_list_05, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 5) 

spi_att06 <- pluck(spi_list_06, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 6) 

spi_att09 <- pluck(spi_list_09, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 9) 

spi_att12 <- pluck(spi_list_12, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 12) 

spi_att18 <- pluck(spi_list_18, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 18) 

spi_att24 <- pluck(spi_list_24, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 24) 

spi_att30 <- pluck(spi_list_30, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 30) 

spi_att36 <- pluck(spi_list_36, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 36) 

spi_att42 <- pluck(spi_list_42, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 42) 

spi_att48 <- pluck(spi_list_48, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 48) 

spi_att54 <- pluck(spi_list_54, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 54) 

spi_att60 <- pluck(spi_list_60, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 60) 

#create a list, and join SPI attributes 
spi_att <- list(spi_att01, spi_att02, spi_att03, spi_att04, 
                   spi_att05, spi_att06, spi_att09, spi_att12, 
                   spi_att18, spi_att24, spi_att30, spi_att36, 
                   spi_att42, spi_att48, spi_att54, spi_att60) %>%
  reduce(left_join, by = c("property", "month")) %>%
  mutate(month = as.integer(month)) %>% 
  gather(key, spi_len, -month, -property, -starts_with("val")) %>% 
  select(-key) %>% 
  gather(key, val, -month, -property, -spi_len) %>% 
  select(-key) 

# clean up environment 
rm(spi_att01, spi_att02, spi_att03, spi_att04, 
                   spi_att05, spi_att06, spi_att09, spi_att12, 
                   spi_att18, spi_att24, spi_att30, spi_att36, 
                   spi_att42, spi_att48, spi_att54, spi_att60) 

rm(spi_list_01, spi_list_02, spi_list_03, spi_list_04, 
                   spi_list_05, spi_list_06, spi_list_09, spi_list_12, 
                   spi_list_18, spi_list_24, spi_list_30, spi_list_36, 
                   spi_list_42, spi_list_48, spi_list_54, spi_list_60) 
rm(spi_index_na)
```

```{r examine-SCI-values, eval=FALSE}  
# This code chunk examines & fixes the really large negative values
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Visualize initial SPI values----
ggplot(spi_index, aes(as.integer(spi_length), spi_index)) + 
  geom_jitter() +  
  facet_grid(rows = vars(sta)) + 
  theme_bw() +
  ggtitle("Initial SPI Plot", 
          subtitle = "Typical values are between -4 to 4") 

# save intial plot
#ggplot2::ggsave(filename = "figure/initial_spi.png", 
#                width = 6, height = 6, units = "in")

# split outliers from typical values----
spi_index_outlier <- spi_index %>% 
  filter(spi_index < -4) %>% 
  as.tibble() 

spi_index_typical <- spi_index %>% 
  filter(spi_index > -4) %>%
  arrange(spi_index)

# plot outlier years----
sta_outlier <- sta_raw %>% 
  gather(key = sta, value = depth, -date, -year, -month) %>% 
  filter(date == "1977-02-01" | 
    date == "2005-01-01" | 
    date == "2007-01-01" | 
    date == "2012-11-01" | 
    date == "1976-05-01" |
    date == "1981-09-01" |
    date == "2007-02-01"
      ) 

ggplot(sta_outlier, aes(date, depth, color = factor(sta))) + 
  geom_point() + 
  theme_bw() +
  ggtitle("Plot of outlier years") 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# spi_index_typical - calculated above 
# these are the min SPI vals from Cottonwood during depression 
#  date       sta   spi_index spi_length 
#   <date>     <chr>     <dbl>      <dbl> 
# 1 1936-10-01 cot       -3.31          6 
# 2 1937-01-01 cot       -3.27          9 
# 3 1936-09-01 cot       -3.26          4 
# 4 1936-09-01 cot       -3.25          5 
# 5 1936-10-01 cot       -3.25          9  

# spi_index_outlier - calculated above 
# date       sta   spi_index spi_length
#  <date>     <chr>     <dbl>      <dbl>
#1 1977-02-01 ora       -8.03          5   * caused by a zero depth
#2 2005-01-01 ora       -7.51         12
#3 2007-01-01 ora       -8.01          4
#4 2012-09-01 int       -8.03          1 
#5 2012-11-01 ora       -7.71         12 
#6 1976-05-01 ora       -7.93         30
#7 1981-09-01 ora       -7.73         36
#8 2007-02-01 ora       -8.01         36

# Outlier values could be approximated by the mean SPI  
# or the nearest neighbor (Oral <- Oelrichs) - I used mean SPI  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# create replacement values
spi_index_replace <- spi_index_typical %>%          # pulls outliers
  filter(date == "1977-02-01" &  spi_length == 5   | 
           date == "2005-01-01" & spi_length == 12 |  
           date == "2007-01-01" & spi_length == 4  | 
           date == "2012-11-01" & spi_length == 12 | 
           date == "2012-09-01" & spi_length == 1  | 
           date == "1976-05-01" & spi_length == 30 | 
           date == "1981-09-01" & spi_length == 30 | 
           date == "2007-02-01" & spi_length == 30  
         )  %>% 
    group_by(date, spi_length) %>% 
  summarize(spi_index = mean(spi_index)) %>%
  ungroup()                      # calculates mean values 

# update replacement values & join with typical values 
spi_index_replace <- spi_index_outlier %>% 
  select(date, sta) %>% 
  full_join(., spi_index_replace, by = "date")

spi_index2 <- bind_rows(spi_index_typical, spi_index_replace) %>% 
  arrange(sta, spi_length, date)

# check the values after the fix - the outlier values are now fixed 
# spi_index2
# A tibble: 48,011 x 4
#   date       sta   spi_index spi_length
#   <date>     <chr>     <dbl>      <dbl>
# 1 1936-10-01 cot       -3.31          6
# 2 1937-01-01 cot       -3.27          9
# 3 1936-09-01 cot       -3.26          4
# 4 1936-09-01 cot       -3.25          5
# 5 1936-10-01 cot       -3.25          9

# update the index in memory 
spi_index <- spi_index2 %>% 
  arrange(spi_index)

# clean up
rm(spi_index_outlier, spi_index_replace, 
   spi_index_typical, spi_index2, sta_outlier)
```

```{r final-spi-plot}  
spi_index_plot <- spi_index %>% 
  filter(date > "1990-01-01")  %>% 
  filter(spi_length == 1 | 
           spi_length == 6 |
           spi_length == 12 | 
           spi_length == 24 |
           spi_length == 36 |
           spi_length == 48 | 
           spi_length == 60
         ) %>%
  mutate(year = year(date))

ggplot(spi_index_plot, aes(year, spi_index)) + 
  geom_point() + 
  geom_smooth() + 
  geom_hline(yintercept = 0, color = "gray") +
  facet_grid(rows = vars(spi_length), 
             cols = vars(sta)) +
  theme_classic() + 
  ggtitle("SPI Index values for Pine Ridge reservation stations",
          subtitle = "1990-01-01 to 2018-05-30")

ggplot2::ggsave(filename = "figure/spi_vals.png", 
                width = 8, height = 8, units = "in") 

# the model shows spi values are highly correlated with only real 
# difference being the number of years in the calculation.
# However, individual droughts differ in space & time 

#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() + 
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0) + 
#  scale_x_date(limits = as.Date(c('1980-01-01','2018-01-01')), 
#                 date_breaks = "1 year")  
#labels = date_format("%b-%Y")
```

```{r spi-correlation-analysis}
# visually check results
#spi_index_plot <- spi_index %>% 
#  filter(spi_length == 12)
  
#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() +
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0, aes)

#ggplot2::ggsave(path = "figure/", filename = "spi_1mo.png", 
#                width = 6, height = 6, units = "in")  

# create correlation matrix inputs
spi_M <- spi_index %>% 
  select(-date) %>% 
  group_by(sta, spi_length) %>% 
  mutate(grouped_id = row_number()) %>% 
  spread(key = sta, value = spi_index) %>% 
  drop_na() %>% 
  ungroup() %>% 
  select(-grouped_id) %>% 
  select(spi_length, cot, int, oel, ora, rap) # ensure vars order 

# create correlation matrix names from the correlation matrix vars
spi_M_names <- spi_M %>% 
  filter(spi_length == 1) %>% 
  cor() %>% 
  as.tibble() %>% 
  names() %>% 
  as.tibble() %>% 
  slice(-1) %>% 
  mutate(value2 = value) %>% 
  mutate(value3 = value) %>%  
  mutate(value4 = value) %>%
  mutate(value5 = value) %>% 
  gather(key, sta2) %>% 
  select(-key) %>% 
  rownames_to_column() 

# create second station names column 
spi_M_names2 <- spi_M_names %>% 
  arrange(sta2) %>% 
  rename(sta1 = sta2) %>% 
  select(-rowname)

# bind the names columns
spi_M_names <- bind_cols(spi_M_names, spi_M_names2)  
rm(spi_M_names2)

# create a correlation matrix from SPI vals
spi_M <- spi_M %>% 
  split(.$spi_length) %>% 
  purrr::map_dfr(~ cor(.)) %>% 
  drop_na() %>% 
  slice(-1) %>% 
  rownames_to_column() 

# bind names to the correlation matrix 
spi_M <- full_join(spi_M_names, spi_M, by = "rowname")
spi_M <- spi_M %>% 
  select(sta1, sta2, everything()) %>%
  select(-rowname)
  
# prepare lookup table of lat-lons
sta_loc <- sta_meta %>% 
  arrange(name) %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  select(sta, lat, lon, dur_year)

# join the 'from' lat lons
spi_corr <- full_join(spi_M, sta_loc, by = c("sta1" = "sta")) %>% 
  rename(lat1 = lat) %>% 
  rename(lon1 = lon) 

# join the 'to' lat lons
spi_corr <- full_join(spi_corr, sta_loc, by = c("sta2" = "sta")) %>% 
  rename(lat2 = lat) %>% 
  rename(lon2 = lon) %>% 
  mutate(year_dif = abs(dur_year.x - dur_year.y))
  

# convert the 'to' and 'from' lat-lons to northings & eastings & distance
lat_to_km <- 111.03 # 1 degree lat to km @ lat 40-degrees 
lon_to_km <- 85.39  # 1 degree lon to km @ lat 40-degrees 
  
spi_corr <- spi_corr %>% 
  mutate(northing = abs((lat1 - lat2)) * lat_to_km) %>% 
  mutate(easting = abs((lon1 - lon2)) * lat_to_km) %>% 
  mutate(distance = sqrt(northing^2 + easting^2)) %>% 
  select(year_dif, everything()) %>%
  select(-(lat1:easting)) %>% 
  mutate(stations = paste(sta1, sta2, sep = "_")) %>% 
  gather(key = spi_length, value = pears_r, -distance, 
         -stations, -sta1, -sta2, -year_dif) %>% 
  filter(distance > 0) %>% 
  mutate(spi_length = as.double(spi_length))


# clean up the global environment 
rm(spi_M, spi_M_names, sta_loc, lat_to_km, lon_to_km)

# model effect of averaging time & distance on correlation----
# fit a linear model
spi_lm <- lm(pears_r ~ distance + spi_length + year_dif, 
             data = spi_corr)

# augment & gather the original data
spi_corr_aug <- augment(spi_lm, spi_corr) 

spi_corr_gath <- spi_corr_aug %>% 
  select(-(sta1:sta2)) %>%
  select(year_dif:.fitted) %>% 
  gather(key = factor, val, -stations, -pears_r) %>% 
  mutate(val = as.double(val))  

# plot the original data and fitted model for SPI----  
ggplot(spi_corr_gath, aes(val, pears_r)) + 
  geom_point(aes(color = factor(stations))) +
  facet_wrap(ncol = 1, vars(factor), scales = "free") +
    geom_smooth(method = lm) + 
  theme_classic()

spi_lm_fit <- glance(spi_lm) 

spi_lm_tidy <- tidy(spi_lm) %>% 
  mutate(
    low = estimate - std.error,
    high = estimate + std.error
  )

# clean-up Global Environment----
spi_corr <- spi_corr_aug 
rm(spi_corr_aug, spi_corr_gath, spi_lm ,spi_lm_fit)


rm(spi_corr, spi_lm_tidy)
```

# finalize the diagnostic plots
```{r SCI-diagnostic-plots, eval=FALSE}
# Diagnostic plots of SPI transforms 

# PE3 scale plot---- 
spi_scale <- spi_att %>% 
  filter(property == "scale")

ggplot(spi_scale, aes(as.factor(month), spi_value)) +  
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_scale.png", 
                width = 6, height = 6, units = "in") 

# PE3 location plot---- 
spi_location <- spi_att %>% 
  filter(property == "location")

ggplot(spi_location, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_location.png", 
                width = 6, height = 6, units = "in") 

# PE3 shape plot---- 
spi_shape <- spi_att %>% 
  filter(property == "shape")

ggplot(spi_shape, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_shape.png", 
                width = 6, height = 6, units = "in") 

# clean up Global Environment----
rm(spi_location, spi_shape, spi_scale) 
```


```{r streamflow-data-organization}  
# The streamflow data should be 1990-2016.  However, many stations  
# have missing periods of record.
# The 'wet-cycle' transitions to 'dry cycle' in 2002, 
#    and back in xxxx 

# import and append daily flow values > 1990
gage_cont <- import("data/gage_cont.csv") %>% 
  clean_names() %>%                                # N = 437,680
  filter(water_year >= 1990)                        # n = 217,335  

gage_disc <- import("data/gage_disc.csv") %>% 
  clean_names() %>%                                 # N = 186,608
  filter(water_year >= 1990)                        # n =  27,280 

gage_other <- import("data/gage_other.csv") %>% 
  clean_names() %>%                                 # N = 116,927
  filter(water_year >= 1990)                        # n =  42,339  

# bind the gage dfs together, make min & max yr vars, clean up 
gage <- bind_rows(gage_cont, gage_disc, gage_other)  %>%  
  distinct() %>%                                    # n = 286,954
  group_by(sta) %>% 
  mutate(min_yr = min(water_year)) %>% 
  mutate(max_yr = max(water_year)) %>% 
           ungroup()

# find actual count of years
gage_summary <- gage %>% 
  group_by(sta, water_year) %>% 
  summarise(days = n()) %>% 
  group_by(sta) %>% 
  summarise(count_yrs_act = n()) %>% 
  ungroup()  

# remove stations with < 5 yr records
gage <- full_join(gage, gage_summary, by = "sta") # n = 286,954 
rm(gage_cont, gage_disc, gage_other)

gage <- gage %>% 
  filter(count_yrs_act >= 5)                      # n = 284,037

# summarize results - find & drop - incomplete years - update  
gage_summary <- gage %>% 
  group_by(water_year, sta) %>% 
  summarise(days_record = n()) %>% 
  ungroup() 

gage_incomp <- gage_summary %>%   
  filter(days_record < 340) 

gage <- gage %>%                                  # n = 277,838
  filter(sta != "bad_mid") %>% 
  filter(sta != "che_ang")  

gage_summary <- gage %>% 
  group_by(water_year, sta) %>% 
  summarise(days_record = n()) %>% 
  ungroup() 

gage_incomp <- gage_summary %>%   
  filter(days_record < 340) 


# filter the first set of complete stations - starting at 1990
gage_sum_run01 <- gage_summary %>% 
  filter(water_year < 2002) %>% 
  spread(water_year, days_record) %>% 
  filter(!is.na(`1990`)) %>%   # 26 sta w/ complete recs [1990, 1995]
  select(sta)

gage_run01 <- inner_join(gage, gage_sum_run01) 
gage_run01 <- gage_run01 %>%  
  filter(water_year <= 1995)                   # n = 56,108

# check na vals for missing data & incomplete years

#gage_sta_na <- gage %>% 
#  filter(is.na(q) | 
  is.na(count_yrs_act)
         ) %>%    
 group_by(sta, water_year) %>% 
   summarise(q = mean(q))      # bad_mid & 

gage_na <- gage %>% 
    filter(is.na(count_yrs_act)) %>%     # bad river midlands removed
   mutate(count_yrs_exp = max_yr - min_yr)             # n = 286,239


gage <- gage %>% 
    filter(!is.na(count_yrs_act)) %>%     # bad river midlands removed
   mutate(count_yrs_exp = max_yr - min_yr)             # n = 286,239

rm(gage_yr_summary)

# find and filter stations with missing years
gage_miss_yr <- gage %>% 
  filter(count_yrs_exp > count_yrs_act)               # n =  21,208

gage_comp <- gage %>% 
  filter(count_yrs_exp <= count_yrs_act)              # n = 265,031

# remove stations with very short records & summarize stations 
gage_comp <- gage_comp %>% 
  filter(count_yrs_act >= 5)               # n =  263,209

gage_miss_yr <- gage_miss_yr %>% 
  filter(count_yrs_act >= 5)               # n =  14,662 

gage_comp_summary <- gage_comp %>% 
  group_by(sta) %>% 
  summarise(min = min(min_yr), 
            max = max(max_yr)) 

gage_miss_yr_summary <- gage_miss_yr %>% 
  group_by(sta) %>% 
  summarise(min = min(min_yr), 
            max = max(max_yr),
            yr_count_act = first(count_yrs_act), 
            yr_count_exp = first(count_yrs_exp)) 


# find missing years 
gage_miss_spread <- gage_miss_yr %>% 
  select(sta, water_year, q)


#    mutate(min_yr = case_when(
#    .$water_year < 1990 ~ 1990,
#    .$water_year >= 1990 ~ as.double(.$water_year))) %>% 
#  mutate(max_yr = case_when(
#    .$water_year < 1990 ~ 1990,
#    .$water_year >= 1990 ~ as.double(.$water_year))) 




# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
gage_meta_full <- import("data/gage_meta_full.csv") 

#gage_meta_lt80 <- import("data/gage_lt80_meta.csv") %>% 
#gage_meta_9016 <- import("data/gage_meta_9016.csv") %>% 

gage_9016 <- import("data/gage_9016.csv") %>% 
  select(1:4) %>% 
  distinct() 

gage_9015 <- import("data/gage_9015.csv") %>% 
  select(1:4) %>% 
  distinct() 

test <- anti_join(gage_9016, gage_9015)

#  combine records and prepare to look for missing years 
gage <- bind_rows(gage_9016, test) %>% 
  distinct() %>% 
  mutate(min_yr = case_when(
    .$min_yr < 1990 ~ 1990,
    .$min_yr >= 1990 ~ as.double(.$min_yr))) %>% 
  mutate(min_yr = case_when(
      sta == "lcr_abv" ~ 1996, 
      TRUE ~ .$min_yr
    )  # fix the starting year for lcr_abv
  ) %>% 
  mutate(count_yr = max_yr - min_yr) # recalculate count
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  








# split to stations with missing & nonmissing data
gage_miss <- gage %>% 
  filter(count_yr < 27) %>% 
  mutate(completeness = "missing") 

gage_comp <- anti_join(gage, gage_miss) 

gage_comp <- gage_comp %>% 
    mutate(completeness = "complete") 

# Fix missing early years - 1991:1992 

gage_miss <- gage_miss %>% 
  mutate(
    completeness = case_when(
      min_yr == 1991 ~ "1_fix 91-92", 
      min_yr == 1992 ~ "1_fix 91-92", 
            TRUE ~ .$completeness
    )
) 

# clean global environment 
rm(gage_9016, gage_9398, gage)

# next stations to fix
gage_yr_max_lt04 <- gage_yr_miss %>% 
  filter(max_yr < 2004) %>% 
  as.tibble()

# gage_yr_max_lt04 # these are stations missing years after 1997 
# A tibble: 4 x 4 
# sta     min_yr max_yr count_yr
#  <chr>    <dbl>  <int>    <dbl>
#1 whi_slm   1990   1997        7
#2 wcc_ogl   1990   1999        9
#3 bev_abf   1991   1997        6
#4 wkc_wok   1992   1997        5


```

```{r PCA_exploratory_analysis_93-98}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# import data 
pca_input <- import("data/gage_9398.csv") %>% 
  mutate(date = Date) 


# Eliminate effects of watershed size----
# Calculate daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

pca_input <- pca_input %>% 
  as.tibble() %>% 
  clean_names() %>% # puts the names into snake case
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na()  # not really needed in this expression

# Transform using BoxCox to approach normality---- 
# Daily flow data are highly skewed.  BoxCox transformation utilizes 
# a lambda value to transform a dataset to an ~ normal distribution.

# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

lambda_q1 <- BoxCox.lambda(pca_input$q1_depth)   #  q1 ==> 0.010   
lambda_q7 <- BoxCox.lambda(pca_input$q7_depth)   #  q7 ==> 0.105  
lambda_q30 <- BoxCox.lambda(pca_input$q30_depth) # q30 ==> 0.264    

# Standardize data by z-score---- 
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

pca_input <- pca_input %>% 
  mutate(q1_tr = BoxCox(.$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(.$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(.$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>%   # means
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>%   
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% # sds
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>%  
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>% # transfroms the data
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

 
# Calculate PCA matrix & summary info---- 
# prcomp() requires a dataset with only the variables, split data 
# into PCA input & names to connect to result 


pca_matrix <- pca_input %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) %>%
  prcomp(., scale = TRUE)      
# note that . is passing select(q1_depth, q7_depth, q30_depth)
  
#pca_meta <- pca_input %>% 
 # select(sta, date, q1_depth, q7_depth, q30_depth) <- should delete

# Gather & summarize PCA results----
# Eigenvectors--results about PC axes
#pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
#pca_vars <-  tidy(pca_matrix, matrix = "variables") 

# Drop the 3rd PC-axis because it's not useful 
#pca_vars <- pca_vars %>% 
#  filter(PC != 3) %>% 
#  rename(var = column) %>% 
#  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix - summarize & clean up
gage_aug <- augment(pca_matrix, data = pca_input) %>% 
  select(-c(.rownames, date_2, q1_mean:q30_sd, .fittedPC3)) %>% 
  mutate(q1_q30_diff = q1_depth - q30_depth)      

gage_sum <- gage_aug %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            q1_mean = mean(q1_depth), 
            q7_mean = mean(q7_depth),
            q30_mean = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),
            date_min = min(dec_year), 
            date_max = max(dec_year) 
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean)

rm(lambda_q1, lambda_q7, lambda_q30, pca_input, pca_matrix) 

# PCA results & interpretation
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A tibble: 3 x 4
#     PC std.dev percent cumulative
#  <dbl>   <dbl>   <dbl>      <dbl>
#1     1   1.67  0.932        0.932 
#2     2   0.419 0.0586       0.991
#3     3   0.167 0.00935      1     

## A tibble: 6 x 3
#  var          PC  value
#  <fct>     <dbl>  <dbl>
#1 q1_depth      1  0.577
#2 q7_depth      1  0.591
#3 q30_depth     1  0.563
#4 q1_depth      2  0.577
#5 q7_depth      2  0.193
#6 q30_depth     2 -0.794

# eigenvecter:
#   93% of covarience is explained by PCA1 & 6% of varience by PCA2
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30;
#   PC2 - large positive loading of Q1 & large negative loading of Q30  
#   PC1 is about the hydrologic export of the system with larger
#     values exporting a greater amount of water 
#   PC2 is about the contribution of baseflow vs event-flow
# Find nearest neighbors to extend stations

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#pca_meta <- import("data/gage_meta_9398.csv") 
#input_chk <- pca_input %>% 
#  group_by(sta) %>% 
#  summarise(start_wat_yr = min(DecYear), 
#            end_wat_yr = max(DecYear)
#            ) %>% 
#  arrange(desc(start_wat_yr)) %>% 
#  arrange(end_wat_yr) 
# four stations are 93:97, the other 23 are 93:98 
```

```{r fill_missing_data_wet_years}
# overview----
# find nearest neighbors for stations with missing data
# also, drop stations with very short records: 
#     nio_bbb, nio_gor, nio_abb, cas_hot

# table of nearest neighbors by PCA above by water year
# station  1990-1992  1992   1998  neighbor
# bev_pri      y        n      n    bev_buf
# blp_bel      y        n      n    brsf_co
# lwr_abv      y        n      n    lwr_ros
# ros_ros      y        n      y    lwr_abv
# whi_slm      y        n      y    whi_ogl
# wkc_wok      y        n      y    bev_buf
# bev_abf      n        y      y    frn_fai
# rap_far      n        y      n    fal_hot 

# import data----
gage_9092 <- import("data/gage_9016.csv") %>% 
  clean_names() %>% 
  select(c("sta", "min_yr", "max_yr", "count_yr", "date", "i", 
           "julian", "month", "day", "dec_year", "month_seq", 
           "water_year", "qualifier", "log_q", "q", "q7", "q30", 
           "contrib_drain_area_va")) %>% 
  filter(water_year < 1993) 

gage_9398 <- import("data/gage_9398.csv") %>% 
  clean_names() %>% 
  select(c("sta", "min_yr", "max_yr", "count_yr", "date", "i", 
           "julian", "month", "day", "dec_year", "month_seq", 
           "water_year", "qualifier", "log_q", "q", "q7", "q30", 
           "contrib_drain_area_va")) 

gage_9098 <- bind_rows(gage_9092, gage_9398) %>% 
  distinct() %>%            # tests to ensure no duplicates
  mutate(min_yr = case_when(
    .$min_yr < 1990 ~ 1990,
    .$min_yr >= 1990 ~ as.double(.$min_yr))) %>%  # updated min yr
  mutate(min_yr = case_when(
      sta == "lcr_abv" ~ 1996, 
      TRUE ~ .$min_yr
    )  # fix min year for lcr_abv
  ) %>% 
  mutate(count_yr = max_yr - min_yr) %>%      # recalculate count 
  filter(sta != "nio_bbb") %>% 
  filter(sta != "nio_gor") %>% 
  filter(sta != "nio_abb") %>% 
  filter(sta != "cas_hot") %>% 
  filter(sta != "whi_cra")  # these are very short records 

rm(gage_9092, gage_9398) 

# fill missing data for 1990-1992 :: bev_pri <- bev_buf---- 
gage_from <- gage_9098 %>% 
  filter(sta == "bev_buf") %>% 
  filter(water_year <= 1992) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "bev_pri")

gage_to <- gage_9098 %>% 
  filter(sta == "bev_pri") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# fill missing data for 1990-1992 :: blp_bel <- brsf_co----
gage_from <- gage_9098 %>% 
  filter(sta == "brsf_co") %>% 
  filter(water_year <= 1992) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "blp_bel")

gage_to <- gage_9098 %>% 
  filter(sta == "blp_bel") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# fill missing data for 1990-1992 :: lwr_abv <- lwr_ros----  
gage_from <- gage_9098 %>% 
  filter(sta == "lwr_ros") %>% 
  filter(water_year <= 1992) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "lwr_abv")

gage_to <- gage_9098 %>% 
  filter(sta == "lwr_abv") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# fill missing data for 1990-1992 :: ros_ros <- lwr_abv----  
gage_from <- gage_9098 %>% 
  filter(sta == "lwr_abv") %>% 
  filter(water_year <= 1992 | 
           water_year <= 1998) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "ros_ros")

gage_to <- gage_9098 %>% 
  filter(sta == "ros_ros") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill)  

# fill missing data for 1990-1992 :: whi_slm <- whi_ogl---- 
gage_from <- gage_9098 %>% 
  filter(sta == "whi_ogl") %>% 
  filter(water_year <= 1992 | 
           water_year <= 1998) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "whi_slm")

gage_to <- gage_9098 %>% 
  filter(sta == "whi_slm") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# fill missing data for 1990-1992 :: wkc_wok <- bev_buf---- 
gage_from <- gage_9098 %>% 
  filter(sta == "bev_buf") %>% 
  filter(water_year <= 1992 | 
          water_year <= 1998) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "wkc_wok")

gage_to <- gage_9098 %>% 
  filter(sta == "wkc_wok") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# fill missing data 1990 & 1998   :: bev_abf <- frn_fai----
gage_from <- gage_9098 %>% 
  filter(sta == "frn_fai") %>% 
  filter(water_year == 1990 | 
           water_year == 1998) %>% 
  select(-c(2:4)) %>% 
  mutate(sta = "bev_abf")

gage_to <- gage_9098 %>% 
  filter(sta == "bev_abf") %>% 
  slice(1) %>% 
  select(1:4)

gage_fill <- full_join(gage_to, gage_from, by = "sta")
gage_9098 <- bind_rows(gage_9098, gage_fill) 

# summarize & clean up  
gage_9098_sum <- gage_9098 %>% 
  group_by(sta) %>% 
  summarize(date_min = min(dec_year), 
            date_max = max(dec_year),
            count_yr = mean(count_yr)
            ) %>% 
  ungroup() %>% 
  arrange(count_yr) 

rm(gage_fill, gage_from, gage_sum, gage_to)
```

```{r PCA_exploratory_analysis_90-98}

# Data is from above
pca_input <- gage_9098 

# Eliminate effects of watershed size----
# Calculate daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

pca_input <- pca_input %>% 
  as.tibble() %>% 
  clean_names() %>% # puts the names into snake case
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na()  # not really needed in this expression

# Transform using BoxCox to approach normality---- 
# Daily flow data are highly skewed.  BoxCox transformation utilizes 
# a lambda value to transform a dataset to an ~ normal distribution.

# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

lambda_q1 <- BoxCox.lambda(pca_input$q1_depth)   #  q1 ==> 0.006    
lambda_q7 <- BoxCox.lambda(pca_input$q7_depth)   #  q7 ==> 0.098   
lambda_q30 <- BoxCox.lambda(pca_input$q30_depth) # q30 ==> 0.193    

# Standardize data by z-score---- 
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

pca_input <- pca_input %>% 
  mutate(q1_tr = BoxCox(.$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(.$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(.$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>%   # means
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>%   
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% # sds
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>%  
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>% # transfroms the data
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

# Calculate PCA matrix & summary info---- 
# prcomp() requires a dataset with only the variables, split data 
# into PCA input & names to connect to result 

pca_matrix <- pca_input %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) %>%
  prcomp(., scale = TRUE)      
# note that . is passing select(q1_depth, q7_depth, q30_depth)
  
# Gather & summarize PCA results----
# Eigenvectors--results about PC axes
# pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# A tibble: 3 x 4
#     PC std.dev percent cumulative perc_init cum_init
#  <dbl>   <dbl>   <dbl>      <dbl>   <dbl>    <dbl>
#1     1   1.69  0.947        0.947   0.932    0.932
#2     2   0.367 0.0448       0.992   0.0586   0.991
#3     3   0.155 0.00797      1    


# PCA variables--the loadings on the PCA axes.  
# pca_vars <-  tidy(pca_matrix, matrix = "variables") 

# Drop the 3rd PC-axis because it's not useful 
#pca_vars <- pca_vars %>% 
#  filter(PC != 3) %>% 
#  rename(var = column) %>% 
#  mutate(var = as.factor(var)) 

#  pca_vars
#   var        PC   value  val_init
#   <fct>     <dbl>  <dbl>  <dbl>
# q1_depth      1  0.577    0.577
# q7_depth      1  0.587    0.591
# q30_depth     1  0.568    0.563
# q1_depth      2  0.588    0.577
# q7_depth      2  0.184    0.193
# q30_depth     2 -0.788   -0.794


## A tibble: 6 x 3
#  var          PC  value
#  <fct>     <dbl>  <dbl>
#1 q1_depth      1  0.577
#2 q7_depth      1  0.591
#3 q30_depth     1  0.563
#4 q1_depth      2  0.577
#5 q7_depth      2  0.193
#6 q30_depth     2 -0.794

# Bind sample vals to PCA matrix - summarize & clean up
gage_aug <- augment(pca_matrix, data = pca_input) %>% 
  select(-c(.rownames, q1_mean:q30_sd, .fittedPC3)) %>% 
  mutate(q1_q30_diff = q1_depth - q30_depth)      

gage_sum <- gage_aug %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            q1_mean = mean(q1_depth), 
            q7_mean = mean(q7_depth),
            q30_mean = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),
            date_min = min(dec_year), 
            date_max = max(dec_year) 
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean)

rm(lambda_q1, lambda_q7, lambda_q30, pca_input, pca_matrix) 
```

```{r plot_PCA}

# mean eigenvector plot
ggplot(gage_sum, aes(PC1_mean, PC2_mean)) + 
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
            nudge_y = 0.02) + 
  theme_classic() + 
#  scale_x_continuous(limits = c(-3.5, 2)) + 
  xlab("PC axis 1 mean") +
  ylab("PC axis 2 mean") 

# transformed variable plot 
ggplot(gage_9398_sum, aes(q7_mean, q1_q30_mean)) + 
  geom_jitter() +  
  geom_text(aes(label = sta), check_overlap = TRUE,  
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2.0, 1.5)) + 
  scale_y_continuous(limits = c(-1, 0.75)) + 
  geom_density2d() + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  xlab("hydrologic export index") +
  ylab("stability index") 

# PC1 explanatory plot 
ggplot(gage_aug, 
       aes(q7_depth, .fittedPC1, color = factor(sta))) + 
  geom_jitter() + 
  theme_classic() +
  xlab("Q7 depth") +
  ylab("Fitted PC1") +  
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = "strm_pca1.png", 
#                width = 6, height = 6, units = "in")  
  
# PC2 explanatory plot
ggplot(gage_aug, aes(q1_q30_diff, .fittedPC2, 
                          color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q1 depth minus Q30 depth") +
  ylab("Fitted PC2") +
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = #"strm_pca1.png", 
#                width = 6, height = 6, units = "in")  



#ggplot2::ggsave(path = "figure/", filename = "strm_dens_mean.png", 
#                width = 6, height = 6, units = "in")  

# create density plot of all points
ggplot(gage_aug, aes(q7_depth, q1_q30_diff)) +
  geom_jitter(aes(color = sta, shape = ".")) + 
  geom_density2d(aes(color = NULL)) + 
  geom_point(data = gage_sum, mapping = aes(q7_mean, q1_q30_mean)) + 
  geom_text(data = gage_sum, 
           mapping = aes(q7_mean, q1_q30_mean, label = sta), 
            check_overlap = TRUE) + 
  scale_x_continuous(limits = c(-2, 1.5)) + 
  scale_y_continuous(limits = c(-1.0, 0.75)) + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  theme(legend.position = "none") 

#ggplot2::ggsave(path = "figure/", filename = "strm_dens_all.png", 
#                width = 6, height = 6, units = "in")  
```

```{r Model-Based_Clustering-9098} 

# The traditional clustering methods, such as hierarchical clustering 
# and k-means clustering, are heuristic and are not based on formal  
# models. An alternative is model-based clustering, which consider the  
# data as coming from a distribution that is mixture of two or more  
# clusters (Fraley and Raftery 2002, Fraley #et al. (2012)). 
# Model-based clustering uses a soft assignment, where each data point 
# has a probability of belonging to each cluster.  In model-based 
# clustering, the data is considered as coming from a mixture of 
# density.  Each component (i.e. cluster) k is modeled by the normal 
# or Gaussian distribution which is characterized by the parameters: 
#   μk\mu_k: mean vector, 
#   ∑k\sum_k: covariance matrix, 
#   An associated probability in the mixture. Each point has a 
#     probability of belonging to each cluster.  

# The model parameters can be estimated using the Expectation-
# Maximization (EM) algorithm initialized by hierarchical model-based 
# clustering. Each cluster k is centered at the means μk\mu_k, with 
# increased density for points near the mean.
# Geometric features (shape, volume, orientation) of each cluster are 
# determined by the covariance matrix ∑k\sum_k. 

# Different possible parameterizations of ∑k\sum_k are available in 
# the R package mclust (see ?mclustModelNames).
# The available model options, in mclust package, are represented by 
# identifiers including: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV 
# and VVV. 

# The first identifier refers to volume, the second to shape and the 
# third to orientation. E stands for "equal", V for "variable" and I 
# for "coordinate axes".

# The Mclust package uses maximum likelihood to fit all these models, 
# with different covariance matrix parameterizations, for a range of 
# k components.

# The best model is selected using the Bayesian Information Criterion 
# or BIC. A large BIC score indicates strong evidence for the 
# corresponding model. 

# Preparing to cluster by the central tendancy
gage_clust_input <- gage_sum %>% 
  select(q1_mean:q30_mean)

gage_clust_input_meta <- gage_sum %>% 
  select(sta)

# apply & summarize model 
gage_clust_l <- Mclust(gage_clust_input) # Model-based-clustering 
summary(gage_clust_l) # Print a summary 

#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 

#Mclust EEV (ellipsoidal, equal volume and shape) 
# EEV model with 5 components using 93-98 data
# EEV model with 5 components using 90-98 data

# log.likelihood  n df     BIC      ICL
#        130.961 27 37   139.976 138.2098
#        172.397 27 51   176.706 175.8812

#Clustering tables:
# 1  2  3  4  5  93-98 data
# 3  7 13  2  2 
 
# 1 2 3 4 5 6 7  90-98 data
# 2 2 1 5 2 6 9 


# Model-based clustering selected a model with five components 
# (i.e. clusters). The optimal selected model name is VEV model. 
# That is the five components are ellipsoidal with varying volume, 
# and orientation, and equal shape. The summary contains also the 
# clustering table specifying the number of observations in each 
# clusters.

#gage_clust_l$modelName # Optimal selected model ==> "EEV" 
#gage_clust_l$G # Optimal number of cluster => 5 

# Extract results to a dataframe & add metadata
gage_clust <- as.tibble(gage_clust_l$z) 
gage_clust <- bind_cols(gage_clust_input_meta, gage_clust) 

# remove low probability results
gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.7) 

# join together summary results & rearrange
gage_sum <- full_join(gage_sum, gage_clust, 
                             by = "sta")
gage_clust <- gage_clust %>% 
  dplyr::select(sta, group, everything())

# test output & clean up environment
test <- anti_join(gage_9398_sum, gage_clust, by = "sta")  

rm(gage_clust, gage_clust_input, 
   gage_clust_input_meta, test)
```

```{r Clustering_visualization}

# Visualizing model-based clustering
# Model-based clustering results can be drawn using the base 
# function plot.Mclust() [in mclust package]. We'll use the 
# function fviz_mclust() [in factoextra package] to create beautiful 
# plots based on ggplot2.

# Where the data contain more than two variables, fviz_mclust() 
# uses a principal component analysis to reduce the dimensionnality 
# of the data. The first two principal components are used to produce 
# a scatter plot of the data. However, if you want to plot the data 
# using only two variables of interest, c("insulin", "sspg"), 
# you can specify that in the fviz_mclust() function using the 
# argument choose.vars = c("insulin", "sspg").

library(factoextra)  # this might screw up the select!!!

# BIC values used for choosing the number of clusters 
fviz_mclust(gage_clust_l, "BIC", palette = "jco") 

# Classification uncertainty 
# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations 
fviz_mclust(gage_clust_l, "uncertainty", palette = "jco")

# Classification: plot showing the clustering 
fviz_mclust(gage_clust_l, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 

#test <- left_join(gage_sum, gage_clust)


# plot clusters in PCA space
ggplot(gage_9398_sum, aes(q7_mean, q1_q30_mean, color = as.factor(group))) + 
  geom_density2d(na.rm = FALSE, contour = TRUE, aes(color = as.factor(group)))  + 
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2, 1.1)) + 
  scale_y_continuous(limits = c(-1.0, 0.4)) + 
  theme_classic() 

 #ggplot2::ggsave(path = "figure/", filename = "strm_clust.png", 
#                width = 6, height = 6, units = "in")  

gage_clust_plot <- gage_9398_sum %>% 
  select(sta, group, q7_mean, q1_q30_mean) %>% 
  rename(hydro_exp_coeff = q7_mean) %>% 
  rename(stability_coeff = q1_q30_mean) %>% 
  gather(key = parameter, value = value, -sta, -group)

ggplot(gage_clust_plot, aes(group, value)) + 
  facet_grid(rows = vars(parameter)) + 
         geom_boxplot() + 
  theme_bw()

#ggplot2::ggsave(path = "figure/", filename = #"strm_clust_box.png", 
#                width = 6, height = 3, units = "in")  

gage_clust <- gage_clust %>% 
  select(group, sta, station_nm, everything()) %>% 
  arrange(group)

# moderately low hydrologic export, highest Q1-Q30 = stable-low export
#1     V1 whi_slm          WHITE RIVER AT SLIM BUTTE, SD
#2     V1 wcc_ogl           WHITE CLAY CR NEAR OGLALA SD
#3     V1 spr_her              SPRING CR NEAR HERMOSA,SD
#4     V1 che_was          CHEYENNE RIVER NEAR WASTA, SD
#5     V1 whi_ogl                 WHITE R NEAR OGLALA SD
#6     V1 whi_sta            WHITE R NR NE-SD STATE LINE

# average hydrologic export, high Q1-Q30 = stable-average export
#7     V2 wkc_wok WOUNDED KNEE CREEK AT WOUNDED KNEE, SD
#8     V2 bev_buf          BEAVER CR NEAR BUFFALO GAP,SD
#9     V2 bat_bhr             BATTLE CR BELOW HERMOSA,SD
#10    V2 bev_pri          BEAVER CREEK NEAR PRINGLE, SD

# averarage Q1-Q30, highes hydrologic export = GW controlled
#11    V3 bev_abf     BEAVER CREEK ABOVE BUFFALO GAP, SD
#12    V3 ros_ros               ROSEBUD CR AT ROSEBUD SD
#13    V3 lwr_abv          LITTLE WHITE R ABV ROSEBUD SD
#14    V3 lwr_mar          LITTLE WHITE R NEAR MARTIN,SD
#15    V3 lcr_bel   LAKE CR BELOW REFUGE NEAR TUTHILL,SD
#16    V3 lwr_ros         LITTLE WHITE R NEAR ROSEBUD SD
#17    V3 bat_key             BATTLE CR NEAR KEYSTONE,SD
#18    V3 rap_far           RAPID CR NEAR FARMINGDALE,SD
#19    V3 fal_hot               FALL R AT HOT SPRINGS,SD
#20    V3 bat_her                BATTLE CR AT HERMOSA,SD
#21    V3 lwr_whi    LITTLE WHITE R BELOW WHITE RIVER,SD
#22    V3 lcr_vet           LITTLE WHITE R NEAR VETAL,SD
#23    V3 frn_fai            FRENCH CR ABOVE FAIRBURN SD

# similiar export to V1, but much lower  = flashy-low export
#24    V4 whi_kad                 WHITE R NEAR KADOKA,SD
#25    V4 brsf_co    SOUTH FORK BAD R NEAR COTTONWOOD,SD
#26    V4 blp_bel      BLACK PIPE CREEK NR BELVIDERE, SD

# ephemeral
#27    V5 hat_edg                HAT CR NEAR EDGEMONT,SD
#28    V5 hor_oel            HORSEHEAD CR AT OELRICHS,SD

```



```{r PCA_exploratory_analysis-old}

# Examines data from 1993-1998 - wet years with Wounded Knee station  

# Data preparation steps: import data, normalize by watershed area, 
#     remove missing values, calculate z-scores by BoxCox & 
#     standardize & center data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# import data----

gage_9398_raw <- import("data/gage_9398.csv") 


# The original data is saved in a long format with rows as daily 
# observations and columns as variables

# glimpse(gage_9398_raw)
# Variables: 26
# count_yr              <int> 67, 67, 67... count of years of record
# sta                   <chr> "bat_bhr"...  station name
# site_no               <int> 6406500...    USGS site number 
# station_nm            <chr> "BATTLE CR BELOW HERMOSA,SD"... 
# dec_lat_va            <dbl> 43.72543...   latitude 
# dec_long_va           <dbl> -102.9061...  longitude 
# state_cd              <int> 46, 46...     fips? code for State  
# county_cd             <int> 103, 103...   fips? code for County  
# alt_va                <dbl> 2800.32...    altitude in feet 
# drain_area_va         <dbl> 284, 284...   drainage area in sq mi. 
# contrib_drain_area_va <dbl> 284, 284...   contrib. da in sq mi.
# min_yr                <int> 1951...       first year of record 
# max_yr                <int> 2017...       last year of record 
# Date                  <chr> "1991-10-01"  date 
# i                     <int> 2191, 2192... count from 1st day of rec. 
# Q                     <dbl> 0.12518933... discharge in cms
# Julian                <int> 51772...      Julian calendar date  
# Month                 <int> 10, 10...     month 
# Day                   <int> 275, 276...   day  
# DecYear               <dbl> 1991.749...   decimal year  
# MonthSeq              <int> 1702...       NA 
# waterYear             <int> 1992...       water year 
# Qualifier             <chr> "A", "A"...   data quality code  
# LogQ                  <dbl> -2.077928...  log of zero-adj discharge 
# Q7                    <dbl> 0.14420207... 7-day moving average  
# Q30                   <dbl> 0.1342507...  30-day moving average 

# check gages----
# check gage data -- Should probably dig into this some more!
sta_chk <- gage_9398_raw %>% 
  group_by(sta) %>% 
  summarise(start_wat_yr = min(waterYear), 
            end_wat_yr = max(waterYear)) %>% 
  arrange(desc(start_wat_yr)) %>% 
  arrange(end_wat_yr)

# PCA method discussion----
# Overview 
# ~~~~~~~~ 
# PCA is a method to summarize data using fewer variables by 
# constructing new linear descriptors from variables. The new 
# linear discriptors describe maximum variation across observations. 
# The new discriptors can be used to predict, or "reconstruct",
# observations as well as possible.

# PCA finds the axis the describes the maximum covariance and 
# projecting all of the observations points onto this line. The line 
# called the "first principal component" simultanesous describes 
# maximal variation of values along the line & minimizes the 
# reconstruction error 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Prepare data for exploratory PCA----
# 2. Eliminate the effects of different sizes of watersheds:  
#    I calculated daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

gage_depth <- gage_9398_raw %>% 
  as.tibble() %>% 
  clean_names() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na()

# 3. Transform using BoxCox to approach normality 
# daily flow data are highly skewed & tend to approach a logrithmic 
# distribution.  BoxCox transformation utilizes a lamda value to 
# transform a dataset to approach a normal distribution.
# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

# find lambda values for BoxCox transformation  
lambda_q1 <- BoxCox.lambda(gage_depth$q1_depth)   #  q1 ==> 0.010   
lambda_q7 <- BoxCox.lambda(gage_depth$q7_depth)   #  q7 ==> 0.105  
lambda_q30 <- BoxCox.lambda(gage_depth$q30_depth) # q30 ==> 0.264    

# 4.Standardize data by z-score---- 
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

# Apply transformation and scale data
gage_depth <- gage_depth %>% 
  mutate(q1_tr = BoxCox(gage_depth$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(gage_depth$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(gage_depth$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>% 
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

 
# Select numeric data for PCA---- 
# ~~~~~~~~~~~~~~~~~~~
# prcomp() requires a dataset with only the variables.  
# Below splits data into PCA input & names to connect to result 

pca_input <- gage_depth %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) 

pca_meta <- gage_depth %>% 
  select(sta, date, q1_depth, q7_depth, q30_depth) 

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE) 

# this plots the PCA biplot--takes a LONG time
# biplot(gage_pc, scale = 0)

# Gather & summarize PCA results----
# The broom package is used to gather results into tibbles.

# Eigenvectors--results about PC axes
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
pca_vars <-  tidy(pca_matrix, matrix = "variables") 

# Drop the 3rd PC-axis because it's not useful 
pca_vars <- pca_vars %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix 
gage_aug <- augment(pca_matrix, data = pca_input) 
gage_aug <- bind_cols(pca_meta, gage_aug) 

gage_aug <- gage_aug %>% 
  select(-.fittedPC3, -.rownames, 
         -q1_depth1, -q7_depth1, -q30_depth1) %>% 
  mutate(q1_q30_diff = q1_depth - q30_depth) 

# summarize pca of gages
gage_sum <- gage_aug %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            q1_mean = mean(q1_depth), 
            q7_mean = mean(q7_depth),
            q30_mean = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff)) %>% 
  ungroup() %>% 
  arrange(q1_q30_mean) %>% 
  arrange(q7_mean)
  
pca_eigen 
# PCA results & interpretation
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A tibble: 3 x 4
#     PC std.dev percent cumulative
#  <dbl>   <dbl>   <dbl>      <dbl>
#1     1   1.67  0.932        0.932
#2     2   0.419 0.0586       0.991
#3     3   0.167 0.00935      1     

pca_vars 
## A tibble: 6 x 3
#  var          PC  value
#  <fct>     <dbl>  <dbl>
#1 q1_depth      1  0.577
#2 q7_depth      1  0.591
#3 q30_depth     1  0.563
#4 q1_depth      2  0.577
#5 q7_depth      2  0.193
#6 q30_depth     2 -0.794


# The PCA explains the following: 
# eigenvecter:
#   93% of covarience is explained by PCA1 & 6% of varience by PCA2
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30;
#   PC2 - large positive loading of Q1 & large negative loading of Q30  
#   PC1 is about the hydrologic export of the system with larger
#     values exporting a greater amount of water 
#   PC2 is about the contribution of baseflow vs event-flow
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# clean up Global Environment----
gage_9398_sum <- full_join(gage_9398_meta, gage_sum, by = "sta")

gage_9398_raw <- gage_9398_raw %>% 
  mutate(date = Date) 

gage_9398_aug <- left_join(gage_aug, gage_9398_raw, 
                           by = c("sta", "date")) 

# note:looks like gage_x_raw has same info as gage_x_aug
rm(lambda_q1, lambda_q7, lambda_q30,  gage_9398_meta, 
   pca_eigen, pca_meta, pca_vars, gage_aug, pca_input, 
   pca_matrix, gage_sum, sta_chk, gage_9398_raw)
```
```{r EDA-PCA-9015-gages}
# Examines data from 1993-2017 without 1996 & 2007 

gage_9015_meta <- import("data/gage_meta_9015.csv") 
gage_9015_raw <- import("data/gage_9015.csv") 

gage_9015_sum <- gage_9015_raw %>% 
  group_by(sta) %>%  
  summarise(start_yr = min(waterYear), 
            end_yr = max(waterYear)) 

gage_9015_sum 
# A tibble: 21 x 3
#   sta     start_yr end_yr
#   <chr>      <dbl>  <dbl>
# 1 bat_bhr     1990   2015
# 2 bat_her     1990   2015
# 3 bev_buf     1990   2015
# 4 brsf_co     1990   2015
# 5 che_buf     2008   2015
# 6 che_was     1990   2015
# 7 fal_hot     1990   2015
# 8 frn_fai     1990   2015
# 9 hat_edg     1990   2015
#10 hor_oel     1990   2015
#11 lcr_abv     1997   2015
#12 lcr_bel     1990   2015
#13 lcr_vet     1990   2015
#14 lwr_mar     1990   2015
#15 lwr_ros     1990   2015
#16 lwr_whi     1990   2015
#17 rap_far     1991   2015
#18 whi_int     2003   2015
#19 whi_kad     1990   2015
#20 whi_ogl     1990   2015
#21 whi_sta     1990   2015


# Eliminate the effects of different sizes of watersheds & 
# remove missing values created by rolling averages

gage_9015_depth <- gage_9015_raw %>% 
  as.tibble() %>% 
  clean_names() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na() 

# Transform using BoxCox to approach normality 

# find lambda values for BoxCox transformation  
lambda_q1 <- BoxCox.lambda(gage_9015_depth$q1_depth)   # q1 ==>  0.009   
lambda_q7 <- BoxCox.lambda(gage_9015_depth$q7_depth)   # q7 ==>  0.113   
lambda_q30 <- BoxCox.lambda(gage_9015_depth$q30_depth) # q30 ==> 0.214  

# 5.Standardize data by z-score:
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

# Apply transformation and scale data
gage_9015_depth <- gage_9015_depth %>% 
  mutate(q1_tr = BoxCox(gage_9015_depth$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(gage_9015_depth$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(gage_9015_depth$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>% 
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

# Conduct exploratory PCA---- 

# Select numeric data 
# ~~~~~~~~~~~~~~~~~~~
# prcomp() requires a dataset with only the variables.  
# Below splits data into PCA input & names to connect to result 

pca_input <- gage_9015_depth %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) 

pca_meta <- gage_9015_depth %>% 
  select(sta, date, q1_depth, q7_depth, q30_depth) 

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE) 

# Gather & summarize PCA results----
# The broom package is used to gather results into tibbles.

# Eigenvectors--results about PC axes
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
pca_vars <-  tidy(pca_matrix, matrix = "variables") 

# Drop the 3rd PC-axis because it's not useful 
pca_vars <- pca_vars %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix 
gage_9015_aug <- augment(pca_matrix, data = pca_input) 
gage_9015_aug <- bind_cols(pca_meta, gage_9015_aug) 

gage_9015_aug <- gage_9015_aug %>% 
  select(-.fittedPC3, -.rownames, 
         -q1_depth1, -q7_depth1, -q30_depth1) %>% 
  mutate(q1_q30_diff = q1_depth - q30_depth) 

# summarize pca of gages
gage_9015_sum <- gage_9015_aug %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            q1_mean = mean(q1_depth), 
            q7_mean = mean(q7_depth),
            q30_mean = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff)) %>% 
  ungroup() %>% 
  arrange(q1_q30_mean) %>% 
  arrange(q7_mean)
  
pca_eigen
# A tibble: 3 x 4
#     PC std.dev percent cumulative
#  <dbl>   <dbl>   <dbl>      <dbl>
#1     1   1.69  0.948        0.948
#2     2   0.369 0.0454       0.993
#3     3   0.141 0.00659      1    

pca_vars
# A tibble: 6 x 3
#  var          PC  value
#  <fct>     <dbl>  <dbl>
#1 q1_depth      1  0.577
#2 q7_depth      1  0.588
#3 q30_depth     1  0.567
#4 q1_depth      2  0.596
#5 q7_depth      2  0.172
#6 q30_depth     2 -0.784

# clean up Global Environment----
gage_9015_sum <- full_join(gage_9015_meta, gage_9015_sum)

rm(lambda_q1, lambda_q7, lambda_q30, gage_9015_depth, gage_9015_meta, 
   pca_eigen, pca_meta, pca_vars, pca_input, pca_matrix)


# plot PCA----
# PC1 explanatory plot
ggplot(gage_9015_aug, aes(q7_depth, .fittedPC1, color = factor(sta))) + 
  geom_jitter() + 
  theme_classic() +
  xlab("Q7 depth") +
  ylab("Fitted PC1") + 
  theme(legend.position = "bottom")
  
# PC2 explanatory plot
ggplot(gage_9015_aug, aes(q1_q30_diff, .fittedPC2, color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q1 depth minus Q30 depth") +
  ylab("Fitted PC2") +
  theme(legend.position = "bottom") 

# mean eigenvector plot
ggplot(gage_9015_sum, aes(PC1_mean, PC2_mean)) +
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
            nudge_y = 0.02) + 
  theme_classic() + 
  scale_x_continuous(limits = c(-3.5, 2)) + 
  xlab("PC axis 1 mean") +
  ylab("PC axis 2 mean") 


gage_9015_sum <- gage_9015_sum %>% 
  mutate(period = "2000-15") 

gage_9398_sum <- gage_9398_sum %>% 
  mutate(period = "1993-98") 

gage_sum <- bind_rows(gage_9398_sum, gage_9015_sum)

# transformed variable plot
#ggplot(gage_9015_sum, aes(q7_mean, q1_q30_mean)) +
#  geom_jitter() + 
#  geom_density2d() + 
#  geom_text(aes(label = sta), check_overlap = TRUE, 
#              nudge_y = 0.02) + 
#  scale_x_continuous(limits = c(-2.5, 2)) + 
#  scale_y_continuous(limits = c(-1, 0.75)) + 
#  geom_vline(xintercept = 0, aes(size = 2)) + 
#  geom_hline(yintercept = 0, aes(size = 1)) + 
#  theme_classic() + 
#  xlab("Average transformed Q7 depth") +
#  ylab("Transformed Q1 and Q30 average difference") 


# 
ggplot(gage_sum, aes(q7_mean, q1_q30_mean)) + 
  facet_wrap(ncol = 2, vars(period)) + 
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2.5, 2)) + 
  scale_y_continuous(limits = c(-1, 0.75)) + 
  geom_density2d() + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  xlab("hydrologic export index") +
  ylab("stability index") 




```


```{r Model-Based_Clustering-9015}
# creates models for 2003-2017 waterYears

# create density plots----

# create density plot of centroids 
#ggplot(gage_9015_sum, aes(q7_mean, q1_q30_mean)) +
#  geom_jitter() + 
#  geom_density2d() + 
#  geom_text(aes(label = sta), check_overlap = TRUE, 
#              nudge_y = 0.02) + 
#  scale_x_continuous(limits = c(-2, 1)) + 
#  geom_vline(xintercept = 0, aes(size = 2)) + 
#  geom_hline(yintercept = 0, aes(size = 1)) + 
#  theme_classic() 


# Preparing to cluster by the central tendancy
mc_input <- gage_9015_sum %>% 
  select(q1_mean:q30_mean)

mc_input_meta <- gage_9015_sum %>% 
  select(sta)

gage_clust_l <- Mclust(mc_input) # Model-based-clustering 
summary(gage_clust_l) # Print a summary 

#---------------------------------------------------- 
#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 

#Mclust EEV (ellipsoidal, equal volume and shape) model with 3 components: 

# log.likelihood  n df      BIC      ICL
#       90.50279 21 23 110.9816 110.9624

#Clustering table:
# 1  2  3 
# 7 11  3 

gage_clust_l$modelName # Optimal selected model ==> "EEV" 
gage_clust_l$G # Optimal number of cluster => 3 

# Take a look at the results
gage_clust <- as.tibble(gage_clust_l$z) 
gage_clust <- bind_cols(mc_input_meta, gage_clust) 

gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.8) 

gage_clust <- full_join(gage_9015_sum, gage_clust, by = "sta")
gage_clust <- gage_clust %>% 
  select(sta, group, everything())

# Visualizing model-based clustering

# BIC values used for choosing the number of clusters 
fviz_mclust(gage_clust_l, "BIC", palette = "jco") 

# Classification uncertainty 
# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations 
fviz_mclust(gage_clust_l, "uncertainty", palette = "jco")

# Classification: plot showing the clustering 
fviz_mclust(gage_clust_l, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 

# Plot clusters in PCA space
ggplot(gage_9398_sum, aes(q7_mean, q1_q30_mean, color = group)) +
  geom_jitter() + 
  geom_density2d() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2, 1.6)) + 
  scale_y_continuous(limits = c(-0.8, 0.55)) + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() 

gage_clust_plot <- gage_9398_sum %>% 
  select(sta, group, q7_mean, q1_q30_mean) %>% 
  rename(hydro_exp_coeff = q7_mean) %>% 
  rename(stability_coeff = q1_q30_mean) %>% 
  gather(key = parameter, value, -sta, -group) 


ggplot(gage_clust_plot, aes(group, value)) + 
  facet_grid(rows = vars(parameter)) + 
         geom_boxplot() + 
  theme_bw()

# This is OLD!
# Cluster 1: High hydrologic export 
#   BATTLE CR BELOW HERMOSA,SD
#   BEAVER CR NEAR BUFFALO GAP,SD
#   FRENCH CR ABOVE FAIRBURN SD
#   WOUNDED KNEE CREEK AT WOUNDED KNEE, SD
# Cluster 2: Very high hydrologic export - highest gw 
#   BATTLE CR AT HERMOSA,SD
#   BATTLE CR NEAR KEYSTONE,SD
#   BEAVER CREEK ABOVE BUFFALO GAP, SD
#   FALL R AT HOT SPRINGS,SD 
#   LAKE CR BELOW REFUGE NEAR TUTHILL,SD 
#   LITTLE WHITE R NEAR VETAL,SD
#   LITTLE WHITE R ABV ROSEBUD SD
#   LITTLE WHITE R NEAR MARTIN,SD
#   LITTLE WHITE R NEAR ROSEBUD SD
#   LITTLE WHITE R BELOW WHITE RIVER,SD
#   RAPID CR NEAR FARMINGDALE,SD
#   ROSEBUD CR AT ROSEBUD SD
# Cluster 3: Moderate hydrologic export; low flashiness
#   BEAVER CREEK NEAR PRINGLE, SD 
#   CHEYENNE RIVER NEAR WASTA, SD 
#   SPRING CR NEAR HERMOSA,SD
#   WHITE CLAY CR NEAR OGLALA SD
#   WHITE R NEAR OGLALA SD
#   WHITE RIVER AT SLIM BUTTE, SD
#   WHITE R NR NE-SD STATE LINE
# Cluster 4: Moderate hydrologic export; higher flashiness
#   BLACK PIPE CREEK NR BELVIDERE, SD
#   SOUTH FORK BAD R NEAR COTTONWOOD,SD
#   WHITE R NEAR KADOKA,SD
# Cluster 5: Very low hydrologic export coefficient; flashy - ephemeral
#   HAT CR NEAR EDGEMONT,SD
#   HORSEHEAD CR AT OELRICHS,SD 
```


```{r PROBABLY-GARBABE}
# this is the simplist case approach
#gage_scale_l <- gage_prep %>% 
#  select(sta, q_depth, date) %>% 
#  mutate(q_mean = mean(q_depth)) %>% 
#  mutate(q_sd = sd(q_depth)) %>% 
#  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
#  select(sta, q_scaled, date)

# calculate summary values
gage_sum <- gage_scale_l %>% 
  group_by(sta) %>% 
  summarise(mean = mean(q_scaled),
            sd = sd(q_scaled)) %>%
    ungroup()

# standardize the variables
gage_scale1_l <- gage_prep %>% 
  select(sta, q_depth, date) %>% 
  mutate(q_mean = mean(q_depth)) %>% 
  mutate(q_sd = sd(q_depth)) %>% 
  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
  mutate(avg_period = "1") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q_scaled, date)

gage_scale7_l <- gage_prep %>%  
 filter(!is.na(q7_depth)) %>% 
  mutate(q7_mean = mean(q7_depth)) %>% 
  mutate(q7_sd = sd(q7_depth)) %>% 
  mutate(q7_scaled = (q7_depth - q7_mean)/q7_sd) %>% 
  mutate(avg_period = "7") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q7_scaled, date)
    
gage_scale30_l <- gage_prep %>% 
  filter(!is.na(q30_depth)) %>% 
  mutate(q30_mean = mean(q30_depth)) %>% 
  mutate(q30_sd = sd(q30_depth)) %>% 
  mutate(q30_scaled = (q30_depth - q30_mean)/q30_sd) %>% 
  mutate(avg_period = "30") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q30_scaled, date)

# spread the long variables
gage_scale1 <- gage_scale_l %>% 
  spread(sta_type, q_scaled) 

gage_scale7 <- gage_scale7_l %>% 
  spread(sta_type, q7_scaled) 

gage_scale30 <- gage_scale30_l %>% 
  spread(sta_type, q30_scaled) 

gage_scale <- full_join(gage_scale1, gage_scale7)
gage_scale <- full_join(gage_scale, gage_scale30)

rm(gage_scale_l, gage_scale1, gage_scale1_l, gage_scale7, 
   gage_scale7_l, gage_scale30, gage_scale30_l)

# A check on the data finds Wounded Knee Creek has ~11% NA values
#plot_missing(gage_scale)

check_sta <- gage_scale %>% 
  select(wkc_wok30, date) %>% 
  filter(is.na(wkc_wok30))

# drop the NA values (num obs drops from 2192 to 1916)
gage_scale <- gage_scale %>% 
  drop_na() 

gage_scale_l <- gage_scale %>%
  gather(key = )


$xi−center(x)scale(x) \frac{x_i - center(x)}{scale(x)}$
$xi−mean(x)sd(x) \frac{x_i - mean(x)}{(x)}$

Where center(x) can be the mean or the median of x values, and scale(x) can be the standard deviation (SD), the interquartile range, or the MAD (median absolute deviation).
```

```{r old_code_2_delete, eval=FALSE}
# this code chunk joins the various [spi1, spi48] data slices

# get names for Pearson-III distribution - done once
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#att_names <- spi.cot.l$dist.para %>% 
#  rownames() %>% 
#  as_data_frame() %>% 
#  rename(property = value) 

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
#att_cot <- spi.cot.l$dist.para %>% 
#  as_data_frame() %>% 
#  mutate(sta = "cot")

#att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
#att_int <- spi.int.l$dist.para %>% 
#  as_data_frame() %>% 
#  mutate(sta = "int")

#att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
#att_oel <- spi.oel.l$dist.para %>% 
##  as_data_frame() %>% 
#  mutate(sta = "oel")

#att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
#att_ora <- spi.ora.l$dist.para %>% 
#  as_data_frame() %>% 
#  mutate(sta = "ora")

#att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
#att_rap <- spi.rap.l$dist.para %>% 
#  as_data_frame() %>% 
#  mutate(sta = "rap")

#att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
#  rename("1" = M1) %>% 
#  rename("2" = M2) %>% 
#  rename("3" = M3) %>% 
 # rename("4" = M4) %>% 
#  rename("5" = M5) %>% 
#  rename("6" = M6) %>% 
#  rename("7" = M7) %>% 
#  rename("8" = M8) %>% 
#  rename("9" = M9) %>% 
#  rename("10" = M10) %>% 
#  rename("11" = M11) %>% 
#  rename("12" = M12) 

#rm(att_cot, att_int, att_oel, att_ora, att_rap)

#   1mon-check for warnings from SPI ----
#flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
#flag_int <- as.tibble(spi.int.l$dist.para.flag) # M2 (feb) flag => 3
#flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
#flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
#flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

#flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
#                      flag_ora, flag_rap) 

# the flag means ML values did not converge
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Sta  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# Int  0.30    4.40    9.20   12.53   17.10   70.30 * is the max why?
# Cot  0.30    4.80   10.40   13.54   18.30   62.00 
# Oel  0.00    5.85   11.50   12.08   16.40   37.80 
# Ora  0.00    5.05   10.10   12.35   17.50   42.70 
# Rap  0.60    5.10    8.90   11.56   15.95   43.40  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# gather months for PE3
#spi_att <- spi_att %>% 
#  select(sta, property, everything()) %>% 
 # gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
#spi_check <- spi_att %>% 
 # filter(property == "N" |
 #          property == "P0" |
#           property == "N.P0") %>% 
 # arrange(property)

#spi_att <- spi_att %>% 
 # filter(property != "N") %>% 
#  filter(property != "P0") %>% 
#  filter(property != "N.P0") %>% 
#  mutate(month = as.integer(month)) 
# prepare for join- create length
spi_att1 <-  mutate(spi_att1, spi_length = 1) 
spi_att2 <-  mutate(spi_att2, spi_length = 2) 
spi_att3 <-  mutate(spi_att3, spi_length = 3) 
spi_att4 <-  mutate(spi_att4, spi_length = 4) 
spi_att5 <-  mutate(spi_att5, spi_length = 5) 
spi_att6 <-  mutate(spi_att6, spi_length = 6) 
spi_att9 <-  mutate(spi_att9, spi_length = 9)  
spi_att12 <- mutate(spi_att12, spi_length = 12)  
spi_att18 <- mutate(spi_att18, spi_length = 18) 
spi_att24 <- mutate(spi_att48, spi_length = 24) 

# prepare for join- differenciate values
spi_att1 <-  mutate(spi_att1, spi1 = value) 
spi_att2 <-  mutate(spi_att2, spi2 = value) 
spi_att3 <-  mutate(spi_att3, spi3 = value) 
spi_att4 <-  mutate(spi_att4, spi4 = value) 
spi_att5 <-  mutate(spi_att5, spi5 = value) 
spi_att6 <-  mutate(spi_att6, spi6 = value) 
spi_att9 <-  mutate(spi_att9, spi9 = value)  
spi_att12 <- mutate(spi_att12, spi12 = value)  
spi_att18 <- mutate(spi_att18, spi18 = value) 
spi_att24 <- mutate(spi_att24, spi24 = value) 

# create a list of the spi_att dataframes
att_list <- list(spi_att1, spi_att2, spi_att3, 
                            spi_att4, spi_att5, spi_att6, 
                            spi_att9, spi_att12, spi_att18, 
                            spi_att24)

# flatten the list to a dataframe & select values
spi_att <- flatten_dfc(att_list) %>% 
  select(sta, spi_length, property, month, spi1, spi2, spi3, spi4, 
         spi5, spi6, spi9, spi12, spi18, spi24) %>% 
  gather(key = length, value = spi_value, -spi_length, 
         -property, -sta, -month) %>% 
  select(-length)
 

# join spi check values----
# prepare for join- create length
spi_check1 <- mutate(spi_check1, spi_length = 1) 
spi_check2 <- mutate(spi_check2, spi_length = 2) 
spi_check3 <- mutate(spi_check3, spi_length = 3) 
spi_check4 <- mutate(spi_check4, spi_length = 4) 
spi_check5 <- mutate(spi_check5, spi_length = 5) 
spi_check6 <- mutate(spi_check6, spi_length = 6) 
spi_check9 <- mutate(spi_check9, spi_length = 9)  
spi_check12 <- mutate(spi_check12, spi_length = 12)  
spi_check18 <- mutate(spi_check18, spi_length = 18) 
spi_check24 <- mutate(spi_check24, spi_length = 24) 

# bind rows of spi correlations for different averaging periods
spi_check <- bind_rows(spi_check1, spi_check2) 
spi_check <- bind_rows(spi_check, spi_check3) 
spi_check <- bind_rows(spi_check, spi_check4) 
spi_check <- bind_rows(spi_check, spi_check5) 
spi_check <- bind_rows(spi_check, spi_check6) 
spi_check <- bind_rows(spi_check, spi_check9) 
spi_check <- bind_rows(spi_check, spi_check12) 
spi_check <- bind_rows(spi_check, spi_check18) 
spi_check <- bind_rows(spi_check, spi_check24) 

# check non-zero precip vals
zero_prob_prcp <- spi_check %>% 
  filter(property != "N") %>% 
  mutate(value = as.numeric(value)) %>% 
  mutate(month = as.integer(month)) %>% 
  drop_na() %>% 
  filter(property == "P0") %>% 
  group_by(month, spi_length) %>% 
  summarize(mean_val_perc = 100 * mean(value)) %>% 
  filter(mean_val_perc > 0)

# join spi index values & clean up global environment----
# prepare for join -add lengths & get dataframes to same length
spi_gath1 <- spi_gath1 %>% 
  mutate(spi_length = 1) 

spi_gath2 <- spi_gath2 %>% 
  mutate(spi_length = 2) 

spi_gath3 <- spi_gath3 %>% 
  mutate(spi_length = 3) 

spi_gath4 <- spi_gath4 %>% 
  mutate(spi_length = 4) 

spi_gath5 <- spi_gath5 %>% 
  mutate(spi_length = 5) 

spi_gath6 <- spi_gath6 %>% 
  mutate(spi_length = 6) 

spi_gath9 <- spi_gath9 %>% 
  mutate(spi_length = 9) 

spi_gath12 <- spi_gath12 %>% 
  mutate(spi_length = 12) 

spi_gath18 <- spi_gath18 %>% 
  mutate(spi_length = 18) 

spi_gath24 <- spi_gath24 %>% 
  mutate(spi_length = 24)  

# bind rows is probably the easiest approach here to get the 
# individual dataframes into a single dataframe.  The dataframes are 
# of different lengths because of the averaging periods

spi_index <- bind_rows(spi_gath1, spi_gath2) 
spi_index <- bind_rows(spi_index, spi_gath3)  
spi_index <- bind_rows(spi_index, spi_gath4)  
spi_index <- bind_rows(spi_index, spi_gath5)  
spi_index <- bind_rows(spi_index, spi_gath6)  
spi_index <- bind_rows(spi_index, spi_gath9)  
spi_index <- bind_rows(spi_index, spi_gath12)  
spi_index <- bind_rows(spi_index, spi_gath18)  
spi_index <- bind_rows(spi_index, spi_gath24)  


rm(spi_att1, spi_att2, spi_att3, spi_att4, spi_att5, 
   spi_att6, spi_att9, spi_att12, spi_att18, spi_att24)

rm(spi_check1, spi_check2, spi_check3, spi_check4, 
   spi_check5, spi_check6, spi_check9, spi_check12, spi_check18, 
   spi_check24) 

rm(spi_corr_cot, spi_corr_int, spi_corr_oel, spi_corr_ora, 
   lat_to_km, lon_to_km, sta_loc)

rm(spi_gath1, spi_gath2, spi_gath3, spi_gath4, spi_gath5, 
   spi_gath6, spi_gath9, spi_gath12, spi_gath18, spi_gath24) 

rm(spi_check, att_list) 

#r precip-clustering, include=FALSE, eval=FALSE
# Didn't work well...

# Prepare data for exploratory PCA -----------------------------------

# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  

# The steps are as follows :
# 1. Transform using BoxCox transformation to approach normality & 
#    standardize.
# 2. Drop rows with missing observations from wide data
#    (e.g. the station records are of different lengths)
# 3. Apply transformation and the scale data after making the data 
#    long.
# 4. Drop intermediate variables & rows with missing observations 
#    

# BoxCox transformation utilizes a lamda value to transform a dataset 
# to approach a normal distribution.
#   lambda = 1 is normal distribution (no change), 
#   lambda = 0.5 is a square-root transformation, 
#   lamda = 2 is a square transformation,
#   lambda = 0 is a logrithmic transformation.
# find lambda values for BoxCox transformation  
lambda_prcp <- BoxCox.lambda(sta_grp$depth) 

# lambda vals
#  prcp ==> 0.108 

# glimpse(sta_scaled) 
# sta          <chr> "oel", "oel", "oel", "oel", "oel", "oel", "oel"
# date         <date> 2018-05-01, 2018-04-01, 2018-03-01, 2018-02-01
# depth        <dbl> 139.8, 33.5, 24.0, 18.3, 13.3, 14.8, 16.5, 19.0.
# depth_tr     <dbl> 6.528617, 4.270760, 3.792030, 3.415306, 2.985829.
# depth_scaled <dbl> 1.320188955, 0.340849965, 0.133202243 

sta_mon_wide <- sta_int %>% 
  mutate(depth_tr = BoxCox(sta_int$depth, lambda_prcp)) %>% 
  mutate(depth_mean = mean(depth_tr, na.rm = TRUE)) %>% 
  mutate(depth_sd = sd(depth_tr, na.rm = TRUE)) %>% 
  mutate(depth_scaled = (depth_tr - depth_mean)/depth_sd) %>% 
  dplyr::select(-date) %>% 
  select(-sta_abrev, -mean_depth, -deviation)

sta_mon <- sta_mon_wide %>%  
  mutate(sta = as.factor(sta)) %>%  
  select(-depth, -yr_mon, -depth_mean, -depth_sd, -depth_tr, ) %>%
  spread(month, depth_scaled, convert = TRUE) %>% 
  drop_na() %>% 
  rename(jan = "1") %>% 
  rename(feb = "2") %>% 
  rename(mar = "3") %>% 
  rename(apr = "4") %>% 
  rename(may = "5") %>% 
  rename(jun = "6") %>% 
  rename(jul = "7") %>% 
  rename(aug = "8") %>% 
  rename(sep = "9") %>% 
  rename(oct = "10") %>% 
  rename(nov = "11") %>% 
  rename(dec = "12") 

# glimpse(sta_mon) 
# Observations: 230
# Variables: 14
# year <int> 1972, 1972, 1972, 1972, 1972, 1973, 1973, 1973, 1973, 
# sta  <fct> cot, int, oel, ora, rap, cot, int, oel, ora, rap, cot, 
# jan  <dbl> -1.1705655, -1.8359887, -1.0710896, -0.5540255, -0.781 
# feb  <dbl> -0.5138541, -1.7302116, -1.1024868, -1.3120703, -0.400 
# mar  <dbl> -0.49461927, -0.43412364, -0.01338585, -0.13223232, -0 
# apr  <dbl> -0.1172139, 1.0387562, 0.6274294, 0.3555747, 0.8642143 
# may  <dbl> 1.37710556, 0.70367637, 0.96166778, 0.84463790, 0.9909 
# jun  <dbl> 0.63876170, 0.36154511, 1.03170176, 1.35395955, 1.1668 
# jul  <dbl> 1.1087770, 0.7467736, 0.2690885, 0.3312313, 0.4877920, 
# aug  <dbl> 0.32709997, 0.79595891, 0.29970391, 0.47757034, 0.7806 
# sep  <dbl> -0.53364445, -0.99892436, -0.25600144, -0.10249695, -0 
# oct  <dbl> -0.25151491, -0.25151491, -0.01338585, -0.10981856, -0 
# nov  <dbl> -0.39471011, -0.39471011, -0.78171073, -0.32692017, -0 
# dec  <dbl> -0.84271133, -6.16974428, -0.83213761, -1.11880814, -0 

# Select numeric data for PCA input &  names to connect to result 
# prcomp() requires a dataset with only the variables for the 
# unsupervised classification 

pca_meta <- sta_mon %>% 
  select(year, sta)
pca_input <- sta_mon %>% 
  select(-year, -sta)

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE) 

# Gather PCA results----
# The broom package is used to gather results into tibbles.

# this plots the PCA biplot 
biplot <- biplot(pca_matrix, scale = 0) 

# Eigenvectors
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# A tibble: 12 x 4
#      PC std.dev percent cumulative
#   <dbl>   <dbl>   <dbl>      <dbl>
# 1     1   1.25   0.130       0.130
# 2     2   1.20   0.120       0.250
# 3     3   1.14   0.108       0.357
# 4     4   1.11   0.103       0.460
# 5     5   1.07   0.0955      0.556
# 6     6   1.03   0.0877      0.643
# 7     7   0.984  0.0807      0.724
# 8     8   0.924  0.0711      0.795
# 9     9   0.885  0.0652      0.860
#10    10   0.846  0.0597      0.92 
#11    11   0.728  0.0441      0.964
#12    12   0.656  0.0359      1  
# This means that 13% of covarience is explained by PCA1 & 
# 12% of varience by PCA2

# PCA variables 
# these are the loadings on the PCA axes.  
pca_vars <-  tidy(pca_matrix, matrix = "variables") 
pca_vars <- pca_vars %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) %>% 
  arrange(value) %>% 
  arrange(PC)

pca_rotation <- pca_matrix$rotation

# Bind sample vals to PCA matrix 
pca_au <- augment(pca_matrix, data = pca_input) %>% 
  select(jan:.fittedPC3)
pca_au <- bind_cols(pca_meta, pca_au) 
rm(pca_meta)

# Calculate mean eigenvectors 
# this summarizes the individual points by station - used in plots
pca_summary <- pca_au %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            PC3_mean = mean(.fittedPC3), 
            mar_mean = mean(mar), 
            apr_mean = mean(apr), 
            may_mean = mean(may),
            jun_mean = mean(jun), 
            jul_mean = mean(jul), 
            aug_mean = mean(aug),
            sep_mean = mean(sep), 
            oct_mean = mean(oct), 
            nov_mean = mean(nov)) %>% 
  ungroup()    


# PCA Interpretation-------------------------- 
pca_expl <- pca_vars %>%
  filter(abs(value) > 0.3) %>% 
  filter(PC < 4) %>% 
  filter(var != "dec") %>% 
  filter(var != "jan") %>% 
  arrange(value) %>% 
  arrange(PC)

pca_expl
# A tibble: 9 x 3
#  var      PC  value
#  <fct> <dbl>  <dbl>
#1 jun       1 -0.612
#2 mar       1  0.357
#3 sep       1  0.368
#4 may       2 -0.485
#5 aug       2 -0.438
#6 apr       2  0.330
#7 mar       2  0.438
#8 oct       3 -0.540
#9 may       3 -0.409

# thus PC1 is var in jun vs mar & sep
#      PC2 is var in may & aug vs apr
#      PC3 is var in oct & may

#PCA1:PCA2
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC1_mean, PC2_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

# PC1:: 
# Interior has some unusually a dry March & Jun records
# Cottonwood has some unusually dry Jun & Sept records
pca_pc1 <- pca_au %>% 
  select(sta, mar, jun, sep, .fittedPC1) %>% 
  gather(key = month, value = z_depth, -.fittedPC1, -sta)

ggplot(pca_pc1, aes(.fittedPC1, z_depth, color = sta)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(cols = vars(month))

# PC2:: 
# Oral & Interior have a slightly wetter May & August
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC2_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC2_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 


pca_pc2 <- pca_au %>% 
  select(sta, apr, may, aug, .fittedPC2) %>% 
  gather(key = month, value = z_depth, -.fittedPC2, -sta)

ggplot(pca_pc2, aes(.fittedPC2, z_depth, color = sta)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(rows = vars(month)) + 
  theme_classic() 


#PCA1:PCA3
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC1_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC1_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

#PCA2:PCA3
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC2_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC2_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

#fviz_pca_ind(prcomp(pca_input), 
#             title = "PCA - Iris data", 
#             habillage = pca_meta$sta, 
#             palette = "jco", geom = "point", 
#             ggtheme = theme_classic(), 
#             legend = "bottom")  
```