<!--
Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0  Formulate your question  
2.0   Read in your data  
3.0  Check the dataset 
3.1  Check the number of rows and columns.
3.2  Check the types of data
3.3  Look at the top and the bottom of your data 
3.4  Check your “n”s & NAs 
3.5  Validate with at least one external data source  
4.0  Try the easy solution first to answer question
5.0  Challenge your solution 
6.0  Follow up questions 

## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?

## Narrower question:
What is the underlying distribution of precipitation data?  

## Analysis Steps & progress
1. Recreated analysis from the lmomco text ch 12 (author?) in Tidyverse
2. Imported cleaned precipitation data (see 04_prcp-data_munging)   
3. Applied sqrt & log10 transform to explore effects on skew 
4. Explored the data with box plots, violin plot.
5. Applied Weibull plotting position and graphed the data on sqrt plot
6. Calculated L-moments and L-moment ratios 
7. Calculated SPI for "cot" dataset


## Issues: SPI calculations not as expected; -Inf for 2 mo
## Next Steps: 
1. Clean up code leading up to SPI
2. Map the variable as a function

Other:
1. figure out how to reference stuff 
-->

## Variable naming convention:   
sta          precipitation station  
_meta        metadata  
_mon         monthly precipitation depths  
_grp         wide data changed to long data
_notzero     non-zero precip values
_zero        precip values equal to zero
_log         log10 of monthly precipitation depths
_count       number of months in a given record

lmom_sta     L-moments for stations
lmom_reg     Weighted mean of L-moments for stations

min          minimum non-zero value
L1           first L-moment, similiar to mean
L_CV         first L-moment ratio, similiar to coefficient of var
L_skew       second L-moment ratio, similiar to skewness 
L_kurtosis   third L-moment ratio, similiar to kurtosis
n            number of months in a given record
int#         intermediate variable used to bind rows; # = 1, 2, ...

# Used in the L-moment diagram
aep4         4-Parameter Asymmetric Exponential Power Distribution 
gev          Generalized Extreme Value Distribution
glo          Generalized Logistic Distribution
gpa          Generalized Pareto Distribution
####gno      Generalized Logistic Distribution #need to fix as GLO
gov          Govindarajulu Distribution
pe3          Pearson Type III Distribution


## Thoughts - the orig depth vs plotting vals look j-shaped.  
## Sqrt trans vs plotting vals look slightly sinusoidal; nice boxplots
## Log-tranformation looks like the mirror of orig depth vs plotting

## Results: Fits a PE3 distribution
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, include=FALSE, message=FALSE} 
# Sets up the library of packages 
library("lmomco") # lmoments to find distribution 
library("tidyverse")
library("magrittr") # contains easier ways to say things about lists
library("here") # identifies where to save work 
library("rio") # more robust I/O - to import and clean data
library("lubridate") # fixes dates 
library("SPEI") # Calculates SPI-index
library("jsonlite") # Convert between JSON data and R objects
library("curl") # Drop-in replacement for base url
library("listviewer") # htmlwidget for interactive views of R lists
library("janitor") # tools to clean dirty data 

#lmomco <- citation("lmomco")
#toBibtex(lmomco)

# Session Info
a_session <- devtools::session_info()

```

```{r import-data} 
# General Purpose: prepare data for drought index   
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) 

# add a small value to zeros to solve a downstream issue 
# maybe have this fixed now using ts rather than date class
#sta_mon <- sta_mon %>% 
#  gather(key = "station", value = "depth", -date, -year, -month) %>%
#  mutate(depth = replace(depth, depth == 0.0, 0.15)) %>%
#  spread(station, depth)

# make the wide data long, remove NA vals, sqrt transform
sta_grp <- sta_mon %>%
  gather(key = "station", value = "depth", -date, -year, -month) %>%
  drop_na(depth)  # %>% 
#  mutate(sqrt_depth = sqrt(depth)) 

# Check on log transformation
#sta_notzero <- sta_grp %>%
#  filter(depth != 0)

#min <- min(sta_notzero$depth)

#sta_zero <- sta_grp %>%
#  filter(depth == 0) %>%
#  mutate(depth = depth + min/2)

#sta_log <- bind_rows(sta_zero, sta_notzero)
#sta_log <- sta_log %>%
#  mutate(log_depth = log10(depth))

#rm(min)

# plot the data
ggplot(sta_grp, aes(as.factor(station), depth)) +
  geom_violin() +
  geom_boxplot() +
#  scale_y_sqrt() +
#  scale_y_log10() +
  scale_y_sqrt() +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") +
  xlab("") + 
  ylab("Monthly depth in mm")

#ggplot2::ggsave(filename = "rf_boxplot.png", 
#                width = 6, height = 6, units = "in")
```

```{r eda_fiddling, include=FALSE, eval=FALSE}
sta_big <- sta_grp %>%
  filter(depth > 100)
summary(sta_big)
ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```

```{r station_plotting_postion, include=FALSE, eval=FALSE} 
# find the plotting position: using Weibull; a = 0  
sta_grp <- sta_grp %>% 
  group_by(station) %>% 
  arrange(depth) %>% 
  mutate(position = pp(depth)) %>% 
  ungroup() 

# find the plotting position for log-data: using Weibull; a = 0
#sta_log <- sta_log %>% 
#  group_by(station) %>% 
# arrange(log_depth) %>% 
#  mutate(position = pp(log_depth)) %>% 
#  ungroup() 

# plot the depths as a function of plotting position
ggplot(sta_grp, aes(position, depth)) + 
  facet_grid(.~station) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

#ggplot2::ggsave(filename = "rf_freq_plot.png", 
#                width = 6, height = 6, units = "in")

```

```{r lmoments, include=FALSE, eval=FALSE}
# This is a long bit of code that takes things out of a list 

lmom_sta <- sta_grp %>% 
  split(.$station) %>%
  map(~ lmoms(.$depth)) %>%
  transpose() %>%
  as_tibble() %>%
  select(lambdas, ratios) %>%
    mutate(lambdas = map(lambdas, ~as_tibble(t(.x))))  %>%
    mutate(lambdas = map(lambdas, 
                         ~set_names(.x, c("L1", "L2", "L3", 
                                          "L4", "L5")))) %>%
      mutate(ratios = map(ratios, ~as_tibble(t(.x))))  %>%
    mutate(ratios = map(ratios, 
                         ~set_names(.x, c("T1", "T2", "T3", 
                                          "T4", "T5")))) %>% 
  unnest(lambdas) %>%
  unnest(ratios) %>%
  mutate(station = c("oel", "cot", "rap", 
         "int", "ora")) %>%
  select(-T1) %>%
  rename(L_CV = T2) %>% 
  rename(L_skew = T3) %>%
  rename(L_kurtosis = T4) %>%
  select(station, L1, L_CV, L_skew, L_kurtosis)

# Note that we might consider Weiss 1964 bias value of 1.018 for L1

# get the lenths of the datasets
sta_count <- sta_grp %>%
  count(station)  

# join the number of years to the station
lmom_sta <- left_join(lmom_sta, sta_count, by = "station")

# calculate weighted means for regional L-moments 
L1  <- weighted.mean(lmom_sta$L1, lmom_sta$n)
L_CV  <- weighted.mean(lmom_sta$L_CV, lmom_sta$n)
L_skew  <- weighted.mean(lmom_sta$L_skew, lmom_sta$n)
L_kurtosis  <- weighted.mean(lmom_sta$L_kurtosis, lmom_sta$n)
n       <- sum(lmom_sta$n)

# combine the output into a single weighted mean
int1     <- cbind(L1, L_CV)
int2     <- cbind(int1, L_skew)
int3     <- cbind(int2, L_kurtosis)
lmom_reg <- cbind(int3, n)

rm(L1, L_CV, L_skew, L_kurtosis, n, int1, int2, int3, sta_count)

# finalize the regional L-moment
lmom_reg <- as.tibble(lmom_reg) %>%
  mutate(station = "WtMean") %>%
  select(station, everything())
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2  
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list 
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble()

gev <- lmrdia %>% 
  extract2(5) %>% as.tibble()

glo <- lmrdia %>%
  extract2(6) %>% as.tibble()

gpa <- lmrdia %>%
  extract2(7) %>% as.tibble()

gno <- lmrdia %>%
  extract2(9) %>% as.tibble()

gov <- lmrdia %>%
  extract2(10) %>% as.tibble()

pe3 <- lmrdia %>%
  extract2(12) %>% as.tibble()

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
               rename(GEV = V2.x) %>% 
               rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything()) 

rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}
# plots the theo distributions, the sample vals, and regional mean 
ggplot() + 
  geom_line(data = lmom_theo, aes(L_skew, L_kurtosis, 
                                  group = distribution, 
                                  linetype = distribution)) +
  geom_point(data = lmom_sta, aes(L_skew, L_kurtosis)) +
  geom_point(data = lmom_reg, aes(L_skew, L_kurtosis, 
                                  size = 2, show.legend = NA)) +
  theme_bw() + 
 # xlim(0.25, 0.5) +
 # ylim(0, 0.25) +
  xlim(0, 0.5) +
  ylim(0, 0.5) +
  ggtitle("L-moment diagram for monthly precipitation depth", 
          subtitle = "Weather stations near Pine Ridge Reservation, 1909-2018")

#ggplot2::ggsave(filename = "lmom_plot.png", 
#                width = 6, height = 6, units = "in")
```

```{r example-on-lists, include=FALSE, eval=FALSE}
n = c(2, 3, 5) 
s = c("aa", "bb", "cc", "dd", "ee") 
b = c(TRUE, FALSE, TRUE, FALSE, FALSE) 
x = list(n, s, b, 3)   # x contains copies of n, s, b

# List Slicing
#We retrieve a list slice with the single square bracket "[]" operator. # The following is a slice containing the second member of x, which is # a copy of s.

b <- as.data.frame(x[2])
> x[2] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee"

With an index vector, we can retrieve a slice with multiple members. Here a slice containing the second and fourth members of x.
> x[c(2, 4)] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee" 
 
[[2]] 
[1] 3 
```

```{r example-on-tidy-lists, include=FALSE, eval=FALSE}
 
# Simplifying lists with purrr 
library(jsonlite) # Convert between JSON data and R objects.
library(curl) # Drop-in replacement for base url
library(repurrrsive) # Recursive lists for teaching & examples. 
library(listviewer) # htmlwidget for interactive views of R lists

# Not all lists are easily coerced into data frames by simply calling 
# fromJSON() %>% as_tibble(). Unless your list is perfectly 
# structured, this will not work.

#Inspecting and exploring lists

#Before you can apply functions to a list, you should understand it. 
#Especially when dealing with poorly documented APIs, you may not know 
# in advance the structure of your list, or it may not be the same as 
# the documentation. str() is the base R method for inspecting a list 
# by printing the structure of the list to the console. If you have a 
# large list, this will be a lot of output. max.levels and list.len 
# can be used to print only a partial structure for this list. 
# Alternatively, you can use jsonedit() to interactively view the list # within RStudio.

# Let’s look at got_chars, which is a list of information on the 29 
# point-of-view characters from the first five books in A Song of Ice 
# and Fire by George R.R. Martin.

#Each element corresponds to one character and contains 18 sub-elements
# which are named atomic vectors of various lengths and types.

got_chars <- got_chars
str(got_chars, list.len = 3)
jsonedit(got_chars, mode = "view", elementId = "got_chars")

# Extract elements
# Quick review of purrr::map()

#Map functions in R for Data Science
#    Notes on map functions

# We can use purrr::map() to extract elements from lists.
# Name and position shortcuts

# Let’s extract the name element for each Game of Thrones character. 
# To do this, we can use map() and extract list elements based on their name:

chars_1to4 <- map(got_chars[1:4], "name")

# A companion shortcut is to extract elements by their integer 
# position in the list. For example, extract the 3rd element of each 
# character’s list like so:

chars_5to8 <- map(got_chars[5:8], 3)

# To recap, here are two shortcuts for making the .f function that 
# map() will apply:
#    Provide “TEXT” to extract the element named “TEXT”
#        Equivalent to function(x) x[["TEXT"]]
#    Provide i to extract the i-th element
#        Equivalent to function(x) x[[i]]

# And as always, we can use map() with the pipe %>%:

chars_name <- got_chars %>% map("name")
chars_name <- got_chars %>% map(3)

# Type-specific map
# map() always returns a list, but if you know that the elements are 
# all the same type (e.g. numeric, character, boolean) and are each 
# of length one, you can use the map_() function appropriate for that # type of vector.

chars_chr_9to12  <- as.tibble(map_chr(got_chars[9:12], "name"))
chars_chr_13to16 <- as.tibble(map_chr(got_chars[13:16], 3))

# Extract multiple values
# What if you want to retrieve elements? What if you want to know the 
# character’s name and gender? For a single user, we can use  
# traditional subsetting:

# Victarion element
chars_element3 <- got_chars[[3]]

# specific elements for Victarion
chars_ele3_sub <- got_chars[[3]][c("name", 
                                   "culture", "gender", "born")]

# We use a single square bracket indexing and a character vector to 
# index by name. To adapt this to the map() framework, recall:

# map(.x, .f, ...)

#The function .f will be [ and ... will be the character vector #identifying the names of the elements to extract.

char_map <- map(got_chars, `[`, c("name", "culture", "gender", "born"))
str(char_map[16:17])

# The result is a list of 40 with fewer elements (e.g. name, culture,
#   gender, born)

# Alternatively, we can use magrittr::extract() to do the same thing. 
# It looks a bit more clean:

library(magrittr)

char_map <- map(got_chars, extract, c("name", "culture", "gender", "born"))
str(char_map[18:19])

#Data frame output

# Notice that even by extracting multiple elements at once, we are 
# still left with a list. But we want a simplified data frame! 
# Remember that the output of map() is always a list. To force the 
# output to be a data frame, use map_df():

char_df <- map_df(got_chars, extract, c("name", "culture", "gender", "id", "born", "alive"))

# Now we have an automatically type converted data frame. It was 
# quite simple to perform, however it is not very robust. 

#It takes more code, but it is generally better to explicitly specify 
# the type of each column to ensure the output is as you would expect:

char_tibble <- got_chars %>% {
  tibble(
    name = map_chr(., "name"),
    culture = map_chr(., "culture"),
    gender = map_chr(., "gender"),       
    id = map_int(., "id"),
    born = map_chr(., "born"),
    alive = map_lgl(., "alive")
  )
}

# The dot . above is the placeholder for the primary input: got_chars 
# in this case. The curly braces {} surrounding the tibble() call 
# prevent got_chars from being passed in as the first argument of 
# tibble().

#Exercise: simplify gh_users

# repurrsive provides information on 6 GitHub users in a list named 
# gh_users. It is a recursive list:

#   One element for each of the 6 GitHub users
#   Each element is, in turn, a list with information on the user
#   What is in the list? Let’s take a look:

str(gh_users, list.len = 3)
jsonedit(gh_users, mode = "view", elementId = "gh_users") 

# Extract each user’s real name, username, GitHub ID, location, date 
# of account creation, and number of public repositories. Store this 
# information in a tidy data frame.

get_exam <- gh_users %>% { 
  tibble(
    name = map_chr(., "name"),
    username = map_chr(., "login"), 
    create_date = map_chr(., "created_at") %>%
      lubridate::ymd_hms(),
    num_repos = map_int(., "public_repos")
  )
}

# List inside a data frame

# gh_users has a single primary level of nesting, but you regularly 
# will encounter even more levels. gh_repos is a list with:

#    One element for each of the 6 GitHub users
#    Each element is another list of that user’s repositories (or the 
#      first 30 if the user has more)
#     Several of the list elements are also a list

str(gh_repos, list.len = 2)
jsonedit(gh_repos, mode = "view", elementId = "gh_repos")

# Vector input to extraction shortcuts

# Now we use the indexing shortcuts in a more complicated setting. 
# Instead of providing a single name or position, we use a vector:
#    the j-th element addresses the j-th level of the hierarchy

# Here we get the full name (element 3) of the first repository 
#   listed for each user.

gh_repos %>%
  map_chr(c(1, 3))

# Note that this does NOT give elements 1 and 3 of gh_repos. It 
# extracts the first repo for each user and, within that, the 3rd 
# piece of information for the repo.

# Get it into a data frame

# Our objective is to get a data frame with one row per repository, 
# with variables identifying which GitHub user owns it, the 
# repository name, etc.

# Create a data frame with usernames and gh_repos

# First let’s create a data frame with gh_repos as a list-column 
# along with identifying GitHub usernames. To do this, we extract 
# the user names using the approach outlined above, set them as the 
# names on gh_repos, then convert the named list into a tibble:

(unames <- map_chr(gh_repos, c(1, 4, 1)))

(udf <- gh_repos %>%
  set_names(unames) %>% 
    enframe("username", "gh_repos"))

# Next let’s extract some basic piece of information from gh_repos. 
#  For instance, how many repos are associated with each user?

udf <- udf %>% 
  mutate(n_repos = map_int(gh_repos, length))

# Practice on a single user
# Before attempting to map() functions to the entire data frame, 
# let’s first practice on a single user.

# one_user is a list of repos for one user
one_user <- udf$gh_repos[[1]]

# one_user[[1]] is a list of info for one repo
one_repo <- one_user[[1]]
str(one_repo, max.level = 1, list.len = 5)

# a highly selective list of tibble-worthy info for one repo
one_repo[c("name", "fork", "open_issues")]


# make a data frame of that info for all a user's repos
map_df(one_user, `[`, c("name", "fork", "open_issues"))
map_df(one_user, extract, c("name", "fork", "open_issues"))

# Scale up to all users
# Next let’s scale this up to all the users in the data frame by 
# executing a map() inside of a map():

udf %>% 
  mutate(repo_info = gh_repos %>%
           map(. %>%
                 map_df(extract, c("name", "fork", "open_issues"))))

# Tidy the data frame
# Now that we extracted our user-specific information, we want to 
# make this a tidy data frame. All the info we want is in repo_info, 
# so we can remove gh_repos and unnest() the data frame:

(rdf <- udf %>% 
   mutate(
     repo_info = gh_repos %>%
       map(. %>%
             map_df(extract, c("name", "fork", "open_issues")))
   ) %>% 
   select(-gh_repos) %>% 
   tidyr::unnest())

# Acknowledgments
# Examples and data files drawn from Jenny Bryan’s purrr tutorial
# This work is licensed under the CC BY-NC 4.0 Creative 
#   Commons License.
```

```{r spi-algorithm-coeff-cot, include=FALSE, eval=FALSE} 
# clean up environment 
rm(lmom_reg, lmom_sta, lmom_theo) 

# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months.  
# Code is mostly following the SPI vignette. 

# Add Cottonwoods dataset and change to time series 
sta_cot <- sta_mon %>% 
  select(year, month, cot) %>% 
  ts(end = c(2018, 05), frequency = 12)

# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot  <- spi(sta_cot[,'cot'], 1) 
spi_2cot  <- spi(sta_cot[,'cot'], 2) 
spi_3cot  <- spi(sta_cot[,'cot'], 3) 
spi_4cot  <- spi(sta_cot[,'cot'], 4) 
spi_5cot  <- spi(sta_cot[,'cot'], 5) 
spi_6cot  <- spi(sta_cot[,'cot'], 6) 
spi_9cot  <- spi(sta_cot[,'cot'], 9) 
spi_12cot <- spi(sta_cot[,'cot'], 12)  
spi_18cot <- spi(sta_cot[,'cot'], 18) 
spi_24cot <- spi(sta_cot[,'cot'], 24) 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_2cot_coeff <- as.tibble(spi_2cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_3cot_coeff <- as.tibble(spi_3cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_4cot_coeff <- as.tibble(spi_4cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_5cot_coeff <- as.tibble(spi_5cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_6cot_coeff <- as.tibble(spi_6cot$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9cot_coeff <- as.tibble(spi_9cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_12cot_coeff <- as.tibble(spi_12cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_18cot_coeff <- as.tibble(spi_18cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_24cot_coeff <- as.tibble(spi_24cot$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- spi_1cot_coeff %>% 
  rename(cot1_alpha = alpha) %>%
   rename(cot1_beta = beta) 

spi_2cot_coeff <- spi_2cot_coeff %>% 
  rename(cot2_alpha = alpha) %>%
   rename(cot2_beta = beta) 

spi_3cot_coeff <- spi_3cot_coeff %>% 
  rename(cot3_alpha = alpha) %>%
   rename(cot3_beta = beta) 

spi_4cot_coeff <- spi_4cot_coeff %>% 
  rename(cot3_alpha = alpha) %>%
   rename(cot4_beta = beta) 

spi_5cot_coeff <- spi_5cot_coeff %>% 
  rename(cot5_alpha = alpha) %>%
   rename(cot5_beta = beta) 

spi_6cot_coeff <- spi_6cot_coeff %>% 
  rename(cot6_alpha = alpha) %>%
   rename(cot6_beta = beta) 

spi_9cot_coeff <- spi_9cot_coeff %>% 
  rename(cot9_alpha = alpha) %>%
   rename(cot9_beta = beta) 

spi_12cot_coeff <- spi_12cot_coeff %>% 
  rename(cot12_alpha = alpha) %>%
   rename(cot12_beta = beta) 

spi_18cot_coeff <- spi_18cot_coeff %>% 
  rename(cot18_alpha = alpha) %>%
   rename(cot18_beta = beta) 

spi_24cot_coeff <- spi_24cot_coeff %>% 
  rename(cot24_alpha = alpha) %>%
   rename(cot24_beta = beta) 

# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
int1 <- bind_cols(spi_1cot_coeff, spi_2cot_coeff) 
rm(spi_1cot_coeff, spi_2cot_coeff) 

int2 <- bind_cols(int1, spi_3cot_coeff) 
rm(int1, spi_3cot_coeff) 

int3 <- bind_cols(int2, spi_4cot_coeff) 
rm(int2, spi_4cot_coeff) 

int4 <- bind_cols(int3, spi_5cot_coeff) 
rm(int3, spi_5cot_coeff) 

int5 <- bind_cols(int4, spi_6cot_coeff) 
rm(int4, spi_6cot_coeff) 

int6 <- bind_cols(int5, spi_9cot_coeff) 
rm(int5, spi_9cot_coeff) 

int7 <- bind_cols(int6, spi_12cot_coeff) 
rm(int6, spi_12cot_coeff) 

int8 <- bind_cols(int7, spi_18cot_coeff) 
rm(int7, spi_18cot_coeff) 

spi_coeff_cot <- bind_cols(int8, spi_24cot_coeff) 
rm(int8, spi_24cot_coeff) 

# export(spi_coeff_cot, "data/spi_coeff_cot.csv") 
rm(spi_coeff_cot)
```

```{r spi-algorithm-fit-cot}
# clean up environment 
rm(lmom_reg, lmom_sta, lmom_theo) 

# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months.  
# Code is mostly following the SPI vignette. 

# Add Cottonwoods dataset and change to time series 
sta_cot <- sta_mon %>% 
  select(year, month, cot) %>% 
  ts(end = c(2018, 05), frequency = 12)

# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot  <- spi(sta_cot[,'cot'], 1) 
spi_2cot  <- spi(sta_cot[,'cot'], 2) 
spi_3cot  <- spi(sta_cot[,'cot'], 3) 
spi_4cot  <- spi(sta_cot[,'cot'], 4) 
spi_5cot  <- spi(sta_cot[,'cot'], 5) 
spi_6cot  <- spi(sta_cot[,'cot'], 6) 
spi_9cot  <- spi(sta_cot[,'cot'], 9) 
spi_12cot <- spi(sta_cot[,'cot'], 12)  
spi_18cot <- spi(sta_cot[,'cot'], 18) 
spi_24cot <- spi(sta_cot[,'cot'], 24) 

# save SPI values
spi_1cot <- as.tibble(spi_1cot$fitted) %>% 
  mutate(duration = 1)

spi_2cot <- as.tibble(spi_2cot$fitted) %>% 
  mutate(duration = 2)

spi_3cot <- as.tibble(spi_3cot$fitted) %>% 
  rename(cot_3 = "Series 1") %>% 
  mutate(duration = 3) 

spi_4cot <- as.tibble(spi_4cot$fitted) %>% 
  rename(cot_4 = "Series 1") %>% 
  mutate(duration = 4) 

spi_5cot <- as.tibble(spi_5cot$fitted) %>% 
  rename(cot_5 = "Series 1") %>% 
  mutate(duration = 5) 

spi_6cot <- as.tibble(spi_6cot$fitted) %>% 
  rename(cot_6 = "Series 1") %>% 
  mutate(duration = 6) 

spi_9cot <- as.tibble(spi_9cot$fitted) %>% 
  rename(cot_9 = "Series 1") %>% 
  mutate(duration = 9)

spi_12cot <- as.tibble(spi_12cot$fitted) %>% 
  rename(cot_12 = "Series 1") %>% 
  mutate(duration = 12) 

spi_18cot <- as.tibble(spi_18cot$fitted) %>% 
  rename(cot_18 = "Series 1") %>% 
  mutate(duration = 18) 

spi_24cot <- as.tibble(spi_24cot$fitted) %>% 
  rename(cot_24 = "Series 1") %>% 
  mutate(duration = 24) 
# export(site, "data/site_meta.csv")



# this does not work!  Starts the shorter value series at 1909
# sta_in <- ts(sta_in[,-c(1,2)], end = c(2018, 05), frequency = 12)




# Add Cottonwoods dataset and change to time series 
sta_cot <- sta_mon %>% 
  select(year, month, cot) 

sta_cot <- ts(sta_cot[,-c(1,2)], end = c(2018, 05), frequency = 12)
# plot(sta_cot)

# Calculate 1-, 2-, 3-, 4-, 5-, 6-, 9-, 12-, 24-month SPI 
# check on using 'map' to do this.

spi_1cot <- spi(sta_cot[,'cot'], 1) 
#spi_2cot <- spi(sta_cot[,'cot'], 2) 
#spi_3cot <- spi(sta_cot[,'cot'], 3) 
#spi_4cot <- spi(sta_cot[,'cot'], 4) 
#spi_5cot <- spi(sta_cot[,'cot'], 5) 
#spi_6cot <- spi(sta_cot[,'cot'], 6) 
#spi_9cot <- spi(sta_cot[,'cot'], 9) 
spi_12cot <- spi(sta_cot[,'cot'], 12) 
#spi_18cot <- spi(sta_cot[,'cot'], 18) 
#spi_24cot <- spi(sta_cot[,'cot'], 24) 

#  Extract information from spei object: summary, 
#         call function, fitted values, and coefficients

spi_names <- as.tibble(names(spi_1cot))
# spi_call <- as.list(spi_1cot$call)
spi_1cot_fitted <- as.tibble(spi_1cot$fitted)
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients) 

spi_12cot_names <- as.tibble(names(spi_12cot))
spi_12cot_call <- as.list(spi_12cot$call)
spi_12cot_fitted <- as.tibble(spi_12cot$fitted)
spi_12cot_coeff <- as.tibble(spi_12cot$coefficients)

# Next steps
# Computing several time series at a time
# Dataset balance contains time series of the climatic water balance at 12 locations
#data(balance)
#head(balance)
#bal_spei12 <- spei(balance, 12)
# plot(bal_spei12)
```

```{r check-spi-algorithm}
# This code chunk checks the SPI algorithm because I am
# having some difficulties with SPI for 2 months, getting -Inf vals.

# 1.0 This is from the vignette
# 1.1 add Wichita dataset, clean names changes to time series, 
data(wichita) 
wichita <- wichita %>% 
  clean_names()

wichita <- ts(wichita[,-c(1,2)], end = c(2011, 10), frequency = 12)
plot(wichita)

# 1.2 - Calculate one and twelve-month SPI with Wichita dataset
spi_1 <- spi(wichita[,'prcp'], 1)
spi_12 <- spi(wichita[,'prcp'], 12) 

# 1.3 - Extract information from spei object: summary, 
#         call function, fitted values, and coefficients

# spi_1_sum <- summary(spi_1) # this does not save!
spi_1_names <- as.tibble(names(spi_1))
spi_1_call <- as.list(spi_1$call)
spi_1_fitted <- as.tibble(spi_1$fitted)
spi_1_coeff <- as.tibble(spi_1$coefficients)

# 1.3 - Plot
plot(spi_1, 'Wichita, SPI-1')
plot(spi_12, 'Wichita, SPI-12')

# 2.0 - same approach using PRR data
# 2.1 add Cottonwoods dataset, change to time series, 
sta_cot <- sta_mon %>% 
  select(year, month, cot) 

sta_cot <- ts(sta_cot[,-c(1,2)], end = c(2018, 05), frequency = 12)
plot(sta_cot)

# 2.2 - Calculate one and twelve-month SPI with Wichita dataset
spi_1cot <- spi(sta_cot[,'cot'], 1)
spi_12cot <- spi(sta_cot[,'cot'], 12) 

# 1.3 - Extract information from spei object: summary, 
#         call function, fitted values, and coefficients

# spi_1_sum <- summary(spi_1) # this does not save!
spi_1cot_names <- as.tibble(names(spi_1cot))
spi_1cot_call <- as.list(spi_1cot$call)
spi_1cot_fitted <- as.tibble(spi_1cot$fitted)
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients)

spi_12cot_names <- as.tibble(names(spi_12cot))
spi_12cot_call <- as.list(spi_12cot$call)
spi_12cot_fitted <- as.tibble(spi_12cot$fitted)
spi_12cot_coeff <- as.tibble(spi_12cot$coefficients)



# 1.3 - Plot
plot(spi_1cot, 'test, SPI-1')
plot(spi_12cot, 'test, SPI-12')



# Compute potential evapotranspiration (PET) and climatic water balance (BAL)
#wichita$PET <- thornthwaite(wichita$TMED, 37.6475)
#wichita$BAL <- wichita$PRCP-wichita$PET

# Convert to a ts (time series) object for convenience
#wichita <- ts(wichita[,-c(1,2)], end = c(2011, 10), frequency = 12)
#plot(wichita)

# One and twelve-month SPEI
#spei1 <- spei(wichita[,'BAL'], 1)
#spei12 <- spei(wichita[,'BAL'], 12)
#class(spei1)


#spi(data, scale, kernel = list(type = 'rectangular', shift = 0),
#	distribution = 'PersonIII', fit = 'ub-pwm', na.rm = FALSE,
#	ref.start=NULL, ref.end=NULL, x=FALSE, params=NULL, ...)





# Time series not starting in January
par(mfrow=c(1,1))
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12))

# Using a particular reference period (1980-2000) for computing the parameters
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12,
	ref.start=c(1980,1), ref.end=c(2000,1)))

# Using different kernels
spei24 <- spei(wichita[,'BAL'],24)
spei24_gau <- spei(wichita[,'BAL'], 24, kernel=list(type='gaussian', shift=0))
par(mfrow=c(2,1))
plot(spei24, main='SPEI-24 with rectangular kernel')
plot(spei24_gau, main='SPEI-24 with gaussian kernel')

# Computing several time series at a time
# Dataset balance contains time series of the climatic water balance at 12 locations
data(balance)
head(balance)
bal_spei12 <- spei(balance, 12)
plot(bal_spei12)

# Using custom (user provided) parameters
coe <- spei1$coefficients
dim(coe)
spei(wichita[,'BAL'], 1, params=coe)
```

```{r SPI-1month}


# Arrange data and calculate tidy-SPI 
sta_mon <- sta_mon %>% 
  arrange(date)

spi_1mon_list <- sta_mon %>% 
  select(-(date:month)) %>%
  spi(distribution = 'PearsonIII', scale = 1, na.rm = TRUE) 

# Look at the list 
# jsonedit(spi_1mon, mode = "view", elementId = "call")

# extract the fitted data
spi_1mon <- as.data.frame(spi_1mon_list["fitted"]) # this works.  
# But, not sure how to do do the same in a Tidyverse way.  Moving fwd.

# clean up the dataframe
# grab cols from sta_mon
dates <- sta_mon %>% 
  select(1:3)

# note - I would really prefer to use a map function of some sort
spi_1mon <- spi_1mon %>%
  rename(oel = fitted.oel) %>% 
  rename(cot = fitted.cot) %>% 
  rename(rap = fitted.rap) %>%
  rename(int = fitted.int) %>% 
  rename(ora = fitted.ora)

spi_1mon <- bind_cols(dates, spi_1mon)
rm(spi_1mon_list)

spi_1mon_grp <- spi_1mon %>%
  gather(key = "station", value = "value", -date, -year, -month) %>%
  drop_na(value)

ggplot(spi_1mon_grp, aes(date, value)) +
  geom_line() +
  facet_grid(station~.) +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "One-month duration; Pine Ridge Reservation, SD") +
       xlab("") +
       ylab("SPI-value")





# Compute potential evapotranspiration (PET) and climatic water balance (BAL)
wichita$PET <- thornthwaite(wichita$TMED, 37.6475)
wichita$BAL <- wichita$PRCP-wichita$PET

# Convert to a ts (time series) object for convenience
wichita <- ts(wichita[,-c(1,2)], end = c(2011, 10), frequency = 12)
plot(wichita)

# One and twelve-month SPEI
spei1 <- spei(wichita[,'BAL'], 1)
spei12 <- spei(wichita[,'BAL'], 12)
class(spei1)


spi(data, scale, kernel = list(type = 'rectangular', shift = 0),
	distribution = 'PersonIII', fit = 'ub-pwm', na.rm = FALSE,
	ref.start=NULL, ref.end=NULL, x=FALSE, params=NULL, ...)


# Extract information from spei object: summary, call function, fitted values, and coefficients
summary(spei1)
names(spei1)
spei1$call
spei1$fitted
spei1$coefficients

# Plot spei object
par(mfrow = c(2,1))
plot(spei1, main = 'Wichita, SPEI-1')
plot(spei12, main = 'Wichita, SPEI-12')

# One and tvelwe-months SPI
spi_1 <- spi(wichita[,'PRCP'], 1)
spi_12 <- spi(wichita[,'PRCP'], 12)

par(mfrow=c(2,1))
plot(spi_1, 'Wichita, SPI-1')
plot(spi_12, 'Wichita, SPI-12')

# Time series not starting in January
par(mfrow=c(1,1))
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12))

# Using a particular reference period (1980-2000) for computing the parameters
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12,
	ref.start=c(1980,1), ref.end=c(2000,1)))

# Using different kernels
spei24 <- spei(wichita[,'BAL'],24)
spei24_gau <- spei(wichita[,'BAL'], 24, kernel=list(type='gaussian', shift=0))
par(mfrow=c(2,1))
plot(spei24, main='SPEI-24 with rectangular kernel')
plot(spei24_gau, main='SPEI-24 with gaussian kernel')

# Computing several time series at a time
# Dataset balance contains time series of the climatic water balance at 12 locations
data(balance)
head(balance)
bal_spei12 <- spei(balance, 12)
plot(bal_spei12)

# Using custom (user provided) parameters
coe <- spei1$coefficients
dim(coe)
spei(wichita[,'BAL'], 1, params=coe)
```

```{r SPI-2month}
# clean up 
rm(spi_1mon, spi_1mon_grp) 

# the approach above did not work for 2 month data.  Getting -Inf vals

# try on individual parts of the dataset
#sta <- as.tibble(names(sta_mon)) %>%
#  slice(-(1:3))

sta_cot <- sta_mon %>% 
  select(date, cot) 

sta_int <- sta_mon %>% 
  select(date, int) 

sta_oel <- sta_mon %>% 
  select(date, oel) 

sta_ora <- sta_mon %>% 
  select(date, ora) 

sta_rap <- sta_mon %>% 
  select(date, rap) 

# calculate the SPI vals for each
spi_2cot_list <- sta_cot %>% 
  select(-date) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 

spi_2int_list <- sta_int %>% 
  select(-date) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 

spi_2oel_list <- sta_oel %>% 
  select(-date) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 

spi_2ora_list <- sta_ora %>% 
  select(-date) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 

spi_2rap_list <- sta_rap %>% 
  select(-date) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 


# extract the fitted data
spi_2cot <- as.data.frame(spi_2cot_list["fitted"]) # this works.
spi_2cot <- spi_2cot %>%
  rename(spi_cot = cot) 
spi_2cot <- bind_cols(sta_mon, spi_2cot)

spi_2int <- as.data.frame(spi_2int_list["fitted"]) # this works.
spi_2int <- spi_2int %>%
  rename(spi_int = int) 
spi_2int <- bind_cols(sta_mon, spi_2int) 

spi_2oel <- as.data.frame(spi_2oel_list["fitted"]) # this works.
spi_2oel <- spi_2oel %>%
  rename(spi_oel = oel) 
spi_2oel <- bind_cols(sta_mon, spi_2oel)

spi_2ora <- as.data.frame(spi_2ora_list["fitted"]) # this works.
spi_2ora <- spi_2ora %>%
  rename(spi_ora = ora) 
spi_2ora <- bind_cols(sta_mon, spi_2ora)

spi_2rap <- as.data.frame(spi_2rap_list["fitted"]) # this works.
spi_2rap <- spi_2rap %>%
  rename(spi_rap = rap) 
spi_2rap <- bind_cols(sta_mon, spi_2rap)

rm(spi_2cot_list, spi_2int_list,spi_2oel_list,
   spi_2ora_list, spi_2rap_list)

summary(spi_2int$spi_int)

ggplot(spi_2int, aes(spi_int, int)) +
  geom_point()





# this is new code...
# Arrange data and calculate SPI
#sta_mon_nest <- sta_grp %>% 
#  mutate(station = as_factor(station)) %>%
#  group_by(station) %>% 
#  nest()

#(sta_mon_nest <- sta_mon_nest %>% 
#   mutate(fit = map(data, le_vs_yr)))

# this was recopied above
spi_2mon_list <- sta_mon %>% 
  select(-(date:month)) %>%
  spi(distribution = 'PearsonIII', scale = 2, na.rm = TRUE) 

spi_2mon <- as.data.frame(spi_2mon_list["fitted"]) # this works.  
test <- bind_cols(sta_mon, spi_2mon)


# Look at the list 
# jsonedit(spi_2mon, mode = "view", elementId = "call")

# extract the fitted data
spi_2mon <- as.data.frame(spi_2mon_list["fitted"]) # this works.  
# But, not sure how to do do the same in a Tidyverse way.  Moving fwd.

# clean up the dataframe
# grab cols from sta_mon
dates <- sta_mon %>%
  select(1:3)

# note - I would really prefer to use a map function of some sort
spi_2mon <- spi_2mon %>%
  rename(oel = fitted.oel) %>% 
  rename(cot = fitted.cot) %>% 
  rename(rap = fitted.rap) %>%
  rename(int = fitted.int) %>% 
  rename(ora = fitted.ora)

spi_2mon <- bind_cols(dates, spi_2mon)
rm(spi_2mon_list, dates)

spi_2mon_grp <- spi_2mon %>%
  gather(key = "station", value = "value", -date, -year, -month) %>%
  drop_na(value)

ggplot(spi_2mon_grp, aes(date, value)) + 
  geom_line() +
  facet_grid(station~.) +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "Two-month duration; Pine Ridge Reservation, SD") +
       xlab("") +
       ylab("SPI-value")
```

```{r ggplot_2month_SPI}
ggplot(spi_2mon_grp, aes(date, value)) +
  geom_line() +
  facet_grid(station~.) +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "One-month duration; Pine Ridge Reservation, SD") +
       xlab("") +
       ylab("SPI-value")
```


```{r boxplots_1month}
ggplot(spi_1mon_grp, aes(month, value, group = month)) +
  geom_boxplot() +
  facet_wrap(~station) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")
```





```{r SPI-example}
# Load data 
data(wichita) 

# Compute potential evapotranspiration (PET) and climatic water balance (BAL)
wichita$PET <- thornthwaite(wichita$TMED, 37.6475)
wichita$BAL <- wichita$PRCP-wichita$PET

# Convert to a ts (time series) object for convenience
wichita <- ts(wichita[,-c(1,2)], end = c(2011, 10), frequency = 12)
plot(wichita)

# One and twelve-month SPEI
spei1 <- spei(wichita[,'BAL'], 1)
spei12 <- spei(wichita[,'BAL'], 12)
class(spei1)

# Extract information from spei object: summary, call function, fitted values, and coefficients
summary(spei1)
names(spei1)
spei1$call
spei1$fitted
spei1$coefficients

# Plot spei object
par(mfrow = c(2,1))
plot(spei1, main = 'Wichita, SPEI-1')
plot(spei12, main = 'Wichita, SPEI-12')

# One and tvelwe-months SPI
spi_1 <- spi(wichita[,'PRCP'], 1)
spi_12 <- spi(wichita[,'PRCP'], 12)

par(mfrow=c(2,1))
plot(spi_1, 'Wichita, SPI-1')
plot(spi_12, 'Wichita, SPI-12')

# Time series not starting in January
par(mfrow=c(1,1))
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12))

# Using a particular reference period (1980-2000) for computing the parameters
plot(spei(ts(wichita[,'BAL'], freq=12, start=c(1980,6)), 12,
	ref.start=c(1980,1), ref.end=c(2000,1)))

# Using different kernels
spei24 <- spei(wichita[,'BAL'],24)
spei24_gau <- spei(wichita[,'BAL'], 24, kernel=list(type='gaussian', shift=0))
par(mfrow=c(2,1))
plot(spei24, main='SPEI-24 with rectangular kernel')
plot(spei24_gau, main='SPEI-24 with gaussian kernel')

# Computing several time series at a time
# Dataset balance contains time series of the climatic water balance at 12 locations
data(balance)
head(balance)
bal_spei12 <- spei(balance, 12)
plot(bal_spei12)

# Using custom (user provided) parameters
coe <- spei1$coefficients
dim(coe)
spei(wichita[,'BAL'], 1, params=coe)
```




## Introduction


## Methods
I imported Global Historical Climatology Network (GHCN) daily precipitation records for candidate "WEATHER STATIONS" into R-Studio (REF1) using the "rnoaa" package.
I used Theissen polygons and the length and continuity of precipitation records to select stations for further analysis.
I used 'dplyr' to fill NA values with data from nearest station
I used 'dplyr' to create monthly vals from daily vals.
I removed short records: Oral & Long Valley after checking for 
covariance.

<!-- 
Work on finishing describing methods for EDA.
-->

## Results
<!--
Work on describing results of EDA.
