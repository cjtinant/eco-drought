<!--
2.1    Standardized Precipitation Index
2.1.1  Download & munge precipitation data: complete; _04_prco-data_munging
2.1.2  Identify distribution: complete;  _05_L-moment_diagram
2.1.3   Calculate SPI: started but having difficulties!; created a test case to send to author
2.2    Stream Drought Index
2.2.1  Download & mungeStream data: finished - July 16
2.2.2 Learn purrr::map(): ok.finished http://r4ds.had.co.nz/iteration.html#the-map-functions
2.2.3   Cluster time series - in progress: following Kassambara, "Practical Guide to Cluster Analysis in R
2.2.4   Calculate SDI
2.2.5   Delineate watersheds
2.2.6   Cluster ungaged stations
2.2.7 Learn 'rf' function
2.3   Disseminate results
2.3.1 Identify journal

Title (13-words or less): Relating Precipitation and Hydrologic Drought in a Semi-arid Climate with gSSURO Data  
Subtitle: Pine Ridge Reservation and surrounding Areas in Southwestern South Dakota 

## Broad questions:
How does the precipitation and hydrologic drought history differ?
Can these differences be explained using gSSURGO data

## Narrower questions:
What is the underlying distribution of precipitation data?  
What is the underlying distribution of hydrologic data?  
How does streamflow co-vary across the landscape?
Can these covariences be explained using gSSURGO data?

# Specific Aims
1. Refine an approach for relating precipitation drought to hydrological drought in gaged watersheds
2. Develop a method to estimate regional hydrologic group membership for ungaged watersheds using gSSURGO data

## Introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

What are the gaps this paper hopes to address?
Defining precipitation records for the analysis
Working with short records for streamflow
Relating these records to a physical process

As an example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

## Methods
1. Identify precipitation records for drought analysis
     I imported Global Historical Climatology Network (GHCN) daily precipitation    records for candidate "WEATHER STATIONS" into R-Studio (REF1) using the "rnoaa" package.
    I used Theissen polygons and the length and continuity of precipitation records to select stations for further analysis.
    I used 'dplyr' to fill NA values with data from nearest station
    I used 'dplyr' to create monthly vals from daily vals.
   I removed short records: Oral & Long Valley after checking for 
covariance.

2. Identify streamflow records for hydrology analysis
(list) 

3. Exploratory EDA of streamflow to identify membership of short records


## Analysis Steps & progress
1. Recreated analysis from the lmomco text ch 12 (author?) in Tidyverse
2. Imported cleaned precipitation data (see 04_prcp-data_munging)   
3. Applied sqrt & log10 transform to explore effects on skew 
4. Explored the data with box plots, violin plot.
5. Applied Weibull plotting position and graphed the data on sqrt plot
6. Calculated L-moments and L-moment ratios 
7. Calculated SPI for 'cot', 'oel', 'rap', 'int', and 'ora' datasets using Pearson III.
8. Completed exploratory PCA 
9. Clustered streamflows during wet period

# Next steps: 
1. look at a second package for SPI - on Twitter bookmarks.
2. find drought years & wet years for precip
3. Examine clusters for dry years
4. Supervised classification - random forest should cross-validate?

# Someday - Maybe
1. Map the variable as a function - might put off, but ugly and long code below!
2. figure out how to reference stuff with - grateful package

## Variable naming convention:   
sta          precipitation station  
_meta        metadata  
_mon         monthly precipitation depths  
_grp         wide data changed to long data #might change to _gath
_notzero     non-zero precip values
_zero        precip values equal to zero
_log         log10 of monthly precipitation depths
_count       number of months in a given record

min          minimum non-zero value
n            number of months in a given record

prep#         intermediate variable used to bind rows; # = 1, 2, ...
intc#        intermediate variable used to bind cols; # = 1, 2, ...
intr#        intermediate variable used to bind rows; # = 1, 2, ...

# SPI output variables
spi          Standardized Precipitation Index vals for a station 
 _gath       Dataframe is long rather than wide
 _cot        Cottonwood station
 _oel        Oelrichs station 
 _rap        Rapid City station 
 _int        Interior station 
 -ora        Oral station

# Gage variables 
gage
_raw         original inputted data 
_prep        intermediate variable 
_scale       standardized (z-score) data, number is averaging period 
_sum         summary data 
_l           data in long format

## Thoughts - the orig depth vs plotting vals look j-shaped.  
## Sqrt trans vs plotting vals look slightly sinusoidal; nice boxplots
## Log-tranformation looks like the mirror of orig depth vs plotting

## Results: Precipitation fits a PE3 distribution






<!-- 
Work on finishing describing methods for EDA.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ten simple rules for structuring papers

Citation: Mensh B, Kording K (2017) Ten simple rules for structuring papers. PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619

Principles (Rules 1–4)
Rule 1: Focus your paper on a central contribution, which you communicate in the title

Your communication efforts are successful if readers can still describe the main contribution of your paper to their colleagues a year after reading it. Although it is clear that a paper often needs to communicate a number of innovations on the way to its final message, it does not pay to be greedy. Focus on a single message; papers that simultaneously focus on multiple contributions tend to be less convincing about each and are therefore less memorable.

The most important element of a paper is the title—think of the ratio of the number of titles you read to the number of papers you read. The title is typically the first element a reader encounters, so its quality [3] determines whether the reader will invest time in reading the abstract.

The title not only transmits the paper’s central contribution but can also serve as a constant reminder (to you) to focus the text on transmitting that idea. Science is, after all, the abstraction of simple principles from complex data. The title is the ultimate refinement of the paper’s contribution. Thinking about the title early—and regularly returning to hone it—can help not only the writing of the paper but also the process of designing experiments or developing theories.

This Rule of One is the most difficult rule to optimally implement because it comes face-to-face with the key challenge of science, which is to make the claim and/or model as simple as the data and logic can support but no simpler. In the end, your struggle to find this balance may appropriately result in “one contribution” that is multifaceted. For example, a technology paper may describe both its new technology and a biological result using it; the bridge that unifies these two facets is a clear description of how the new technology can be used to do new biology.

Rule 2: Write for flesh-and-blood human beings who do not know your work

Because you are the world’s leading expert at exactly what you are doing, you are also the world’s least qualified person to judge your writing from the perspective of the naïve reader. The majority of writing mistakes stem from this predicament. Think like a designer—for each element, determine the impact that you want to have on people and then strive to achieve that objective [4]. Try to think through the paper like a naïve reader who must first be made to care about the problem you are addressing (see Rule 6) and then will want to understand your answer with minimal effort.

Define technical terms clearly because readers can become frustrated when they encounter a word that they don’t understand. Avoid abbreviations and acronyms so that readers do not have to go back to earlier sections to identify them.

The vast knowledge base of human psychology is useful in paper writing. For example, people have working memory constraints in that they can only remember a small number of items and are better at remembering the beginning and the end of a list than the middle [5]. Do your best to minimize the number of loose threads that the reader has to keep in mind at any one time.

Rule 3: Stick to the context-content-conclusion (C-C-C) scheme

The vast majority of popular (i.e., memorable and re-tellable) stories have a structure with a discernible beginning, a well-defined body, and an end. The beginning sets up the context for the story, while the body (content) advances the story towards an ending in which the problems find their conclusions. This structure reduces the chance that the reader will wonder “Why was I told that?” (if the context is missing) or “So what?” (if the conclusion is missing).

There are many ways of telling a story. Mostly, they differ in how well they serve a patient reader versus an impatient one [6]. The impatient reader needs to be engaged quickly; this can be accomplished by presenting the most exciting content first (e.g., as seen in news articles). The C-C-C scheme that we advocate serves a more patient reader who is willing to spend the time to get oriented with the context. A consequent disadvantage of C-C-C is that it may not optimally engage the impatient reader. This disadvantage is mitigated by the fact that the structure of scientific articles, specifically the primacy of the title and abstract, already forces the content to be revealed quickly. Thus, a reader who proceeds to the introduction is likely engaged enough to have the patience to absorb the context. Furthermore, one hazard of excessive “content first” story structures in science is that you may generate skepticism in the reader because they may be missing an important piece of context that makes your claim more credible. For these reasons, we advocate C-C-C as a “default” scientific story structure.

The C-C-C scheme defines the structure of the paper on multiple scales. At the whole-paper scale, the introduction sets the context, the results are the content, and the discussion brings home the conclusion. Applying C-C-C at the paragraph scale, the first sentence defines the topic or context, the body hosts the novel content put forth for the reader’s consideration, and the last sentence provides the conclusion to be remembered.

Deviating from the C-C-C structure often leads to papers that are hard to read, but writers often do so because of their own autobiographical context. During our everyday lives as scientists, we spend a majority of our time producing content and a minority amidst a flurry of other activities. We run experiments, develop the exposition of available literature, and combine thoughts using the magic of human cognition. It is natural to want to record these efforts on paper and structure a paper chronologically. But for our readers, most details of our activities are extraneous. They do not care about the chronological path by which you reached a result; they just care about the ultimate claim and the logic supporting it (see Rule 7). Thus, all our work must be reformatted to provide a context that makes our material meaningful and a conclusion that helps the reader to understand and remember it.

Rule 4: Optimize your logical flow by avoiding zig-zag and using parallelism
Avoiding zig-zag.

Only the central idea of the paper should be touched upon multiple times. Otherwise, each subject should be covered in only one place in order to minimize the number of subject changes. Related sentences or paragraphs should be strung together rather than interrupted by unrelated material. Ideas that are similar, such as two reasons why we should believe something, should come one immediately after the other.
Using parallelism.

Similarly, across consecutive paragraphs or sentences, parallel messages should be communicated with parallel form. Parallelism makes it easier to read the text because the reader is familiar with the structure. For example, if we have three independent reasons why we prefer one interpretation of a result over another, it is helpful to communicate them with the same syntax so that this syntax becomes transparent to the reader, which allows them to focus on the content. There is nothing wrong with using the same word multiple times in a sentence or paragraph. Resist the temptation to use a different word to refer to the same concept—doing so makes readers wonder if the second word has a slightly different meaning.
The components of a paper (Rules 5–8)

The individual parts of a paper—abstract, introduction, results, and discussion—have different objectives, and thus they each apply the C-C-C structure a little differently in order to achieve their objectives. We will discuss these specialized structures in this section and summarize them in Fig 1.
thumbnail

Fig 1. Summary of a paper’s structural elements at three spatial scales: Across sections, across paragraphs, and within paragraphs.

Note that the abstract is special in that it contains all three elements (Context, Content, and Conclusion), thus comprising all three colors.

https://doi.org/10.1371/journal.pcbi.1005619.g001
Rule 5: Tell a complete story in the abstract

The abstract is, for most readers, the only part of the paper that will be read. This means that the abstract must convey the entire message of the paper effectively. To serve this purpose, the abstract’s structure is highly conserved. Each of the C-C-C elements is detailed below.

The context must communicate to the reader what gap the paper will fill. The first sentence orients the reader by introducing the broader field in which the particular research is situated. Then, this context is narrowed until it lands on the open question that the research answered. A successful context section sets the stage for distinguishing the paper’s contributions from the current state of the art by communicating what is missing in the literature (i.e., the specific gap) and why that matters (i.e., the connection between the specific gap and the broader context that the paper opened with).

The content (“Here we”) first describes the novel method or approach that you used to fill the gap or question. Then you present the meat—your executive summary of the results.

Finally, the conclusion interprets the results to answer the question that was posed at the end of the context section. There is often a second part to the conclusion section that highlights how this conclusion moves the broader field forward (i.e., “broader significance”). This is particularly true for more “general” journals with a broad readership.

This structure helps you avoid the most common mistake with the abstract, which is to talk about results before the reader is ready to understand them. Good abstracts usually take many iterations of refinement to make sure the results fill the gap like a key fits its lock. The broad-narrow-broad structure allows you to communicate with a wider readership (through breadth) while maintaining the credibility of your claim (which is always based on a finite or narrow set of results).

Rule 6: Communicate why the paper matters in the introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

As an example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

Rule 7-Results: Deliver the results as a sequence of statements, supported by figures, that connect logically to support the central contribution

The results section needs to convince the reader that the central claim is supported by data and logic. Every scientific argument has its own particular logical structure, which dictates the sequence in which its elements should be presented.

For example, a paper may set up a hypothesis, verify that a method for measurement is valid in the system under study, and then use the measurement to disprove the hypothesis. Alternatively, a paper may set up multiple alternative (and mutually exclusive) hypotheses and then disprove all but one to provide evidence for the remaining interpretation. The fabric of the argument will contain controls and methods where they are needed for the overall logic.

In the outlining phase of paper preparation (see Rule 9), sketch out the logical structure of how your results support your claim and convert this into a sequence of declarative statements that become the headers of subsections within the results section (and/or the titles of figures). Most journals allow this type of formatting, but if your chosen journal does not, these headers are still useful during the writing phase and can either be adapted to serve as introductory sentences to your paragraphs or deleted before submission. Such a clear progression of logical steps makes the paper easy to follow.

Figures, their titles, and legends are particularly important because they show the most objective support (data) of the steps that culminate in the paper’s claim. Moreover, figures are often viewed by readers who skip directly from the abstract in order to save time. Thus, the title of the figure should communicate the conclusion of the analysis, and the legend should explain how it was done. Figure making is an art unto itself; the Edward Tufte books remain the gold standard for learning this craft [7,8].

The first results paragraph is special in that it typically summarizes the overall approach to the problem outlined in the introduction, along with any key innovative methods that were developed. Most readers do not read the methods, so this paragraph gives them the gist of the methods that were used.

Each subsequent paragraph in the results section starts with a sentence or two that set up the question that the paragraph answers, such as the following: “To verify that there are no artifacts…,” “What is the test-retest reliability of our measure?,” or “We next tested whether Ca2+ flux through L-type Ca2+ channels was involved.” The middle of the paragraph presents data and logic that pertain to the question, and the paragraph ends with a sentence that answers the question. For example, it may conclude that none of the potential artifacts were detected. This structure makes it easy for experienced readers to fact-check a paper. Each paragraph convinces the reader of the answer given in its last sentence. This makes it easy to find the paragraph in which a suspicious conclusion is drawn and to check the logic of that paragraph. The result of each paragraph is a logical statement, and paragraphs farther down in the text rely on the logical conclusions of previous paragraphs, much as theorems are built in mathematical literature.
Rule 8: Discuss how the gap was filled, the limitations of the interpretation, and the relevance to the field

The discussion section explains how the results have filled the gap that was identified in the introduction, provides caveats to the interpretation, and describes how the paper advances the field by providing new opportunities. This is typically done by recapitulating the results, discussing the limitations, and then revealing how the central contribution may catalyze future progress. The first discussion paragraph is special in that it generally summarizes the important findings from the results section. Some readers skip over substantial parts of the results, so this paragraph at least gives them the gist of that section.

Each of the following paragraphs in the discussion section starts by describing an area of weakness or strength of the paper. It then evaluates the strength or weakness by linking it to the relevant literature. Discussion paragraphs often conclude by describing a clever, informal way of perceiving the contribution or by discussing future directions that can extend the contribution.

For example, the first paragraph may summarize the results, focusing on their meaning. The second through fourth paragraphs may deal with potential weaknesses and with how the literature alleviates concerns or how future experiments can deal with these weaknesses. The fifth paragraph may then culminate in a description of how the paper moves the field forward. Step by step, the reader thus learns to put the paper’s conclusions into the right context.
Process (Rules 9 and 10)

To produce a good paper, authors can use helpful processes and habits. Some aspects of a paper affect its impact more than others, which suggests that your investment of time should be weighted towards the issues that matter most. Moreover, iteratively using feedback from colleagues allows authors to improve the story at all levels to produce a powerful manuscript. Choosing the right process makes writing papers easier and more effective.
Rule 9: Allocate time where it matters: Title, abstract, figures, and outlining

The central logic that underlies a scientific claim is paramount. It is also the bridge that connects the experimental phase of a research effort with the paper-writing phase. Thus, it is useful to formalize the logic of ongoing experimental efforts (e.g., during lab meetings) into an evolving document of some sort that will ultimately steer the outline of the paper.

You should also allocate your time according to the importance of each section. The title, abstract, and figures are viewed by far more people than the rest of the paper, and the methods section is read least of all. Budget accordingly.

The time that we do spend on each section can be used efficiently by planning text before producing it. Make an outline. We like to write one informal sentence for each planned paragraph. It is often useful to start the process around descriptions of each result—these may become the section headers in the results section. Because the story has an overall arc, each paragraph should have a defined role in advancing this story. This role is best scrutinized at the outline stage in order to reduce wasting time on wordsmithing paragraphs that don’t end up fitting within the overall story.
Rule 10: Get feedback to reduce, reuse, and recycle the story

Writing can be considered an optimization problem in which you simultaneously improve the story, the outline, and all the component sentences. In this context, it is important not to get too attached to one’s writing. In many cases, trashing entire paragraphs and rewriting is a faster way to produce good text than incremental editing.

There are multiple signs that further work is necessary on a manuscript (see Table 1). For example, if you, as the writer, cannot describe the entire outline of a paper to a colleague in a few minutes, then clearly a reader will not be able to. You need to further distill your story. Finding such violations of good writing helps to improve the paper at all levels.
thumbnail

Table 1. A summary of the ten rules and how to tell if they are being violated.

https://doi.org/10.1371/journal.pcbi.1005619.t001

Successfully writing a paper typically requires input from multiple people. Test readers are necessary to make sure that the overall story works. They can also give valuable input on where the story appears to move too quickly or too slowly. They can clarify when it is best to go back to the drawing board and retell the entire story. Reviewers are also extremely useful. Non-specific feedback and unenthusiastic reviews often imply that the reviewers did not “get” the big picture story line. Very specific feedback usually points out places where the logic within a paragraph was not sufficient. It is vital to accept this feedback in a positive way. Because input from others is essential, a network of helpful colleagues is fundamental to making a story memorable. To keep this network working, make sure to pay back your colleagues by reading their manuscripts.
Discussion

This paper focused on the structure, or “anatomy,” of manuscripts. We had to gloss over many finer points of writing, including word choice and grammar, the creative process, and collaboration. A paper about writing can never be complete; as such, there is a large body of literature dealing with issues of scientific writing [9,10,11,12,13,14,15,16,17].

Personal style often leads writers to deviate from a rigid, conserved structure, and it can be a delight to read a paper that creatively bends the rules. However, as with many other things in life, a thorough mastery of the standard rules is necessary to successfully bend them [18]. In following these guidelines, scientists will be able to address a broad audience, bridge disciplines, and more effectively enable integrative science.
Acknowledgments

We took our own advice and sought feedback from a large number of colleagues throughout the process of preparing this paper. We would like to especially thank the following people who gave particularly detailed and useful feedback:

Sandra Aamodt, Misha Ahrens, Vanessa Bender, Erik Bloss, Davi Bock, Shelly Buffington, Xing Chen, Frances Cho, Gabrielle Edgerton, multiple generations of the COSMO summer school, Jason Perry, Jermyn See, Nelson Spruston, David Stern, Alice Ting, Joshua Vogelstein, Ronald Weber.
References

    1. Hirsch JE (2005) An index to quantify an individual's scientific research output. Proc Natl Acad Sci U S A. 102: 16569–16572. pmid:16275915
    2. Acuna DE, Allesina S, Kording KP (2012) Future impact: Predicting scientific success. Nature. 489: 201–202. pmid:22972278
    3. Paiva CE, Lima JPSN, Paiva BSR (2012) Articles with short titles describing the results are cited more often. Clinics. 67: 509–513. pmid:22666797
    4. Carter M (2012) Designing Science Presentations: A Visual Guide to Figures, Papers, Slides, Posters, and More: Academic Press.
    5. Murdock BB Jr (1968) Serial order effects in short-term memory. J Exp Psychol. 76: Suppl:1–15.
    6. Schimel J (2012) Writing science: how to write papers that get cited and proposals that get funded. USA: OUP.
    7. Tufte ER (1990) Envisioning information. Graphics Press.
    8. Tufte ER The Visual Display of Quantitative Information. Graphics Press.
    9. Lisberger SG (2011) From Science to Citation: How to Publish a Successful Scientific Paper. Stephen Lisberger.
    10. Simons D (2012) Dan's writing and revising guide. http://www.dansimons.com/resources/Simons_on_writing.pdf [cited 2017 Sep 9].
    11. Sørensen C (1994) This is Not an Article—Just Some Thoughts on How to Write One. Syöte, Finland: Oulu University, 46–59.
    12. Day R (1988) How to write and publish a scientific paper. Phoenix: Oryx.
    13. Lester JD, Lester J (1967) Writing research papers. Scott, Foresman.
    14. Dumont J-L (2009) Trees, Maps, and Theorems. Principiae. http://www.treesmapsandtheorems.com/ [cited 2017 Sep 9].
    15. Pinker S (2014) The Sense of Style: The Thinking Person’s Guide to Writing in the 21st Century. Viking Adult.
    16. Bern D (1987) Writing the empirical journal. The compleat academic: A practical guide for the beginning social scientist. 171.
    17. George GD, Swan JA (1990) The science of scientific writing. Am Sci. 78: 550–558.
    18. Strunk W (2007) The elements of style. Penguin. 

# Exploratory PCA--------------------------------------------------- 
# PCA is related to eigenvectors and eigenvalues.  The variance or 
# spread of the observations is measured as the average squared 
# distance from the center of the point cloud to each observation (c).  
# The total reconstruction error is measured as the average squared 
# length of the errors (b), and distance along the principal axis (a) 
# can also be measured.  Therefore the sum of the square of the errors 
# plus the sum of the square distance along the principal axis equals 
# the average squared distance between the center of the point cloud 
# each observation; this is precisely Pythagoras theorem. 

# You can imagine that the PC axis is a solid rod and each error 
# is a spring. The energy of the spring is proportional to its squared 
# length (this is known in physics as the Hooke's law), so the rod 
# will orient itself such as to minimize the sum of these squared 
# distances. 

# Regarding eigenvectors and eigenvalues. A 2×2 matrix given by: 
#   (1.07     0.63)
#   (0.63     0.64)

# The variance of the x variable is 1.07, 
# the variance of the y variable is 0.64, 
# and the covariance between them is 0.63. 

# As it is a square symmetric matrix, it can be diagonalized by 
# choosing a new orthogonal coordinate system, given by its 
# eigenvectors (incidentally, this is called spectral theorem); 
# corresponding eigenvalues will then be located on the diagonal. 
# In this new coordinate system, the covariance matrix is diagonal 
# and looks like this:
#   (1.52     0) 
#   (0     0.19)

# The correlation between points is now zero. It becomes clear that 
# the variance of any projection will be given by a weighted average 
# of the eigenvalues.  Consequently, the maximum possible variance 
# (1.52) will be achieved if we simply take the projection on the 
# first coordinate axis. It follows that the direction of the first 
# principal component is given by the first eigenvector of the 
# covariance matrix. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-->

## Results
<!--
Work on describing results of EDA.
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Sets up the library of packages 
#library("magrittr") # contains easier ways to say things about lists
library("here") # identifies where to save work 
library("rio") # more robust I/O - to import and clean data
library("lubridate") # fixes dates 
library("tidyverse") 
library("janitor") # tools to clean dirty data 

library("forecast") # using the BoxCox function
library("standardize")
library("cluster")
library("factoextra") 
library("broom") # tidies up objects 

#library("DataExplorer")

# library("SPEI") # Calculates SPI-index # having some issues...
library("SCI")

library("clValid")
# library("grateful") - not yet ready for R 3.5.0

#library("lintr")
#library("test_that")
#library("jsonlite") # Convert between JSON data and R objects
#library("curl") # Drop-in replacement for base url
#library("listviewer") # htmlwidget for interactive views of R lists



#lmomco <- citation("lmomco")
#toBibtex(lmomco)

# Session Info
a_session <- devtools::session_info()
```

```{r import-precip-data} 
# General Purpose: prepare data for drought index    
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) 

# make the wide data long, remove NA vals 
sta_grp <- sta_mon %>% 
  gather(key = "station", value = "depth", -date, -year, -month) %>% 
  drop_na(depth)  # %>%  
#  mutate(sqrt_depth = sqrt(depth)) 
```

```{r check-precip-data}
# not sure why these zeros are thought of as NA values
sta_na <- sta_grp %>% 
    filter(depth == is.na(depth)) 

sta_grp$depth <- ifelse(sta_grp$depth == is.na(sta_grp$depth), 0, sta_grp$depth)  

sta_na <- sta_grp %>% 
    filter(depth == is.na(depth)) 
```

```{r precip-boxplot, include=FALSE, eval=FALSE} 
sta_sum <- as.tibble(summary(sta_mon))  
  
# plot the precip data as a boxplot
ggplot(sta_grp, aes(as.factor(station), depth)) +
  geom_violin() +
  geom_boxplot() +
#  scale_y_sqrt() +
#  scale_y_log10() +
  scale_y_sqrt() +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") +
  xlab("") + 
  ylab("Monthly depth in mm") +
  NULL

ggplot2::ggsave(filename = "prcp_boxplot.png", 
                width = 6, height = 6, units = "in")
```

```{r eda_fiddling, include=FALSE, eval=FALSE}
sta_big <- sta_grp %>% 
  filter(depth > 100) 
summary(sta_big) 

ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```

```{r testing, include=FALSE, eval=FALSE}
# Mini-library
library("tidyverse") 
library("lubridate") 
library("rio") 
library("SPEI")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# load data
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  arrange(date)
  
# prepare date for joining later
Date <- sta_mon %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use a single station - cot
sta_cot_ts <- sta_mon %>% 
  select(cot) %>%
  ts(end = c(2018, 05), frequency = 12)

spi_list  <- spi(sta_cot_ts, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_cot <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_ts <- as.tibble(spi_list$fitted)  
spi1_cot_ts <- bind_cols(Date, spi1_cot_ts) 

# Results
 as.tibble(summary(spi1_cot_ts))
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data for testing Lubridate object
sta_cot_lub <- sta_mon %>% 
  select(cot) 

spi_list  <- spi(sta_cot_lub, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 
 
spi1_coeff_cot_lub <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_lub <- as.tibble(spi_list$fitted) 
spi1_cot_lub <- bind_cols(Date, spi1_cot_lub) 
rm(sta_cot_lub)

# Results
as.tibble(summary(spi1_cot_lub))
# A tibble: 12 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use lubridate-created object for date for a batch process
# results are as above for 'cot' but have -Inf for 'rap' 

# prepare data for batch
sta_all <- sta_mon %>% 
  select(-c(date, year, month))  
 
spi_list  <- spi(sta_all, 1,  
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_all <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_all <- as.tibble(spi_list$fitted) 
spi1_all <- bind_cols(Date, spi1_all) 

# Selected Results
summary(spi1_all$cot)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -2.40240 -0.69324  0.02341  0.01383  0.68317  3.30822 

summary(spi1_all$date)
# Min.      1st Qu.       Median     Mean      3rd Qu.       Max.
# "1909-06-01" "36-08-24" "63-11-16" "63-11-16" "91-02-08" "2018-05-01" 
summary(spi1_all$rap)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#   -Inf -0.6472  0.0022    -Inf  0.6657  2.9000     467 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use only 'rap' including NA vals 
# Summary: It looks like the "bug" is in handling the na.rm?
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data for 'rap' only
sta_rap <- sta_mon %>% 
  select(rap) 

spi_list  <- spi(sta_rap, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble()  

spi1_rap <- as.tibble(spi_list$fitted) 
spi1_rap <- bind_cols(Date, spi1_rap) 

# Results
as.tibble(summary(spi1_rap))
# A tibble: 14 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  " 
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     date" NA                    
# 8 ""    "     rap"  "Min.   :   -Inf  "   
# 9 ""    "     rap"  "1st Qu.:-0.6472  "   
#10 ""    "     rap"  "Median : 0.0022  "   
#11 ""    "     rap"  "Mean   :   -Inf  "   
#12 ""    "     rap"  "3rd Qu.: 0.6657  "   
#13 ""    "     rap"  "Max.   : 2.9000  "   
#14 ""    "     rap"  "NA's   :467  "  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use only 'rap' without NA vals 
# Summary: I'm confused about the bug!

# prepare data for 'rap' without NA only
sta_rap <- sta_mon %>% 
  select(rap, date) %>%
  filter(rap != is.na(rap))

# update date for joining later
Date_rap <- sta_rap %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)

# drop date
sta_rap <- sta_rap %>% select(-date)

spi_list  <- spi(sta_rap, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble()  

spi1_rap <- as.tibble(spi_list$fitted) 
spi1_rap <- bind_cols(Date_rap, spi1_rap) 

spi1_rap <- spi1_rap %>%
  rename(spi1 = rap) %>%
  arrange(spi1)

spi1_rap <- bind_cols(sta_rap, spi1_rap) 
spi_rap_test <- spi1_rap
```

```{r testing2}
# this is a new test to check on why differences between two ways of 
# calculating code

sta_rap3 <- sta_mon %>%
  arrange(date) %>%
  slice(468:1308) %>%
  select(date, rap)

spi_list  <- spi(sta_rap3[, 'rap'], 1, 
                 distribution = 'PearsonIII') 

spi1_rap3 <- as.tibble(spi_list$fitted) 

spi1_rap3 <- bind_cols(spi1_rap3, sta_rap3)

ggplot(spi1_rap3, aes(rap, rap1)) +
  geom_point()


# prepare data for 'rap' without NA only
sta_rap1 <- sta_mon %>% 
  select(rap, date) %>%
  filter(rap != is.na(rap)) %>%
  arrange(date) # This might be the issue!

# update date for joining later
Date_rap1 <- sta_rap1 %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)

# drop date
#sta_rap1 <- sta_rap1 %>% select(-date)

# calculate SPI
spi_list  <- spi(sta_rap1[, 'rap'], 1, 
                 distribution = 'PearsonIII') 

#spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
#  t() %>% as.tibble()  

spi1_rap1 <- as.tibble(spi_list$fitted) 
spi1_rap1 <- bind_cols(Date_rap1, spi1_rap1) 

spi1_rap1 <- spi1_rap1 %>%
  rename(spi1 = rap) %>%
  arrange(spi1)

spi1_rap1 <- bind_cols(sta_rap1, spi1_rap1) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Second approach 
sta_trans <- sta_mon %>% 
  select(date, year, month, rap) %>% 
  arrange(date) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Pull apart the NA and non-NA vals
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   1. Split the raw data into two parts at 1948-05-01
  sta_ge49      <- sta_trans %>% filter(year >= 1949) # yr above
  sta_48m05     <- sta_trans %>% filter(year == 1948 & month >= 5)
sta_rap2         <- bind_rows(sta_ge49, sta_48m05) # this is active
  rm(sta_ge49, sta_48m05)
# remove year and month
sta_rap2 <- sta_rap2 %>%
  select(date, rap)
#  2. Save NA observations  
  sta_lt48      <- sta_trans %>% filter(year < 1948)
  sta_48m01_m05 <- sta_trans %>% filter(year == 1948 & month < 5)
sta_NA          <- bind_rows(sta_lt48, sta_48m01_m05) # this is not
  rm(sta_lt48, sta_48m01_m05, sta_trans)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Calculate Rapid City SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi1_rap2  <- spi(sta_rap2[,'rap'],  1, distribution = 'PearsonIII') 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save & rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
#spi_1rap_coeff <- as.tibble(spi_1rap$coefficients) %>% 
 # t() %>% as.tibble() 

#spi_1rap_coeff <- rownames_to_column(spi_1rap_coeff, "month")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi1_rap2 <- as.tibble(spi1_rap2$fitted) %>% 
  mutate(duration = 1)



# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




# Why does sta_rap1 have 839 $ sta_rap2 have 841???




summary(sta_rap1$date)
#  Min.      1st Qu.       Median         Mean      3rd Qu. 
#"1948-05-01" "1965-11-16" "1983-05-01" "1983-05-03" "2000-10-16" 
#        Max. 
#"2018-05-01" 
summary(sta_rap2$date)
#        Min.      1st Qu.       Median         Mean      3rd Qu. 
#"1948-05-01" "1965-11-01" "1983-05-01" "1983-05-02" "2000-11-01" 
#        Max. 
#"2018-05-01" 
sta_rap1_a <- sta_rap1 %>% 
  arrange(date) %>%
  slice(131:150)
sta_rap2_a <- sta_rap2 %>%
  arrange(date) %>%
  slice(131:150)

test_a <- full_join(sta_rap1_a, sta_rap2_a, by = "date")
test_a <- test_a %>%
  mutate(dif = rap.y - rap.x)

```

```{r spi-with-SCI}
# Standardized Climate Index (SCI)

# Description
#fitSCI identifies parameters for the Standardized Climate Index (SCI) 
#transformation. transformSCI applies the transformation

# Usage

fitSCI(x, ...)
## Default S3 method:
fitSCI(x, first.mon, time.scale, distr, p0, 
       p0.center.mass=FALSE, scaling=c("no","max","sd"),mledist.par =  list(),
       start.fun = dist.start, start.fun.fix = FALSE, warn = TRUE, ...)

transformSCI(x, ...)
## Default S3 method:
transformSCI(x, first.mon, obj, sci.limit = Inf, warn=TRUE, ...)

#Arguments
# x	- numeric vector, representing a monthly univariate time series.
# first.mon	- value in 1:12 indicating month of the first element of x 

# time.scale	- The time scale (integer) of the SCI calculation. 
# The time scale is the window length of an backward looking running mean.

# distr	- A character string "name" naming a distribution for which 
# the corresponding density function (dname), the corresponding 
# distribution function (pname) and the quantile function (qname) must 
# be defined (see for example GammaDist)

# p0 - if TRUE, model Probability of zero (precipitation) months is 
# modeled with a mixed distribution as D(x) = p0 + (1-p0)G(x), 
# where G(x) > 0 is the reference distribution (e.g. Gamma) p0 is the 
# probability of a zero (precipitation) month.

# p0.center.mass - If TRUE, the Probability of zero (precipitation) is 
# estimated using a "center of mass" estimate based on the Weibull 
# plotting position function (see details). Only applies if p0=TRUE.

# scaling	- Indicates whether to do some scaling of x prior to 
# parameter identification. "no" (the default) indicates no scaling. 
# "max" indicates scaling by the maximum of x, such that 
# x <- x/max(x,na.rm=TRUE). "sd" stands for scaling by the standard 
# deviation. Scaling can stabilize parameter estimation.

# mledist.par	- named list that can be used to pass parameters to 
# mledist in package fitdistrplus.

# start.fun	- Function with arguments x and distr estimating initial 
# parameters of the function distr for each month. The function should 
# return a named list corresponding to the parameters of distr. 
# (See also dist.start)

# start.fun.fix	- logical argument, indicating if parameter estimates 
# of start.fun should be used if maximum likelihood estimation breaks 
# down. This stabilizes the implementation but can introduce biases in 
# the resulting SCI.

# obj	- an object of class fitSCI, output from fitSCI.

# sci.limit	- Truncate absolute values of SCI that are lage than 
# sci.limit. See details.

# warn	- Issue warnings if problems in parameter estimation occur.

# Details
# fitSCI estimates the parameters for transforming a meteorological 
# and environmental time series to a Standardized Climate Index (SCI). 

# transformSCI applies the standardisation. 

# Typical SCI are the Standardized Precipitation Index (SPI), the 
# Standardized Runoff Index (SRI) or the Standardized Precipitation 
# Evapotranspiration Index (SPEI).

# To reduce biases in the presence of many zero (precipitation) events, 
# the probability of these events (p0) can be estimated using a 
# "center of mass" estimate based on the Weibull plotting position 
# function (p0.center.mass=TRUE). Following Stagge et al. (2014) the 
# probability of zero events is then estimated as p0 = (n_p)/(n + 1), 
# where np refers to the number of zero events and n is the sample 
# size. The resulting mixed distribution used fro SCI transformation 
# is then: 
# g(x) = if(x > 0) p0 + (1 - p0)G(x) else if(x == 0) (np + 1)/(2(n + 1))
#   where G(x)>0 is a model (e.g. gamma) distribution.

# Uncertainty in distribution parameters can cause unrealistically 
# large (small) SCI values if values in x exceed the values used for 
# parameter estimation (see fitSCI). Therefore transformSCI allows for 
# a truncation of the SCI series such that abs(sci)<=sci.limit. The 
# truncation can be disabled by setting sci.limit=Inf.

# Value
# fitSCI returns an object of class "fitSCI" with the following components:

# dist.para	
# A column matrix containing the parameters of distribution distr for 
# each month. Row names correspond to the distribution parameters. 
# If p0=TUE an additional row named P0 is introduced, indicating the 
# probability of zero (precipitation) events.

# dist.para.flag	
# an vector indicating possible issues occurring throughout parameter 
# estimation. Possible values are: 0. no problems occurred; 1. 
# starting values could not be estimated; 2. mledist crashed with 
# unknown error; 3. mledist did not converge; 4. all values in this 
# month are NA; 5. all values in this month are constant, distribution 
# not defined.

# time.scale	
# The time scale (integer) of the SCI calculation.

# distr	
# A character string "name" naming a distribution used

# p0	
# logical indicating whether probability of zero (precipitation) 
# events is estimated separately.

# p0.center.mass	
# logical indicating whether probability of zero (precipitation) 
# events is estimated using the "centre of mass" estimator 
# (see Stagge et al. (2014) for details).

# scaling	
# numeric value that has been used to scale x (see argument scaling). 
# A value of 1 results from scaling="no", other values are the maximum 
# value or the standard deviation of x, depending on the choice of the 
# parameter scaling.

# call	
# the function call transform SCI returns a numeric vector containing 
# the SCI, having values of the standard normal distribution.

# Note
# This function is intended to be used together with transformSCI.

# Author(s)
# Lukas Gudmundsson & James Stagge

# References
# Stagee, J.H. ; Tallaksen, L.M.; Gudmundsson, L.; van Loon, A.; 
# Stahl, K.: Candidate Distributions for Climatological Drought 
# Indices (SPI and SPEI), 2015, International Journal of Climatology, 
# 35, 4027-4040, doi:10.1002/joc.4267.

# Stagee, J.H. ; Tallaksen, L.M.; Gudmundsson, L.; van Loon, A.; 
# Stahl, K.: Response to comment on "Candidate Distributions for 
# Climatological Drought Indices (SPI and SPEI)", 2016, International 
# Journal of Climatology, 36, 2132-2138, doi:10.1002/joc.4564.

# McKee, T.; Doesken, N. & Kleist, J.: The relationship of drought 
# frequency and duration to time scales Preprints, 8th Conference on 
# Applied Climatology, 1993, 179-184.

# Shukla, S. & Wood, A. W.: Use of a standardized runoff index for 
# characterizing hydrologic drought Geophysical Research Letters, 
# 2008, 35, L02405.

# Vicente-Serrano, S. M.; Begueria, S. & Lopez-Moreno, J. I.: A 
# Multiscalar Drought Index Sensitive to Global Warming: The 
# Standardized Precipitation Evapotranspiration Index J. Climate, 
# Journal of Climate, American Meteorological Society, 2009, 23, 
# 1696-1718.

# See Also
# dist.start

# Examples
## generate artificial data
##
set.seed(101)
n.years <- 60
date <- as.tibble(rep(1:n.years,each = 12) + 1950 + 
  rep((0:11)/12,times = n.years))

## Precipitation
PRECIP <- (0.25*sin( 2 * pi * date) + 0.3) * 
  rgamma(n.years*12, shape = 3, scale = 1)
PRECIP[PRECIP < 0.1] <- 0

## Potential Evapotranspiration
PET <- 0.5 * sin( 2 * pi * date) + 1.2 + rnorm(n.years * 12, 0, 0.2)

## display test data
matplot(date,cbind(PRECIP,PET),t=c("h","l"),col=c("blue","red"),lty=1)
legend("topright",legend=c("PRECIPitation","temperature"),fill=c("blue","red"))

##
## example SPI
##
spi.para <- fitSCI(PRECIP,first.mon=1,distr="gamma",time.scale=6,p0=TRUE)
spi.para
spi <- transformSCI(PRECIP,first.mon=1,obj=spi.para)
plot(date,spi,t="l")

##
## effect of time.scale on SPI
##
spi.1.para <- fitSCI(PRECIP,first.mon=1,time.scale=1,distr="gamma",p0=TRUE)
spi.12.para <- fitSCI(PRECIP,first.mon=1,time.scale=12,distr="gamma",p0=TRUE)
spi.1 <- transformSCI(PRECIP,first.mon=1,obj=spi.1.para)
spi.12 <- transformSCI(PRECIP,first.mon=1,obj=spi.12.para)
matplot(date,cbind(spi.1,spi.12),t="l",lty=1,col=c("red","blue"),lwd=c(1,2))
legend("topright",legend=c("time.scale=1","time.scale=12"),fill=c("red","blue"))

##
## example SPEI
##
if(require(evd)){
    spei.para <- fitSCI(PRECIP-PET,first.mon=1,time.scale=6,distr="gev",p0=FALSE)
    spei <- transformSCI(PRECIP-PET,first.mon=1,obj=spei.para)
    plot(date,spei,t="l")
}

##
## effect of changing different distribution for SPEI computation
##
spei.genlog.para <- fitSCI(PRECIP-PET,first.mon=1,time.scale=6,distr="genlog",p0=FALSE)
spei.genlog <- transformSCI(PRECIP-PET,first.mon=1,obj=spei.genlog.para)
if(require(evd)){lines(date,spei.genlog, col="red")} else {plot(date,spei.genlog,t="l")}
## in this case: only limited effect.
## generally: optimal choice of distribution: user responsibility.

##
## use a 30 year reference period for SPI parameter estimation
##
sel.date <- date>=1970 & date<2000
spi.ref.para <- fitSCI(PRECIP[sel.date],first.mon=1,distr="gamma",time.scale=6,p0=TRUE)
## apply the the parameters of the reference period to all data
## also outside the reference period
spi.ref <- transformSCI(PRECIP,first.mon=1,obj=spi.ref.para)
plot(date,spi.ref,t="l",col="blue",ylim=c(-5,5),lwd=2)
lines(date[sel.date],spi.ref[sel.date],col="red",lwd=3)
legend("bottom",legend=c("reference period","extrapolation period"),fill=c("red","blue"),
       horiz=TRUE)

##
## use "start.fun.fix" in instances where maximum likelyhood estimation fails
##
## force failure of maximum likelyhood estimation by adding "strange" value
## a warning should be issued
xx <- PRECIP-PET; xx[300] <- 1000
spei.para <- fitSCI(xx,first.mon=2,time.scale=1,p0=FALSE,distr="gev")
spei.para$dist.para
## use start.fun, usually ment for estimating inital values for
## parameter optimisation if maximum likelihood estimation fails
spei.para <- fitSCI(xx,first.mon=2,time.scale=1,p0=FALSE,distr="gev",
                    start.fun.fix=TRUE)
spei.para$dist.para

##
## usage of sci.limit to truncate unrealistic SCI values
## 
PRECIP.mod <- PRECIP
PRECIP.mod[300] <- 100 ## introduce spuriously large value
spi.mod.para <- fitSCI(PRECIP.mod,first.mon=1,time.scale=3,p0=TRUE,distr="gamma")
plot(transformSCI(PRECIP.mod,first.mon=1,obj=spi.mod.para,sci.limit=Inf),
     t="l",col="blue",lwd=2)
lines(transformSCI(PRECIP.mod,first.mon=1,obj=spi.mod.para,sci.limit=4),col="red")

##
## how to modify settings of function "mledist" used for parameter identification
## 
## identify parameters with standard settings
spi.para <- fitSCI(PRECIP,first.mon=1,distr="gamma",time.scale=6,p0=TRUE)
## add lower and upper limits for parameter identification
lower.lim <- apply(spi.para$dist.para,1,min) - 0.5*apply(spi.para$dist.para,1,sd)
upper.lim <- apply(spi.para$dist.para,1,max) + 0.5*apply(spi.para$dist.para,1,sd)
spi.para.limit <- fitSCI(PRECIP,first.mon=1,distr="gamma",time.scale=6,p0=TRUE,
                         mledist.par=list(lower=lower.lim, upper=upper.lim))

##
## how to write an own start.fun
## (required if distributions not mentioned in "dist.start" are used)
##
## function with same arguments as "dist.start" 
my.start <- function(x,distr="gamma"){
### code based on "mmedist" in package "fitdistrplus"
    ppar <- try({
        n <- length(x)
        m <- mean(x)
        v <- (n - 1)/n * var(x)
        shape <- m^2/v
        rate <- m/v
        list(shape = shape, rate = rate)},TRUE)
    if (class(ppar) == "try-error") ## function has to be able to return NA parameters
        ppar <- list(shape = NA, rate = NA)
    return(ppar)
}
my.start(PRECIP)
spi.para <- fitSCI(PRECIP,first.mon=1,time.scale=6,p0=TRUE,
                   distr="gamma",start.fun=my.start)

```



```{r spi-cot} 
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Cottonwood, SD. 
# Code is mostly following the SPI vignette. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
# Select Cottonwoods dataset 
sta_cot <- sta_mon %>% 
  select(date, cot) %>% 
  arrange(date) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot  <- spi(sta_cot[,'cot'], 1, distribution = 'PearsonIII') 
spi_2cot  <- spi(sta_cot[,'cot'], 2, distribution = 'PearsonIII') 
spi_3cot  <- spi(sta_cot[,'cot'], 3, distribution = 'PearsonIII') 
spi_4cot  <- spi(sta_cot[,'cot'], 4, distribution = 'PearsonIII') 
spi_5cot  <- spi(sta_cot[,'cot'], 5, distribution = 'PearsonIII') 
spi_6cot  <- spi(sta_cot[,'cot'], 6, distribution = 'PearsonIII') 
spi_9cot  <- spi(sta_cot[,'cot'], 9, distribution = 'PearsonIII') 
spi_12cot <- spi(sta_cot[,'cot'], 12, distribution = 'PearsonIII')  
spi_18cot <- spi(sta_cot[,'cot'], 18, distribution = 'PearsonIII') 
spi_24cot <- spi(sta_cot[,'cot'], 24, distribution = 'PearsonIII') 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_2cot_coeff <- as.tibble(spi_2cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_3cot_coeff <- as.tibble(spi_3cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_4cot_coeff <- as.tibble(spi_4cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_5cot_coeff <- as.tibble(spi_5cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_6cot_coeff <- as.tibble(spi_6cot$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9cot_coeff <- as.tibble(spi_9cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_12cot_coeff <- as.tibble(spi_12cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_18cot_coeff <- as.tibble(spi_18cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_24cot_coeff <- as.tibble(spi_24cot$coefficients) %>% 
  t() %>% as.tibble() 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- spi_1cot_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2cot_coeff <- spi_2cot_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3cot_coeff <- spi_3cot_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4cot_coeff <- spi_4cot_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5cot_coeff <- spi_5cot_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6cot_coeff <- spi_6cot_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9cot_coeff <- spi_9cot_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12cot_coeff <- spi_12cot_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18cot_coeff <- spi_18cot_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24cot_coeff <- spi_24cot_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1cot_coeff, spi_2cot_coeff) 
rm(spi_1cot_coeff, spi_2cot_coeff) 

prep2 <- bind_cols(prep1, spi_3cot_coeff) 
rm(prep1, spi_3cot_coeff) 

prep3 <- bind_cols(prep2, spi_4cot_coeff) 
rm(prep2, spi_4cot_coeff) 

prep4 <- bind_cols(prep3, spi_5cot_coeff) 
rm(prep3, spi_5cot_coeff) 

prep5 <- bind_cols(prep4, spi_6cot_coeff) 
rm(prep4, spi_6cot_coeff) 

prep6 <- bind_cols(prep5, spi_9cot_coeff) 
rm(prep5, spi_9cot_coeff) 

prep7 <- bind_cols(prep6, spi_12cot_coeff) 
rm(prep6, spi_12cot_coeff) 

prep8 <- bind_cols(prep7, spi_18cot_coeff) 
rm(prep7, spi_18cot_coeff) 

spi_coeff_cot <- bind_cols(prep8, spi_24cot_coeff) 
rm(prep8, spi_24cot_coeff, sta_cot) 

spi_coeff_cot <- rownames_to_column(spi_coeff_cot, "month")
# export(spi_coeff_cot, "data/spi_coeff_cot.csv") 
# rm(spi_coeff_cot)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi_1cot <- as.tibble(spi_1cot$fitted) %>% 
  mutate(duration = 1) 

spi_2cot <- as.tibble(spi_2cot$fitted) %>% 
  mutate(duration = 2)

spi_3cot <- as.tibble(spi_3cot$fitted) %>% 
  mutate(duration = 3) 

spi_4cot <- as.tibble(spi_4cot$fitted) %>% 
  mutate(duration = 4) 

spi_5cot <- as.tibble(spi_5cot$fitted) %>% 
  mutate(duration = 5) 

spi_6cot <- as.tibble(spi_6cot$fitted) %>% 
  mutate(duration = 6) 

spi_9cot <- as.tibble(spi_9cot$fitted) %>% 
  mutate(duration = 9)

spi_12cot <- as.tibble(spi_12cot$fitted) %>% 
  mutate(duration = 12) 

spi_18cot <- as.tibble(spi_18cot$fitted) %>% 
  mutate(duration = 18) 

spi_24cot <- as.tibble(spi_24cot$fitted) %>% 
  mutate(duration = 24) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# add date to the SPI data 
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1cot  <-  bind_cols(date, spi_1cot) 
spi_2cot  <-  bind_cols(date, spi_2cot) 
spi_3cot  <-  bind_cols(date, spi_3cot) 
spi_4cot  <-  bind_cols(date, spi_4cot) 
spi_5cot  <-  bind_cols(date, spi_5cot) 
spi_6cot  <-  bind_cols(date, spi_6cot) 
spi_9cot  <-  bind_cols(date, spi_9cot) 
spi_12cot <-  bind_cols(date, spi_12cot) 
spi_18cot <-  bind_cols(date, spi_18cot) 
spi_24cot <-  bind_cols(date, spi_24cot) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rowwise join of SPI data
intr1 <- bind_rows(spi_1cot, spi_2cot) 
rm(spi_1cot, spi_2cot) 

intr2 <- bind_rows(intr1, spi_3cot) 
rm(intr1, spi_3cot) 

intr3 <- bind_rows(intr2, spi_4cot) 
rm(intr2, spi_4cot) 

intr4 <- bind_rows(intr3, spi_5cot) 
rm(intr3, intc3, spi_5cot) 

intr5 <- bind_rows(intr4, spi_6cot) 
rm(intr4, spi_6cot) 

intr6 <- bind_rows(intr5, spi_9cot) 
rm(intr5, spi_9cot) 

intr7 <- bind_rows(intr6, spi_12cot) 
rm(intr6, spi_12cot) 

intr8 <- bind_rows(intr7, spi_18cot) 
rm(intr7, spi_18cot) 

spi_cot <- bind_rows(intr8, spi_24cot) 
rm(intr8, spi_24cot)  

# export(spi_cot, "data/spi_cot.csv") 
# rm(spi_cot)
```

```{r spi-oel}   
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Oelrichs  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Select Oelrichs dataset
sta_oel <- sta_mon %>%  
  select(date, oel) %>% 
  arrange(date)  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel  <- spi(sta_oel[,'oel'], 1, distribution = 'PearsonIII') 
spi_2oel  <- spi(sta_oel[,'oel'], 2, distribution = 'PearsonIII') 
spi_3oel  <- spi(sta_oel[,'oel'], 3, distribution = 'PearsonIII') 
spi_4oel  <- spi(sta_oel[,'oel'], 4, distribution = 'PearsonIII') 
spi_5oel  <- spi(sta_oel[,'oel'], 5, distribution = 'PearsonIII') 
spi_6oel  <- spi(sta_oel[,'oel'], 6, distribution = 'PearsonIII') 
spi_9oel  <- spi(sta_oel[,'oel'], 9, distribution = 'PearsonIII') 
spi_12oel <- spi(sta_oel[,'oel'], 12, distribution = 'PearsonIII')  
spi_18oel <- spi(sta_oel[,'oel'], 18, distribution = 'PearsonIII') 
spi_24oel <- spi(sta_oel[,'oel'], 24, distribution = 'PearsonIII') 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel_coeff <- as.tibble(spi_1oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_2oel_coeff <- as.tibble(spi_2oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_3oel_coeff <- as.tibble(spi_3oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_4oel_coeff <- as.tibble(spi_4oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_5oel_coeff <- as.tibble(spi_5oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_6oel_coeff <- as.tibble(spi_6oel$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9oel_coeff <- as.tibble(spi_9oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_12oel_coeff <- as.tibble(spi_12oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_18oel_coeff <- as.tibble(spi_18oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_24oel_coeff <- as.tibble(spi_24oel$coefficients) %>% 
  t() %>% as.tibble() 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel_coeff <- spi_1oel_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2oel_coeff <- spi_2oel_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3oel_coeff <- spi_3oel_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4oel_coeff <- spi_4oel_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5oel_coeff <- spi_5oel_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6oel_coeff <- spi_6oel_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9oel_coeff <- spi_9oel_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12oel_coeff <- spi_12oel_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18oel_coeff <- spi_18oel_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24oel_coeff <- spi_24oel_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma)  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1oel_coeff, spi_2oel_coeff) 
rm(spi_1oel_coeff, spi_2oel_coeff) 

prep2 <- bind_cols(prep1, spi_3oel_coeff) 
rm(prep1, spi_3oel_coeff) 

prep3 <- bind_cols(prep2, spi_4oel_coeff) 
rm(prep2, spi_4oel_coeff) 

prep4 <- bind_cols(prep3, spi_5oel_coeff) 
rm(prep3, spi_5oel_coeff) 

prep5 <- bind_cols(prep4, spi_6oel_coeff) 
rm(prep4, spi_6oel_coeff) 

prep6 <- bind_cols(prep5, spi_9oel_coeff) 
rm(prep5, spi_9oel_coeff) 

prep7 <- bind_cols(prep6, spi_12oel_coeff) 
rm(prep6, spi_12oel_coeff) 
 
prep8 <- bind_cols(prep7, spi_18oel_coeff) 
rm(prep7, spi_18oel_coeff) 

spi_coeff_oel <- bind_cols(prep8, spi_24oel_coeff) 
rm(prep8, spi_24oel_coeff) 

spi_coeff_oel <- rownames_to_column(spi_coeff_oel, "month")
# export(spi_coeff_oel, "data/spi_coeff_oel.csv") 
# rm(spi_coeff_oel, sta_oel)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi_1oel <- as.tibble(spi_1oel$fitted) %>% 
  mutate(duration = 1)

spi_2oel <- as.tibble(spi_2oel$fitted) %>% 
  mutate(duration = 2)

spi_3oel <- as.tibble(spi_3oel$fitted) %>% 
  mutate(duration = 3) 

spi_4oel <- as.tibble(spi_4oel$fitted) %>% 
  mutate(duration = 4) 

spi_5oel <- as.tibble(spi_5oel$fitted) %>% 
  mutate(duration = 5) 

spi_6oel <- as.tibble(spi_6oel$fitted) %>% 
  mutate(duration = 6) 

spi_9oel <- as.tibble(spi_9oel$fitted) %>% 
  mutate(duration = 9)

spi_12oel <- as.tibble(spi_12oel$fitted) %>% 
  mutate(duration = 12) 

spi_18oel <- as.tibble(spi_18oel$fitted) %>% 
  mutate(duration = 18) 

spi_24oel <- as.tibble(spi_24oel$fitted) %>% 
  mutate(duration = 24) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1oel  <-  bind_cols(date, spi_1oel) 
spi_2oel  <-  bind_cols(date, spi_2oel) 
spi_3oel  <-  bind_cols(date, spi_3oel) 
spi_4oel  <-  bind_cols(date, spi_4oel) 
spi_5oel  <-  bind_cols(date, spi_5oel) 
spi_6oel  <-  bind_cols(date, spi_6oel) 
spi_9oel  <-  bind_cols(date, spi_9oel) 
spi_12oel <-  bind_cols(date, spi_12oel) 
spi_18oel <-  bind_cols(date, spi_18oel) 
spi_24oel <-  bind_cols(date, spi_24oel) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rowwise join of SPI data
intr1 <- bind_rows(spi_1oel, spi_2oel) 
rm(spi_1oel, spi_2oel) 

intr2 <- bind_rows(intr1, spi_3oel) 
rm(intr1, spi_3oel) 

intr3 <- bind_rows(intr2, spi_4oel) 
rm(intr2, spi_4oel) 

intr4 <- bind_rows(intr3, spi_5oel) 
rm(intr3, spi_5oel) 

intr5 <- bind_rows(intr4, spi_6oel) 
rm(intr4, intc4, spi_6oel) 

intr6 <- bind_rows(intr5, spi_9oel) 
rm(intr5, spi_9oel) 

intr7 <- bind_rows(intr6, spi_12oel) 
rm(intr6, spi_12oel) 

intr8 <- bind_rows(intr7, spi_18oel) 
rm(intr7, spi_18oel) 

spi_oel <- bind_rows(intr8, spi_24oel) 
rm(intr8, spi_24oel, sta_oel) 

# export(spi_oel, "data/spi_oel.csv") 
# rm(spi_oel)
```

```{r spi-rap} 
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Rapid City, SD.
# Note that this resulted in -Inf vals before
# Added na.rm = TRUE to handle the missing vals; didn't
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Select Rapid City dataset 
sta_trans <- sta_mon %>% 
  select(date, year, month, rap) %>% 
  arrange(date) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Pull apart the NA and non-NA vals
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# reusing code from _04_prcp-data_munging
# Use nearest year as split-point
#   1. Split the raw data into two parts at 1948-05-01
  sta_ge49      <- sta_trans %>% filter(year >= 1949) # yr above
  sta_48m05     <- sta_trans %>% filter(year == 1948 & month >= 5)
sta_rap         <- bind_rows(sta_ge49, sta_48m05) # this is active
  rm(sta_ge49, sta_48m05)
# remove year and month
sta_rap <- sta_rap %>%
  select(date, rap)
#  2. Save NA observations  
  sta_lt48      <- sta_trans %>% filter(year < 1948)
  sta_48m01_m05 <- sta_trans %>% filter(year == 1948 & month < 5)
sta_NA          <- bind_rows(sta_lt48, sta_48m01_m05) # this is not
  rm(sta_lt48, sta_48m01_m05, sta_trans)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Calculate Rapid City SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
# Added na.rm = TRUE to handle the missing vals, but didn't work!
spi_1rap  <- spi(sta_rap[,'rap'],  1, distribution = 'PearsonIII') 
spi_2rap  <- spi(sta_rap[,'rap'],  2, distribution = 'PearsonIII') 
spi_3rap  <- spi(sta_rap[,'rap'],  3, distribution = 'PearsonIII') 
spi_4rap  <- spi(sta_rap[,'rap'],  4, distribution = 'PearsonIII') 
spi_5rap  <- spi(sta_rap[,'rap'],  5, distribution = 'PearsonIII') 
spi_6rap  <- spi(sta_rap[,'rap'],  6, distribution = 'PearsonIII') 
spi_9rap  <- spi(sta_rap[,'rap'],  9, distribution = 'PearsonIII') 
spi_12rap <- spi(sta_rap[,'rap'], 12, distribution = 'PearsonIII')  
spi_18rap <- spi(sta_rap[,'rap'], 18, distribution = 'PearsonIII') 
spi_24rap <- spi(sta_rap[,'rap'], 24, distribution = 'PearsonIII') 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1rap_coeff <- as.tibble(spi_1rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_2rap_coeff <- as.tibble(spi_2rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_3rap_coeff <- as.tibble(spi_3rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_4rap_coeff <- as.tibble(spi_4rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_5rap_coeff <- as.tibble(spi_5rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_6rap_coeff <- as.tibble(spi_6rap$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9rap_coeff <- as.tibble(spi_9rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_12rap_coeff <- as.tibble(spi_12rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_18rap_coeff <- as.tibble(spi_18rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_24rap_coeff <- as.tibble(spi_24rap$coefficients) %>% 
  t() %>% as.tibble() 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1rap_coeff <- spi_1rap_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2rap_coeff <- spi_2rap_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3rap_coeff <- spi_3rap_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4rap_coeff <- spi_4rap_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5rap_coeff <- spi_5rap_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6rap_coeff <- spi_6rap_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9rap_coeff <- spi_9rap_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12rap_coeff <- spi_12rap_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18rap_coeff <- spi_18rap_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24rap_coeff <- spi_24rap_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1rap_coeff, spi_2rap_coeff) 
rm(spi_1rap_coeff, spi_2rap_coeff) 

prep2 <- bind_cols(prep1, spi_3rap_coeff) 
rm(prep1, spi_3rap_coeff) 

prep3 <- bind_cols(prep2, spi_4rap_coeff) 
rm(prep2, spi_4rap_coeff) 

prep4 <- bind_cols(prep3, spi_5rap_coeff) 
rm(prep3, spi_5rap_coeff) 

prep5 <- bind_cols(prep4, spi_6rap_coeff) 
rm(prep4, spi_6rap_coeff) 

prep6 <- bind_cols(prep5, spi_9rap_coeff) 
rm(prep5, spi_9rap_coeff) 

prep7 <- bind_cols(prep6, spi_12rap_coeff) 
rm(prep6, spi_12rap_coeff) 

prep8 <- bind_cols(prep7, spi_18rap_coeff) 
rm(prep7, spi_18rap_coeff) 

spi_coeff_rap <- bind_cols(prep8, spi_24rap_coeff) 
rm(prep8, spi_24rap_coeff) 

spi_coeff_rap <- rownames_to_column(spi_coeff_rap, "month")
# export(spi_coeff_rap, "data/spi_coeff_rap.csv") 
# rm(spi_coeff_rap)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi_1rap <- as.tibble(spi_1rap$fitted) %>% 
  mutate(duration = 1)

spi_2rap <- as.tibble(spi_2rap$fitted) %>% 
  mutate(duration = 2)

spi_3rap <- as.tibble(spi_3rap$fitted) %>% 
  mutate(duration = 3) 

spi_4rap <- as.tibble(spi_4rap$fitted) %>% 
  mutate(duration = 4) 

spi_5rap <- as.tibble(spi_5rap$fitted) %>% 
  mutate(duration = 5) 

spi_6rap <- as.tibble(spi_6rap$fitted) %>% 
  mutate(duration = 6) 

spi_9rap <- as.tibble(spi_9rap$fitted) %>% 
  mutate(duration = 9)

spi_12rap <- as.tibble(spi_12rap$fitted) %>% 
  mutate(duration = 12) 

spi_18rap <- as.tibble(spi_18rap$fitted) %>% 
  mutate(duration = 18) 

spi_24rap <- as.tibble(spi_24rap$fitted) %>% 
  mutate(duration = 24) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# add date to the SPI data & bind cols
date <- sta_rap %>%
  select(date) %>%
  arrange(date)
#rm(sta_rap)
# ~~~~~~~~~~~~~~~
spi_1rap  <-  bind_cols(date, spi_1rap) 
spi_2rap  <-  bind_cols(date, spi_2rap) 
spi_3rap  <-  bind_cols(date, spi_3rap) 
spi_4rap  <-  bind_cols(date, spi_4rap) 
spi_5rap  <-  bind_cols(date, spi_5rap) 
spi_6rap  <-  bind_cols(date, spi_6rap) 
spi_9rap  <-  bind_cols(date, spi_9rap) 
spi_12rap <-  bind_cols(date, spi_12rap) 
spi_18rap <-  bind_cols(date, spi_18rap) 
spi_24rap <-  bind_cols(date, spi_24rap) 
# Rowwise join of SPI data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
intr1 <- bind_rows(spi_1rap, spi_2rap) 
rm(spi_1rap, spi_2rap) 

intr2 <- bind_rows(intr1, spi_3rap) 
rm(intr1, spi_3rap) 

intr3 <- bind_rows(intr2, spi_4rap) 
rm(intr2, spi_4rap) 

intr4 <- bind_rows(intr3, spi_5rap) 
rm(intr3, spi_5rap) 

intr5 <- bind_rows(intr4, spi_6rap)  
rm(intr4, spi_6rap) 

intr6 <- bind_rows(intr5, spi_9rap) 
rm(intr5, spi_9rap) 

intr7 <- bind_rows(intr6, spi_12rap) 
rm(intr6, spi_12rap) 

intr8 <- bind_rows(intr7, spi_18rap) 
rm(intr7, spi_18rap) 

spi_rap <- bind_rows(intr8, spi_24rap) 
rm(intr8, spi_24rap) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# check on the -Inf vals
spi_1rap <- spi_rap %>%
  filter(duration == 1)
# ~~~~~~~~~~~~~~~~~~~~~~
# join data and rename
sta_rap <- full_join(sta_rap, spi_1rap, by = "date") 
sta_rap <- sta_rap %>%
  rename(depth = rap.x) %>%
  rename(spi1 = rap.y) %>%
  arrange(spi1)

broken_rap <- sta_rap %>%
  filter(spi1 < -5)

 
```

```{r spi-algorithm-coeff-int}  
# This code chunk calculates SPI coefficients for  
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Interior, SD. 

# Add Interior dataset and change to time series 
sta_int <- sta_mon %>% 
  arrange(date) %>% 
  select(year, month, int) %>% 
  ts(end = c(2018, 05), frequency = 12)

# Calculate SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
# Added na.rm = TRUE to handle the missing vals
spi_1int  <- spi(sta_int[,'int'], 1, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_2int  <- spi(sta_int[,'int'], 2, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_3int  <- spi(sta_int[,'int'], 3, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_4int  <- spi(sta_int[,'int'], 4, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_5int  <- spi(sta_int[,'int'], 5, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_6int  <- spi(sta_int[,'int'], 6, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_9int  <- spi(sta_int[,'int'], 9, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_12int <- spi(sta_int[,'int'], 12, na.rm = TRUE, 
                 distribution = 'PearsonIII')  
spi_18int <- spi(sta_int[,'int'], 18, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_24int <- spi(sta_int[,'int'], 24, na.rm = TRUE, 
                 distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1int_coeff <- as.tibble(spi_1int$coefficients) %>% 
  t() %>% as.tibble() 

spi_2int_coeff <- as.tibble(spi_2int$coefficients) %>% 
  t() %>% as.tibble() 

spi_3int_coeff <- as.tibble(spi_3int$coefficients) %>% 
  t() %>% as.tibble() 

spi_4int_coeff <- as.tibble(spi_4int$coefficients) %>% 
  t() %>% as.tibble() 

spi_5int_coeff <- as.tibble(spi_5int$coefficients) %>% 
  t() %>% as.tibble() 

spi_6int_coeff <- as.tibble(spi_6int$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9int_coeff <- as.tibble(spi_9int$coefficients) %>% 
  t() %>% as.tibble() 

spi_12int_coeff <- as.tibble(spi_12int$coefficients) %>% 
  t() %>% as.tibble() 

spi_18int_coeff <- as.tibble(spi_18int$coefficients) %>% 
  t() %>% as.tibble() 

spi_24int_coeff <- as.tibble(spi_24int$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1int_coeff <- spi_1int_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2int_coeff <- spi_2int_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3int_coeff <- spi_3int_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4int_coeff <- spi_4int_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5int_coeff <- spi_5int_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6int_coeff <- spi_6int_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9int_coeff <- spi_9int_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12int_coeff <- spi_12int_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18int_coeff <- spi_18int_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24int_coeff <- spi_24int_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 


# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1int_coeff, spi_2int_coeff) 
rm(spi_1int_coeff, spi_2int_coeff) 

prep2 <- bind_cols(prep1, spi_3int_coeff) 
rm(prep1, spi_3int_coeff) 

prep3 <- bind_cols(prep2, spi_4int_coeff) 
rm(prep2, spi_4int_coeff) 

prep4 <- bind_cols(prep3, spi_5int_coeff) 
rm(prep3, spi_5int_coeff) 

prep5 <- bind_cols(prep4, spi_6int_coeff) 
rm(prep4, spi_6int_coeff) 

prep6 <- bind_cols(prep5, spi_9int_coeff) 
rm(prep5, spi_9int_coeff) 

prep7 <- bind_cols(prep6, spi_12int_coeff) 
rm(prep6, spi_12int_coeff) 

int8 <- bind_cols(int7, spi_18int_coeff) 
rm(int7, spi_18int_coeff) 

spi_coeff_int <- bind_cols(int8, spi_24int_coeff) 
rm(int8, spi_24int_coeff, sta_int) 

# export(spi_coeff_int, "data/spi_coeff_int.csv") 
# rm(spi_coeff_int)
```

```{r spi-algorithm-fit-int}  
# This code chunk calculates SPI fitted values for  
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Interior, SD. 
# It is a continuation of 'spi-algorithm-coeff-int'

# extract SPI values & add duration
spi_1int <- as.tibble(spi_1int$fitted) %>% 
  mutate(duration = 1)

spi_2int <- as.tibble(spi_2int$fitted) %>% 
  mutate(duration = 2)

spi_3int <- as.tibble(spi_3int$fitted) %>% 
  mutate(duration = 3) 

spi_4int <- as.tibble(spi_4int$fitted) %>% 
  mutate(duration = 4) 

spi_5int <- as.tibble(spi_5int$fitted) %>% 
  mutate(duration = 5) 

spi_6int <- as.tibble(spi_6int$fitted) %>% 
  mutate(duration = 6) 

spi_9int <- as.tibble(spi_9int$fitted) %>% 
  mutate(duration = 9)

spi_12int <- as.tibble(spi_12int$fitted) %>% 
  mutate(duration = 12) 

spi_18int <- as.tibble(spi_18int$fitted) %>% 
  mutate(duration = 18) 

spi_24int <- as.tibble(spi_24int$fitted) %>% 
  mutate(duration = 24) 

# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1int  <-  bind_cols(date, spi_1int) 
spi_2int  <-  bind_cols(date, spi_2int) 
spi_3int  <-  bind_cols(date, spi_3int) 
spi_4int  <-  bind_cols(date, spi_4int) 
spi_5int  <-  bind_cols(date, spi_5int) 
spi_6int  <-  bind_cols(date, spi_6int) 
spi_9int  <-  bind_cols(date, spi_9int) 
spi_12int <-  bind_cols(date, spi_12int) 
spi_18int <-  bind_cols(date, spi_18int) 
spi_24int <-  bind_cols(date, spi_24int) 

# join the SPI data by wide and long
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1- & 2-month
intr1 <- bind_rows(spi_1int, spi_2int) 

intc1 <- bind_cols(spi_1int, spi_2int) 
intc1 <- intc1 %>%
  select(-c(date1, duration, duration1)) %>%
  rename(int_1 = "Series 1") %>%
  rename(int_2 = "Series 11")
rm(spi_1int, spi_2int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3-month
intr2 <- bind_rows(intr1, spi_3int) 

intc2 <- bind_cols(intc1, spi_3int) 
intc2 <- intc2 %>%
  select(-c(date1, duration)) %>%
  rename(int_3 = "Series 1") 
rm(intr1, intc1, spi_3int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 4-month
intr3 <- bind_rows(intr2, spi_4int) 

intc3 <- bind_cols(intc2, spi_4int) 
intc3 <- intc3 %>%
  select(-c(date1, duration)) %>%
  rename(int_4 = "Series 1") 
rm(intr2, intc2, spi_4int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 5-month
intr4 <- bind_rows(intr3, spi_5int) 

intc4 <- bind_cols(intc3, spi_5int) 
intc4 <- intc4 %>%
  select(-c(date1, duration)) %>%
  rename(int_5 = "Series 1") 
rm(intr3, intc3, spi_5int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 6-month
intr5 <- bind_rows(intr4, spi_6int) 

intc5 <- bind_cols(intc4, spi_6int) 
intc5 <- intc5 %>%
  select(-c(date1, duration)) %>%
  rename(int_6 = "Series 1") 
rm(intr4, intc4, spi_6int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9-month
intr6 <- bind_rows(intr5, spi_9int) 

intc6 <- bind_cols(intc5, spi_9int) 
intc6 <- intc6 %>%
  select(-c(date1, duration)) %>%
  rename(int_9 = "Series 1") 
rm(intr5, intc5, spi_9int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 12-month
intr7 <- bind_rows(intr6, spi_12int) 

intc7 <- bind_cols(intc6, spi_12int) 
intc7 <- intc7 %>%
  select(-c(date1, duration)) %>%
  rename(int_12 = "Series 1") 
rm(intr6, intc6, spi_12int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 18-month
intr8 <- bind_rows(intr7, spi_18int) 

intc8 <- bind_cols(intc7, spi_18int) 
intc8 <- intc8 %>%
  select(-c(date1, duration)) %>%
  rename(int_18 = "Series 1") 
rm(intr7, intc7, spi_18int) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 24-month
spi_int_gath <- bind_rows(intr8, spi_24int) 
spi_int_gath <- spi_int_gath %>%
  rename(spi_int = "Series 1")

spi_int <- bind_cols(intc8, spi_24int) 
spi_int <- spi_int %>%
  select(-c(date1, duration)) %>%
  rename(int_24 = "Series 1") 

rm(intr8, intc8, spi_24int) 
# Next steps:
# distribution = 'PersonIII',
```

```{r spi-algorithm-coeff-ora}  
# This code chunk calculates SPI coefficients for  
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Oral, SD.

# Add Oral dataset and change to time series 
sta_ora <- sta_mon %>% 
  arrange(date) %>% 
  select(year, month, ora) %>% 
  ts(end = c(2018, 05), frequency = 12)

# Calculate SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
# Added na.rm = TRUE to handle the missing vals
spi_1ora  <- spi(sta_ora[,'ora'], 1, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_2ora  <- spi(sta_ora[,'ora'], 2, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_3ora  <- spi(sta_ora[,'ora'], 3, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_4ora  <- spi(sta_ora[,'ora'], 4, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_5ora  <- spi(sta_ora[,'ora'], 5, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_6ora  <- spi(sta_ora[,'ora'], 6, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_9ora  <- spi(sta_ora[,'ora'], 9, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_12ora <- spi(sta_ora[,'ora'], 12, na.rm = TRUE, 
                 distribution = 'PearsonIII')  
spi_18ora <- spi(sta_ora[,'ora'], 18, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_24ora <- spi(sta_ora[,'ora'], 24, na.rm = TRUE, 
                 distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1ora_coeff <- as.tibble(spi_1ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_2ora_coeff <- as.tibble(spi_2ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_3ora_coeff <- as.tibble(spi_3ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_4ora_coeff <- as.tibble(spi_4ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_5ora_coeff <- as.tibble(spi_5ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_6ora_coeff <- as.tibble(spi_6ora$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9ora_coeff <- as.tibble(spi_9ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_12ora_coeff <- as.tibble(spi_12ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_18ora_coeff <- as.tibble(spi_18ora$coefficients) %>% 
  t() %>% as.tibble() 

spi_24ora_coeff <- as.tibble(spi_24ora$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1ora_coeff <- spi_1ora_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2ora_coeff <- spi_2ora_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3ora_coeff <- spi_3ora_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4ora_coeff <- spi_4ora_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5ora_coeff <- spi_5ora_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6ora_coeff <- spi_6ora_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9ora_coeff <- spi_9ora_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12ora_coeff <- spi_12ora_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18ora_coeff <- spi_18ora_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24ora_coeff <- spi_24ora_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 


# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1ora_coeff, spi_2ora_coeff) 
rm(spi_1ora_coeff, spi_2ora_coeff) 

prep2 <- bind_cols(prep1, spi_3ora_coeff) 
rm(prep1, spi_3ora_coeff) 

prep3 <- bind_cols(prep2, spi_4ora_coeff) 
rm(prep2, spi_4ora_coeff) 

prep4 <- bind_cols(prep3, spi_5ora_coeff) 
rm(prep3, spi_5ora_coeff) 

prep5 <- bind_cols(prep4, spi_6ora_coeff) 
rm(prep4, spi_6ora_coeff) 

prep6 <- bind_cols(prep5, spi_9ora_coeff) 
rm(prep5, spi_9ora_coeff) 

prep7 <- bind_cols(prep6, spi_12ora_coeff) 
rm(prep6, spi_12ora_coeff) 

prep8 <- bind_cols(prep7, spi_18ora_coeff) 
rm(prep7, spi_18ora_coeff) 

spi_coeff_ora <- bind_cols(prep8, spi_24ora_coeff) 
rm(prep8, spi_24ora_coeff, sta_ora) 

# export(spi_coeff_ora, "data/spi_coeff_ora.csv") 
# rm(spi_coeff_ora)
```

```{r spi-algorithm-fit-ora} 
# This code chunk calculates SPI fitted values for  
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Oral, SD. 
# It is a continuation of 'spi-algorithm-coeff-ora'

# extract SPI values & add duration
spi_1ora <- as.tibble(spi_1ora$fitted) %>% 
  mutate(duration = 1)

spi_2ora <- as.tibble(spi_2ora$fitted) %>% 
  mutate(duration = 2)

spi_3ora <- as.tibble(spi_3ora$fitted) %>% 
  mutate(duration = 3) 

spi_4ora <- as.tibble(spi_4ora$fitted) %>% 
  mutate(duration = 4) 

spi_5ora <- as.tibble(spi_5ora$fitted) %>% 
  mutate(duration = 5) 

spi_6ora <- as.tibble(spi_6ora$fitted) %>% 
  mutate(duration = 6) 

spi_9ora <- as.tibble(spi_9ora$fitted) %>% 
  mutate(duration = 9)

spi_12ora <- as.tibble(spi_12ora$fitted) %>% 
  mutate(duration = 12) 

spi_18ora <- as.tibble(spi_18ora$fitted) %>% 
  mutate(duration = 18) 

spi_24ora <- as.tibble(spi_24ora$fitted) %>% 
  mutate(duration = 24) 

# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1ora  <-  bind_cols(date, spi_1ora) 
spi_2ora  <-  bind_cols(date, spi_2ora) 
spi_3ora  <-  bind_cols(date, spi_3ora) 
spi_4ora  <-  bind_cols(date, spi_4ora) 
spi_5ora  <-  bind_cols(date, spi_5ora) 
spi_6ora  <-  bind_cols(date, spi_6ora) 
spi_9ora  <-  bind_cols(date, spi_9ora) 
spi_12ora <-  bind_cols(date, spi_12ora) 
spi_18ora <-  bind_cols(date, spi_18ora) 
spi_24ora <-  bind_cols(date, spi_24ora) 

# join the SPI data by wide and long
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1- & 2-month
intr1 <- bind_rows(spi_1ora, spi_2ora) 

intc1 <- bind_cols(spi_1ora, spi_2ora) 
intc1 <- intc1 %>%
  select(-c(date1, duration, duration1)) %>%
  rename(int_1 = "Series 1") %>%
  rename(int_2 = "Series 11")
rm(spi_1ora, spi_2ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3-month
intr2 <- bind_rows(intr1, spi_3ora) 

intc2 <- bind_cols(intc1, spi_3ora) 
intc2 <- intc2 %>%
  select(-c(date1, duration)) %>%
  rename(ora_3 = "Series 1") 
rm(intr1, intc1, spi_3ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 4-month
intr3 <- bind_rows(intr2, spi_4ora) 

intc3 <- bind_cols(intc2, spi_4ora) 
intc3 <- intc3 %>%
  select(-c(date1, duration)) %>%
  rename(ora_4 = "Series 1") 
rm(intr2, intc2, spi_4ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 5-month
intr4 <- bind_rows(intr3, spi_5ora) 

intc4 <- bind_cols(intc3, spi_5ora) 
intc4 <- intc4 %>%
  select(-c(date1, duration)) %>%
  rename(ora_5 = "Series 1") 
rm(intr3, intc3, spi_5ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 6-month
intr5 <- bind_rows(intr4, spi_6ora) 

intc5 <- bind_cols(intc4, spi_6ora) 
intc5 <- intc5 %>%
  select(-c(date1, duration)) %>%
  rename(ora_6 = "Series 1") 
rm(intr4, intc4, spi_6ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9-month
intr6 <- bind_rows(intr5, spi_9ora) 

intc6 <- bind_cols(intc5, spi_9ora) 
intc6 <- intc6 %>%
  select(-c(date1, duration)) %>%
  rename(ora_9 = "Series 1") 
rm(intr5, intc5, spi_9ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 12-month
intr7 <- bind_rows(intr6, spi_12ora) 

intc7 <- bind_cols(intc6, spi_12ora) 
intc7 <- intc7 %>%
  select(-c(date1, duration)) %>%
  rename(ora_12 = "Series 1") 
rm(intr6, intc6, spi_12ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 18-month
intr8 <- bind_rows(intr7, spi_18ora) 

intc8 <- bind_cols(intc7, spi_18ora) 
intc8 <- intc8 %>%
  select(-c(date1, duration)) %>%
  rename(ora_18 = "Series 1") 
rm(intr7, intc7, spi_18ora) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 24-month
spi_ora_gath <- bind_rows(intr8, spi_24ora) 
spi_ora_gath <- spi_ora_gath %>%
  rename(spi_ora = "Series 1")

spi_ora <- bind_cols(intc8, spi_24ora) 
spi_ora <- spi_ora %>%
  select(-c(date1, duration)) %>%
  rename(ora_24 = "Series 1") 

rm(intr8, intc8, spi_24ora) 
# Next steps:
# distribution = 'PersonIII',
```

```{r spi-plot-cot, include=FALSE, eval=FALSE}
# visual check of output for cottonwood SPI
ggplot(spi_cot_gath, aes(date, spi_cot)) + 
  geom_line() +
  facet_grid(duration~.) +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "Cottonwood, SD") +
       xlab("") + 
       ylab("SPI-value") +
  NULL 
```

```{r spi-plot-oel, include=FALSE, eval=FALSE} 
# visual check of Oelrichs SPI 
ggplot(spi_oel_gath, aes(date, spi_oel)) +  
  geom_line() + 
  facet_grid(duration~.) + 
  theme_classic() +  
  labs(title = "Standardized Precipitation Index (SPI)", 
       subtitle = "Oral, SD") + 
       xlab("") + 
       ylab("SPI-value") 
```

```{r spi-plot-rap, include=FALSE, eval=FALSE} 
# visual check of Rapid City SPI 
ggplot(spi_rap_gath, aes(date, spi_rap)) +  
  geom_line(aes(na.rm = TRUE)) + 
  facet_grid(duration~.) + 
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)", 
       subtitle = "Rapid City, SD") + 
       xlab("") + 
       ylab("SPI-value") 
# Next steps : how to remove NA from all of the facets 
```

```{r spi-plot-int, include=FALSE, eval=FALSE} 
# visual check of Interior SPI  
ggplot(spi_int_gath, aes(date, spi_int)) +  
  geom_line(aes(na.rm = TRUE)) + 
  facet_grid(duration~.) +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "Interior, SD") +
       xlab("") +
       ylab("SPI-value")
# Next steps : how to remove NA from all of the facets
```

```{r spi-plot-ora, include=FALSE, eval=FALSE} 
ggplot(spi_ora_gath, aes(date, spi_ora)) +  
  geom_line(aes(na.rm = TRUE)) + 
  facet_grid(duration~.) + 
  theme_classic() +  
  labs(title = "Standardized Precipitation Index (SPI)", 
       subtitle = "Oral, SD") + 
       xlab("") + 
       ylab("SPI-value") 
# Next steps : how to remove NA from all of the facets  
```

```{r spi-algorithm-coeff-cot-DEPR} 
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Cottonwood, SD. 
# Code is mostly following the SPI vignette. 

# Add Cottonwoods dataset and change to time series 
sta_cot <- sta_mon %>% 
  select(date, cot) %>% 
  arrange(date) 




# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot  <- spi(sta_cot[,'cot'], 1, distribution = 'PearsonIII') 
spi_2cot  <- spi(sta_cot[,'cot'], 2, distribution = 'PearsonIII') 
spi_3cot  <- spi(sta_cot[,'cot'], 3, distribution = 'PearsonIII') 
spi_4cot  <- spi(sta_cot[,'cot'], 4, distribution = 'PearsonIII') 
spi_5cot  <- spi(sta_cot[,'cot'], 5, distribution = 'PearsonIII') 
spi_6cot  <- spi(sta_cot[,'cot'], 6, distribution = 'PearsonIII') 
spi_9cot  <- spi(sta_cot[,'cot'], 9, distribution = 'PearsonIII') 
spi_12cot <- spi(sta_cot[,'cot'], 12, distribution = 'PearsonIII')  
spi_18cot <- spi(sta_cot[,'cot'], 18, distribution = 'PearsonIII') 
spi_24cot <- spi(sta_cot[,'cot'], 24, distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_2cot_coeff <- as.tibble(spi_2cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_3cot_coeff <- as.tibble(spi_3cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_4cot_coeff <- as.tibble(spi_4cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_5cot_coeff <- as.tibble(spi_5cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_6cot_coeff <- as.tibble(spi_6cot$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9cot_coeff <- as.tibble(spi_9cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_12cot_coeff <- as.tibble(spi_12cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_18cot_coeff <- as.tibble(spi_18cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_24cot_coeff <- as.tibble(spi_24cot$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- spi_1cot_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2cot_coeff <- spi_2cot_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3cot_coeff <- spi_3cot_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4cot_coeff <- spi_4cot_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5cot_coeff <- spi_5cot_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6cot_coeff <- spi_6cot_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9cot_coeff <- spi_9cot_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12cot_coeff <- spi_12cot_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18cot_coeff <- spi_18cot_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24cot_coeff <- spi_24cot_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 

# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1cot_coeff, spi_2cot_coeff) 
rm(spi_1cot_coeff, spi_2cot_coeff) 

prep2 <- bind_cols(prep1, spi_3cot_coeff) 
rm(prep1, spi_3cot_coeff) 

prep3 <- bind_cols(prep2, spi_4cot_coeff) 
rm(prep2, spi_4cot_coeff) 

prep4 <- bind_cols(prep3, spi_5cot_coeff) 
rm(prep3, spi_5cot_coeff) 

prep5 <- bind_cols(prep4, spi_6cot_coeff) 
rm(prep4, spi_6cot_coeff) 

prep6 <- bind_cols(prep5, spi_9cot_coeff) 
rm(prep5, spi_9cot_coeff) 

prep7 <- bind_cols(prep6, spi_12cot_coeff) 
rm(prep6, spi_12cot_coeff) 

prep8 <- bind_cols(prep7, spi_18cot_coeff) 
rm(prep7, spi_18cot_coeff) 

spi_coeff_cot <- bind_cols(prep8, spi_24cot_coeff) 
rm(prep8, spi_24cot_coeff, sta_cot) 

# export(spi_coeff_cot, "data/spi_coeff_cot.csv") 
# rm(spi_coeff_cot)
```

```{r combine-spi-coeff}   
# prepare for combining
# THIS COULD BE DONE BETTER
spi_coeff_cot <- spi_coeff_cot %>% 
  mutate(sta = "cot") %>%
  mutate(month = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)) 

spi_coeff_oel <- spi_coeff_oel %>% 
  mutate(sta = "oel") %>%
  mutate(month = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)) 

spi_coeff_rap <- spi_coeff_rap %>% 
  mutate(sta = "rap") %>%
  mutate(month = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)) 

spi_coeff_int <- spi_coeff_int %>% 
  mutate(sta = "int") %>%
  mutate(month = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)) 

spi_coeff_ora <- spi_coeff_ora %>% 
  mutate(sta = "ora") %>%
  mutate(month = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)) 

# combine in steps
prep1 <- bind_rows(spi_coeff_cot, spi_coeff_oel) 
rm(spi_coeff_cot, spi_coeff_oel) 

prep2 <- bind_rows(prep1, spi_coeff_rap) 
rm(prep1, spi_coeff_rap) 

prep3 <- bind_rows(prep2, spi_coeff_int) 
rm(prep2, spi_coeff_int) 

spi_coeff <- bind_rows(prep3, spi_coeff_ora) 
rm(prep3, spi_coeff_ora) 
```

```{r boxplots_1month, include=FALSE, eval=FALSE}
#ggplot(spi_1mon_grp, aes(month, value, group = month)) + 
#  geom_boxplot() + 
#  facet_wrap(~station) + 
#  theme_classic() +  
#  labs(title = "Monthly precipitation depths", 
#       subtitle = "Pine Ridge Reservation, SD for 1910-2017") + 
#       xlab("Date") + 
#       ylab("Depth, mm") 
```

```{r munging-DEPR, include=FALSE, eval=FALSE}

# add a small value to zeros to solve a downstream issue 
# maybe have this fixed now using ts rather than date class
#sta_mon <- sta_mon %>% 
#  gather(key = "station", value = "depth", -date, -year, -month) %>%
#  mutate(depth = replace(depth, depth == 0.0, 0.15)) %>%
#  spread(station, depth) 

# Check on log transformation
#sta_notzero <- sta_grp %>%
#  filter(depth != 0)

#min <- min(sta_notzero$depth)

#sta_zero <- sta_grp %>%
#  filter(depth == 0) %>%
#  mutate(depth = depth + min/2)

#sta_log <- bind_rows(sta_zero, sta_notzero)
#sta_log <- sta_log %>%
#  mutate(log_depth = log10(depth))

#rm(min) 
```

```{r purrr-DEPR, include=FALSE, eval=FALSE} 
# This code chunk applies the SPI function across list of stations
# There seems to be an error with zeros and NA vals in 'SPEI'
# The sta_grp data from import-data code chunk above has NA removed

# Change the data into a simple list 
by_sta <- sta_grp %>% 
  select(station, depth, date) %>% 
    split(.$station) %>%
  transpose() # invert the list
by_sta <- by_sta$depth # drop the unneeded character elements

# Apply SPI to the list
spi_list  <- by_sta %>%  
  purrr::map(~spi(data = ., 1,  
                 distribution = 'PearsonIII', na.rm = TRUE)) %>%
  transpose()
spi_fit <- as.array(spi_list$fitted, make.names = TRUE)

test <- as.tibble(unlist(spi_fit))

cot <- as.tibble(spi_fit$cot)

# Next Steps - work with dates to join the lists???
by_sta_date <- sta_grp %>% 
  select(station, date) %>% 
    split(.$station) %>%
  transpose() # invert the list
multi_join <- function(list_of_loaded_data, join_func, ...){
    require("dplyr")
    output <- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)
    return(output)
}
merged_data <- multi_join(spi_list, full_join)

by_sta_date <- by_sta_date$date # drop the unneeded character elements

test <- as.tibble(import("data/stations_monthly.csv")) %>% 
  select(date, cot)

Date <- test %>% 
  select(date) %>%
  mutate(date = ymd(date)) 

test <- test %>%
  ts(end = c(2018, 05), frequency = 12)

spi_list  <- spi(test[,'cot'], 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

test_coeff <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi_test <- as.tibble(spi_list$fitted) %>%
  rename(cot = "Series 1")
spi_test <- bind_cols(Date, spi_test) 

# visual check of output for cottonwood SPI
ggplot(spi_test, aes(date, cot)) + 
  geom_line() +
  theme_classic() + 
  labs(title = "Standardized Precipitation Index (SPI)",
       subtitle = "Cottonwood, SD") +
       xlab("") + 
       ylab("SPI-value") +
  NULL 
```

```{r testing-DEPR, include=FALSE, eval=FALSE}
# Mini-library
library("tidyverse") 
library("lubridate") 
library("rio") 
library("SPEI")

# load data
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  arrange(date)
  
# prepare date for joining later
Date <- sta_mon %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use a single station - cot
sta_cot_ts <- sta_mon %>% 
  select(cot) %>%
  ts(end = c(2018, 05), frequency = 12)

spi_list  <- spi(sta_cot_ts, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_cot <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_ts <- as.tibble(spi_list$fitted)  
spi1_cot_ts <- bind_cols(Date, spi1_cot_ts) 

# Results
# as.tibble(summary(spi1_cot_ts))
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "  

#sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
#sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
 # arrange(date)

# prepare to rejoin date
#Date <- sta_mon %>% 
#  select(date) %>%
#  mutate(date = ymd(date)) %>%
#  arrange(date)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data for testing Lubridate object
sta_cot_lub <- sta_mon %>% 
  select(cot) 

spi_list  <- spi(sta_cot_lub, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 
 
spi1_coeff_cot_lub <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_lub <- as.tibble(spi_list$fitted) 
spi1_cot_lub <- bind_cols(Date, spi1_cot_lub) 
rm(sta_cot_lub)

# Results
# as.tibble(summary(spi1_cot_lub))
# A tibble: 12 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "   

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use lubridate-created object for date for a batch process
# results are as above for 'cot' but have -Inf for 'rap' 

#library("tidyverse") 
#library("lubridate")
#library("rio") 
#library("SPEI")

#sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
#sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
#  arrange(date)

# prepare to rejoin date
# Date <- sta_mon %>% 
#  select(date) %>%
#  mutate(date = ymd(date)) %>%
#  arrange(date)

# prepare data for batch
sta_all <- sta_mon %>% 
  select(-c(date, year, month))  
 
spi_list  <- spi(sta_all, 1,  
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_all <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_all <- as.tibble(spi_list$fitted) 
spi1_all <- bind_cols(Date, spi1_all) 

# Selected Results
summary(spi1_all$cot)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -2.40240 -0.69324  0.02341  0.01383  0.68317  3.30822 

summary(spi1_all$date)
# Min.      1st Qu.       Median     Mean      3rd Qu.       Max.
# "1909-06-01" "36-08-24" "63-11-16" "63-11-16" "91-02-08" "2018-05-01" 
summary(spi1_all$rap)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#   -Inf -0.6472  0.0022    -Inf  0.6657  2.9000     467 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use only 'rap' with NA vals 
# Summary: It looks like the "bug" is in handling the na.rm?
# Work around: Split, apply combine.

# Use lubridate-created object for date for a batch process
# results are as above for 'cot' but have -Inf for 'rap' 

#library("tidyverse") 
#library("lubridate")
#library("rio") 
#library("SPEI")

#sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
#sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
#  arrange(date)

# prepare to rejoin date
#Date <- sta_mon %>% 
#  select(date) %>%
 # mutate(date = ymd(date)) %>%
 # arrange(date)

# prepare data for 'rap' only
sta_rap <- sta_mon %>% 
  select(rap) 

spi_list  <- spi(sta_rap, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble()  

spi1_rap <- as.tibble(spi_list$fitted) 
spi1_rap <- bind_cols(Date, spi1_rap) 

# Results
as.tibble(summary(spi1_rap))
# A tibble: 14 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  " 
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     date" NA                    
# 8 ""    "     rap"  "Min.   :   -Inf  "   
# 9 ""    "     rap"  "1st Qu.:-0.6472  "   
#10 ""    "     rap"  "Median : 0.0022  "   
#11 ""    "     rap"  "Mean   :   -Inf  "   
#12 ""    "     rap"  "3rd Qu.: 0.6657  "   
#13 ""    "     rap"  "Max.   : 2.9000  "   
#14 ""    "     rap"  "NA's   :467  "       
```

```{r spi-cot-DEPR} 
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Cottonwood, SD. 
# Code is mostly following the SPI vignette. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Add Cottonwoods dataset and change to time series 
sta_cot <- sta_mon %>% 
  select(date, cot) %>% 
  arrange(date) 

# Calculate Cottonwood SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot  <- spi(sta_cot[,'cot'], 1, distribution = 'PearsonIII') 
spi_2cot  <- spi(sta_cot[,'cot'], 2, distribution = 'PearsonIII') 
spi_3cot  <- spi(sta_cot[,'cot'], 3, distribution = 'PearsonIII') 
spi_4cot  <- spi(sta_cot[,'cot'], 4, distribution = 'PearsonIII') 
spi_5cot  <- spi(sta_cot[,'cot'], 5, distribution = 'PearsonIII') 
spi_6cot  <- spi(sta_cot[,'cot'], 6, distribution = 'PearsonIII') 
spi_9cot  <- spi(sta_cot[,'cot'], 9, distribution = 'PearsonIII') 
spi_12cot <- spi(sta_cot[,'cot'], 12, distribution = 'PearsonIII')  
spi_18cot <- spi(sta_cot[,'cot'], 18, distribution = 'PearsonIII') 
spi_24cot <- spi(sta_cot[,'cot'], 24, distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- as.tibble(spi_1cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_2cot_coeff <- as.tibble(spi_2cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_3cot_coeff <- as.tibble(spi_3cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_4cot_coeff <- as.tibble(spi_4cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_5cot_coeff <- as.tibble(spi_5cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_6cot_coeff <- as.tibble(spi_6cot$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9cot_coeff <- as.tibble(spi_9cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_12cot_coeff <- as.tibble(spi_12cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_18cot_coeff <- as.tibble(spi_18cot$coefficients) %>% 
  t() %>% as.tibble() 

spi_24cot_coeff <- as.tibble(spi_24cot$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1cot_coeff <- spi_1cot_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2cot_coeff <- spi_2cot_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3cot_coeff <- spi_3cot_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4cot_coeff <- spi_4cot_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5cot_coeff <- spi_5cot_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6cot_coeff <- spi_6cot_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9cot_coeff <- spi_9cot_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12cot_coeff <- spi_12cot_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18cot_coeff <- spi_18cot_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24cot_coeff <- spi_24cot_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 

# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1cot_coeff, spi_2cot_coeff) 
rm(spi_1cot_coeff, spi_2cot_coeff) 

prep2 <- bind_cols(prep1, spi_3cot_coeff) 
rm(prep1, spi_3cot_coeff) 

prep3 <- bind_cols(prep2, spi_4cot_coeff) 
rm(prep2, spi_4cot_coeff) 

prep4 <- bind_cols(prep3, spi_5cot_coeff) 
rm(prep3, spi_5cot_coeff) 

prep5 <- bind_cols(prep4, spi_6cot_coeff) 
rm(prep4, spi_6cot_coeff) 

prep6 <- bind_cols(prep5, spi_9cot_coeff) 
rm(prep5, spi_9cot_coeff) 

prep7 <- bind_cols(prep6, spi_12cot_coeff) 
rm(prep6, spi_12cot_coeff) 

prep8 <- bind_cols(prep7, spi_18cot_coeff) 
rm(prep7, spi_18cot_coeff) 

spi_coeff_cot <- bind_cols(prep8, spi_24cot_coeff) 
rm(prep8, spi_24cot_coeff, sta_cot) 

spi_coeff_cot <- rownames_to_column(spi_coeff_cot, "month")

# export(spi_coeff_cot, "data/spi_coeff_cot.csv") 
# rm(spi_coeff_cot)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi_1cot <- as.tibble(spi_1cot$fitted) %>% 
  mutate(duration = 1) 

spi_2cot <- as.tibble(spi_2cot$fitted) %>% 
  mutate(duration = 2)

spi_3cot <- as.tibble(spi_3cot$fitted) %>% 
  mutate(duration = 3) 

spi_4cot <- as.tibble(spi_4cot$fitted) %>% 
  mutate(duration = 4) 

spi_5cot <- as.tibble(spi_5cot$fitted) %>% 
  mutate(duration = 5) 

spi_6cot <- as.tibble(spi_6cot$fitted) %>% 
  mutate(duration = 6) 

spi_9cot <- as.tibble(spi_9cot$fitted) %>% 
  mutate(duration = 9)

spi_12cot <- as.tibble(spi_12cot$fitted) %>% 
  mutate(duration = 12) 

spi_18cot <- as.tibble(spi_18cot$fitted) %>% 
  mutate(duration = 18) 

spi_24cot <- as.tibble(spi_24cot$fitted) %>% 
  mutate(duration = 24) 

# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1cot  <-  bind_cols(date, spi_1cot) 
spi_2cot  <-  bind_cols(date, spi_2cot) 
spi_3cot  <-  bind_cols(date, spi_3cot) 
spi_4cot  <-  bind_cols(date, spi_4cot) 
spi_5cot  <-  bind_cols(date, spi_5cot) 
spi_6cot  <-  bind_cols(date, spi_6cot) 
spi_9cot  <-  bind_cols(date, spi_9cot) 
spi_12cot <-  bind_cols(date, spi_12cot) 
spi_18cot <-  bind_cols(date, spi_18cot) 
spi_24cot <-  bind_cols(date, spi_24cot) 

# join the SPI data by wide and long
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1- & 2-month
intr1 <- bind_rows(spi_1cot, spi_2cot) 

#intc1 <- bind_cols(spi_1cot, spi_2cot) 
#intc1 <- intc1 %>%
#  select(-c(date1, duration, duration1)) %>%
#  rename(cot_1 = "Series 1") %>%
#  rename(cot_2 = "Series 11")
rm(spi_1cot, spi_2cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3-month
intr2 <- bind_rows(intr1, spi_3cot) 

#intc2 <- bind_cols(intc1, spi_3cot) 
#intc2 <- intc2 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_3 = "Series 1") 
rm(intr1, intc1, spi_3cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 4-month
intr3 <- bind_rows(intr2, spi_4cot) 

#intc3 <- bind_cols(intc2, spi_4cot) 
#intc3 <- intc3 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_4 = "Series 1") 
rm(intr2, intc2, spi_4cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 5-month
intr4 <- bind_rows(intr3, spi_5cot) 

#intc4 <- bind_cols(intc3, spi_5cot) 
#intc4 <- intc4 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_5 = "Series 1") 
rm(intr3, intc3, spi_5cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 6-month
intr5 <- bind_rows(intr4, spi_6cot) 

#intc5 <- bind_cols(intc4, spi_6cot) 
#intc5 <- intc5 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_6 = "Series 1") 
rm(intr4, intc4, spi_6cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9-month
intr6 <- bind_rows(intr5, spi_9cot) 

#intc6 <- bind_cols(intc5, spi_9cot) 
#intc6 <- intc6 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_9 = "Series 1") 
rm(intr5, intc5, spi_9cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 12-month
intr7 <- bind_rows(intr6, spi_12cot) 

#intc7 <- bind_cols(intc6, spi_12cot) 
#intc7 <- intc7 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_12 = "Series 1") 
rm(intr6, intc6, spi_12cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 18-month
intr8 <- bind_rows(intr7, spi_18cot) 

#intc8 <- bind_cols(intc7, spi_18cot) 
#intc8 <- intc8 %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_18 = "Series 1") 
rm(intr7, intc7, spi_18cot) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 24-month
spi_cot <- bind_rows(intr8, spi_24cot) 

#spi_cot <- bind_cols(intc8, spi_24cot) 
#spi_cot <- spi_cot %>%
#  select(-c(date1, duration)) %>%
#  rename(cot_24 = "Series 1") 

# clean up
rm(intr8, intc8, spi_24cot)  
```

```{r spi-oel-DEPR}   
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Oelrichs  
 
# Add Oelrichs dataset and change to time series 
sta_oel <- sta_mon %>% 
  arrange(date) %>% 
  select(year, month, oel) %>% 
  ts(end = c(2018, 05), frequency = 12) 

# Calculate SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel  <- spi(sta_oel[,'oel'], 1, distribution = 'PearsonIII') 
spi_2oel  <- spi(sta_oel[,'oel'], 2, distribution = 'PearsonIII') 
spi_3oel  <- spi(sta_oel[,'oel'], 3, distribution = 'PearsonIII') 
spi_4oel  <- spi(sta_oel[,'oel'], 4, distribution = 'PearsonIII') 
spi_5oel  <- spi(sta_oel[,'oel'], 5, distribution = 'PearsonIII') 
spi_6oel  <- spi(sta_oel[,'oel'], 6, distribution = 'PearsonIII') 
spi_9oel  <- spi(sta_oel[,'oel'], 9, distribution = 'PearsonIII') 
spi_12oel <- spi(sta_oel[,'oel'], 12, distribution = 'PearsonIII')  
spi_18oel <- spi(sta_oel[,'oel'], 18, distribution = 'PearsonIII') 
spi_24oel <- spi(sta_oel[,'oel'], 24, distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel_coeff <- as.tibble(spi_1oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_2oel_coeff <- as.tibble(spi_2oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_3oel_coeff <- as.tibble(spi_3oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_4oel_coeff <- as.tibble(spi_4oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_5oel_coeff <- as.tibble(spi_5oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_6oel_coeff <- as.tibble(spi_6oel$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9oel_coeff <- as.tibble(spi_9oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_12oel_coeff <- as.tibble(spi_12oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_18oel_coeff <- as.tibble(spi_18oel$coefficients) %>% 
  t() %>% as.tibble() 

spi_24oel_coeff <- as.tibble(spi_24oel$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1oel_coeff <- spi_1oel_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2oel_coeff <- spi_2oel_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3oel_coeff <- spi_3oel_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4oel_coeff <- spi_4oel_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5oel_coeff <- spi_5oel_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6oel_coeff <- spi_6oel_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9oel_coeff <- spi_9oel_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12oel_coeff <- spi_12oel_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18oel_coeff <- spi_18oel_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24oel_coeff <- spi_24oel_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma)  


# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1oel_coeff, spi_2oel_coeff) 
rm(spi_1oel_coeff, spi_2oel_coeff) 

prep2 <- bind_cols(prep1, spi_3oel_coeff) 
rm(prep1, spi_3oel_coeff) 

prep3 <- bind_cols(prep2, spi_4oel_coeff) 
rm(prep2, spi_4oel_coeff) 

prep4 <- bind_cols(prep3, spi_5oel_coeff) 
rm(prep3, spi_5oel_coeff) 

prep5 <- bind_cols(prep4, spi_6oel_coeff) 
rm(prep4, spi_6oel_coeff) 

prep6 <- bind_cols(prep5, spi_9oel_coeff) 
rm(prep5, spi_9oel_coeff) 

prep7 <- bind_cols(prep6, spi_12oel_coeff) 
rm(prep6, spi_12oel_coeff) 
 
prep8 <- bind_cols(prep7, spi_18oel_coeff) 
rm(prep7, spi_18oel_coeff) 

spi_coeff_oel <- bind_cols(prep8, spi_24oel_coeff) 
rm(prep8, spi_24oel_coeff) 

# export(spi_coeff_oel, "data/spi_coeff_oel.csv") 
# rm(spi_coeff_oel, sta_oel)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This code chunk calculates SPI fitted values for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Oelrichs.  
# It is a continuation of 'spi-algorithm-coeff-oel'

# extract SPI values & add duration
spi_1oel <- as.tibble(spi_1oel$fitted) %>% 
  mutate(duration = 1)

spi_2oel <- as.tibble(spi_2oel$fitted) %>% 
  mutate(duration = 2)

spi_3oel <- as.tibble(spi_3oel$fitted) %>% 
  mutate(duration = 3) 

spi_4oel <- as.tibble(spi_4oel$fitted) %>% 
  mutate(duration = 4) 

spi_5oel <- as.tibble(spi_5oel$fitted) %>% 
  mutate(duration = 5) 

spi_6oel <- as.tibble(spi_6oel$fitted) %>% 
  mutate(duration = 6) 

spi_9oel <- as.tibble(spi_9oel$fitted) %>% 
  mutate(duration = 9)

spi_12oel <- as.tibble(spi_12oel$fitted) %>% 
  mutate(duration = 12) 

spi_18oel <- as.tibble(spi_18oel$fitted) %>% 
  mutate(duration = 18) 

spi_24oel <- as.tibble(spi_24oel$fitted) %>% 
  mutate(duration = 24) 

# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1oel  <-  bind_cols(date, spi_1oel) 
spi_2oel  <-  bind_cols(date, spi_2oel) 
spi_3oel  <-  bind_cols(date, spi_3oel) 
spi_4oel  <-  bind_cols(date, spi_4oel) 
spi_5oel  <-  bind_cols(date, spi_5oel) 
spi_6oel  <-  bind_cols(date, spi_6oel) 
spi_9oel  <-  bind_cols(date, spi_9oel) 
spi_12oel <-  bind_cols(date, spi_12oel) 
spi_18oel <-  bind_cols(date, spi_18oel) 
spi_24oel <-  bind_cols(date, spi_24oel) 

# join the SPI data by wide and long
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1- & 2-month
intr1 <- bind_rows(spi_1oel, spi_2oel) 

intc1 <- bind_cols(spi_1oel, spi_2oel) 
intc1 <- intc1 %>%
  select(-c(date1, duration, duration1)) %>%
  rename(oel_1 = "Series 1") %>%
  rename(oel_2 = "Series 11")
rm(spi_1oel, spi_2oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3-month
intr2 <- bind_rows(intr1, spi_3oel) 

intc2 <- bind_cols(intc1, spi_3oel) 
intc2 <- intc2 %>%
  select(-c(date1, duration)) %>%
  rename(oel_3 = "Series 1") 
rm(intr1, intc1, spi_3oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 4-month
intr3 <- bind_rows(intr2, spi_4oel) 

intc3 <- bind_cols(intc2, spi_4oel) 
intc3 <- intc3 %>%
  select(-c(date1, duration)) %>%
  rename(oel_4 = "Series 1") 
rm(intr2, intc2, spi_4oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 5-month
intr4 <- bind_rows(intr3, spi_5oel) 

intc4 <- bind_cols(intc3, spi_5oel) 
intc4 <- intc4 %>%
  select(-c(date1, duration)) %>%
  rename(oel_5 = "Series 1") 
rm(intr3, intc3, spi_5oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 6-month
intr5 <- bind_rows(intr4, spi_6oel) 

intc5 <- bind_cols(intc4, spi_6oel) 
intc5 <- intc5 %>%
  select(-c(date1, duration)) %>%
  rename(oel_6 = "Series 1") 
rm(intr4, intc4, spi_6oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9-month
intr6 <- bind_rows(intr5, spi_9oel) 

intc6 <- bind_cols(intc5, spi_9oel) 
intc6 <- intc6 %>%
  select(-c(date1, duration)) %>%
  rename(oel_9 = "Series 1") 
rm(intr5, intc5, spi_9oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 12-month
intr7 <- bind_rows(intr6, spi_12oel) 

intc7 <- bind_cols(intc6, spi_12oel) 
intc7 <- intc7 %>%
  select(-c(date1, duration)) %>%
  rename(oel_12 = "Series 1") 
rm(intr6, intc6, spi_12oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 18-month
intr8 <- bind_rows(intr7, spi_18oel) 

intc8 <- bind_cols(intc7, spi_18oel) 
intc8 <- intc8 %>%
  select(-c(date1, duration)) %>%
  rename(oel_18 = "Series 1") 
rm(intr7, intc7, spi_18oel) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 24-month
spi_oel_gath <- bind_rows(intr8, spi_24oel) 
spi_oel_gath <- spi_oel_gath %>%
  rename(spi_oel = "Series 1")

spi_oel <- bind_cols(intc8, spi_24oel) 
spi_oel <- spi_oel %>%
  select(-c(date1, duration)) %>%
  rename(oel_24 = "Series 1") 

rm(intr8, intc8, spi_24oel, sta_oel) 
# Next steps:
# distribution = 'PersonIII',
```

```{r spi-rap-DEPR} 
# This code chunk calculates SPI coefficients for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Rapid City, SD.

# Add Rapid City dataset and change to time series 
sta_rap <- sta_mon %>% 
  arrange(date) %>% 
  select(year, month, rap) %>% 
  ts(end = c(2018, 05), frequency = 12)

# Calculate SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
# Added na.rm = TRUE to handle the missing vals
spi_1rap  <- spi(sta_rap[,'rap'], 1, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_2rap  <- spi(sta_rap[,'rap'], 2, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_3rap  <- spi(sta_rap[,'rap'], 3, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_4rap  <- spi(sta_rap[,'rap'], 4, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_5rap  <- spi(sta_rap[,'rap'], 5, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_6rap  <- spi(sta_rap[,'rap'], 6, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_9rap  <- spi(sta_rap[,'rap'], 9, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_12rap <- spi(sta_rap[,'rap'], 12, na.rm = TRUE, 
                 distribution = 'PearsonIII')  
spi_18rap <- spi(sta_rap[,'rap'], 18, na.rm = TRUE, 
                 distribution = 'PearsonIII') 
spi_24rap <- spi(sta_rap[,'rap'], 24, na.rm = TRUE, 
                 distribution = 'PearsonIII') 

# save coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1rap_coeff <- as.tibble(spi_1rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_2rap_coeff <- as.tibble(spi_2rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_3rap_coeff <- as.tibble(spi_3rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_4rap_coeff <- as.tibble(spi_4rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_5rap_coeff <- as.tibble(spi_5rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_6rap_coeff <- as.tibble(spi_6rap$coefficients) %>% 
  t() %>%
  as.tibble() 

spi_9rap_coeff <- as.tibble(spi_9rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_12rap_coeff <- as.tibble(spi_12rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_18rap_coeff <- as.tibble(spi_18rap$coefficients) %>% 
  t() %>% as.tibble() 

spi_24rap_coeff <- as.tibble(spi_24rap$coefficients) %>% 
  t() %>% as.tibble() 

# Rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi_1rap_coeff <- spi_1rap_coeff %>% 
  rename(mu_1 = mu) %>%
   rename(sigma_1 = sigma) %>% 
   rename(gamma_1 = gamma)
  
spi_2rap_coeff <- spi_2rap_coeff %>% 
  rename(mu_2 = mu) %>%
   rename(sigma_2 = sigma) %>% 
   rename(gamma_2 = gamma) 

spi_3rap_coeff <- spi_3rap_coeff %>% 
  rename(mu_3 = mu) %>%
   rename(sigma_3 = sigma) %>% 
   rename(gamma_3 = gamma) 

spi_4rap_coeff <- spi_4rap_coeff %>% 
  rename(mu_4 = mu) %>%
   rename(sigma_4 = sigma) %>% 
   rename(gamma_4 = gamma) 

spi_5rap_coeff <- spi_5rap_coeff %>% 
  rename(mu_5 = mu) %>%
   rename(sigma_5 = sigma) %>% 
   rename(gamma_5 = gamma) 

spi_6rap_coeff <- spi_6rap_coeff %>% 
  rename(mu_6 = mu) %>%
   rename(sigma_6 = sigma) %>% 
   rename(gamma_6 = gamma) 

spi_9rap_coeff <- spi_9rap_coeff %>% 
  rename(mu_9 = mu) %>%
   rename(sigma_9 = sigma) %>% 
   rename(gamma_9 = gamma) 

spi_12rap_coeff <- spi_12rap_coeff %>% 
  rename(mu_12 = mu) %>%
   rename(sigma_12 = sigma)  %>% 
   rename(gamma_12 = gamma) 

spi_18rap_coeff <- spi_18rap_coeff %>% 
  rename(mu_18 = mu) %>%
   rename(sigma_18 = sigma) %>% 
   rename(gamma_18 = gamma) 

spi_24rap_coeff <- spi_24rap_coeff %>% 
  rename(mu_24 = mu) %>%
   rename(sigma_24 = sigma) %>% 
   rename(gamma_24 = gamma) 


# Join the values 
# THIS CODE COULD BE DONE MUCH BETTER! 
prep1 <- bind_cols(spi_1rap_coeff, spi_2rap_coeff) 
rm(spi_1rap_coeff, spi_2rap_coeff) 

prep2 <- bind_cols(prep1, spi_3rap_coeff) 
rm(prep1, spi_3rap_coeff) 

prep3 <- bind_cols(prep2, spi_4rap_coeff) 
rm(prep2, spi_4rap_coeff) 

prep4 <- bind_cols(prep3, spi_5rap_coeff) 
rm(prep3, spi_5rap_coeff) 

prep5 <- bind_cols(prep4, spi_6rap_coeff) 
rm(prep4, spi_6rap_coeff) 

prep6 <- bind_cols(prep5, spi_9rap_coeff) 
rm(prep5, spi_9rap_coeff) 

prep7 <- bind_cols(prep6, spi_12rap_coeff) 
rm(prep6, spi_12rap_coeff) 

prep8 <- bind_cols(prep7, spi_18rap_coeff) 
rm(prep7, spi_18rap_coeff) 

spi_coeff_rap <- bind_cols(prep8, spi_24rap_coeff) 
rm(prep8, spi_24rap_coeff, sta_rap) 

# export(spi_coeff_rap, "data/spi_coeff_rap.csv") 
# rm(spi_coeff_rap)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This code chunk calculates SPI fitted values for 
# 1, 2, 3, 4, 5, 6, 9, 12, 24 months for Rapid City, SD. 
# It is a continuation of 'spi-algorithm-coeff-rap'

# extract SPI values & add duration
spi_1rap <- as.tibble(spi_1rap$fitted) %>% 
  mutate(duration = 1)

spi_2rap <- as.tibble(spi_2rap$fitted) %>% 
  mutate(duration = 2)

spi_3rap <- as.tibble(spi_3rap$fitted) %>% 
  mutate(duration = 3) 

spi_4rap <- as.tibble(spi_4rap$fitted) %>% 
  mutate(duration = 4) 

spi_5rap <- as.tibble(spi_5rap$fitted) %>% 
  mutate(duration = 5) 

spi_6rap <- as.tibble(spi_6rap$fitted) %>% 
  mutate(duration = 6) 

spi_9rap <- as.tibble(spi_9rap$fitted) %>% 
  mutate(duration = 9)

spi_12rap <- as.tibble(spi_12rap$fitted) %>% 
  mutate(duration = 12) 

spi_18rap <- as.tibble(spi_18rap$fitted) %>% 
  mutate(duration = 18) 

spi_24rap <- as.tibble(spi_24rap$fitted) %>% 
  mutate(duration = 24) 

# add date to the SPI data
date <- sta_mon %>%
  select(date) %>%
  arrange(date)

spi_1rap  <-  bind_cols(date, spi_1rap) 
spi_2rap  <-  bind_cols(date, spi_2rap) 
spi_3rap  <-  bind_cols(date, spi_3rap) 
spi_4rap  <-  bind_cols(date, spi_4rap) 
spi_5rap  <-  bind_cols(date, spi_5rap) 
spi_6rap  <-  bind_cols(date, spi_6rap) 
spi_9rap  <-  bind_cols(date, spi_9rap) 
spi_12rap <-  bind_cols(date, spi_12rap) 
spi_18rap <-  bind_cols(date, spi_18rap) 
spi_24rap <-  bind_cols(date, spi_24rap) 

# join the SPI data by wide and long
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1- & 2-month
intr1 <- bind_rows(spi_1rap, spi_2rap) 

intc1 <- bind_cols(spi_1rap, spi_2rap) 
intc1 <- intc1 %>%
  select(-c(date1, duration, duration1)) %>%
  rename(int_1 = "Series 1") %>%
  rename(int_2 = "Series 11")
rm(spi_1rap, spi_2rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3-month
intr2 <- bind_rows(intr1, spi_3rap) 

intc2 <- bind_cols(intc1, spi_3rap) 
intc2 <- intc2 %>%
  select(-c(date1, duration)) %>%
  rename(rap_3 = "Series 1") 
rm(intr1, intc1, spi_3rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 4-month
intr3 <- bind_rows(intr2, spi_4rap) 

intc3 <- bind_cols(intc2, spi_4rap) 
intc3 <- intc3 %>%
  select(-c(date1, duration)) %>%
  rename(rap_4 = "Series 1") 
rm(intr2, intc2, spi_4rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 5-month
intr4 <- bind_rows(intr3, spi_5rap) 

intc4 <- bind_cols(intc3, spi_5rap) 
intc4 <- intc4 %>%
  select(-c(date1, duration)) %>%
  rename(rap_5 = "Series 1") 
rm(intr3, intc3, spi_5rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 6-month
intr5 <- bind_rows(intr4, spi_6rap) 

intc5 <- bind_cols(intc4, spi_6rap) 
intc5 <- intc5 %>%
  select(-c(date1, duration)) %>%
  rename(rap_6 = "Series 1") 
rm(intr4, intc4, spi_6rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9-month
intr6 <- bind_rows(intr5, spi_9rap) 

intc6 <- bind_cols(intc5, spi_9rap) 
intc6 <- intc6 %>%
  select(-c(date1, duration)) %>%
  rename(rap_9 = "Series 1") 
rm(intr5, intc5, spi_9rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 12-month
intr7 <- bind_rows(intr6, spi_12rap) 

intc7 <- bind_cols(intc6, spi_12rap) 
intc7 <- intc7 %>%
  select(-c(date1, duration)) %>%
  rename(rap_12 = "Series 1") 
rm(intr6, intc6, spi_12rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 18-month
intr8 <- bind_rows(intr7, spi_18rap) 

intc8 <- bind_cols(intc7, spi_18rap) 
intc8 <- intc8 %>%
  select(-c(date1, duration)) %>%
  rename(rap_18 = "Series 1") 
rm(intr7, intc7, spi_18rap) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 24-month
spi_rap_gath <- bind_rows(intr8, spi_24rap) 
spi_rap_gath <- spi_rap_gath %>%
  rename(spi_rap = "Series 1")

spi_rap <- bind_cols(intc8, spi_24rap) 
spi_rap <- spi_rap %>%
  select(-c(date1, duration)) %>%
  rename(rap_24 = "Series 1") 

rm(intr8, intc8, spi_24rap) 
# Next steps:
# distribution = 'PersonIII'
```



```{r exploratory-PCA-gages}
# Overview
# ~~~~~~~~
# PCA is a method to summarize data using fewer variables by 
# constructing new linear descriptors from variables. The new 
# linear discriptors describe maximum variation across observations. 
# The new discriptors can be used to predict, or "reconstruct",
# observations as well as possible.

# PCA finds the axis the describes the maximum covariance and 
# projecting all of the observations points onto this line. The line 
# called the "first principal component" simultanesous describes 
# maximal variation of values along the line & minimizes the 
# reconstruction error 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# The original data is saved in a long format with rows as daily 
# observations and columns as variables

# glimpse(gage_raw)
# Variables: 26
# count_yr              <int> 67, 67, 67... count of years of record
# sta                   <chr> "bat_bhr"...  station name
# site_no               <int> 6406500...    USGS site number 
# station_nm            <chr> "BATTLE CR BELOW HERMOSA,SD"... 
# dec_lat_va            <dbl> 43.72543...   latitude 
# dec_long_va           <dbl> -102.9061...  longitude 
# state_cd              <int> 46, 46...     fips? code for State  
# county_cd             <int> 103, 103...   fips? code for County  
# alt_va                <dbl> 2800.32...    altitude in feet 
# drain_area_va         <dbl> 284, 284...   drainage area in sq mi. 
# contrib_drain_area_va <dbl> 284, 284...   contrib. da in sq mi.
# min_yr                <int> 1951...       first year of record 
# max_yr                <int> 2017...       last year of record 
# Date                  <chr> "1991-10-01"  date 
# i                     <int> 2191, 2192... count from 1st day of rec. 
# Q                     <dbl> 0.12518933... discharge in cms
# Julian                <int> 51772...      Julian calendar date  
# Month                 <int> 10, 10...     month 
# Day                   <int> 275, 276...   day  
# DecYear               <dbl> 1991.749...   decimal year  
# MonthSeq              <int> 1702...       NA 
# waterYear             <int> 1992...       water year 
# Qualifier             <chr> "A", "A"...   data quality code  
# LogQ                  <dbl> -2.077928...  log of zero-adj discharge 
# Q7                    <dbl> 0.14420207... 7-day moving average  
# Q30                   <dbl> 0.1342507...  30-day moving average 

# Prepare data for exploratory PCA -----------------------------------
# 1. Import data & drop Cheyenne River at Angustora with ~50% NA vals 
gage_raw <- import("data/gage_92_97.csv") %>%  
  filter(sta != "chr_ang")
gage_meta <- import("data/gage_meta_92_97.csv") 

# 2. Eliminate the effects of different sizes of watersheds:  
#    I calculated daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

# 3. Estimate or remove missing values in the data 
#    The NA vals created by calculating Q7 & Q30 need to be removed or 
#    estimated.  Removal seems the more conservative approach - 
#    especially as the NA values are very small given the number of 
#    observations of 365 obs/yr * 7 years. 

library(dplyr)
gage_scaled <- gage_raw %>% 
  as.tibble() %>% 
  clean_names() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na() 

# 4. Transform using BoxCox to approach normality 
# daily flow data are highly skewed & tend to approach a logrithmic 
# distribution.  BoxCox transformation utilizes a lamda value to 
# transform a dataset to approach a normal distribution.
# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

# find lambda values for BoxCox transformation  
lambda_q1 <- BoxCox.lambda(gage_scaled$q1_depth) 
lambda_q7 <- BoxCox.lambda(gage_scaled$q7_depth) 
lambda_q30 <- BoxCox.lambda(gage_scaled$q30_depth) 

# lambda vals
#  q1 ==> 0.013 
#  q7 ==> 0.109 
# q30 ==> 0.202

# 5.Standardize the data:
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

# Apply transformation and scale data
gage_scaled <- gage_scaled %>% 
  mutate(q1_tr = BoxCox(gage_scaled$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(gage_scaled$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(gage_scaled$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_scaled = (q1_tr - q1_mean)/q1_sd) %>% 
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_scaled = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_scaled = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
         q1_scaled, q7_scaled, q30_scaled) 

# glimpse(gage_scale) 
# sta        <chr> "bat_bhr", "bat_bhr", "bat_bhr", "bat_bhr", ... 
# date       <chr> "1991-10-01", "1991-10-02", "1991-10-03", "1... 
# q1_depth   <dbl> 14.704930, 14.704930, 16.700614, 18.696298, ... 
# q7_depth   <dbl> 16.93820, 16.46303, 16.32048, 16.46303, 16.4... 
# q30_depth  <dbl> 15.76929, 15.81364, 15.93560, 16.12408, 16.2... 
# q1_scaled  <dbl> -0.4163661, -0.4163661, -0.3542800, -0.29912... 
# q7_scaled  <dbl> -0.5215705, -0.5363362, -0.5408399, -0.53633... 
# q30_scaled <dbl> -0.7615155, -0.7600352, -0.7559816, -0.74976... 

# Select numeric data for PCA input &  names to connect to result 
# prcomp() requires a dataset with only the variables for the 
# unsupervised classification 
pca_input <- gage_scaled %>% 
  dplyr::select(q1_scaled, q7_scaled, q30_scaled) 

pca_meta <- gage_scaled %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth) 

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE)

# this plots the PCA biplot - takes a LONG time
# biplot(gage_pc, scale = 0)

# Gather PCA results----
# The broom package is used to gather results into tibbles.

# Eigenvectors
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

#gage_pc_eigen
#    PC std.dev percent cumulative
#  <dbl>   <dbl>   <dbl>      <dbl>
#1     1   1.68  0.941        0.941
#2     2   0.385 0.0494       0.990
#3     3   0.170 0.00958      1  
# This means that 94% of covarience is explained by PCA1 & 
# 5% of varience by PCA2

# PCA variables 
# these are the loadings on the PCA axes.  I dropped the 3rd PCA 
# because it's not useful
pca_vars <-  tidy(pca_matrix, matrix = "variables") 
pca_vars <- pca_vars %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var))

#gage_pc_vars
#var           PC  value
#  <fct>      <dbl>  <dbl>
#1 q1_scaled      1  0.576
#2 q7_scaled      1  0.589
#3 q30_scaled     1  0.567
#4 q1_scaled      2  0.598
#5 q7_scaled      2  0.169
#6 q30_scaled     2 -0.784 
# This explains approximately equal loadings of Q1, Q7, Q30 on PCA1 & 
# large positive loading of Q1 & large negative loading of Q30. 

# Bind sample vals to PCA matrix 
pca_au <- augment(pca_matrix, data = pca_input) 
pca_au <- bind_cols(pca_meta, pca_au)
rm(pca_meta)

# Calculate mean eigenvectors 
# this summarizes the individual points by station - used in plots
pca_summary <- pca_au %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
           q1_mean = mean(log10(q1_depth)), 
            q7_mean = mean(log10(q7_depth)),
            q30_mean = mean(log10(q30_depth)), 
            q1_sd = sd(log10(q1_depth)), 
            q7_sd = sd(log10(q7_depth)),
            q30_sd = sd(log10(q30_depth))) %>% 
  mutate(q1_minus_q30 = q1_mean - q30_mean) %>% 
  ungroup()


# PCA1 Interpretation-------------------------- 
# The PCA1 axis describes increase in the watershed export coefficient
pca_au_small <- pca_au %>% 
  filter(sta == "hor_oel" | 
           sta == "whi_ogl" |
           sta == "whi_kad" |
           sta == "blp_bel" |
           sta == "wkc_wok" |
           sta == "lwr_mar" |
           sta == "lcr_vet" |
           sta == "lwr_abv" |
           sta == "fal_hot" |
         sta == "lcr_bel") %>% 
  dplyr::select(sta, q1_depth, q7_depth, q30_depth) 

# set levels for plotting
pca_au_small$sta <- as.factor(pca_au_small$sta)
pca_au_small$sta <- factor(pca_au_small$sta, 
                          levels = c("hor_oel", "whi_ogl", 
                                     "whi_kad", "blp_bel",
                                     "wkc_wok", "lwr_mar", 
                                     "lcr_vet", "lwr_abv", 
                                     "fal_hot", "lcr_bel")
                          )

# plot gradient along axis
ggplot(pca_au_small, aes(sta, q7_depth)) +
  geom_boxplot() +
  scale_y_log10() + 
  theme_classic() +
  xlab("") +
  ylab("log Q7 depth")

# Examine the second axis loadings 
pca_au_small <- pca_au %>% 
      filter(sta == "hor_oel" | 
           sta == "wcc_ogl" |
           sta == "blp_bel" |
         sta == "lcr_bel") 


pca_input_small <- gage_scaled %>% 
      filter(sta == "hor_oel" | 
           sta == "wcc_ogl" |
           sta == "blp_bel" |
         sta == "lcr_bel") %>%
  dplyr::select(q1_scaled, q7_scaled, q30_scaled) 

pca_meta_small <- gage_scaled %>% 
      filter(sta == "hor_oel" | 
           sta == "wcc_ogl" |
           sta == "blp_bel" |
         sta == "lcr_bel") %>%
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth) 

#pca_matrix_small <- prcomp(pca_input_small, scale = TRUE)

#pca_au_small <- augment(pca_matrix_small, data = pca_meta_small) 
#pca_au_small <- bind_cols(pca_meta_small, pca_au_small)
#rm(pca_meta_small)

pca_summary_small <- pca_summary %>% 
      filter(sta == "hor_oel" | 
           sta == "wcc_ogl" |
           sta == "blp_bel" |
         sta == "lcr_bel")

ggplot() +
  geom_jitter(data = pca_au_small, 
             mapping = aes(x = .fittedPC1, y = .fittedPC2, 
                           color = factor(sta))) +
  geom_point(data = pca_summary, 
             mapping = aes(PC1_mean, PC2_mean)) + 
    geom_text(data = pca_summary_small,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 
  



#fviz_pca_ind(prcomp(gage_matrix_small), 
#             title = "PCA - Iris data", 
#             habillage = gage_matrix_small_name$sta, 
#             palette = "jco", geom = "point", 
#             ggtheme = theme_classic(), 
#             legend = "bottom") 
```

```{r Model-Based_Clustering}
# The traditional clustering methods, such as hierarchical clustering
# and k-means clustering, are heuristic and are not based on formal 
# models. An alternative is model-based clustering, which consider the 
# data as coming from a distribution that is mixture of two or more 
# clusters 

# (Fraley and Raftery 2002, Fraley #et al. (2012)).

# Model-based clustering uses a soft assignment, where each data point 
# has a probability of belonging to each cluster.  In model-based 
# clustering, the data is considered as coming from a mixture of 
# density.  Each component (i.e. cluster) k is modeled by the normal 
# or Gaussian distribution which is characterized by the parameters:
#   μk\mu_k: mean vector, 
#   ∑k\sum_k: covariance matrix, 
#   An associated probability in the mixture. Each point has a 
#     probability of belonging to each cluster.

# "Old faithful geyser data" Example 

# Load the data 
#library("MASS") 
#data("geyser")

# Scatter plot 
library("ggpubr") 
ggscatter(geyser, x = "duration", y = "waiting") +
geom_density2d()  # Add 2D density 
  

# The plot suggests at least 3 clusters in the mixture. 
# The shape of each of the 3 clusters appears to be approximately 
# elliptical suggesting three bivariate normal distributions. As the 
# 3 ellipses seems to be similar in terms of volume, shape and 
# orientation, we might anticipate that the three components of this 
# mixture might have homogeneous covariance matrices.  

# gage-data - first a summary then the whole set
ggscatter(pca_summary, x = "PC1_mean", y = "PC2_mean") + 
geom_density2d()  # Add 2D density 
    geom_text(label = "sta") 

ggscatter(pca_au, x = ".fittedPC1", y = ".fittedPC2") +
geom_density2d() # Add 2D density 

# The plot suggests 3 clusters in the mixture. 
# The shape of the 3 clusters appears to be approximately 
# elliptical suggesting three bivariate normal distributions. 
# As the ellipses are dissimilar in terms of volume
# Shape and orientation are more homogeneous covariance matrices. 
# Possibly, a VEE?

# Estimating model parameters------
# The model parameters can be estimated using the Expectation-
# Maximization (EM) algorithm initialized by hierarchical model-based 
# clustering. Each cluster k is centered at the means μk\mu_k, with 
# increased density for points near the mean.
# Geometric features (shape, volume, orientation) of each cluster are 
# determined by the covariance matrix ∑k\sum_k.

# Different possible parameterizations of ∑k\sum_k are available in 
# the R package mclust (see ?mclustModelNames).
# The available model options, in mclust package, are represented by 
# identifiers including: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV 
# and VVV. 

# The first identifier refers to volume, the second to shape and the 
# third to orientation. E stands for "equal", V for "variable" and I 
# for "coordinate axes".

# For example:
# EVI denotes a model in which the volumes of all clusters are 
# equal (E), the shapes of the clusters may vary (V), and the 
# orientation is the identity (I) or "coordinate axes. EEE means that 
# the clusters have the same volume, shape and orientation in 
# p-dimensional space. VEI means that the clusters have variable 
# volume, the same shape and orientation equal to coordinate axes.


# Choosing the best model
# The Mclust package uses maximum likelihood to fit all these models, 
# with different covariance matrix parameterizations, for a range of 
# k components.
# The best model is selected using the Bayesian Information Criterion 
# or BIC. A large BIC score indicates strong evidence for the 
# corresponding model.

# Computing model-based clustering in R 
# Model-based clustering can be applied on univariate or multivariate 
# data.

# M-clust Example 
# Model-based clustering on the diabetes data set [mclust package] 
# giving three measurements and the diagnosis for 145 subjects 
# described as follow:
library("mclust") 
data("diabetes") 
head(diabetes, 3)

## class glucose insulin sspg 
## 1 Normal 80 356 124 
## 2 Normal 97 289 117 
## 3 Normal 105 319 143

# class: the diagnosis: normal, chemically diabetic, and overtly 
# diabetic. Excluded from the cluster analysis. 
# glucose: plasma glucose response to oral glucose 
# insulin: plasma insulin response to oral glucose 
# sspg: steady-state plasma glucose (measures insulin resistance)

# Model-based clustering can be computed using the function Mclust() 
library(mclust) 
df <- scale(diabetes[, -1]) # Standardize the data 

mc <- Mclust(df) # Model-based-clustering 

summary(mc) # Print a summary 
## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) 
## log.likelihood   n df  BIC  ICL 
##           -169 145 29 -483 -501 
## 
## Clustering table: 
##  1  2  3  
## 81 36 28 

# You can access the results by:
mc$modelName # Optimal selected model ==> "VVV" 

mc$G # Optimal number of cluster => 3 

head(mc$z, 10) # Probability to belong to a given cluster 
#        [,1]        [,2]         [,3]
#1  0.9906745 0.008991332 3.341728e-04
#2  0.9822128 0.017783229 3.974744e-06
#3  0.9777871 0.022157665 5.527579e-05
#4  0.9774763 0.022312280 2.113743e-04
#5  0.9208978 0.079034264 6.789759e-05
#6  0.9863472 0.012977950 6.748263e-04
#7  0.9429761 0.056900375 1.235066e-04
#8  0.9768888 0.022939764 1.714394e-04
#9  0.9739547 0.025861249 1.840708e-04
#10 0.9841312 0.015858549 1.028654e-05

mc_gage <- Mclust(pca_input) # Model-based-clustering 
summary(mc_gage) # Print a summary 



#---------------------------------------------------- 
#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 
#Mclust VVV (ellipsoidal, varying volume, shape, and orientation) 
#model with 8 components: 

# log.likelihood     n df       BIC       ICL
#      -32617.22 60881 79 -66104.76 -98845.13

#Clustering table:
#    1     2     3     4     5     6     7     8 
#   2669  2287 10456  4485 11405 15100  1158 13321 

# Model-based clustering selected a model with eight components 
# (i.e. clusters). The optimal selected model name is VVV model. 
# That is the eight components are ellipsoidal with varying volume, 
# shape, and orientation. The summary contains also the clustering 
# table specifying the number of observations in each clusters.

# create an input of mean values
mc_input <- pca_summary %>% 
  dplyr::select(q1_mean, q7_mean, q30_mean)

mc_mean <- Mclust(mc_input) # Model-based-clustering 
summary(mc_mean) # Print a summary 


#---------------------------------------------------- 
#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 

#Mclust EEE (ellipsoidal, equal volume, shape and orientation) 
# model with 7 components: 

# log.likelihood  n df      BIC      ICL
#       182.2984 28 33 254.6341 253.5411

#Clustering table:
# 1 2 3 4 5 6 7 
# 7 9 7 1 1 2 1 


# You can access the results by:
mc_mean$modelName # Optimal selected model ==> "EEE" 

mc_mean$G # Optimal number of cluster => 7 

mc_mean_prob <- as.tibble(mc_mean$z)
mc_mean_prob <- bind_cols(pca_summary, mc_mean_prob) 
 

mc_mean_class <- as.tibble(mc_mean$classification) 
mc_mean_class <- bind_cols(pca_summary, mc_mean_class) %>% 
  dplyr::select(sta, value, everything()) %>% 
  rename(classification = value)


ggplot() +
  geom_point(data = mc_mean_class, 
             mapping = aes(PC1_mean, PC2_mean, 
                           color = factor(classification))) + 
    geom_text(data = mc_mean_class,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

  geom_jitter(data = pca_au_small, 
             mapping = aes(x = .fittedPC1, y = .fittedPC2, 
                           color = factor(sta))) +




head(mc_class, 10) 
# Cluster assignment of each observation ==> 6 for
# 1  2  3  4  5  6  7  8  9 10  
# 1  1  1  1  1  1  1  1  1  1  

# Visualizing model-based clustering
# Model-based clustering results can be drawn using the base 
# function plot.Mclust() [in mclust package]. We'll use the 
# function fviz_mclust() [in factoextra package] to create beautiful 
# plots based on ggplot2.

# Where the data contain more than two variables, fviz_mclust() 
# uses a principal component analysis to reduce the dimensionnality 
# of the data. The first two principal components are used to produce 
# a scatter plot of the data. However, if you want to plot the data 
# using only two variables of interest, c("insulin", "sspg"), 
# you can specify that in the fviz_mclust() function using the 
# argument choose.vars = c("insulin", "sspg").

library(factoextra) 

# BIC values used for choosing the number of clusters 
fviz_mclust(mc, "BIC", palette = "jco") 

# Classification: plot showing the clustering 
fviz_mclust(mc_mean, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 

# Classification uncertainty 
fviz_mclust(mc_mean, "uncertainty", palette = "jco")

# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations.


ggplot() +
  geom_point(data = mc_mean_class, 
             mapping = aes(PC1_mean, PC2_mean, 
                           color = factor(classification))) + 
    geom_text(data = mc_mean_class,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic()  + 
  xlim(2, -5) +
  ylim(-0.6, 0.3) 



```





Several R packages available from CRAN or Bioconducto perform cluster validation, including: 

|    Package   |    Function(s)   |      Author       |      Notes    |
|:------------:|:----------------:|:-----------------:|:-------------:|
|   cclust     | clustIndex()     |    Dimitriadou    | No user guide |
|     fpc      | cluster.stats()  |       Hennig      | No user guide |
|              | clusterboot()    |                   |               |
| clusterRepro |                  | Kapp & Tibshirani | not general   |
|  clusterSim  |                  | Walesiak & Dudek  | poor user guide |
|  clusterStab |                  | MacDonald et al.  | narrow vignette |
|     clue     | cl_validity() +  | Hornik, September |     maybe...    |
|     e1071    | fclustIndex() ++ | Dimitriadou et al.| 2006  | unk.

+ validation for both paritioning methods (“dissimilarity accounted for”) and hierarchical methods (“variance accounted for”) 
++ fuzzy cluster validation measures.

pam() in recommended pack- age cluster (Rousseeuw, Struyf, Hubert, and Maechler, 2005; Struyf, Hubert, and Rousseeuw, 1996), and Mclust() in package mclust (Fraley, Raftery, and Wehrens, 2005; Fraley and Raftery, 2003), are available as components named cluster, clustering, and classification, 

RWeka (Hornik, Hothorn, and Karatzoglou, 2006), cba (Buchta and Hahsler, 2005), cclust (Dimitriadou, 2005), cluster, e1071 (Dimitriadou, Hornik, Leisch, Meyer, and Weingessel, 2005), flexclust (Leisch, 2006), flexmix (Leisch, 2004), kernlab (Karatzoglou, Smola, Hornik, and Zeileis, 2004), and mclust (and of course, clue itself).









# this is the simplist case approach
#gage_scale_l <- gage_prep %>% 
#  select(sta, q_depth, date) %>% 
#  mutate(q_mean = mean(q_depth)) %>% 
#  mutate(q_sd = sd(q_depth)) %>% 
#  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
#  select(sta, q_scaled, date)

# calculate summary values
gage_sum <- gage_scale_l %>% 
  group_by(sta) %>% 
  summarise(mean = mean(q_scaled),
            sd = sd(q_scaled)) %>%
    ungroup()

# standardize the variables
gage_scale1_l <- gage_prep %>% 
  select(sta, q_depth, date) %>% 
  mutate(q_mean = mean(q_depth)) %>% 
  mutate(q_sd = sd(q_depth)) %>% 
  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
  mutate(avg_period = "1") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q_scaled, date)

gage_scale7_l <- gage_prep %>%  
 filter(!is.na(q7_depth)) %>% 
  mutate(q7_mean = mean(q7_depth)) %>% 
  mutate(q7_sd = sd(q7_depth)) %>% 
  mutate(q7_scaled = (q7_depth - q7_mean)/q7_sd) %>% 
  mutate(avg_period = "7") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q7_scaled, date)
    
gage_scale30_l <- gage_prep %>% 
  filter(!is.na(q30_depth)) %>% 
  mutate(q30_mean = mean(q30_depth)) %>% 
  mutate(q30_sd = sd(q30_depth)) %>% 
  mutate(q30_scaled = (q30_depth - q30_mean)/q30_sd) %>% 
  mutate(avg_period = "30") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q30_scaled, date)

# spread the long variables
gage_scale1 <- gage_scale_l %>% 
  spread(sta_type, q_scaled) 

gage_scale7 <- gage_scale7_l %>% 
  spread(sta_type, q7_scaled) 

gage_scale30 <- gage_scale30_l %>% 
  spread(sta_type, q30_scaled) 

gage_scale <- full_join(gage_scale1, gage_scale7)
gage_scale <- full_join(gage_scale, gage_scale30)

rm(gage_scale_l, gage_scale1, gage_scale1_l, gage_scale7, 
   gage_scale7_l, gage_scale30, gage_scale30_l)

# A check on the data finds Wounded Knee Creek has ~11% NA values
#plot_missing(gage_scale)

check_sta <- gage_scale %>% 
  select(wkc_wok30, date) %>% 
  filter(is.na(wkc_wok30))

# drop the NA values (num obs drops from 2192 to 1916)
gage_scale <- gage_scale %>% 
  drop_na() 

gage_scale_l <- gage_scale %>%
  gather(key = )
```

$xi−center(x)scale(x) \frac{x_i - center(x)}{scale(x)}$
$xi−mean(x)sd(x) \frac{x_i - mean(x)}{(x)}$

Where center(x) can be the mean or the median of x values, and scale(x) can be the standard deviation (SD), the interquartile range, or the MAD (median absolute deviation).


