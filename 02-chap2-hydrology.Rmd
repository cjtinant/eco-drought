
Title (13-words or less): Hydrological Drought Assessment of Heterogenious Catchments with Short Periods of Record  

<!--
## Variable naming convention:   



gage         USGS stream gages with daily values 
    _dv       daily values; used for the first set of gages selected
    _meta     metadata; without a modifier is the set after initial site removal

-->

```{r setup, include=FALSE, message=FALSE}  
#knitr::opts_chunk$set(echo = FALSE)  
options(tibble.print_max = 70) # sets tibble output for printing  

# Sets up the library of packages   
library("here") # identifies where to save work  
library("EGRET") # Exploration and Graphics for RivEr Trends 
library("rio") # more robust I/O - to import and clean data  
library("lubridate") # easier dates 
library("tidyverse") 
library("janitor") # tools for examining and cleaning dirty data  
library("dataRetrieval") # USGS data import  
```

```{r streamflow-fix-NA}    
# Stream flow is being analyzed over a period of Water Year 1980-2017.   
# This code chunk fixes some NA values for Q7 & Q30  
# at the start of record 

# 1. import gage daily values 
gage_dv_part01 <- read_csv("data/gage_dv_part01.csv") %>% 
  mutate(site_no = as.character(site_no)) %>% 
    mutate(county_cd = as.character(county_cd))   
gage_dv_part02 <- read_csv("data/gage_dv_part02.csv") %>% 
  mutate(site_no = as.character(site_no)) %>% 
    mutate(county_cd = as.character(county_cd)) 
gage_dv_part03 <- read_csv("data/gage_dv_part03.csv") %>% 
  mutate(site_no = as.character(site_no)) %>% 
    mutate(county_cd = as.character(county_cd)) 


# 2. join daily values & clean up
gage_dv_all <- bind_rows(gage_dv_part01, gage_dv_part02, gage_dv_part03) %>% 
  mutate(site_no = zeroPad(site_no, 8)) %>% 
  mutate(state_cd = as.character(state_cd)) %>% 
  mutate(huc_cd = as.character(huc_cd)) %>% 
  select(sta, Date, Q, everything()) %>% 
  group_by(sta) %>%                               
  filter(!is.na(Q30)) %>%          # drops NA vals from start of record 
  ungroup() 

# 3. remove metadata from discharge values 
gage_meta <- gage_dv_all %>% 
  select(sta, min_year:contrib_drain_area_va) %>% 
  distinct() 


# 4. check for incomplete year records 
gage_incomp <- gage_dv_all %>% 
  group_by(sta, waterYear) %>% 
  summarize(i_count = n()) %>% 
  filter(i_count <= 364) %>% 
  ungroup() %>% 
  filter(waterYear > 1990) %>% 
  arrange(sta, waterYear, i_count) 

gage_dv_all <- full_join(gage_dv_all, gage_incomp, 
                     by = c("sta", "waterYear")) %>% 
  arrange(sta, Date) 


# 4. fix low flows ---- needed for the PCA  
# this code chunk fixes low flow values by:
#   1) substituting 0.01 cfs for zero-flow values &
#   2) log_q is removed from the data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
gage_dv_gath <- gage_dv_all %>% 
  select(-LogQ) %>% 
  gather(key = q_type, val = q_val,       # prepares to censor to 0.01 cfs
         -c(sta, waterYear, date, qualifier)) 

gage_low <- gage_orig %>% 
  filter(q_val < 0.01) %>%               
  mutate(q_val = 0.01) %>% 
  spread(qualifier, q_val) %>% 
    rename("Ac" = "A") %>%      # changes names to 'c' for censored
    rename("A:<c" = "A:<") %>% 
    rename("A:ec" = "A:e") %>% 
    rename("Pc" = "P") %>% 
    rename("P:ec" = "P:e") %>%
  gather(qualifier, q_val, -c(sta, water_year, date, q_type)) %>% 
  filter(!is.na(q_val))

gage_high <- gage_orig %>% 
  filter(q_val >= 0.01)                

gage_orig <- bind_rows(gage_high, gage_low) 

gage_qual <- gage_orig %>% 
  select(sta, date, qualifier) %>% 
  distinct()

gage_orig <- gage_orig %>% 
  select(-qualifier) %>% 
  group_by(date) %>% 
  spread(q_type, q_val) %>% 
  ungroup()

gage_orig <- full_join(gage_orig, gage_qual, 
                       by = c("sta", "date")) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# test the data for duplicates & remove duplicate vals
gage_test <- gage_orig %>%  
  group_by(sta, water_year) %>% 
  summarize(count = n()) %>% 
  ungroup() %>% 
  filter(count > 366) 

export(gage_orig, file = "data/gage_inputs.csv") 

# 4. test the data for duplicates & remove duplicate vals
gage_test <- gage_dv_all %>%  
  group_by(sta, waterYear) %>% 
  summarize(count = n()) %>% 
  ungroup() %>% 
  filter(count > 366) 

```

# 5. separate incomplete years from full years 
gage_dv_incomp <- gage_dv_all %>% 
  filter(!is.na(i_count)) 

gage_dv <- gage_dv_all %>% 
  filter(is.na(i_count)) 


```{r streamflow-data-organization}    
# Stream flow is being analyzed over a period of 1990-2016.       
# This code chunk identifies missing gage-year combinations    
# An iterative approach is used to identify cluster memberships &  
# fill missing years with the streamflow depths of the nearest   
# neighbor in the group.   

# The 'wet-cycle' transitions to 'dry cycle' in 2002  

# import and append daily flow values > 1990 
gage_orig       <- import(file = "data/gage_inputs.csv")   # n = 245,440 obs 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv") %>% 
  select(-contrib_drain_area_va) 
                                                      # run01 n =  12,783 obs 

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")  %>% 
  select(-contrib_drain_area_va)                      # run02 n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  %>% 
 select(-contrib_drain_area_va)                       # run03 n =   9,972 obs

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  %>% 
  select(-contrib_drain_area_va)                      # run04 n =   1,634 obs

gage_est_9017a  <- import(file = "data/gage_est_9017a.csv") %>% 
  select(-contrib_drain_area_va)                      # run05 n =   9,496 obs

gage_est_9017b  <- import(file = "data/gage_est_9017b.csv") 
                                                      # run06 n =  15,064 obs

gage_est_9017c  <- import(file = "data/gage_est_9017c.csv") %>% 
  select(-contrib_drain_area_va)                      # run07 n =  21,916 obs

# bind the gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002a, gage_est_0317a,  
                  gage_est_9002b, gage_est_0317b, gage_est_9017a,  
                gage_est_9017b, gage_est_9017c) 
                                               # orig  n = 249,247 obs    
                                               # run01 n = 258,281  
                                               # run02 n = 258,954 
                                               # run03 n = 268,926 
                                               # run04 n = 270,560   
                                               # run04 n = 280,056   
                                               # run06 n = 295,120      
                                               # run07 n = 317,036   

rm(gage_est_9002a, gage_est_0317a, gage_est_9002b, gage_est_0317b, 
   gage_est_9017a, gage_est_9017b, gage_est_9017c, gage_orig)

# update gage summary complete days of record 
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                  
  summarise(days_record = n()) %>%                   
  ungroup()                                    

# find & split stations-years with incomplete days of record 
gage_incomp <- gage %>%                          
  group_by(sta, water_year) %>%                     
  summarise(days_record = n()) %>%              
  ungroup() %>%                                     
  filter(days_record < 365)                           

gage_incomp <- semi_join(gage, gage_incomp, 
                         by = c("sta", "water_year")) 
                                               # run00 n =   3,279 obs 
                                               # run01 n =   1,454 obs 
                                               # run02 n =   1,454 obs 
                                               # run03 n =     834 obs 
                                               # run04 n =     642 obs 
                                               # run05 n =     642 obs 
                                               # run06 n =       0 obs 
                                               # run07 n =       0 obs
# add area to gage & gage_incomp_year 
gage_meta <- import("data/gage_meta_full.csv") 
gage_meta <- semi_join(gage_meta, gage_summary, by = "sta")

gage <- full_join(gage, gage_meta, 
                  by = "sta") %>% 
  select(date, sta, q, q7, q30, water_year, qualifier, 
         contrib_drain_area_va)

gage_incomp <- full_join(gage_incomp, gage_meta, by = "sta") %>% 
  select(date, sta, q, q7, q30, 
         water_year, contrib_drain_area_va, 
         qualifier) %>% 
  filter(!is.na(q)) 

# check on observations 
gage_sum <- gage_summary %>% 
  spread(water_year, days_record) 

# once the missing vals are complete, then 

gage_input <- gage    

rm(gage_incomp, gage_meta)
```

```{r find_missing_vals_run, eval=FALSE}  
# This code chunk prepared for estimation of missing values.  The years 
# selected for analysis were 1990-2002 (wet), 2003-2017 (dry), and 
# 1990-2017 (all).  Records were filtered by year and model runs 


# Run   Set    Filter   #Sta  #Complete   #Obs    #Incomp 
# 01    wet     1992     26      338     110,665   1,825  
# 02    dry     2006     23      345     125,286       0 
# 03    wet     2002     29      377     127,778     313 
# 04    dry     2017     24      360     129,862     192  
# 05    all     2017     24      672     235,952       0 
# 06    all     2004     27      868     280,056     642 
# 07    all      na      31      868     295,120       0 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Run01--------------------------------------------------------------
gage_run01_sum    <- gage_summary %>% 
  spread(water_year, days_record) %>% 
  select(c(sta:'2002'))  %>% 
  filter(!is.na(`1992`)) %>%                             # n = 26 sta
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                     # n = 338 obs

gage_run01        <- semi_join(gage, gage_run01_sum, # 109,205
                        by = c("sta", "water_year")) # n = 110,665 obs

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run01_na     <- gage_run01 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run01_incomp <- semi_join(gage_incomp, gage_run01_sum, 
                  by = c("sta", "water_year")) %>%  # 1,576
  filter(!is.na(date))                        # n        =   1,748 obs

gage_input        <- gage_run01  
rm(gage_run01_na) 

# Run02--------------------------------------------------------------
gage_run02_sum    <- gage_summary %>% 
  spread(water_year, days_record) %>% 
  select(c(sta,'2003':'2017')) %>% 
  filter(!is.na(`2006`)) %>%                                  # 23 sta 
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 # n =     345 obs

gage_run02        <- semi_join(gage, gage_run02_sum, 
                        by = c("sta", "water_year")) 

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run02_na     <- gage_run02 %>% 
  filter(is.na(q30))                                       # n = 0 obs

# update incomplete observations
gage_run02_incomp <- semi_join(gage_incomp, gage_run02_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        # n        =   0 obs
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# gage_input changes with each run...  sorry!
gage_input        <- gage_run02                       # run02 = 125,286 obs

rm(gage_meta, gage_incomp, gage_run02, 
   gage_run02_na) 

# Run03--------------------------------------------------------------
gage_run03_sum    <- gage_summary %>%               
  spread(water_year, days_record) %>%    
  select(c(sta:'2002'))  %>%                    # subsets the matrix 
  filter(!is.na(`2002`)) %>%                            
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when( 
    is.na(days_record) ~ 0, 
    TRUE ~ as.numeric(days_record)))                 

gage_run03        <- semi_join(gage, gage_run03_sum, 
                        by = c("sta", "water_year")) 

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run03_na     <- gage_run03 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run03_incomp <- semi_join(gage_incomp, gage_run03_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                       
# ~~~~~~~~~~~~~~~~~~~~~~~ 
# gage_input changes with each run...  sorry!
gage_input        <- gage_run03                       

rm(gage_meta, gage_incomp, gage_run03, gage_run03_na) 

# Run04--------------------------------------------------------------
gage_run04_sum    <- gage_summary %>% 
  spread(water_year, days_record)  %>%           
  select(c(sta,'2003':'2017')) %>%                # subsets the matrix 
  filter(!is.na(`2017`)) %>%                                
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>%
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                

gage_run04        <- semi_join(gage, gage_run04_sum, 
                        by = c("sta", "water_year")) 

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run_na       <- gage_run04 %>% 
  filter(is.na(q30))                                       # n = 0 obs

# update incomplete observations
gage_run04_incomp <- semi_join(gage_incomp, gage_run04_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                          
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# gage_input changes with each run...  sorry!
gage_input        <- gage_run04                       
rm(gage_meta, gage_incomp, gage_run04, gage_run_na) 

# Run05--------------------------------------------------------------
gage_run05_sum    <- gage_summary %>%                  
  spread(water_year, days_record)  %>%                
    filter(!is.na(`2017`)) %>%          
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 

gage_run05        <- semi_join(gage, gage_run05_sum, 
                        by = c("sta", "water_year")) 

# check for any NA values
gage_run_na       <- gage_run05 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run05_incomp <- semi_join(gage_incomp, gage_run05_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# gage_input changes with each run...  sorry!
gage_input        <- gage_run05                       
rm(gage_meta, gage_incomp, gage_run05, gage_run_na) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# Run06--------------------------------------------------------------
gage_run06_sum    <- gage_summary %>%      
  spread(water_year, days_record) %>%                 
  filter(!is.na(`2004`)) %>%   
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 

gage_run06        <- semi_join(gage, gage_run06_sum, 
                        by = c("sta", "water_year")) 

# check for any NA values
gage_run_na       <- gage_run06 %>% 
  filter(is.na(q30))                                 # n =       0 obs

# update incomplete observations
gage_run07_incomp <- semi_join(gage_incomp, gage_run06_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))     

# gage_input changes with each run...  sorry!
gage_input        <- gage_run06                       
rm(gage_meta, gage_incomp, gage_run06, gage_run_na) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# Run07--------------------------------------------------------------
gage_run07_sum    <- gage_summary %>%      
  spread(water_year, days_record) %>%                 
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 

gage_run07        <- semi_join(gage, gage_run07_sum, 
                        by = c("sta", "water_year")) 

# check for any NA values
gage_run_na       <- gage_run07 %>% 
  filter(is.na(q30))                                 # n =       0 obs

# update incomplete observations
gage_run07_incomp <- semi_join(gage_incomp, gage_run07_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))     

# gage_input changes with each run...  sorry!
gage_input        <- gage_run07                       
rm(gage_meta, gage_incomp, gage_run07, gage_run_na) 
```

```{r PCA_and_model_selection}  
# Calculates PCA & clusters stations   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# this was first used for estimating missing values 
# needs to be run separately (sorry!) 

#gage_input <- gage %>%      # wet years 
#  filter(water_year < 2003)    

gage_input <- gage %>% 
  filter(water_year >= 2003)  # dry years

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data 
pca_input <- gage_input %>%  
  select(date, sta, q, q7, q30, contrib_drain_area_va)

# training data: 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
#  01    0.013      0.112      0.265     0.942   0.050   0.942   0.992  
#  02   -0.005      0.070      0.223     0.949   0.047   0.949   0.994    
#  03   -0.102     -0.136     -0.148     0.959   0.035   0.959   0.994   
#  04   -0.008      0.071      0.242     0.947   0.047   0.947   0.994   
#  05   -0.056     -0.113     -0.187     0.953   0.041   0.953   0.994 
#  06   -0.031     -0.070     -0.120     0.960   0.034   0.960   0.994 
#  07   -0.055     -0.067     -0.060     0.966   0.029   0.966   0.995 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# vars:
# run  PC  q1_depth  q7_depth  q30_depth       
#   01  1    0.577      0.589     0.566   
#   01  2    0.584      0.186    -0.790     
#   02  1    0.578      0.588     0.567       
#   02  2    0.567      0.210    -0.796      
#   03  1    0.578      0.584     0.569           
#   03  2    0.545      0.243    -0.803       
#   04  1    0.578      0.588     0.566           
#   04  2    0.571      0.205    -0.795       
#   05  1    0.578      0.586     0.568           
#   05  2    0.555      0.228    -0.800       
#   06  1    0.578      0.585     0.569            
#   06  2    0.553      0.233    -0.800       
#   06  1    0.578      0.583     0.571             
#   06  2    0.542      0.248    -0.803  
#   07
#   07
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#Mclust model:
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# num  model  log.likeli  n  df   BIC     ICL     purpose 
#  01  VEV     116.42    26  25  151.4   151.0   estimate NA 
#  02  VEV      82.65    23  17  112.0   111.6   estimate NA 
#  03  VVV     139.30    29  19  214.6   214.5   estimate NA   
#  04  EEV     161.53    24  65  116.5   116.4   estimate NA    
#  05  EEE     139.33    24  37  161.6   161.0   estimate NA    
#  06  EEV     130.93    27  19  199.2   199.2   estimate NA    
#  07  VVV     151.58    31  19  237.9   237.7   estimate NA    
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#    Cl9#
#  01   14      8     4    na    na    na    na      na      na  
#  02   15      8    na    na    na    na    na      na      na  
#  03   14     15    na    na    na    na    na      na      na  
#  04    3      4     3     2     4     2     1       1       4    
#  05    2      9     3     1     5     2     1       1      na  
#  06   14     13    na    na    na    na    na      na      na  
#  07    7     24    na    na    na    na    na      na      na  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# a final cluster analysis was conducted after missing values were found 
# two runs for final cluster analysis are conducted 

# | Run_num | description | num_obs | 
# |:-------:|:-----------:|:-------:|
# |    1    | all years   | 317,036 |                                     
# |    2    | wet years   | 147,188 |
# |    2    | dry years   | 169,848 |

# Eliminate effects of watershed size----
# Calculate daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

pca_input <- pca_input %>% 
  as.tibble() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  
 
# Transform using BoxCox to approach normality---- 
# Daily flow data are highly skewed.  BoxCox transformation utilizes 
# a lambda value to transform a dataset to an ~ normal distribution.

# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

lambda_q1  <- BoxCox.lambda(pca_input$q1_depth)   
lambda_q7  <- BoxCox.lambda(pca_input$q7_depth)   
lambda_q30 <- BoxCox.lambda(pca_input$q30_depth)     

# Standardize data by z-score---- 
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually. 

pca_input <- pca_input %>% 
  mutate(q1_tr = BoxCox(.$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(.$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(.$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>%   # means
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>%   
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>%       # sds
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>%  
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>%    # transfrom data
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

# Calculate PCA matrix & summary info---- 
# prcomp() requires a dataset with only the variables, split data 
# into PCA input & names to connect to result 

pca_check <- pca_input %>% 
  as.tibble() %>%
  select(sta:q30_depth) %>% 
  gather(key = flow_type, val = cms, -sta, -date) %>% 
  filter(is.na(cms)) %>% 
  mutate(year = year(date)) %>% 
  group_by(sta, year) %>%  
  summarise(count = n()) 

pca_matrix <- pca_input %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) %>%
  prcomp(., scale = TRUE)      
# note that . is passing select(q1_depth, q7_depth, q30_depth)
  
# Gather & summarize PCA results----
# Eigenvectors--results about PC axes
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
# & drop the 3rd PC-axis because it's not useful 

pca_vars <-  tidy(pca_matrix, matrix = "variables") %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix - summarize & clean up
gage_pca <- augment(pca_matrix, data = pca_input) %>% 
  select(-c(.rownames, q1_mean:q30_sd, .fittedPC3)) %>% 
#  select(-c(.rownames, date_2, q1_mean:q30_sd, .fittedPC3)) %>% 
      mutate(q1_q30_diff = q1_depth - q30_depth)      

gage_pca_sum <- gage_pca %>% 
  group_by(sta) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean) %>% 
  mutate(eigen_dist = sqrt(PC1_mean^2 + PC2_mean^2)) %>% 
  arrange(eigen_dist) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the series of station runs are as follows: 

pca_eigen 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
# all   -0.082     -0.086     -0.027     0.967   0.028   0.967   0.995 
# wet   -0.060     -0.078     -0.089     0.963   0.031   0.963   0.994 
# dry   -0.145     -0.185     -0.211     0.963   0.032   0.963   0.995
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

pca_vars 
# run  PC  q1_depth  q7_depth  q30_depth   notes    
#  all  1    0.578      0.583     0.571           
#  wet  1    0.578      0.584     0.570           
#  dry  1   -0.578     -0.584    -0.570   need to inverse
#  all  2    0.540      0.251    -0.803       
#  wet  2    0.546      0.243    -0.802    
#  dry  2   -0.546     -0.243     0.802   need to inverse   

# eigenvecter:
#   96% of covarience is explained by PCA1 & 4% of varience by PCA2
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30; 
#         approximates hydrologic export 
#   PC2 - large opposite loadings of Q1 & Q30  
#         contribution of baseflow vs event-flow
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Preparing to cluster by the central tendancy
gage_pca_sum <- gage_pca_sum %>% 
  arrange(sta)

gage_clust <- gage_pca_sum %>% 
  select(q1_mean:q30_mean)

gage_clust_meta <- gage_pca_sum %>% 
  select(sta)

# apply model-based-clustering, extract results & add metadata
gage_clust_l <- Mclust(gage_clust) 

gage_clust <- as.tibble(gage_clust_l$z) 
gage_clust <- bind_cols(gage_clust_meta, gage_clust) 

# drop low probs - join & rearrange 
gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.5) 

gage_clust <- full_join(gage_pca_sum, gage_clust, 
                             by = "sta")

gage_clust <- gage_clust %>% 
  dplyr::select(group, sta, everything()) %>% 
  arrange(sta) %>%  
  arrange(group)

# join cluster to PCA data & arrange
gage_mod <- full_join(gage_pca, gage_clust, by = "sta") 

gage_mod <- gage_mod %>% 
  select(group, sta, everything()) %>% 
  select(-c(q:q30_tr)) %>% 
  select(-eigen_dist) 

summary(gage_clust_l) # Print a summary 

#clean up environment 
rm(pca_input, pca_matrix, pca_check, pca_eigen, pca_vars) 
rm(gage_clust_meta, gage_pca_sum, gage_pca, gage_input) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Mclust model:

# num  model  log.likeli  n  df   BIC     ICL     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all  VEV     190.89    31  41  241.0   240.8  
# wet  EEE     175.15    31  29  250.7   249.4  
# dry  EEE     165.01    31  37  203.0   202.6   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Clustering table:
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all    7     11     5     3     5    na    na      na     
# wet    3      5     1     4    16     2    na      na    
# dry    4     12     5     1     1     6     1       1     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Model-based clustering selected a model with two-three components 
# (i.e. clusters). The optimal selected model name is VEV model. 
# That is the five components are ellipsoidal with varying volume, 
# and orientation, and equal shape. The summary contains also the 
# clustering table specifying the number of observations in each 
# clusters.

```

```{r cluster_testing-sequentially_save_groups, eval=FALSE}
# save groups - need to do sequentially 
# a later step w
gage_clust_all <- gage_clust %>% 
  mutate(type = "all") 

gage_clust_wet <- gage_clust %>% 
  mutate(type = "wet") 
  
gage_clust_dry <- gage_clust %>% 
  mutate(type = "dry") 

# bind to prepare for save 
gage_clusters <- bind_rows(gage_clust_all, gage_clust_wet, gage_clust_dry) 

# save clusters 
export(gage_clusters, file="gage_clusters.csv")
```

```{r cluster-testing_split-vars-all} 
# import saved clustering analysis group results 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_clust <- import(file = "gage_clusters.csv") %>% 
  select(-eigen_dist) 

#    for 'all' stations: 
# 1) split & fix the PC direction of clusters, 
# 2) describe hydrologic groups by flow-types, 
# 3) arrange group descriptions to match largest to smallest hydrologic export 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
gage_clust_all <- gage_clust %>% 
  filter(type                   == "all") %>% 
  mutate(PC1_mean               = PC1_mean) %>% 
  mutate(PC2_mean               = PC2_mean
         ) %>% 
  mutate(group_descr            = case_when(
         group                  == "V4" ~ "ephemeral", 
         group                  == "V5" ~ "event_control", 
         group                  == "V1" ~ "event_mixed", 
         group                  == "V2" ~ "mixed_flow", 
         group                  == "V3" ~ "base_flow" 
  )) %>% 
  mutate(group                  = case_when( 
         group                  == "V4" ~ "5", 
         group                  == "V5" ~ "4", 
         group                  == "V1" ~ "3", 
         group                  == "V2" ~ "2", 
         group                  == "V3" ~ "1" )) %>% 
  select(-c(prob, type))                   
```

```{r cluster-testing_split-vars-wet} 
#    for 'wet' stations:
# 1) split & fix the PC direction of clusters, 
# 2) describe hydrologic groups by flow-types, 
# 3) arrange group descriptions to match largest to smallest hydrologic export 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
gage_clust_wet <- gage_clust %>% 
  filter(type                  == "wet")   %>% 
  mutate(PC1_mean              = PC1_mean) %>% 
  mutate(PC2_mean              = PC2_mean
         ) %>% 
  mutate(group                 = case_when( 
         group                 == "V6" ~ "6", 
         group                 == "V3" ~ "5", 
         group                 == "V4" ~ "4", 
         group                 == "V2" ~ "3", 
         group                 == "V1" ~ "2", 
         group                 == "V5" ~ "1" 
  )) %>% 
  rename(group_wet             = group) %>%
  rename(PC1_wet               = PC1_mean) %>%
  rename(PC2_wet               = PC2_mean) %>%
  rename(q1_wet                = q1_mean) %>%
  rename(q7_wet                = q7_mean) %>%
  rename(q30_wet               = q30_mean) %>%
  rename(q1_q30_wet            = q1_q30_mean) %>%
  rename(prob_wet              = prob) %>%
  select(-type)  
``` 

```{r cluster-testing_split-vars-dry} 
#    for 'dry' stations:
# 1) split & fix the PC direction of clusters, 
# 2) describe hydrologic groups by flow-types, 
# 3) arrange group descriptions to match largest to smallest hydrologic export 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
gage_clust_dry <- gage_clust %>% 
  filter(type               == "dry") %>% 
  mutate(PC1_mean           = -PC1_mean) %>% 
  mutate(PC2_mean           = -PC2_mean
         ) %>% 
  mutate(group                = case_when( 
         group                == "V3" ~ "1", 
         group                == "V2" ~ "2", 
         group                == "V4" ~ "3", 
         group                == "V1" ~ "4", 
         group                == "V6" ~ "5", 
         group                == "V5" ~ "6", 
         group                == "V7" ~ "7", 
         group                == "V8" ~ "8" 
   )) %>% 
  rename(group_dry          = group) %>% 
  rename(PC1_dry            = PC1_mean) %>%
  rename(PC2_dry            = PC2_mean) %>%
  rename(q1_dry             = q1_mean) %>%
  rename(q7_dry             = q7_mean) %>% 
  rename(q30_dry            = q30_mean) %>% 
  rename(q1_q30_dry         = q1_q30_mean) %>% 
  rename(prob_dry           = prob) %>% 
  select(-type) 
``` 

```{r cluster-testing_make_convex-hull}
# Find the convex hull of the 'all' group 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
# this is the idea; but it doesn't return a minimum convex hull
# gage_clust_hull <- gage_clust_all %>% 
#   select(sta, group2, PC1_mean, PC2_mean) %>% 
#   split(.$group2) %>% 
#   map_dfr(chull(.$PC1_mean,.$PC2_mean)) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  

# using this to make it work 
hull_G1 <- gage_clust_all %>% 
  filter(group == "1")  %>% 
  slice(chull(PC1_mean, PC2_mean)) 

hull_G2 <- gage_clust_all %>% 
  filter(group == "2") %>% 
  slice(chull(PC1_mean, PC2_mean)) 

hull_G3 <- gage_clust_all %>% 
  filter(group == "3") %>% 
  slice(chull(PC1_mean, PC2_mean)) 

hull_G4 <- gage_clust_all %>% 
  filter(group == "4") %>% 
  slice(chull(PC1_mean, PC2_mean))

hull_G5 <- gage_clust_all %>% 
  filter(group == "5") %>% 
  slice(chull(PC1_mean, PC2_mean)) 

gage_clust_hull <- bind_rows(hull_G1, hull_G2, hull_G3, hull_G4, hull_G5) %>% 
  mutate(group = as.integer(group))

rm(hull_G1, hull_G2, hull_G3, hull_G4, hull_G5)
```

```{r cluster-testing_rejoin_clusters} 
# add metadata
# rejoin updated cluster data in wide data format, 
# arrange variables, 
# transform group variable names into integers for continuous color scale
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
gage_meta  <- import(file = "data/gage_meta_full.csv") 

gage_clust <- full_join(gage_clust_all, gage_clust_wet, 
                                 by = c("sta"))  

gage_clust <- full_join(gage_clust, gage_clust_dry, 
                                 by = c("sta")) 

gage_clust <- left_join(gage_clust, gage_meta, by = "sta") 

gage_clust <- gage_clust %>% 
  select(sta, group,   group_wet,  group_dry, 
            prob_wet,   prob_dry,
             q1_mean,     q1_wet,     q1_dry, 
             q7_mean,     q7_wet,     q7_dry, 
            q30_mean,    q30_wet,    q30_dry, 
         q1_q30_mean, q1_q30_wet, q1_q30_dry, 
            PC1_mean,    PC1_wet,    PC1_dry, 
            PC2_mean,    PC2_wet,    PC2_dry, 
         everything()) %>% 
  mutate(group                      = as.integer(group)) %>%  
  mutate(group_wet                  = as.integer(group_wet)) %>% 
  mutate(group_dry                  = as.integer(group_dry))  

# clean-up 
rm(gage_clust_all, gage_clust_wet, gage_clust_dry, gage_meta) 
```

```{r cluster_linear_model} 

gage_clust_mod <- lm(q7_mean ~ group, data = gage_clust) 

tidy(mod)  
glance(mod)  
augment(mod) 
gage_clust_au <- augment(mod, data = gage_clust) 

# shows that the one out of normal point is hor_oel 
ggplot(gage_clust_au, aes(.hat, .std.resid)) + 
  geom_vline(size = 2, colour = "white", xintercept = 0) + 
  geom_hline(size = 2, colour = "white", yintercept = 0) + 
  geom_point() + geom_smooth(se = FALSE) 

# shows that the one out of normal point is hor_oel 
plot(mod, which = 6) 
ggplot(au, aes(.hat, .cooksd)) + 
  geom_vline(xintercept = 0, colour = NA) + 
  geom_abline(slope = seq(0, 3, by = 0.5), colour = "white") + 
  geom_smooth(se = FALSE) + 
  geom_point() 

# clean up 
```

```{r find_best_candidate_by_cluster} 
# taking the average is likely to smooth the data.  
# Instead use representative stations with complete years are chosen
 
#    station type     q7_mean  q1_q30_mean   representitive
# 1 - GW SMALL         0.694      0.0234       fal_hot 
# 2 - GROUNDWATER      0.406      0.0377       lwr_whi 
# 3 - MIXED FLOW      -0.122     -0.0200       whi_kad 
# 4 - EVENT DOMINATED -0.436      0.0550       whi_ogl 
# 5 - EPHEMERAL       -1.62      -0.127        hat_edg 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# import gage data
gage <- import(file = "data/gage_inputs.csv") 
  
# create summary with years of complete record
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                  
  summarise(days_record = n()) %>%                   
  ungroup() %>% 
  filter(days_record >= 365) 

gage_sum2 <- gage_summary %>% 
    group_by(sta) %>% 
    summarise(years_record = n()) %>%                   
    ungroup() %>% 
  arrange(desc(years_record)) 

gage_summary <- full_join(gage_summary, gage_sum2, 
                          by = "sta")

# join summary to gage_clust
gage_reps <- left_join(gage_sum2, gage_clust, 
                          by = "sta") %>% 
  filter(years_record == 28) %>% 
  select(sta, years_record, group, q7_mean, q1_q30_mean, 
         contrib_drain_area_va, everything())

# create summary for the table above   
gage_sum2 <- gage_reps %>% 
  group_by(group) %>% 
  summarise(q7_mean = mean(q7_mean), 
            q1_q30_mean = mean(q1_q30_mean)) 

# create a dataframe of representive stations
gage_reps <- gage_reps %>% 
  filter(sta == "fal_hot" | 
         sta == "lwr_whi" | 
         sta == "whi_kad" | 
         sta == "whi_ogl" | 
         sta == "hat_edg") 
```

```{r clustering_visualization}
# sets up a visualization plot of groups (all) and stations (wet -> dry)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# scale_'var'_distiller - from http://colorbrewer2.org
#   more on color theory from: http://www.hclwizard.org/r-colorspace/ & 
#   https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/color.html  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# NEXT STEPS: FIGURE OUT LEGEND 


# identify stations that get relatively wetter during dry period
gage_clust <- gage_clust %>%  
  mutate(group_miscl = case_when( 
         sta         == "wkc_wok" ~ 1, 
         sta         == "bev_abf" ~ 1, 
         sta         == "wcc_ogl" ~ 1, 
         sta         == "spr_her" ~ 1, 
         sta         == "whi_slm" ~ 1, 
         sta         == "che_buf" ~ 1, 
         sta         == "che_red" ~ 1, 
         sta         == "whi_whi" ~ 1, 
         TRUE ~ 0)) 

# plot the stations
ggplot(gage_summary, aes(PC1_mean, PC2_mean)) + 
# set up the color scheme
  scale_fill_distiller(palette = "YlGnBu") + 
  theme_classic() +
# plot convex hulls
  geom_polygon(data = gage_clust_hull, 
               aes(group = group,
                   fill = group)) + 
# add wet to dry arrows 
  geom_segment(data  = gage_clust, 
               arrow = arrow(length = unit(0.2, "cm"), 
                      type = "open"),     
               aes(x = PC1_wet, 
                   y = PC2_wet, 
                   xend = PC1_dry, 
                   yend = PC2_dry, 
                   alpha = 0.5),
               show.legend = FALSE) + 
# add representative gages
  geom_point(data = gage_reps,
             aes(x = PC1_mean, 
                 y = PC2_mean, 
                 shape = "."), 
               show.legend = FALSE) +
# add other gages
  geom_point(data = gage_clust,
             aes(x = PC1_mean, 
                 y = PC2_mean, 
                 shape = ".", 
                 alpha = 0.5), 
               show.legend = FALSE) 
```

```{r sri_prepare_raw_data}   
# Calculate Standardized Runoff Index (SRI) using the Standardized    
# Climate Index (SCI) package.    
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# filter representative station data
gage_sri <- semi_join(gage, gage_reps, 
                      by = "sta") %>% 
  select(sta, date, q) %>% 
  mutate(date = ymd(date))

# check to ensure there are no zero flow values
gage_chk <- gage_sri %>% 
  filter(is.na(q))                    # 19-01-01: returns zero observations

# prepare data for drought index 
gage_sri <- gage_sri %>% 
  select(sta, date, q) %>% 
# make year_month column
  mutate(date = ymd(date)) %>% 
  mutate(mon = as.character(
    month(date)
         )) %>% 
  mutate(year = as.character(
    year(date)
         )) %>% 
  unite("yr_mon", c("year","mon")) %>% 

  # upscale to monthly average flows
  group_by(sta, yr_mon) %>% 
  summarize(q = mean(q)) %>% 
  ungroup() %>% 
  mutate(day = 15) %>% 
  unite("date", c("yr_mon","day"), sep = "-") %>% 
  mutate(date = ymd(date)) %>%
# find the plotting position: using Weibull; a = 0  
  group_by(sta) %>% 
  arrange(q) %>% 
  mutate(position = pp(q)) %>% 
  ungroup() %>% 
# find log-Q 
  mutate(q_log10 = log10(q))

# clean up
rm(gage, gage_clust_hull, gage_reps, gage_summary, gage_chk) 
``` 

```{r stream-flow_boxplot, include=FALSE, eval=FALSE} 
# plot the precip data as a boxplot
ggplot(gage_sri, aes(as.factor(sta), q_log10)) +
  geom_violin() +
  geom_boxplot() +
#  scale_y_sqrt() +
#  scale_y_log10() +
  scale_y_sqrt() +
  theme_bw() +
  ggtitle("Monthly average streamflows for stations near Pine Ridge Reservation, SD", 
          subtitle = "Water Years 1990 to 2018") + 
  xlab("") + 
  ylab("cubic m/sec") +
  NULL

#ggplot2::ggsave(filename = "rf_boxplot.png", 
#                width = 6, height = 6, units = "in")
```

```{r gage_plotting_postion-DELETE, include=FALSE, eval=FALSE} 
# DELETE THIS!!!
# plot q as a function of plotting position
ggplot(gage_sri, aes(position, q)) + 
  facet_grid(.~sta) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

#ggplot2::ggsave(filename = "rf_freq_plot.png", 
#                width = 6, height = 6, units = "in")

```

```{r gage_visualization} 
# NEXT STEPS - FIND OUT WHY THERE ARE INFINATE VALUES ON Y-AXIS
ggplot(gage_sri, aes(position, q)) + 
  facet_grid(.~sta) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
ggtitle("Representitive streams near Pine Ridge Reservation, SD", 
          subtitle = "1990-2018") + 
  xlab("Frequency of occurrance") +
  ylab("Mean monthly discharge [log-cu m/sec]") +
  scale_y_log10() 

ggplot2::ggsave(filename = "figure/draft_streamflow.png", 
                width = 6, height = 6, units = "in")

```

```{r gage_lmoment_ratios}
# calculate lmoment ratios by mapping a list made up of 'gage_sri' vals

# arrange 'gage_sri' 
# Note that we might consider Weiss 1964 bias value of 1.018 for L1 
gage_sri <- gage_sri %>% 
  arrange(sta)

lmom_sri <- gage_sri %>%  
  split(.$sta) %>% 
  map(~ lmoms(.$q_log)) %>% 
  transpose() %>% 
  as_tibble() %>% 
  select(lambdas, ratios) %>% 
    mutate(lambdas = map(lambdas, ~as_tibble(t(.x))))  %>% 
    mutate(lambdas = map(lambdas,   
                         ~set_names(.x, c("L1", "L2", "L3", 
                                          "L4", "L5")))) %>% 
      mutate(ratios = map(ratios, ~as_tibble(t(.x))))  %>% 
    mutate(ratios = map(ratios,  
                         ~set_names(.x, c("T1", "T2", "T3",  
                                          "T4", "T5")))) %>%  
  unnest(lambdas) %>% 
  unnest(ratios) %>% 
  mutate(sta = c("fal_hot", "hat_edg", "lwr_whi", "whi_kad", "whi_ogl")) %>% 
  select(-T1) %>% 
  rename(L_CV = T2) %>%  
  rename(L_skew = T3) %>% 
  rename(L_kurtosis = T4) %>% 
  select(sta, L1, L_CV, L_skew, L_kurtosis, everything()) 

# get the length of the dataset - ?????
clust_count <- lmom_sri %>%
  count(sta)  

# join the number of years to the station - ?????
lmom_sri <- left_join(lmom_sri, clust_count, by = "sta")

# calculate weighted means for regional L-moments -?????
L1  <- weighted.mean(lmom_sri$L1, lmom_sri$n) 
L_CV  <- weighted.mean(lmom_sri$L_CV, lmom_sri$n)
L_skew  <- weighted.mean(lmom_sri$L_skew, lmom_sri$n)
L_kurtosis  <- weighted.mean(lmom_sri$L_kurtosis, lmom_sri$n)
n       <- sum(lmom_sri$n)

# combine the output into a single weighted mean
int1     <- cbind(L1, L_CV)
int2     <- cbind(int1, L_skew)
int3     <- cbind(int2, L_kurtosis)
lmom_reg <- cbind(int3, n)

rm(L1, L_CV, L_skew, L_kurtosis, n, int1, int2, int3, sta_count)

# finalize the regional L-moment
lmom_reg <- as.tibble(lmom_reg) %>%
  mutate(station = "WtMean") %>% 
  select(station, everything())
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2  
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list 
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble()

gev <- lmrdia %>% 
  extract2(5) %>% as.tibble()

glo <- lmrdia %>%
  extract2(6) %>% as.tibble()

gpa <- lmrdia %>%
  extract2(7) %>% as.tibble()

gno <- lmrdia %>%
  extract2(9) %>% as.tibble()

gov <- lmrdia %>%
  extract2(10) %>% as.tibble()

pe3 <- lmrdia %>%
  extract2(12) %>% as.tibble()

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
               rename(GEV = V2.x) %>% 
               rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything())  
  
rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}
# plots the theo distributions, the sample vals, and regional mean 
ggplot() + 
  geom_line(data = lmom_theo, aes(L_skew, L_kurtosis, 
                                  group = distribution, 
                                  linetype = distribution)) +
  geom_point(data = lmom_sri, aes(L_skew, L_kurtosis)) +
  geom_point(data = lmom_reg, aes(L_skew, L_kurtosis, 
                                  size = 2, show.legend = NA)) +
  theme_bw() + 

#  xlim(0, 0.5) +  - these for precip
#  ylim(0, 0.5) +
xlim(-0.25, 0.5) +
# ylim(0.3, 0.75) +
  ggtitle("L-moment diagram for average monthly logrithm of streamflow depth", 
          subtitle = "Weather stations near Pine Ridge Reservation, 1909-2018")

#ggplot2::ggsave(filename = "lmom_plot.png", 
#                width = 6, height = 6, units = "in")
```


# START HERE
# USE LOG PE3 

```{r sri_prepare_raw_data2}   
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Initial values for SCI calculations 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
time_scale <- 1  # sets the length of the averaging period 
distrib <- "pe3" # sets the distribution type 
p_zero <- TRUE   # sets a function to reduce zero-precip bias
p_zero_cm <- TRUE # ?????
scale <- "sd"    # scales input by subtract mean & divide by sd
warn_me <- TRUE  # sets explicit warning

# Set first month for each station 
first_mon_cot <- 6 
first_mon_int <- 11 
first_mon_oel <- 6 
first_mon_ora <- 5  
first_mon_rap <- 5 

# prepare station for analysis 
sta_cot <- sta_raw %>% arrange(date) %>% select(date, cot) 
sta_int <- sta_raw %>% arrange(date) %>% select(date, int) 
sta_ora <- sta_raw %>% arrange(date) %>% select(date, ora) 
sta_oel <- sta_raw %>% arrange(date) %>% select(date, oel) 
sta_rap <- sta_raw %>% arrange(date) %>% select(date, rap) 

 # change tibble to a vector as double
cot <- as.double(sta_cot$cot)
int <- as.double(sta_int$int) 
ora <- as.double(sta_ora$ora)
oel <- as.double(sta_oel$oel) 
rap <- as.double(sta_rap$rap) 

# notes: 
# A note on variable naming convention; the SCI package doesn't like 
#   snake_case variables 
# 1 month spi - Int did not close on february

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1mon-spi-with-SCI-1mo----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - throws a month 7 NA error.
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   1mon- combine & gather the SPI variables---- 
spi_gath01 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_01 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 2mon-spi-with-SCI-2mo----

# set SCI state variables 
time_scale <- 2  # sets the length of the averaging period 

# Cottonwoods SPI - 2 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value)

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   2mon- combine & gather the SPI variables---- 
spi_gath02 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_02 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 3mon-spi-with-SCI---- 
time_scale <- 3  # sets the length of the averaging period 

# Cottonwoods SPI - 3
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib,  
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   3mon - combine & gather the SPI variables---- 
spi_gath03 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_03 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 4mon-spi-with-SCI-4mo----
# notes:  ORAL HAS ONE SPI VAL OF -8
time_scale <- 4  # sets the length of the averaging period 

# Cottonwoods SPI - 4
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results

spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   4mon - combine & gather the SPI variables---- 
spi_gath04 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_04 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#   5mon-spi-with-SCI-5mo----
# notes: ora has an extreme low value & really high skew 
time_scale <- 5  # sets the length of the averaging period 

# Cottonwoods SPI - 5 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value



#   5mon - combine & gather the SPI variables---- 
spi_gath05 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_05 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 6mon-spi-with-SCI-6mo---- 
 time_scale <- 6  # sets the length of the averaging period 

# Cottonwoods SPI - 6
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   6mon - combine & gather the SPI variables---- 
spi_gath06 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_06 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#   9mon-spi-with-SCI-9mo----
# Notes: ora has an extreme low value & really high skew ??

# set SCI state variables 
time_scale <- 9  # sets the length of the averaging period 

# Cottonwoods SPI - 9 
#~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   9mon - combine & gather the SPI variables---- 
spi_gath09 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_09 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 12mon- spi-with-SCI-12mo----
time_scale <- 12  # sets the length of the averaging period 

# Cottonwoods SPI - 12
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)  

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results  
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   12mon - combine & gather the SPI variables---- 
spi_gath12 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_12 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 18mon-spi-with-SCI-18mo----
time_scale <- 18  # sets the length of the averaging period 

# Cottonwoods SPI - 18
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   18mon - combine & gather the SPI variables---- 
spi_gath18 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_18 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 24mon- spi-with-SCI-24mo---- 
time_scale <- 24  # sets the length of the averaging period 

# Cottonwoods SPI - 24
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   24mon - combine & gather the SPI variables---- 
spi_gath24 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_24 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 30mon- spi-with-SCI-30mo---- 
time_scale <- 30  # sets the length of the averaging period 

# Cottonwoods SPI - 30 
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   30mon - combine & gather the SPI variables---- 
spi_gath30 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_30 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 36mon- spi-with-SCI-36mo---- 
time_scale <- 36  # sets the length of the averaging period 

# Cottonwoods SPI - 36 
#~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   36mon - combine & gather the SPI variables---- 
spi_gath36 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_36 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 42mon- spi-with-SCI-42mo---- 
time_scale <- 42  # sets the length of the averaging period 

# Cottonwoods SPI - 42
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   42mon - combine & gather the SPI variables---- 
spi_gath42 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_42 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 48mon- spi-with-SCI-48mo---- 
time_scale <- 48  # sets the length of the averaging period 

# Cottonwoods SPI -  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   48mon - combine & gather the SPI variables---- 
spi_gath48 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_48 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# 54mon- spi-with-SCI-54mo---- 
time_scale <- 54  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   54mon - combine & gather the SPI variables---- 
spi_gath54 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_54 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# 60mon- spi-with-SCI-60mo---- 
time_scale <- 60  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   60mon - combine & gather the SPI variables---- 
spi_gath60 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_60 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
```






```{r Clustering_visualization} 




# plot clusters in PCA space 
ggplot(gage_test,   
       aes(q7_mean, 
           q1_q30_mean,   
           color = as.factor(type), 
           shape = as.factor(sta))) +  
  geom_density2d(na.rm = TRUE, 
                 contour = TRUE, 
                 aes(color = as.factor(group)))  +  
  geom_jitter() + 
  geom_text(aes(label = sta, 
                color = "black"), 
            check_overlap = TRUE,  
            nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-0.5, 1.5)) + 
  scale_y_continuous(limits = c(-0.3, 0.3)) + 
  theme_classic() 
```

```{r}

#ggplot2::ggsave(path = "figure/", filename = "strm_clust.png", 
#                width = 6, height = 6, units = "in")  

gage_clust_plot <- gage_pca %>% 
  select(sta, group, q7_mean, q1_q30_mean) %>% 
  rename(hydro_exp_coeff = q7_mean) %>% 
  rename(stability_coeff = q1_q30_mean) %>% 
  gather(key = parameter, value = value, -sta, -group) 

ggplot(gage_clust_plot, aes(group, value)) + 
  facet_grid(rows = vars(parameter)) + 
         geom_boxplot() + 
  theme_bw() 

#ggplot2::ggsave(path = "figure/", filename = #"strm_clust_box.png", 
#                width = 6, height = 3, units = "in")  

#?gage_clust <- gage_clust %>% 
  select(group, sta, station_nm, everything()) %>% 
  arrange(group) 

# plotting options from factominer---- 
# BIC values used for choosing the number of clusters  
fviz_mclust(gage_clust_l, "BIC", palette = "jco") 
 
# Classification uncertainty 
# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations 
fviz_mclust(gage_clust_l, "uncertainty", palette = "jco")

# Classification: plot showing the clustering 
fviz_mclust(gage_clust_l, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 



```

```{r fill_missing_data, eval=FALSE} 
# this code chunk fills in missing data.   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
 
# Run01-Fill-Missing-Data-------------------------------------------- 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run01_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                  

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                          
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
#  filter(group_sta == group_lead) %>% # drops obs of different groups
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
#  filter(group_sta == group_lag) %>% # drops obs of different groups 
  rename(group = group_sta) %>%                              
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                            

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 80 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(date, sta_orig, q:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)             
  ) %>% 
  ungroup() %>% 
  rename(sta = sta_orig)

#  9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run01_incomp, 
                  by = c("date", "sta"))        

gage_est_miss <- bind_rows(gage_est_miss, gage_run01_incomp)

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9002.csv") 

# 10. test the data for duplicates & remove duplicate vals
#     import orig data and estimates from prior runs 
gage <- import(file = "data/gage_inputs.csv")       # n = 245,440   

gage_est_9002 <- import(file = "data/gage_est_9002.csv")
                                                    # n =  14,608 obs

# bind the gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage, gage_est_9002) %>% 
  group_by(sta) %>%                              
  mutate(min_yr = min(water_year)) %>%     
  mutate(max_yr = max(water_year)) %>%        
           ungroup()                       

# find actual count of years - post1990  
gage_summary <- gage %>%                             
  group_by(sta) %>%                          
  summarise(min_yr = min(water_year),          
            max_yr = max(water_year)            
            ) %>% 
  ungroup() %>% 
  mutate(count_years_act = max_yr - min_yr) 
 
gage <- semi_join(gage, gage_summary, 
                           by = "sta")                   

# update gage summary & gage
gage_summary <- gage %>%                            
  group_by(sta, water_year) %>%                    
  summarise(days_record = n()) %>%                  
  ungroup() %>% 
  filter(days_record > 366) 

gage_dups <- semi_join(gage_est_9002, gage_summary) %>% 
  filter(qualifier != "est")                           # 1,825 obs

gage_est_9002a <- anti_join(gage_est_9002, gage_dups, 
                            by = c("date", "sta"))    # n = 12,783 

export(gage_est_9002a, file = "data/gage_est_9002a.csv") 
rm(gage_dups, gage_summary)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# Run02-Fill-Missing-Data-------------------------------------------- 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run02_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                      
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                             
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n =  4 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(date, sta_orig, q:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>% 
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run02_incomp, 
                  by = c("date", "sta"))                # n = 731 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run02_incomp)
                                                      # n =  731 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_0317a.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")       # n = 245,440 

# import estimates from prior runs 
gage_est_9002 <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317 <- import(file = "data/gage_est_0317a.csv")    # run02 
                                                     # n =     731 obs

# bind the new gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002, gage_est_0317) %>% 
  group_by(sta) %>%                                  # n = 258,954 obs     
  mutate(min_yr = min(water_year)) %>%     
  mutate(max_yr = max(water_year)) %>%      
           ungroup()                     

# update gage summary & gage
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                  
  summarise(days_record = n()) %>%                  
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002) 

gage_dups <- semi_join(gage_est_0317, gage_test, 
                       by = c("date", "sta"))                 # 0 obs

gage_est_0317aa <- anti_join(gage_est_0317, gage_dups, 
                            by = c("date", "sta"))    # n =   731 obs 

export(gage_est_0317aa, file = "data/gage_est_0317a.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Run03-Fill-Missing-Data-------------------------------------------- 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run03_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
# 6. tidy the df to prepare for a join
#gage_pairs <- gage_pairs %>% 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                           
  select(-c(group_lead, group_lag)) %>%
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                        

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 56 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:water_year) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n = 10,592 obs
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run03_incomp, 
                  by = c("date", "sta"))            # n = 9,972 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run03_incomp) 
                                                    # n = 10,592 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9002b.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")       # n = 245,440 

# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")    # run02 
                                                     # n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  # run03
                                                     # n =  10,592 obs

# bind the new gage dfs together & update gage summary
gage <- bind_rows(gage_orig, gage_est_9002a, 
                  gage_est_0317a, gage_est_9002b)  
                                                     # n = 269,546 obs
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 739 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002a, gage_est_0317a) 

gage_dups <- semi_join(gage_est_9002b, gage_test, 
                       by = c("date", "sta")) # 620 obs

gage_est_9002bb <- anti_join(gage_est_9002b, gage_dups, 
                            by = c("date", "sta"))  # n =  9,972 obs 

export(gage_est_9002bb, file = "data/gage_est_9002b.csv") 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Run04-Fill-Missing-Data-------------------------------------------- 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run04_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                 

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                             

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))    # n = 10 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                        # n = 1,826 obs
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run04_incomp, 
                  by = c("date", "sta"))               # n = 1,634 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run04_incomp) 
                                                       # n = 1,826 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_0317b.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv") %>% 
             filter(sta != "che_sce")                    # n = 245,440 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")    # run02 
                                                     # n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  # run03
                                                     # n =   9,972 obs 

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  # run04
                                                     # n =  1,826 obs 

# bind the new gage dfs together & update gage summary 
gage <- bind_rows(gage_orig, gage_est_9002a, gage_est_0317a, 
                  gage_est_9002b, gage_est_0317b)  
                                                     # n = 270,752 obs
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                 # n run03 = 743 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002a, gage_est_9002a, 
                       gage_est_0317a)  

gage_dups <- semi_join(gage_est_0317b, gage_test, 
                       by = c("date", "sta"))         # n =    192 obs

gage_est_9017bb <- anti_join(gage_est_0317b, gage_dups, 
                            by = c("date", "sta"))    # n =  1,634 obs 

export(gage_est_9017bb, file = "data/gage_est_0317b.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Run05-Fill-Missing-Data-------------------------------------------- 

# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run05_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                     
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                            
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                          

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 52 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n =  9,496 obs 
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations 
gage_est_miss <- anti_join(gage_est_miss, gage_run05_incomp, 
                  by = c("date", "sta"))            # n =  9,496 obs 

gage_est_miss <- bind_rows(gage_est_miss, gage_run05_incomp) 
                                                    # n =  9,496 obs 

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9017a.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv") %>% 
             filter(sta != "che_sce")                    # n = 245,440 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")  # run02 
                                                     # n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  # run03
                                                     # n =   9,972 obs 

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  # run04
                                                      # n =  1,634 obs 

gage_est_9017a  <- import(file = "data/gage_est_9017a.csv")  # run05
                                                     # n =  9,496 obs 

# bind the new gage dfs together & update gage summary
gage <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b, gage_est_9017a) 
                                                     # n = 280,056 obs

gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 769 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b) 

gage_dups <- semi_join(gage_est_9017a, gage_test, 
                       by = c("date", "sta"))           # 0 obs

gage_est_9017aa <- anti_join(gage_est_9017a, gage_dups, 
                            by = c("date", "sta"))  # n =  9,496 obs 

export(gage_est_9017aa, file = "data/gage_est_9017a.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Run06-Fill-Missing-Data-------------------------------------------- 

# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run06_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                   

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))   # n = 86 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n = 15,706 obs
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run06_incomp, 
                  by = c("date", "sta"))            # n = 15,064 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run06_incomp) 
                                                    # n = 15,706 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9017b.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv") %>% 
             filter(sta != "che_sce")                    # n = 245,440 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")  # run02 
                                                     # n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  # run03
                                                     # n =   9,972 obs 

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  # run04
                                                      # n =  1,634 obs 

gage_est_9017a  <- import(file = "data/gage_est_9017a.csv")  # run05
                                                      # n =  9,496 obs 

gage_est_9017b  <- import(file = "data/gage_est_9017b.csv")  # run06 
                                                     # n =  15,064 obs 

# bind the new gage dfs together & update gage summary
gage <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b, gage_est_9017a, 
                  gage_est_9017b) 
                                                     # n = 295,120 obs

gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 868 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b, gage_est_9017a) 

gage_dups <- semi_join(gage_est_9017b, gage_test, 
                       by = c("date", "sta"))                # 0 obs

gage_est_9017bb <- anti_join(gage_est_9017b, gage_dups, 
                            by = c("date", "sta"))  # n =  15,064 obs 

export(gage_est_9017bb, file = "data/gage_est_9017b.csv") 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Run07-Fill-Missing-Data-------------------------------------------- 

# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run07_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                   

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) 

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) 

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))   # n = 120 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n = 21,916 obs
  rename(sta = sta_orig)

# 9. replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run07_incomp, 
                  by = c("date", "sta"))            # n = 21,916 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run07_incomp) 
                                                    # n = 21,916 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9017c.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 10. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv") %>% 
             filter(sta != "che_sce")                    # n = 245,440 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv")  # run01
                                                     # n =  12,783 obs

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")  # run02 
                                                     # n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  # run03
                                                     # n =   9,972 obs 

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  # run04
                                                      # n =  1,634 obs 

gage_est_9017a  <- import(file = "data/gage_est_9017a.csv")  # run05
                                                      # n =  9,496 obs 

gage_est_9017b  <- import(file = "data/gage_est_9017b.csv")  # run06 
                                                     # n =  15,064 obs 

gage_est_9017c  <- import(file = "data/gage_est_9017c.csv")  # run07 
                                                     # n =  21,916 obs 

# bind the new gage dfs together & update gage summary
gage <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b, gage_est_9017a, 
                  gage_est_9017b, gage_est_9017c) 
                                                     # n = 317,036 obs

gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 868 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_0317a, gage_est_0317b, 
                  gage_est_9002a, gage_est_9002b, gage_est_9017a, 
                  gage_est_9017b) 

gage_dups <- semi_join(gage_est_9017c, gage_test, 
                       by = c("date", "sta"))                # 0 obs

gage_est_9017cc <- anti_join(gage_est_9017c, gage_dups, 
                            by = c("date", "sta"))  # n =  21,916 obs 

export(gage_est_9017cc, file = "data/gage_est_9017c.csv") 
```


```{r plot_PCA} 
 
# mean eigenvector plot
ggplot(gage_clust, aes(PC1_mean, PC2_mean)) + 
#  scale_y_reverse() + 
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
            nudge_y = 0.02) + 
  theme_classic() + 
#  scale_x_continuous(limits = c(-3.5, 2)) + 
  xlab("PC axis 1 mean") +
  ylab("PC axis 2 mean") 

# transformed variable plot 
ggplot(gage_pca, aes(q7_mean, q1_q30_mean)) + 
  geom_jitter() +  
  geom_text(aes(label = sta), check_overlap = TRUE,  
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2.0, 1.5)) + 
  scale_y_continuous(limits = c(-1, 0.75)) + 
  geom_density2d() + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  xlab("hydrologic export index") +
  ylab("stability index") 

# PC1 explanatory plot 
ggplot(gage_aug, 
       aes(q7_depth, .fittedPC1, color = factor(sta))) + 
  geom_jitter() + 
  theme_classic() +
  xlab("Q7 depth") +
  ylab("Fitted PC1") +  
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = "strm_pca1.png", 
#                width = 6, height = 6, units = "in")  
  
# PC2 explanatory plot
ggplot(gage_aug, aes(q1_q30_diff, .fittedPC2, 
                          color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q1 depth minus Q30 depth") +
  ylab("Fitted PC2") +
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = #"strm_pca1.png", 
#                width = 6, height = 6, units = "in")  



#ggplot2::ggsave(path = "figure/", filename = "strm_dens_mean.png", 
#                width = 6, height = 6, units = "in")  

# create density plot of all points
ggplot(gage_aug, aes(q7_depth, q1_q30_diff)) +
  geom_jitter(aes(color = sta, shape = ".")) + 
  geom_density2d(aes(color = NULL)) + 
  geom_point(data = gage_pca, mapping = aes(q7_mean, q1_q30_mean)) + 
  geom_text(data = gage_pca, 
           mapping = aes(q7_mean, q1_q30_mean, label = sta), 
            check_overlap = TRUE) + 
  scale_x_continuous(limits = c(-2, 1.5)) + 
  scale_y_continuous(limits = c(-1.0, 0.75)) + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  theme(legend.position = "none") 

#ggplot2::ggsave(path = "figure/", filename = "strm_dens_all.png", 
#                width = 6, height = 6, units = "in")  
```

