
Title (13-words or less): Relating Precipitation and Hydrologic Drought in a Semi-arid Climate with gSSURO Data  
Subtitle: Pine Ridge Reservation and surrounding Areas in Southwestern South Dakota 

```{r draft_language, include=FALSE, eval=FALSE}
## Introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

Intro p1: drought is a "creeping disaster"
Intro p2: what makes a drought? Intensity & duration 
Intro p3: different kinds of drought 
Intro p4: working with drought - skewed distribution -> how to fix?

## What are the gaps this paper hopes to address? 
Evaluate a multi-component regional drought history for the region
How to define minimum precipip record set & distributional characteristics 
Identify "pure" streamflow realizations that incorporate short streamflow gage records to characterize hydrologic drought
Identify spatial vars in gSSURGO data to relate lags to a physical process

# Specific Aims
1. Refine an approach for relating precipitation drought to hydrological drought in gaged watersheds
2. Develop a method to estimate regional hydrologic group membership for ungaged watersheds using gSSURGO data 

An example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

## Methods


## Results
Regional monthly precipitation fits a Pearson type-III distribution.
The mean and deviation of monthly precipitation depth among stations are similiar (e.g little spatial difference).  
However, the distribution of wet and dry periods is temporally dependant 
Station distance is a substantial predictor of covarience
=> Next STEP use distance from centroid to create a predictor variable to test significance..
Regional streamflow clusters into 7 groups.
Hydrologic export coefficient describes ~95% of varience and flashiness (e.g Q1 -Q3) describing 5% of varience.
```

```{r spi_overview eval=FALSE} 
# overview of steps---- 
# 2.1    Standardized Precipitation Index 
# note: SPI package not working; created a test case to send to author
# 2.1.1  Download & munge precip data: complete; _04_prco-data_munging
# 2.1.2  Identify distribution: complete;  _05_L-moment_diagram
# 2.1.3   Calculate SPI: completed spi1, spi6

# 2.2    Stream Drought Index
# 2.2.1  Download & mungeStream data: finished - July 16
# 2.2.2  Learn purrr::map(): ok.finished 
#          http://r4ds.had.co.nz/iteration.html#the-map-functions
# 2.2.3 Cluster time series - in progress
#         see Kassambara, "Practical Guide to Cluster Analysis in R
# 2.2.4   Calculate SDI
# 2.2.5   Delineate watersheds
# 2.2.6   Cluster ungaged stations
# 2.2.7 Learn 'rf' function
# 2.3   Disseminate results
# 2.3.1 Identify journal

## Analysis Steps & progress
# 1.    Identify precipitation records for drought analysis
#         I imported Global Historical Climatology Network (GHCN) 
#         daily precipitation records for candidate "WEATHER STATIONS" 
#         into R-Studio (REF1) using the "rnoaa" package.
# 1.2   Cleaned data (see 04_prcp-data_munging)  
# 1.2.1   I used Theissen polygons and the length and continuity of 
#         precipitation records for initial station selection 
# 1.2.2  'dplyr' to fill daily NA values with data from nearest station
# 1.2.3   create monthly vals from daily vals.
# 1.2.4 I removed short record: Long Valley after checking covariance.
# 1.3   Exploratory EDA
# 1.3.1 Applied sqrt & log10 transform to explore effects on skew 
# 1.3.2 Explored the data with box plots, violin plot.
#        Sqrt trans vs plotting vals look slightly sinusoidal
#        Log-tranformation is mirror of orig depth vs plotting
# 1.3.3 Applied Weibull plotting position & graphed on sqrt plot 
# 1.3.4 Completed exploratory PCA => covarience by zero months?
# 1.4.  Calculated L-moments and L-moment ratios => Pearson-type III
# 1.4.1 Calculated 1 & 6 month SPI 
# 1.5   Clustered streamflows during wet period 
# 1.5.1 Identify streamflow records for hydrology analysis (list) 
# 1.5.2 Exploratory EDA of streamflow to identify membership of 
#       short records 

## Thoughts - the orig depth vs plotting vals look j-shaped. 

# Next steps: 
#2. find drought years & wet years for precip
#3. Examine clusters for dry years
#4. Supervised classification - random forest should cross-validate?

# Someday - Maybe
# 1. Map the variable as a function - might put off, but ugly and long 
#   code below!
# 2. figure out how to reference stuff with - grateful package
```

```{r rules_for_papers, eval=FALSE, include=FALSE} 
<!---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ten simple rules for structuring papers 

Citation: Mensh B, Kording K (2017) Ten simple rules for structuring papers. PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619

Principles (Rules 1–4)
Rule 1: Focus your paper on a central contribution, which you communicate in the title

Your communication efforts are successful if readers can still describe the main contribution of your paper to their colleagues a year after reading it. Although it is clear that a paper often needs to communicate a number of innovations on the way to its final message, it does not pay to be greedy. Focus on a single message; papers that simultaneously focus on multiple contributions tend to be less convincing about each and are therefore less memorable.

The most important element of a paper is the title—think of the ratio of the number of titles you read to the number of papers you read. The title is typically the first element a reader encounters, so its quality [3] determines whether the reader will invest time in reading the abstract.

The title not only transmits the paper’s central contribution but can also serve as a constant reminder (to you) to focus the text on transmitting that idea. Science is, after all, the abstraction of simple principles from complex data. The title is the ultimate refinement of the paper’s contribution. Thinking about the title early—and regularly returning to hone it—can help not only the writing of the paper but also the process of designing experiments or developing theories.

This Rule of One is the most difficult rule to optimally implement because it comes face-to-face with the key challenge of science, which is to make the claim and/or model as simple as the data and logic can support but no simpler. In the end, your struggle to find this balance may appropriately result in “one contribution” that is multifaceted. For example, a technology paper may describe both its new technology and a biological result using it; the bridge that unifies these two facets is a clear description of how the new technology can be used to do new biology.

Rule 2: Write for flesh-and-blood human beings who do not know your work

Because you are the world’s leading expert at exactly what you are doing, you are also the world’s least qualified person to judge your writing from the perspective of the naïve reader. The majority of writing mistakes stem from this predicament. Think like a designer—for each element, determine the impact that you want to have on people and then strive to achieve that objective [4]. Try to think through the paper like a naïve reader who must first be made to care about the problem you are addressing (see Rule 6) and then will want to understand your answer with minimal effort.

Define technical terms clearly because readers can become frustrated when they encounter a word that they don’t understand. Avoid abbreviations and acronyms so that readers do not have to go back to earlier sections to identify them.

The vast knowledge base of human psychology is useful in paper writing. For example, people have working memory constraints in that they can only remember a small number of items and are better at remembering the beginning and the end of a list than the middle [5]. Do your best to minimize the number of loose threads that the reader has to keep in mind at any one time.

Rule 3: Stick to the context-content-conclusion (C-C-C) scheme

The vast majority of popular (i.e., memorable and re-tellable) stories have a structure with a discernible beginning, a well-defined body, and an end. The beginning sets up the context for the story, while the body (content) advances the story towards an ending in which the problems find their conclusions. This structure reduces the chance that the reader will wonder “Why was I told that?” (if the context is missing) or “So what?” (if the conclusion is missing).

There are many ways of telling a story. Mostly, they differ in how well they serve a patient reader versus an impatient one [6]. The impatient reader needs to be engaged quickly; this can be accomplished by presenting the most exciting content first (e.g., as seen in news articles). The C-C-C scheme that we advocate serves a more patient reader who is willing to spend the time to get oriented with the context. A consequent disadvantage of C-C-C is that it may not optimally engage the impatient reader. This disadvantage is mitigated by the fact that the structure of scientific articles, specifically the primacy of the title and abstract, already forces the content to be revealed quickly. Thus, a reader who proceeds to the introduction is likely engaged enough to have the patience to absorb the context. Furthermore, one hazard of excessive “content first” story structures in science is that you may generate skepticism in the reader because they may be missing an important piece of context that makes your claim more credible. For these reasons, we advocate C-C-C as a “default” scientific story structure.

The C-C-C scheme defines the structure of the paper on multiple scales. At the whole-paper scale, the introduction sets the context, the results are the content, and the discussion brings home the conclusion. Applying C-C-C at the paragraph scale, the first sentence defines the topic or context, the body hosts the novel content put forth for the reader’s consideration, and the last sentence provides the conclusion to be remembered.

Deviating from the C-C-C structure often leads to papers that are hard to read, but writers often do so because of their own autobiographical context. During our everyday lives as scientists, we spend a majority of our time producing content and a minority amidst a flurry of other activities. We run experiments, develop the exposition of available literature, and combine thoughts using the magic of human cognition. It is natural to want to record these efforts on paper and structure a paper chronologically. But for our readers, most details of our activities are extraneous. They do not care about the chronological path by which you reached a result; they just care about the ultimate claim and the logic supporting it (see Rule 7). Thus, all our work must be reformatted to provide a context that makes our material meaningful and a conclusion that helps the reader to understand and remember it.

Rule 4: Optimize your logical flow by avoiding zig-zag and using parallelism
Avoiding zig-zag.

Only the central idea of the paper should be touched upon multiple times. Otherwise, each subject should be covered in only one place in order to minimize the number of subject changes. Related sentences or paragraphs should be strung together rather than interrupted by unrelated material. Ideas that are similar, such as two reasons why we should believe something, should come one immediately after the other.
Using parallelism.

Similarly, across consecutive paragraphs or sentences, parallel messages should be communicated with parallel form. Parallelism makes it easier to read the text because the reader is familiar with the structure. For example, if we have three independent reasons why we prefer one interpretation of a result over another, it is helpful to communicate them with the same syntax so that this syntax becomes transparent to the reader, which allows them to focus on the content. There is nothing wrong with using the same word multiple times in a sentence or paragraph. Resist the temptation to use a different word to refer to the same concept—doing so makes readers wonder if the second word has a slightly different meaning.
The components of a paper (Rules 5–8)

The individual parts of a paper—abstract, introduction, results, and discussion—have different objectives, and thus they each apply the C-C-C structure a little differently in order to achieve their objectives. We will discuss these specialized structures in this section and summarize them in Fig 1.
thumbnail

Fig 1. Summary of a paper’s structural elements at three spatial scales: Across sections, across paragraphs, and within paragraphs.

Note that the abstract is special in that it contains all three elements (Context, Content, and Conclusion), thus comprising all three colors.

https://doi.org/10.1371/journal.pcbi.1005619.g001
Rule 5: Tell a complete story in the abstract

The abstract is, for most readers, the only part of the paper that will be read. This means that the abstract must convey the entire message of the paper effectively. To serve this purpose, the abstract’s structure is highly conserved. Each of the C-C-C elements is detailed below.

The context must communicate to the reader what gap the paper will fill. The first sentence orients the reader by introducing the broader field in which the particular research is situated. Then, this context is narrowed until it lands on the open question that the research answered. A successful context section sets the stage for distinguishing the paper’s contributions from the current state of the art by communicating what is missing in the literature (i.e., the specific gap) and why that matters (i.e., the connection between the specific gap and the broader context that the paper opened with).

The content (“Here we”) first describes the novel method or approach that you used to fill the gap or question. Then you present the meat—your executive summary of the results.

Finally, the conclusion interprets the results to answer the question that was posed at the end of the context section. There is often a second part to the conclusion section that highlights how this conclusion moves the broader field forward (i.e., “broader significance”). This is particularly true for more “general” journals with a broad readership.

This structure helps you avoid the most common mistake with the abstract, which is to talk about results before the reader is ready to understand them. Good abstracts usually take many iterations of refinement to make sure the results fill the gap like a key fits its lock. The broad-narrow-broad structure allows you to communicate with a wider readership (through breadth) while maintaining the credibility of your claim (which is always based on a finite or narrow set of results).

Rule 6: Communicate why the paper matters in the introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

As an example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

Rule 7-Results: Deliver the results as a sequence of statements, supported by figures, that connect logically to support the central contribution

The results section needs to convince the reader that the central claim is supported by data and logic. Every scientific argument has its own particular logical structure, which dictates the sequence in which its elements should be presented.

For example, a paper may set up a hypothesis, verify that a method for measurement is valid in the system under study, and then use the measurement to disprove the hypothesis. Alternatively, a paper may set up multiple alternative (and mutually exclusive) hypotheses and then disprove all but one to provide evidence for the remaining interpretation. The fabric of the argument will contain controls and methods where they are needed for the overall logic.

In the outlining phase of paper preparation (see Rule 9), sketch out the logical structure of how your results support your claim and convert this into a sequence of declarative statements that become the headers of subsections within the results section (and/or the titles of figures). Most journals allow this type of formatting, but if your chosen journal does not, these headers are still useful during the writing phase and can either be adapted to serve as introductory sentences to your paragraphs or deleted before submission. Such a clear progression of logical steps makes the paper easy to follow.

Figures, their titles, and legends are particularly important because they show the most objective support (data) of the steps that culminate in the paper’s claim. Moreover, figures are often viewed by readers who skip directly from the abstract in order to save time. Thus, the title of the figure should communicate the conclusion of the analysis, and the legend should explain how it was done. Figure making is an art unto itself; the Edward Tufte books remain the gold standard for learning this craft [7,8].

The first results paragraph is special in that it typically summarizes the overall approach to the problem outlined in the introduction, along with any key innovative methods that were developed. Most readers do not read the methods, so this paragraph gives them the gist of the methods that were used.

Each subsequent paragraph in the results section starts with a sentence or two that set up the question that the paragraph answers, such as the following: “To verify that there are no artifacts…,” “What is the test-retest reliability of our measure?,” or “We next tested whether Ca2+ flux through L-type Ca2+ channels was involved.” The middle of the paragraph presents data and logic that pertain to the question, and the paragraph ends with a sentence that answers the question. For example, it may conclude that none of the potential artifacts were detected. This structure makes it easy for experienced readers to fact-check a paper. Each paragraph convinces the reader of the answer given in its last sentence. This makes it easy to find the paragraph in which a suspicious conclusion is drawn and to check the logic of that paragraph. The result of each paragraph is a logical statement, and paragraphs farther down in the text rely on the logical conclusions of previous paragraphs, much as theorems are built in mathematical literature.
Rule 8: Discuss how the gap was filled, the limitations of the interpretation, and the relevance to the field

The discussion section explains how the results have filled the gap that was identified in the introduction, provides caveats to the interpretation, and describes how the paper advances the field by providing new opportunities. This is typically done by recapitulating the results, discussing the limitations, and then revealing how the central contribution may catalyze future progress. The first discussion paragraph is special in that it generally summarizes the important findings from the results section. Some readers skip over substantial parts of the results, so this paragraph at least gives them the gist of that section.

Each of the following paragraphs in the discussion section starts by describing an area of weakness or strength of the paper. It then evaluates the strength or weakness by linking it to the relevant literature. Discussion paragraphs often conclude by describing a clever, informal way of perceiving the contribution or by discussing future directions that can extend the contribution.

For example, the first paragraph may summarize the results, focusing on their meaning. The second through fourth paragraphs may deal with potential weaknesses and with how the literature alleviates concerns or how future experiments can deal with these weaknesses. The fifth paragraph may then culminate in a description of how the paper moves the field forward. Step by step, the reader thus learns to put the paper’s conclusions into the right context.
Process (Rules 9 and 10)

To produce a good paper, authors can use helpful processes and habits. Some aspects of a paper affect its impact more than others, which suggests that your investment of time should be weighted towards the issues that matter most. Moreover, iteratively using feedback from colleagues allows authors to improve the story at all levels to produce a powerful manuscript. Choosing the right process makes writing papers easier and more effective.
Rule 9: Allocate time where it matters: Title, abstract, figures, and outlining

The central logic that underlies a scientific claim is paramount. It is also the bridge that connects the experimental phase of a research effort with the paper-writing phase. Thus, it is useful to formalize the logic of ongoing experimental efforts (e.g., during lab meetings) into an evolving document of some sort that will ultimately steer the outline of the paper.

You should also allocate your time according to the importance of each section. The title, abstract, and figures are viewed by far more people than the rest of the paper, and the methods section is read least of all. Budget accordingly.

The time that we do spend on each section can be used efficiently by planning text before producing it. Make an outline. We like to write one informal sentence for each planned paragraph. It is often useful to start the process around descriptions of each result—these may become the section headers in the results section. Because the story has an overall arc, each paragraph should have a defined role in advancing this story. This role is best scrutinized at the outline stage in order to reduce wasting time on wordsmithing paragraphs that don’t end up fitting within the overall story.
Rule 10: Get feedback to reduce, reuse, and recycle the story

Writing can be considered an optimization problem in which you simultaneously improve the story, the outline, and all the component sentences. In this context, it is important not to get too attached to one’s writing. In many cases, trashing entire paragraphs and rewriting is a faster way to produce good text than incremental editing.

There are multiple signs that further work is necessary on a manuscript (see Table 1). For example, if you, as the writer, cannot describe the entire outline of a paper to a colleague in a few minutes, then clearly a reader will not be able to. You need to further distill your story. Finding such violations of good writing helps to improve the paper at all levels.
thumbnail

https://doi.org/10.1371/journal.pcbi.1005619.t001

Successfully writing a paper typically requires input from multiple people. Test readers are necessary to make sure that the overall story works. They can also give valuable input on where the story appears to move too quickly or too slowly. They can clarify when it is best to go back to the drawing board and retell the entire story. Reviewers are also extremely useful. Non-specific feedback and unenthusiastic reviews often imply that the reviewers did not “get” the big picture story line. Very specific feedback usually points out places where the logic within a paragraph was not sufficient. It is vital to accept this feedback in a positive way. Because input from others is essential, a network of helpful colleagues is fundamental to making a story memorable. To keep this network working, make sure to pay back your colleagues by reading their manuscripts.
Discussion

This paper focused on the structure, or “anatomy,” of manuscripts. We had to gloss over many finer points of writing, including word choice and grammar, the creative process, and collaboration. A paper about writing can never be complete; as such, there is a large body of literature dealing with issues of scientific writing [9,10,11,12,13,14,15,16,17].

Personal style often leads writers to deviate from a rigid, conserved structure, and it can be a delight to read a paper that creatively bends the rules. However, as with many other things in life, a thorough mastery of the standard rules is necessary to successfully bend them [18]. In following these guidelines, scientists will be able to address a broad audience, bridge disciplines, and more effectively enable integrative science.
Acknowledgments

We took our own advice and sought feedback from a large number of colleagues throughout the process of preparing this paper. We would like to especially thank the following people who gave particularly detailed and useful feedback:

Sandra Aamodt, Misha Ahrens, Vanessa Bender, Erik Bloss, Davi Bock, Shelly Buffington, Xing Chen, Frances Cho, Gabrielle Edgerton, multiple generations of the COSMO summer school, Jason Perry, Jermyn See, Nelson Spruston, David Stern, Alice Ting, Joshua Vogelstein, Ronald Weber.
References

    1. Hirsch JE (2005) An index to quantify an individual's scientific research output. Proc Natl Acad Sci U S A. 102: 16569–16572. pmid:16275915
    2. Acuna DE, Allesina S, Kording KP (2012) Future impact: Predicting scientific success. Nature. 489: 201–202. pmid:22972278
    3. Paiva CE, Lima JPSN, Paiva BSR (2012) Articles with short titles describing the results are cited more often. Clinics. 67: 509–513. pmid:22666797
    4. Carter M (2012) Designing Science Presentations: A Visual Guide to Figures, Papers, Slides, Posters, and More: Academic Press.
    5. Murdock BB Jr (1968) Serial order effects in short-term memory. J Exp Psychol. 76: Suppl:1–15.
    6. Schimel J (2012) Writing science: how to write papers that get cited and proposals that get funded. USA: OUP.
    7. Tufte ER (1990) Envisioning information. Graphics Press.
    8. Tufte ER The Visual Display of Quantitative Information. Graphics Press.
    9. Lisberger SG (2011) From Science to Citation: How to Publish a Successful Scientific Paper. Stephen Lisberger.
    10. Simons D (2012) Dan's writing and revising guide. http://www.dansimons.com/resources/Simons_on_writing.pdf [cited 2017 Sep 9].
    11. Sørensen C (1994) This is Not an Article—Just Some Thoughts on How to Write One. Syöte, Finland: Oulu University, 46–59.
    12. Day R (1988) How to write and publish a scientific paper. Phoenix: Oryx.
    13. Lester JD, Lester J (1967) Writing research papers. Scott, Foresman.
    14. Dumont J-L (2009) Trees, Maps, and Theorems. Principiae. http://www.treesmapsandtheorems.com/ [cited 2017 Sep 9].
    15. Pinker S (2014) The Sense of Style: The Thinking Person’s Guide to Writing in the 21st Century. Viking Adult.
    16. Bern D (1987) Writing the empirical journal. The compleat academic: A practical guide for the beginning social scientist. 171.
    17. George GD, Swan JA (1990) The science of scientific writing. Am Sci. 78: 550–558.
    18. Strunk W (2007) The elements of style. Penguin. 
```

```{r SPI_index_discuss, eval=FALSE, include=FALSE}
# The two SCI functions used below are fitSCI & tranformSCI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   fitSCI estimates the parameters for transforming a meteorological 
#   & environmental time series to a Standardized Climate Index (SCI).  

#       fitSCI(x, first.mon, time.scale, distr, p0, 
#         p0.center.mass = FALSE, scaling = c("no","max","sd"), 
#         mledist.par =  list(), start.fun = dist.start, 
#         start.fun.fix = FALSE, warn = TRUE, ...) 

#   transformSCI applies the transformation 
#       transformSCI(x, first.mon, obj, sci.limit = Inf,
#         warn = TRUE, ...) 

# description of arguments for the SCI functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# x	- numeric vector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A monthly univariate time series for SCI input 

# first.mon 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Value in [1:12] indicating month of the first element of x 

# time.scale 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The time scale is the window length of an backward-looking running 
# mean.  Time scale is an integer value. 

# distr	
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A character string "name" naming a distribution for which the 
# corresponding density function (dname), the corresponding 
# distribution function (pname) and the quantile function (qname) must 
# be defined (see for example GammaDist) 

# dist.para 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A column matrix containing the parameters of distribution distr for 
# each month. Row names correspond to the distribution parameters. 

# p0 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated separately.

# If p0=TRUE an additional row named P0 is introduced, indicating the 
# probability of zero (precipitation) events.

# If TRUE, model Probability of zero (precipitation) months is 
# modeled with a mixed distribution as D(x) = p0 + (1-p0)G(x), 
# where G(x) > 0 is the reference distribution (e.g. Gamma) p0 is the 
# probability of a zero (precipitation) month. 

# Following Stagge et al. (2014) the probability of zero events is 
# then estimated as p0 = (n_p)/(n + 1), where np refers to the number 
# of zero events and n is the sample size. 
# The resulting mixed distribution for SCI transformation is then: 

# g(x) = if(x > 0) p0 + (1 - p0) G(x) 
#   else if(x == 0) (np + 1)/(2(n + 1))
#    where G(x) > 0 is a model (e.g. gamma) distribution.

# p0.center.mass 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated using the "centre of mass" estimator 
# (see Stagge et al. (2014) for details).

# The probability of zero precipitation events (p0) can be estimated 
# using a "center of mass" estimate based on the Weibull plotting 
# position function to reduce biases in the presence of many zero 
# precipitation events by 'p0.center.mass = TRUE' 

# If TRUE, the Probability of zero (precipitation) is 
# estimated using a "center of mass" estimate based on the Weibull 
# plotting position function (see details). Only applies if p0=TRUE.

# scaling 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Indicates whether to do some scaling of x prior to parameter 
# identification. Scaling can stabilize parameter estimation. 
# "no" (the default) indicates no scaling. 
# "max" indicates scaling by the maximum of x, such that 
#      x <- x/max(x,na.rm=TRUE). 
# "sd" stands for scaling by the standard deviation. 
# warn	- Issue warnings if problems in parameter estimation occur. 

# mledist.par 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# named list that can be used to pass parameters to mledist in package 
# fitdistrplus.  # STILL NOT SURE ABOUT THIS!!!

# start.fun 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Function with arguments x and distr estimating initial 
# parameters of the function distr for each month. The function should 
# return a named list corresponding to the parameters of distr. 
# (See also dist.start)

# start.fun.fix 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical argument, indicating if parameter estimates 
# of start.fun should be used if maximum likelihood estimation breaks 
# down. This stabilizes the implementation but can introduce biases in 
# the resulting SCI.  Should look at this for Feb Interior.

# obj 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# an object of class fitSCI, output from fitSCI.

# sci.limit 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Truncate absolute values of SCI that are larger than sci.limit. 
# transformSCI allows for a truncation of the SCI series such that 
#   abs(sci) <= sci.limit. Uncertainty in distribution parameters can 
# cause unrealistically large or small SCI values if values in x 
# exceed the values used for parameter estimation. 
# The truncation can be disabled by setting sci.limit = Inf.

# Output flags 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# fitSCI returns an object of class "fitSCI".  Some flags are:
# dist.para.flag
# an vector indicating possible issues occurring throughout parameter 
# estimation. Possible values are: 
#     0. no problems occurred; 
#     1. starting values could not be estimated; 
#     2. mledist crashed with unknown error; 
#     3. mledist did not converge; 
#     4. all values in this month are NA; 
#     5. all values in this mon are constant, distribution not defined

# scaling
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# numeric value that has been used to scale x (see argument scaling). 
# A value of 1 results from scaling="no", other values are the maximum 
# value or the standard deviation of x, depending on the choice of the 
# parameter scaling. 

# call 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the function call transform SCI returns a numeric vector containing 
# the SCI, having values of the standard normal distribution. 
```

```{r PCA_&_clustering_approach, eval=FALSE, include=FALSE}
# Exploratory PCA--------------------------------------------------- 
# PCA is related to eigenvectors and eigenvalues.  The variance or 
# spread of the observations is measured as the average squared 
# distance from the center of the point cloud to each observation (c).  
# The total reconstruction error is measured as the average squared 
# length of the errors (b), and distance along the principal axis (a) 
# can also be measured.  Therefore the sum of the square of the errors 
# plus the sum of the square distance along the principal axis equals 
# the average squared distance between the center of the point cloud 
# each observation; this is precisely Pythagoras theorem. 

# You can imagine that the PC axis is a solid rod and each error 
# is a spring. The energy of the spring is proportional to its squared 
# length (this is known in physics as the Hooke's law), so the rod 
# will orient itself such as to minimize the sum of these squared 
# distances. 

# Regarding eigenvectors and eigenvalues. A 2×2 matrix given by: 
#   (1.07     0.63)
#   (0.63     0.64)

# The variance of the x variable is 1.07, 
# the variance of the y variable is 0.64, 
# and the covariance between them is 0.63. 

# As it is a square symmetric matrix, it can be diagonalized by 
# choosing a new orthogonal coordinate system, given by its 
# eigenvectors (incidentally, this is called spectral theorem); 
# corresponding eigenvalues will then be located on the diagonal. 
# In this new coordinate system, the covariance matrix is diagonal 
# and looks like this:
#   (1.52     0) 
#   (0     0.19)

# The correlation between points is now zero. It becomes clear that 
# the variance of any projection will be given by a weighted average 
# of the eigenvalues.  Consequently, the maximum possible variance 
# (1.52) will be achieved if we simply take the projection on the 
# first coordinate axis. It follows that the direction of the first 
# principal component is given by the first eigenvector of the 
# covariance matrix. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


 
# The traditional clustering methods, such as hierarchical clustering 
# and k-means clustering, are heuristic and are not based on formal  
# models. An alternative is model-based clustering, which consider the  
# data as coming from a distribution that is mixture of two or more  
# clusters (Fraley and Raftery 2002, Fraley #et al. (2012)). 
# Model-based clustering uses a soft assignment, where each data point 
# has a probability of belonging to each cluster.  In model-based 
# clustering, the data is considered as coming from a mixture of 
# density.  Each component (i.e. cluster) k is modeled by the normal 
# or Gaussian distribution which is characterized by the parameters: 
#   μk\mu_k: mean vector, 
#   ∑k\sum_k: covariance matrix, 
#   An associated probability in the mixture. Each point has a 
#     probability of belonging to each cluster.  

# The model parameters can be estimated using the Expectation-
# Maximization (EM) algorithm initialized by hierarchical model-based 
# clustering. Each cluster k is centered at the means μk\mu_k, with 
# increased density for points near the mean.
# Geometric features (shape, volume, orientation) of each cluster are 
# determined by the covariance matrix ∑k\sum_k. 

# Different possible parameterizations of ∑k\sum_k are available in 
# the R package mclust (see ?mclustModelNames).
# The available model options, in mclust package, are represented by 
# identifiers including: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV 
# and VVV. 

# The first identifier refers to volume, the second to shape and the 
# third to orientation. E stands for "equal", V for "variable" and I 
# for "coordinate axes".

# The Mclust package uses maximum likelihood to fit all these models, 
# with different covariance matrix parameterizations, for a range of 
# k components.

# The best model is selected using the Bayesian Information Criterion 
# or BIC. A large BIC score indicates strong evidence for the 
# corresponding model. 

# Visualizing model-based clustering
# Model-based clustering results can be drawn using the base 
# function plot.Mclust() [in mclust package]. We'll use the 
# function fviz_mclust() [in factoextra package] to create beautiful 
# plots based on ggplot2.

# Where the data contain more than two variables, fviz_mclust() 
# uses a principal component analysis to reduce the dimensionnality 
# of the data. The first two principal components are used to produce 
# a scatter plot of the data. However, if you want to plot the data 
# using only two variables of interest, c("insulin", "sspg"), 
# you can specify that in the fviz_mclust() function using the 
# argument choose.vars = c("insulin", "sspg").



# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Several R packages available from CRAN or Bioconducto perform 
# cluster validation, including: 

#|    Package   |    Function(s)   |      Author       |      Notes    |
#|:------------:|:----------------:|:-----------------:|:-------------:|
#|   cclust     | clustIndex()     |    Dimitriadou    | No user guide |
#|     fpc      | cluster.stats()  |       Hennig      | No user guide |
#|              | clusterboot()    |                   |               |
#| clusterRepro |                  | Kapp & Tibshirani | not general   |
#|  clusterSim  |                  | Walesiak & Dudek  | poor user guide |
#|  clusterStab |                  | MacDonald et al.  | narrow vignette |
#|     clue     | cl_validity() +  | Hornik, September |     maybe...    |
#|     e1071    | fclustIndex() ++ | Dimitriadou et al.| 2006  | unk.

# + validation for both paritioning methods 
#   (“dissimilarity accounted for”) and hierarchical methods 
#   (“variance accounted for”) 

# ++ fuzzy cluster validation measures.

# pam() in recommended package cluster 
# (Rousseeuw, Struyf, Hubert, and Maechler, 2005; Struyf, Hubert, and 
# Rousseeuw, 1996), and Mclust() in package mclust (Fraley, Raftery, 
# and Wehrens, 2005; Fraley and Raftery, 2003), are available as 
# components named cluster, clustering, and classification, 

#RWeka (Hornik, Hothorn, and Karatzoglou, 2006), cba 
# (Buchta and Hahsler, 2005), cclust (Dimitriadou, 2005), cluster, 
# e1071 (Dimitriadou, Hornik, Leisch, Meyer, and Weingessel, 2005), 
# flexclust (Leisch, 2006), flexmix (Leisch, 2004), kernlab 
# (Karatzoglou, Smola, Hornik, and Zeileis, 2004), and mclust 
# (and of course, clue itself).

```

```{r precip-EDA, include=FALSE, eval=FALSE}
# Purpose: EDA of precipitation data.
# Outcome: Differences among stations 1971-2018 are small.
#          less than +/-5 mm on average.

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time?  
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.

# drop rows with missing data - this organizes the stations to same 
# time series length
sta_prep <- sta_raw %>%  
  drop_na %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) 

# create abreviated column for plotting & join
abbrev <- data.frame("sta" = c("cot", "int", "oel", "ora", "rap"),
                     "sta_abrev" = c("C", "I", "E", "O", "R")) 

sta_prep <- full_join(sta_prep, abbrev, by = "sta")

# create a year_month column
sta_prep <- sta_prep %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) 

# find mean values by month & join
sta_mean <- sta_prep %>% 
  group_by(month, year) %>% 
  summarize(mean_depth = mean(depth)) %>% 
  ungroup() %>% 
  mutate(mean_depth = round(mean_depth, digits = 2)) %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) %>% 
  select(-year, -month)
 
sta_prep <- full_join(sta_prep, sta_mean, by = "yr_mon") 
rm(abbrev, sta_mean)

# create a column of deviation from the mean by year & month
sta_prep <- sta_prep %>%
  mutate(deviation = depth - mean_depth) 

# plot the precip data as an overall boxplot 
ggplot(sta_prep, aes(as.factor(sta), depth)) + 
  geom_violin() + 
  geom_boxplot() + 
  scale_y_sqrt() + 
  theme_bw() + 
  ggtitle("Weather stations near Pine Ridge Reservation, SD",  
          subtitle = "1971-2018") + 
  xlab("") +  
  ylab("Monthly depth in mm") + 
  NULL 

ggplot2::ggsave(filename = "prcp_boxplot.png", 
                width = 6, height = 6, units = "in") 

# plot the precip depth as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, depth)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# plot the precip deviations as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, deviation)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# prepare summary of station data & clean up
sta_sum <- sta_prep %>% 
  group_by(sta, month) %>% 
  summarise(mean_depth = mean(depth),
            sd_depth = sd(depth),
            mean_dev = mean(deviation)) 

rm(sta_prep)
```

```{r setup, include=FALSE, warning=FALSE}  
knitr::opts_chunk$set(echo = TRUE)      
options(tibble.print_max = 70) # sets tibble output for printing  
 
# Sets up the library of packages 
library("here")       # identifies where to save work 
library("rio")        # more robust I/O - to import and clean data 
library("janitor")    # tools to clean dirty data 
library("broom")      # tidies up objects 
 
library("SCI")        # calculates SPI & RDI 
library("forecast")   # using the BoxCox function
library("mclust")     # clustering algorithms
library("factoextra") # creates exploratory plots of clusters 
library("corrplot")   # creates correlations plots
library("lubridate")  # fixes dates 
library("tidyverse")  # data munging, and tidying tools

# check into before removing
#library("ggpubr") 
# library("munsell") # https://github.com/cwickham/munsell
# library("grateful") - not yet ready for R 3.5.0
# library("standardize")
# lmomco <- citation("lmomco")
# toBibtex(lmomco)

# probably delete from final version
#library("clValid")
#library("cluster")
#library("DataExplorer")
#library("lintr")
#library("test_that")
#library("jsonlite") # Convert between JSON data and R objects
#library("curl") # Drop-in replacement for base url
#library("listviewer") # htmlwidget for interactive views of R lists
# library("SPEI") # Calculates SPI-index # having some issues... 
#library("magrittr") # contains easier ways to say things about lists

# Session Info
a_session <- devtools::session_info() 
```

```{r variables, eval=FALSE, include=FALSE}
# variables 
# a_session    list variable of session information
# sta          precipitation station  
# spi          Standardized Precipitation Index vals  
# gage         streamflow gage 

# general modifiers:
# _meta        metadata  
# _raw         original imported data in wide format
# _gath        wide data changed to long data format
# _na          exploratory na values 
# _outlier     split outlier data 
# _typical     split not outlier data 
# _replace     intermediate value used to replace outlier value 

# _9398        first streamflow cluster by waterYear

# intermediate variables 
# abbrev       abbreviation; a look-up table 
# _mean        intermediate variable; mean value
# _sum         summary data 
# _prep        intermediate variable 

# regression variables:
# _lm          linear model 
# _aug         Added prediction info about each dataset observation 
# _fit         Fitted regression model information 
# _tidy        Summarizes information about model components 

# precipitation station identifiers:
# _cot        Cottonwood precipitation station
# _oel        Oelrichs precipitation station 
# _rap        Rapid City precipitation station 
# _int        Interior precipitation station 
# _ora        Oral precipitation station

# SCI input & intermediate variables:
# time_scale   sets the length of the averaging period 
# distrib      sets the distribution type 
# p_zero       sets a function to reduce zero-precip bias
# scale        scales input by subtract mean & divide by sd
# warn_me      sets explicit warning
# first_mon    sets the first month for SCI; based on the data

# .cot        Cottonwood precipitation station
# .oel        Oelrichs precipitation station 
# .rap        Rapid City precipitation station 
# .int        Interior precipitation station 
# .ora        Oral precipitation station
# .l          a variable as a list 
# _M          Correlation matrix 
# _check      Temporary variable used to check data

# SCI output variables
# zero_prob   probability of a zero depth month
# _att        SPI function attributes: scale, location, shape 
# _corr       final correlation dataframe
# _index      SPI index

# Gage PCA input & intermediate variables:
# _depth      streamflow depth; depth = Q/A 
# _lambda     BoxCox lambda values defined by a ML estimator 
#   _q1       one-day average streamflow volume or depth 
#   _q7       seven-day average streamflow volume or depth 
#   _q30      thirty-day average streamflow volume or depth 

# pca variables
# _input      matrix with only observations
# _meta       matrix with info about observations 
# _matrix     result of prcomp analysis as prcomp object 
# _eigen      tidy eigenvecter summary 
# _vars       tidy summary about pc loadings on variables 
# _aug        added prediction info about each dataset observation 
# _sum        summary of results

```

```{r spi-with-SCI, message=FALSE}   
# The following code calculates SPI using the Standardized   
# Climate Index (SCI) package.   
#   SCI is the Standardized Precipitation Index 
#   SRI is the Standardized Runoff Index (SRI) - used below 
# Another SCI package index is the Standardized Precipitation 
#   Evapotranspiration Index (SPEI) - not used

# import precipitation data---- 
sta_meta    <- as.tibble(import("data/sta_meta.csv")) %>%  
  mutate(min_date = ymd(mindate)) %>% 
  mutate(max_date = ymd(maxdate)) %>% 
  select(-mindate, -maxdate) %>% 
  mutate(dur_year = max_date - min_date) %>% 
  mutate(dur_year = days(dur_year)) %>% 
  mutate(dur_year = time_length(dur_year, unit = "year")) 
  
sta_raw     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  mutate(date = ymd(date)) %>%  
  arrange(date) 
 
# checks for NA based on length of dataframe
sta_chk <- sta_raw %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) %>% 
  drop_na(depth)  %>% 
  mutate(sta = as.factor(sta)) %>% 
    spread(sta, depth) 

rm(sta_chk) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Initial values for SCI calculations 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
time_scale <- 1  # sets the length of the averaging period 
distrib <- "pe3" # sets the distribution type 
p_zero <- TRUE   # sets a function to reduce zero-precip bias
p_zero_cm <- TRUE # ?????
scale <- "sd"    # scales input by subtract mean & divide by sd
warn_me <- TRUE  # sets explicit warning

# Set first month for each station 
first_mon_cot <- 6 
first_mon_int <- 11 
first_mon_oel <- 6 
first_mon_ora <- 5  
first_mon_rap <- 5 

# prepare station for analysis 
sta_cot <- sta_raw %>% arrange(date) %>% select(date, cot) 
sta_int <- sta_raw %>% arrange(date) %>% select(date, int) 
sta_ora <- sta_raw %>% arrange(date) %>% select(date, ora) 
sta_oel <- sta_raw %>% arrange(date) %>% select(date, oel) 
sta_rap <- sta_raw %>% arrange(date) %>% select(date, rap) 

 # change tibble to a vector as double
cot <- as.double(sta_cot$cot)
int <- as.double(sta_int$int) 
ora <- as.double(sta_ora$ora)
oel <- as.double(sta_oel$oel) 
rap <- as.double(sta_rap$rap) 

# notes: 
# A note on variable naming convention; the SCI package doesn't like 
#   snake_case variables 
# 1 month spi - Int did not close on february

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1mon-spi-with-SCI-1mo----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - throws a month 7 NA error.
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   1mon- combine & gather the SPI variables---- 
spi_gath01 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_01 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 2mon-spi-with-SCI-2mo----

# set SCI state variables 
time_scale <- 2  # sets the length of the averaging period 

# Cottonwoods SPI - 2 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value)

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   2mon- combine & gather the SPI variables---- 
spi_gath02 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_02 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 3mon-spi-with-SCI---- 
time_scale <- 3  # sets the length of the averaging period 

# Cottonwoods SPI - 3
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib,  
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   3mon - combine & gather the SPI variables---- 
spi_gath03 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_03 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 4mon-spi-with-SCI-4mo----
# notes:  ORAL HAS ONE SPI VAL OF -8
time_scale <- 4  # sets the length of the averaging period 

# Cottonwoods SPI - 4
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results

spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   4mon - combine & gather the SPI variables---- 
spi_gath04 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_04 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#   5mon-spi-with-SCI-5mo----
# notes: ora has an extreme low value & really high skew 
time_scale <- 5  # sets the length of the averaging period 

# Cottonwoods SPI - 5 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value



#   5mon - combine & gather the SPI variables---- 
spi_gath05 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_05 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 6mon-spi-with-SCI-6mo---- 
 time_scale <- 6  # sets the length of the averaging period 

# Cottonwoods SPI - 6
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   6mon - combine & gather the SPI variables---- 
spi_gath06 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_06 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#   9mon-spi-with-SCI-9mo----
# Notes: ora has an extreme low value & really high skew ??

# set SCI state variables 
time_scale <- 9  # sets the length of the averaging period 

# Cottonwoods SPI - 9 
#~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   9mon - combine & gather the SPI variables---- 
spi_gath09 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_09 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 12mon- spi-with-SCI-12mo----
time_scale <- 12  # sets the length of the averaging period 

# Cottonwoods SPI - 12
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)  

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results  
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   12mon - combine & gather the SPI variables---- 
spi_gath12 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_12 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 18mon-spi-with-SCI-18mo----
time_scale <- 18  # sets the length of the averaging period 

# Cottonwoods SPI - 18
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   18mon - combine & gather the SPI variables---- 
spi_gath18 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_18 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 24mon- spi-with-SCI-24mo---- 
time_scale <- 24  # sets the length of the averaging period 

# Cottonwoods SPI - 24
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   24mon - combine & gather the SPI variables---- 
spi_gath24 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_24 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 30mon- spi-with-SCI-30mo---- 
time_scale <- 30  # sets the length of the averaging period 

# Cottonwoods SPI - 30 
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   30mon - combine & gather the SPI variables---- 
spi_gath30 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_30 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 36mon- spi-with-SCI-36mo---- 
time_scale <- 36  # sets the length of the averaging period 

# Cottonwoods SPI - 36 
#~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   36mon - combine & gather the SPI variables---- 
spi_gath36 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_36 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 42mon- spi-with-SCI-42mo---- 
time_scale <- 42  # sets the length of the averaging period 

# Cottonwoods SPI - 42
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   42mon - combine & gather the SPI variables---- 
spi_gath42 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_42 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 48mon- spi-with-SCI-48mo---- 
time_scale <- 48  # sets the length of the averaging period 

# Cottonwoods SPI -  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   48mon - combine & gather the SPI variables---- 
spi_gath48 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_48 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# 54mon- spi-with-SCI-54mo---- 
time_scale <- 54  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   54mon - combine & gather the SPI variables---- 
spi_gath54 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_54 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# 60mon- spi-with-SCI-60mo---- 
time_scale <- 60  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   60mon - combine & gather the SPI variables---- 
spi_gath60 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_60 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
```

```{r join_spi_values} 

# clean up some of the unneeded variables 
rm(distrib, p_zero, p_zero_cm, scale, time_scale, warn_me, 
   sta_cot, sta_int, sta_oel, sta_ora, sta_rap, 
   cot, int, oel, ora, rap, 
   first_mon_cot, first_mon_int, first_mon_oel, 
   first_mon_ora, first_mon_rap) 

# Fix month 7 Interior 1-mo-SCI (this is actually Feb!)
# finds dates of record - note that uses Mar rather than Feb
spi_ml_fail <- spi_gath01 %>% 
  mutate(year = year(date)) %>%              # creates ymd
  mutate(mon = month(date)) %>% 
  mutate(day = day(date)) %>% 
  filter(mon == 3) %>%                       # filters dates for 'int'
  filter(sta == "int") %>% 
  mutate(mon = 2) %>% 
  mutate(date = str_c(year, mon, day, sep = "-")) %>% 
  mutate(date = ymd(date)) %>%               # creates updated months
  select(date, sta) %>% 
  group_by(sta) %>% 
  summarise(min_date = min(date))            # summarizes to min month
  
# calculates the average value for each of the dates
spi_na_fix <- spi_gath01 %>% 
  mutate(mon = month(date)) %>% 
  filter(mon == 2) %>% 
  spread(key = sta, value = spi_index) %>% 
  select(-mon) %>% 
  select(date, everything()) %>% 
  mutate(spi_index = rowMeans(.[,-1], na.rm = TRUE)) %>% 
  mutate(sta = "int") 

# filters by the min date for 'int' & prepares to append to spi_gath01
spi_na_fix <- spi_na_fix %>% 
  filter(date >= spi_ml_fail$min_date) %>% 
  select(date, sta, spi_index)

# append the missing values & clean up
spi_gath01 <- bind_rows(spi_gath01, spi_na_fix) 
rm(spi_ml_fail, spi_na_fix)

#create a list, join, rename, and gather the SPI_indices 
spi_index <- list(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
                   spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
                   spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
                   spi_gath42, spi_gath48, spi_gath54, spi_gath60) %>%
  reduce(left_join, by = c("date", "sta")) 

# renames as spi_length variables 
names(spi_index)[3:18] =
  c("1","2", "3", "4", "5", "6", 
    "9", "12", "18", "24", "30", "36", 
                   "42", "48","54", "60") 

# gather the dataframe 
spi_index <- spi_index %>% 
  gather(key = spi_length, val = spi_index, -date, -sta) %>% 
  mutate(spi_length = as.integer(spi_length)) 

# remove the joined dataframes
rm(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
                   spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
                   spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
                   spi_gath42, spi_gath48, spi_gath54, spi_gath60) 

# check for na values - these are caused by rolling averages
spi_index_na <- spi_index %>% 
  filter(is.na(spi_index) | 
           is.na(sta)) %>% 
  group_by(sta, spi_length) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(count_dif = spi_length - count) %>% 
  distinct(count_dif)                     # if ans = 1 then ok

# remove the na values
spi_index <- spi_index %>% 
  na.omit()

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a dataframe of spi attributes
spi_att01 <- pluck(spi_list_01, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 1)

spi_att02 <- pluck(spi_list_02, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 2) 

spi_att03 <- pluck(spi_list_03, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 3) 

spi_att04 <- pluck(spi_list_04, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 4) 

spi_att05 <- pluck(spi_list_05, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 5) 

spi_att06 <- pluck(spi_list_06, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 6) 

spi_att09 <- pluck(spi_list_09, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 9) 

spi_att12 <- pluck(spi_list_12, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 12) 

spi_att18 <- pluck(spi_list_18, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 18) 

spi_att24 <- pluck(spi_list_24, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 24) 

spi_att30 <- pluck(spi_list_30, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 30) 

spi_att36 <- pluck(spi_list_36, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 36) 

spi_att42 <- pluck(spi_list_42, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 42) 

spi_att48 <- pluck(spi_list_48, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 48) 

spi_att54 <- pluck(spi_list_54, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 54) 

spi_att60 <- pluck(spi_list_60, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 60) 

#create a list, and join SPI attributes 
spi_att <- list(spi_att01, spi_att02, spi_att03, spi_att04, 
                   spi_att05, spi_att06, spi_att09, spi_att12, 
                   spi_att18, spi_att24, spi_att30, spi_att36, 
                   spi_att42, spi_att48, spi_att54, spi_att60) %>%
  reduce(left_join, by = c("property", "month")) %>%
  mutate(month = as.integer(month)) %>% 
  gather(key, spi_len, -month, -property, -starts_with("val")) %>% 
  select(-key) %>% 
  gather(key, val, -month, -property, -spi_len) %>% 
  select(-key) 

# clean up environment 
rm(spi_att01, spi_att02, spi_att03, spi_att04, 
                   spi_att05, spi_att06, spi_att09, spi_att12, 
                   spi_att18, spi_att24, spi_att30, spi_att36, 
                   spi_att42, spi_att48, spi_att54, spi_att60) 

rm(spi_list_01, spi_list_02, spi_list_03, spi_list_04, 
                   spi_list_05, spi_list_06, spi_list_09, spi_list_12, 
                   spi_list_18, spi_list_24, spi_list_30, spi_list_36, 
                   spi_list_42, spi_list_48, spi_list_54, spi_list_60) 
rm(spi_index_na)
```

```{r examine-SCI-values, eval=FALSE}  
# This code chunk examines & fixes the really large negative values
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Visualize initial SPI values----
ggplot(spi_index, aes(as.integer(spi_length), spi_index)) + 
  geom_jitter() +  
  facet_grid(rows = vars(sta)) + 
  theme_bw() +
  ggtitle("Initial SPI Plot", 
          subtitle = "Typical values are between -4 to 4") 

# save intial plot
#ggplot2::ggsave(filename = "figure/initial_spi.png", 
#                width = 6, height = 6, units = "in")

# split outliers from typical values----
spi_index_outlier <- spi_index %>% 
  filter(spi_index < -4) %>% 
  as.tibble() 

spi_index_typical <- spi_index %>% 
  filter(spi_index > -4) %>%
  arrange(spi_index)

# plot outlier years----
sta_outlier <- sta_raw %>% 
  gather(key = sta, value = depth, -date, -year, -month) %>% 
  filter(date == "1977-02-01" | 
    date == "2005-01-01" | 
    date == "2007-01-01" | 
    date == "2012-11-01" | 
    date == "1976-05-01" |
    date == "1981-09-01" |
    date == "2007-02-01"
      ) 

ggplot(sta_outlier, aes(date, depth, color = factor(sta))) + 
  geom_point() + 
  theme_bw() +
  ggtitle("Plot of outlier years") 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# spi_index_typical - calculated above 
# these are the min SPI vals from Cottonwood during depression 
#  date       sta   spi_index spi_length 
#   <date>     <chr>     <dbl>      <dbl> 
# 1 1936-10-01 cot       -3.31          6 
# 2 1937-01-01 cot       -3.27          9 
# 3 1936-09-01 cot       -3.26          4 
# 4 1936-09-01 cot       -3.25          5 
# 5 1936-10-01 cot       -3.25          9  

# spi_index_outlier - calculated above 
# date       sta   spi_index spi_length
#  <date>     <chr>     <dbl>      <dbl>
#1 1977-02-01 ora       -8.03          5   * caused by a zero depth
#2 2005-01-01 ora       -7.51         12
#3 2007-01-01 ora       -8.01          4
#4 2012-09-01 int       -8.03          1 
#5 2012-11-01 ora       -7.71         12 
#6 1976-05-01 ora       -7.93         30
#7 1981-09-01 ora       -7.73         36
#8 2007-02-01 ora       -8.01         36

# Outlier values could be approximated by the mean SPI  
# or the nearest neighbor (Oral <- Oelrichs) - I used mean SPI  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# create replacement values
spi_index_replace <- spi_index_typical %>%          # pulls outliers
  filter(date == "1977-02-01" &  spi_length == 5   | 
           date == "2005-01-01" & spi_length == 12 |  
           date == "2007-01-01" & spi_length == 4  | 
           date == "2012-11-01" & spi_length == 12 | 
           date == "2012-09-01" & spi_length == 1  | 
           date == "1976-05-01" & spi_length == 30 | 
           date == "1981-09-01" & spi_length == 30 | 
           date == "2007-02-01" & spi_length == 30  
         )  %>% 
    group_by(date, spi_length) %>% 
  summarize(spi_index = mean(spi_index)) %>%
  ungroup()                      # calculates mean values 

# update replacement values & join with typical values 
spi_index_replace <- spi_index_outlier %>% 
  select(date, sta) %>% 
  full_join(., spi_index_replace, by = "date")

spi_index2 <- bind_rows(spi_index_typical, spi_index_replace) %>% 
  arrange(sta, spi_length, date)

# check the values after the fix - the outlier values are now fixed 
# spi_index2
# A tibble: 48,011 x 4
#   date       sta   spi_index spi_length
#   <date>     <chr>     <dbl>      <dbl>
# 1 1936-10-01 cot       -3.31          6
# 2 1937-01-01 cot       -3.27          9
# 3 1936-09-01 cot       -3.26          4
# 4 1936-09-01 cot       -3.25          5
# 5 1936-10-01 cot       -3.25          9

# update the index in memory 
spi_index <- spi_index2 %>% 
  arrange(spi_index)

# clean up
rm(spi_index_outlier, spi_index_replace, 
   spi_index_typical, spi_index2, sta_outlier)
```

```{r final-spi-plot}  
spi_index_plot <- spi_index %>% 
  filter(date > "1990-01-01")  %>% 
  filter(spi_length == 1 | 
           spi_length == 6 |
           spi_length == 12 | 
           spi_length == 24 |
           spi_length == 36 |
           spi_length == 48 | 
           spi_length == 60
         ) %>%
  mutate(year = year(date))

ggplot(spi_index_plot, aes(year, spi_index)) + 
  geom_point() + 
  geom_smooth() + 
  geom_hline(yintercept = 0, color = "gray") +
  facet_grid(rows = vars(spi_length), 
             cols = vars(sta)) +
  theme_classic() + 
  ggtitle("SPI Index values for Pine Ridge reservation stations",
          subtitle = "1990-01-01 to 2018-05-30")

ggplot2::ggsave(filename = "figure/spi_vals.png", 
                width = 8, height = 8, units = "in") 

# the model shows spi values are highly correlated with only real 
# difference being the number of years in the calculation.
# However, individual droughts differ in space & time 

#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() + 
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0) + 
#  scale_x_date(limits = as.Date(c('1980-01-01','2018-01-01')), 
#                 date_breaks = "1 year")  
#labels = date_format("%b-%Y")
```

```{r spi-correlation-analysis}
# visually check results
#spi_index_plot <- spi_index %>% 
#  filter(spi_length == 12)
  
#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() +
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0, aes)

#ggplot2::ggsave(path = "figure/", filename = "spi_1mo.png", 
#                width = 6, height = 6, units = "in")  

# create correlation matrix inputs
spi_M <- spi_index %>% 
  select(-date) %>% 
  group_by(sta, spi_length) %>% 
  mutate(grouped_id = row_number()) %>% 
  spread(key = sta, value = spi_index) %>% 
  drop_na() %>% 
  ungroup() %>% 
  select(-grouped_id) %>% 
  select(spi_length, cot, int, oel, ora, rap) # ensure vars order 

# create correlation matrix names from the correlation matrix vars
spi_M_names <- spi_M %>% 
  filter(spi_length == 1) %>% 
  cor() %>% 
  as.tibble() %>% 
  names() %>% 
  as.tibble() %>% 
  slice(-1) %>% 
  mutate(value2 = value) %>% 
  mutate(value3 = value) %>%  
  mutate(value4 = value) %>%
  mutate(value5 = value) %>% 
  gather(key, sta2) %>% 
  select(-key) %>% 
  rownames_to_column() 

# create second station names column 
spi_M_names2 <- spi_M_names %>% 
  arrange(sta2) %>% 
  rename(sta1 = sta2) %>% 
  select(-rowname)

# bind the names columns
spi_M_names <- bind_cols(spi_M_names, spi_M_names2)  
rm(spi_M_names2)

# create a correlation matrix from SPI vals
spi_M <- spi_M %>% 
  split(.$spi_length) %>% 
  purrr::map_dfr(~ cor(.)) %>% 
  drop_na() %>% 
  slice(-1) %>% 
  rownames_to_column() 

# bind names to the correlation matrix 
spi_M <- full_join(spi_M_names, spi_M, by = "rowname")
spi_M <- spi_M %>% 
  select(sta1, sta2, everything()) %>%
  select(-rowname)
  
# prepare lookup table of lat-lons
sta_loc <- sta_meta %>% 
  arrange(name) %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  select(sta, lat, lon, dur_year)

# join the 'from' lat lons
spi_corr <- full_join(spi_M, sta_loc, by = c("sta1" = "sta")) %>% 
  rename(lat1 = lat) %>% 
  rename(lon1 = lon) 

# join the 'to' lat lons
spi_corr <- full_join(spi_corr, sta_loc, by = c("sta2" = "sta")) %>% 
  rename(lat2 = lat) %>% 
  rename(lon2 = lon) %>% 
  mutate(year_dif = abs(dur_year.x - dur_year.y))
  

# convert the 'to' and 'from' lat-lons to northings & eastings & distance
lat_to_km <- 111.03 # 1 degree lat to km @ lat 40-degrees 
lon_to_km <- 85.39  # 1 degree lon to km @ lat 40-degrees 
  
spi_corr <- spi_corr %>% 
  mutate(northing = abs((lat1 - lat2)) * lat_to_km) %>% 
  mutate(easting = abs((lon1 - lon2)) * lat_to_km) %>% 
  mutate(distance = sqrt(northing^2 + easting^2)) %>% 
  select(year_dif, everything()) %>%
  select(-(lat1:easting)) %>% 
  mutate(stations = paste(sta1, sta2, sep = "_")) %>% 
  gather(key = spi_length, value = pears_r, -distance, 
         -stations, -sta1, -sta2, -year_dif) %>% 
  filter(distance > 0) %>% 
  mutate(spi_length = as.double(spi_length))


# clean up the global environment 
rm(spi_M, spi_M_names, sta_loc, lat_to_km, lon_to_km)

# model effect of averaging time & distance on correlation----
# fit a linear model
spi_lm <- lm(pears_r ~ distance + spi_length + year_dif, 
             data = spi_corr)

# augment & gather the original data
spi_corr_aug <- augment(spi_lm, spi_corr) 

spi_corr_gath <- spi_corr_aug %>% 
  select(-(sta1:sta2)) %>%
  select(year_dif:.fitted) %>% 
  gather(key = factor, val, -stations, -pears_r) %>% 
  mutate(val = as.double(val))  

# plot the original data and fitted model for SPI----  
ggplot(spi_corr_gath, aes(val, pears_r)) + 
  geom_point(aes(color = factor(stations))) +
  facet_wrap(ncol = 1, vars(factor), scales = "free") +
    geom_smooth(method = lm) + 
  theme_classic()

spi_lm_fit <- glance(spi_lm) 

spi_lm_tidy <- tidy(spi_lm) %>% 
  mutate(
    low = estimate - std.error,
    high = estimate + std.error
  )

# clean-up Global Environment----
spi_corr <- spi_corr_aug 
rm(spi_corr_aug, spi_corr_gath, spi_lm ,spi_lm_fit)


rm(spi_corr, spi_lm_tidy)
```

```{r SCI-diagnostic-plots, eval=FALSE}
# Diagnostic plots of SPI transforms 

# PE3 scale plot---- 
spi_scale <- spi_att %>% 
  filter(property == "scale")

ggplot(spi_scale, aes(as.factor(month), spi_value)) +  
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_scale.png", 
                width = 6, height = 6, units = "in") 

# PE3 location plot---- 
spi_location <- spi_att %>% 
  filter(property == "location")

ggplot(spi_location, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_location.png", 
                width = 6, height = 6, units = "in") 

# PE3 shape plot---- 
spi_shape <- spi_att %>% 
  filter(property == "shape")

ggplot(spi_shape, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_shape.png", 
                width = 6, height = 6, units = "in") 

# clean up Global Environment----
rm(spi_location, spi_shape, spi_scale) 
```

# STREAMFLOW DATA ORGANIZATION
```{r streamflow-fix-NA-OLD}  
# Stream flow is being analyzed over a period of 1990-2016.  
# This code chunk fixes some NA values for Q7 & Q30
# at the start of record

# import and append daily flow values > 1990 
gage_cont <- import("data/gage_cont.csv") %>%  
  clean_names() %>%                                # N = 437,680
  filter(water_year >= 1990) %>%                   # n = 217,335  
  select( -c(julian:month_seq, site_no, i)) %>%
  select(sta, water_year, everything()) 
  
gage_disc <- import("data/gage_disc.csv") %>% 
  clean_names() %>%                               # N = 186,608
  filter(water_year >= 1990) %>%                  # n =  27,280 
  select(date:sta)  %>%
  select( -c(julian:month_seq, i)) %>% 
  select(sta, water_year, everything()) %>%       # remove 'P' na vals
  filter(qualifier != "P")                        # n =  27,252

gage_other <- import("data/gage_other.csv") %>% 
  clean_names() %>%                                 # N = 116,927
  filter(water_year >= 1990) %>%                    # n =  42,339  
  select(date:sta) %>% 
  select( -c(julian:month_seq, i)) %>% 
  select(sta, water_year, everything()) 

# bind the gage dfs together, make min & max yr vars - clean up
gage_na <- bind_rows(gage_cont, gage_disc, gage_other) %>% 
  group_by(sta) %>%                               # n = 203 obs   
  filter(is.na(q30))   # drops some NA vals from start of record 

gage_complete <- bind_rows(gage_cont, gage_disc, gage_other) %>% 
  group_by(sta) %>%                               # n = 286,723  
  filter(!is.na(q30))   # drops some NA vals from start of record 

# find actual count of years - post1990  
gage_summary <- gage_complete %>%                      # N = 44 sta
  group_by(sta) %>%                                    # n = 32 sta 
  summarise(min_yr = min(water_year),            
            max_yr = max(water_year)             
            ) %>% 
  ungroup() %>% 
  mutate(count_years_act = max_yr - min_yr) %>% 
  filter(count_years_act >= 5) %>% 
  filter(
    sta != "bad_mid" &         # incomplete days        
    sta != "che_ang" &         # incomplete days
    sta != "whi_cra" &         # dropped for short rec.
    sta != "che_was" &         # dropped for >> area    
    sta != "cas_hot" & 
    sta != "bat_key" &         # dropped for crystaline geology
    sta != "sna_bur" &  
    sta != "sna_dou")            
 
# remove stations with < 5 yr records - clean up 
gage_complete <- semi_join(gage_complete, gage_summary, 
                           by = "sta")                  # n = 249,247 
                                                  
# update gage summary 
gage_summary <- gage_complete %>%                        # n = 689 obs
  group_by(sta, water_year) %>%                 
  summarise(days_record = n()) %>%                  
  ungroup()                                   

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#fit linear model for Q7 missing observations

# filter-join the complete data with the gages with na
gage_nested <- semi_join(gage_complete, gage_na, 
                         by = "sta") 

# nest the gage values to prepare for fitting a linear model
gage_nested <- gage_nested %>% 
  group_by(sta) %>% 
  nest()

# create a linear model function
q_vs_q7 <- function(df) {
  lm(log(q7) ~ log(q), data = df)
}

# calculate q7 coeffs 
q7_coeffs <- gage_nested %>% 
  mutate(fit = map(data, q_vs_q7),
         tidy = map(fit, tidy)) %>% 
  select(sta, tidy) %>% 
  unnest(tidy) %>% 
  select(sta:estimate) %>% 
  spread(term, estimate) %>% 
  rename(beta = '(Intercept)') %>% 
  rename(alpha = 'log(q)') 

# estimate q7 values based on regression coeffs
gage_na <- inner_join(gage_na, q7_coeffs, 
                      by = "sta") %>% 
  mutate(log_q7 = alpha*log_q + beta) %>% 
  mutate(q7_est = exp(log_q7)) %>% 
  mutate(q7 = case_when(
    is.na(q7) ~ q7_est, 
    TRUE ~ as.double(q7)
  )) %>% 
  select(-c(beta:log_q7))

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#fit linear model for Q30 missing observations

# create a linear model function
q_vs_q30 <- function(df) {
  lm(log(q30) ~ log(q), data = df)
}

# calculate q30 coeffs 
q30_coeffs <- gage_nested %>% 
  mutate(fit = map(data, q_vs_q30),
         tidy = map(fit, tidy)) %>% 
  select(sta, tidy) %>% 
  unnest(tidy) %>% 
  select(sta:estimate) %>% 
  spread(term, estimate) %>% 
  rename(beta = '(Intercept)') %>% 
  rename(alpha = 'log(q)') 

# estimate q30 values based on regression coeffs
gage_na <- inner_join(gage_na, q30_coeffs, 
                      by = "sta") %>% 
  mutate(log_q30 = alpha*log_q + beta) %>% 
  mutate(q30_est = exp(log_q30)) %>% 
  mutate(q30 = case_when(
    is.na(q30) ~ q30_est, 
    TRUE ~ as.double(q30)
  )) 

# join the estimated data to the complete data
gage_na <- gage_na %>% 
  select(-c(q7_est:q30_est))

gage <- bind_rows(gage_complete, gage_na)

# clean up
rm(gage_complete, gage_cont, gage_disc, gage_nested, 
   gage_other, gage_summary, q30_coeffs, q7_coeffs, q_vs_q30,
   q_vs_q7)    

export(gage, file = "data/gage_inputs.csv") 


ggplot(gage_na, aes(q30, q30_est)) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  facet_wrap(vars(sta))

whi_whi <- gage %>% 
  filter(sta == "whi_whi")
```

```{r streamflow-fix-NA}   
# Stream flow is being analyzed over a period of 1990-2016.  
# This code chunk fixes some NA values for Q7 & Q30
# at the start of record

# import and append daily flow values > 1990 
gage_cont <- import("data/gage_cont.csv") %>%  
  clean_names() %>%                                # N = 437,680
  filter(water_year >= 1990) %>%                   # n = 217,335  
  select( -c(julian:month_seq, site_no, i)) %>%
  select(sta, water_year, everything()) 
  
gage_disc <- import("data/gage_disc.csv") %>% 
  clean_names() %>%                               # N = 186,608
  filter(water_year >= 1990) %>%                  # n =  27,280 
  select(date:sta)  %>%
  select( -c(julian:month_seq, i)) %>% 
  select(sta, water_year, everything()) %>%       # remove 'P' na vals
  filter(qualifier != "P")                        # n =  27,252

gage_other <- import("data/gage_other.csv") %>% 
  clean_names() %>%                                 # N = 116,927
  filter(water_year >= 1990) %>%                    # n =  42,339  
  select(date:sta) %>% 
  select( -c(julian:month_seq, i)) %>% 
  select(sta, water_year, everything()) 

# bind the gage dfs together, make min & max yr vars - clean up
gage_complete <- bind_rows(gage_cont, gage_disc, gage_other) %>% 
  group_by(sta) %>%                               # n = 286,723  
  filter(!is.na(q30))   # drops some NA vals from start of record 

# find actual count of years - post1990  
gage_summary <- gage_complete %>%                      # N = 44 sta
  group_by(sta) %>%                                    # n = 32 sta 
  summarise(min_yr = min(water_year),            
            max_yr = max(water_year)             
            ) %>% 
  ungroup() %>% 
  mutate(count_years_act = max_yr - min_yr) %>% 
  filter(count_years_act >= 5) %>% 
  filter(
    sta != "bad_mid"  &      # incomplete days        
    sta != "che_ang"  &      # incomplete days
    sta != "whi_cra"  &      # dropped for short rec.
    sta != "che_was"  &      # dropped for >> area    
    sta != "cas_hot"  &      # dropped for short rec.
    sta != "bat_key"  &      # dropped for crystaline geology
    sta != "sna_bur"  &      # dropped for short rec.
    sta != "sna_dou"  &      # dropped for short rec.
    sta != "che_sce")        # dropped for upstream flow control (dam)
 
# remove stations with < 5 yr records - clean up 
gage_complete <- semi_join(gage_complete, gage_summary, 
                           by = "sta")                   # n = 245,440 
                                                  
# update gage summary 
gage_summary <- gage_complete %>%                        # n = 689 obs
  group_by(sta, water_year) %>%                 
  summarise(days_record = n()) %>%                  
  ungroup()                                   

# clean up
rm(gage_other, gage_cont, gage_disc)    


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# test the data for duplicates & remove duplicate vals
gage_test <- gage_complete %>%  
  group_by(sta, water_year) %>% 
  summarize(count = n()) %>% 
  ungroup() %>% 
  filter(count > 366) 

export(gage_complete, file = "data/gage_inputs.csv") 
```

```{r streamflow-data-organization}   
# Stream flow is being analyzed over a period of 1990-2016.  
# This code chunk identifies missing gage-year combinations  
# An iterative approach is used to identify cluster memberships &  
# fill missing years with the streamflow depths of the nearest 
# neighbor in the group. 

# The 'wet-cycle' transitions to 'dry cycle' in 2002, 
#    and back in xxxx  

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")   # n = 245,440 obs 

# import estimates from prior runs 
gage_est_9002  <- import(file = "data/gage_est_9002.csv") 
  # run01
                                                     # n =  15,168 obs

gage_est_0317  <- import(file = "data/gage_est_0317.csv")  %>% # run02 
  select(-contrib_drain_area_va)  
                 # n =   2,365 obs

gage_est_9017a <- import(file = "data/gage_est_9017a.csv") %>% 
  select(-contrib_drain_area_va)                  # run03 
                                                     # n =  17,063 obs

# bind the gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002, 
                  gage_est_0317,  gage_est_9017a) %>% 
  group_by(sta) %>%                            # orig  n = 249,247 obs    
  mutate(min_yr = min(water_year)) %>%         # run01 n = 260,608  
  mutate(max_yr = max(water_year)) %>%         # run02 n = 262,973      
           ungroup()                           # run03 n = 280,056 

# find actual count of years - post1990  
gage_summary <- gage %>%                     
  group_by(sta) %>%                           
  summarise(min_yr = min(water_year),        
            max_yr = max(water_year)       
            ) %>% 
  ungroup() %>% 
  mutate(count_years_act = max_yr - min_yr) 
 
gage <- semi_join(gage, gage_summary, by = "sta")                   

# update gage summary & gage
gage_summary <- gage %>%                           #  orig n = 678 obs
  group_by(sta, water_year) %>%                    # n run01 = 717 obs
  summarise(days_record = n()) %>%                 # n run02 = 723 obs   
  ungroup()                                        # n run03 = 769 obs 
                                                   # n run04 = 769 obs 

# find & split stations-years with incomplete days of record 
gage_incomp <- gage %>%                           # orig  n = 15 obs 
  group_by(sta, water_year) %>%                   # run01 n =  7 obs  
  summarise(days_record = n()) %>%                # run02 n =  6 obs
  ungroup() %>%                                   # run03 n =  4 obs      
  filter(days_record < 365)                           

gage_incomp <- semi_join(gage, gage_incomp, 
                         by = c("sta", "water_year")) 
                                               # run00 n =   3,279 obs 
                                               # run01 n =   1,282 obs 
                                               # run02 n =   1,090 obs  
                                               # run03 n =     642 obs 
                                               # run03 n =     642 obs 

gage <- full_join(gage, gage_summary, 
                  by = c("sta", "water_year")) %>%  
  filter(days_record >= 365)                   # run00 n = 242,161 obs    
                                               # run01 n = 259,326 obs   
                                               # run02 n = 261,883 obs  
                                               # run03 n = 279,414 obs  
                                               # run04 n = 279,414 obs  

# add area to gage & gage_incomp_year 
gage_meta <- import("data/gage_meta_full.csv") 
gage_meta <- semi_join(gage_meta, gage_summary, by = "sta")

gage <- full_join(gage, gage_meta, 
                  by = "sta") %>% 
  select(date, sta, q, log_q:q30, water_year, qualifier, 
         contrib_drain_area_va)

gage_incomp <- full_join(gage_incomp, gage_meta, by = "sta") %>% 
  select(date, sta, q, log_q:q30, 
         water_year, contrib_drain_area_va, 
         qualifier) %>% 
  filter(!is.na(q)) 


# if gage incomplete is zero, then 
gage_input <- gage       # for 1990-2002 data 
```

```{r find_missing_vals_run01, eval=FALSE}  
# Estimate missing station vals - run 01  
# [1990, 2002] filtered based on 2002 complete records 

gage_run01_sum <- gage_summary %>% 
  spread(water_year, days_record) %>% 
  select(c(sta:'2002')) %>% 
  filter(!is.na(`1996`)) %>%                         # 32 sta 
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>%
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 # n =     351 obs

gage_run01 <- semi_join(gage, gage_run01_sum, 
                        by = c("sta", "water_year")) # n = 111,031 obs

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run01_na <- gage_run01 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run01_incomp <- semi_join(gage_incomp, gage_run01_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        # n        =   1,997 obs

# ~~~~~~~~~~~~~~~~~~~~~~~ 
# show a roadmap 
gage_run01_map <- semi_join(gage_summary, gage_run01_sum,
                            by = "sta") %>% 
  spread(water_year, days_record)   
 
gage_input <- gage_run01  

rm(gage_run01_na) 
```

```{r find_missing_vals_run02, eval=FALSE} 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Estimate missing station vals - run 02  
# [2003, 2017] filtered based on 2007 complete records 

gage_run02_sum <- gage_summary %>% 
  spread(water_year, days_record) %>% 
  select(c(sta,'2003':'2017')) %>% 
  filter(!is.na(`2007`)) %>%                         # 25 sta 
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>%
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 # n =     360 obs

gage_run02 <- semi_join(gage, gage_run02_sum, 
                        by = c("sta", "water_year")) # n = 128,939 obs

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run02_na <- gage_run02 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run02_incomp <- semi_join(gage_incomp, gage_run02_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        # n        =   375 obs
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# show a roadmap 
gage_run02_map <- semi_join(gage_summary, gage_run02_sum,
                            by = "sta") %>% 
  spread(water_year, days_record)

# gage_input changes with each run...  sorry!
gage_input <- gage_run02                       # run02 = 132,592 obs

rm(gage_meta, gage_incomp, gage_run02, 
   gage_run02_na) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

```{r find_missing_vals_run03, eval=FALSE}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Estimate missing station vals - run 03 
# [1990, 2017] filtered based on 2005 complete records 

gage_run03_sum <- gage_summary %>% 
  spread(water_year, days_record) %>% 
#  select(c(sta:'2005')) %>%                    # subsets the matrix
  filter(!is.na(`2005`)) %>%                            # 24 sta 
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>%
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 # n =     672 obs

gage_run03 <- semi_join(gage, gage_run03_sum, 
                        by = c("sta", "water_year")) # n = 227,917 obs

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run03_na <- gage_run03 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run03_incomp <- semi_join(gage_incomp, gage_run03_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        # n        =   448 obs
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# show a roadmap 
gage_run03_map <- semi_join(gage_summary, gage_run03_sum,
                            by = "sta") %>% 
  spread(water_year, days_record)

# gage_input changes with each run...  sorry!
gage_input <- gage_run03                       

rm(gage_meta, gage_incomp, gage_run03, gage_run03_na) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

```{r find_missing_vals_run04, eval=FALSE}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Estimate missing station vals - run 04  
# [1990, 2017] filtered based on 2003 complete records
# to fill incomplete years

gage_run04_sum <- gage_summary %>% 
  spread(water_year, days_record) %>%           
#  select(c(sta, '2002':'2017'))  #%>%     # subsets the matrix n = 27 
  filter(!is.na(`2004`))  %>%   
  gather(key = water_year, val = days_record, -sta) %>% 
  mutate(water_year = as.integer(water_year)) %>%
  mutate(days_record = case_when(
    is.na(days_record) ~ 0,
    TRUE ~ as.numeric(days_record)))                 # n =     756 obs

gage_run04 <- semi_join(gage, gage_run04_sum, 
                        by = c("sta", "water_year")) # n = 260,422 obs

#~~~~~~~~~~~~~~~~~~~~~~~~ 
# check for any NA values
gage_run_na <- gage_run04 %>% 
  filter(is.na(q30))                                 # n = 0 obs

# update incomplete observations
gage_run04_incomp <- semi_join(gage_incomp, gage_run04_sum, 
                  by = c("sta", "water_year")) %>%  
  filter(!is.na(date))                        # n        =   642 obs
# ~~~~~~~~~~~~~~~~~~~~~~~ 

# show a roadmap 
gage_run04_map <- semi_join(gage_summary, gage_run04_sum,
                            by = "sta") %>% 
  spread(water_year, days_record)


# gage_input changes with each run...  sorry!
gage_input <- gage_run04                       

rm(gage_meta, gage_incomp, gage_run04, gage_run_na) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

```{r PCA_exploratory_analysis}  
# this dataframe is used to calculate PCA & cluster stations  
# this is used for estimating missing values & for the final 
# cluster analysis 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data 
pca_input <- gage_input %>%  
  select(date, sta, q, q7, q30, contrib_drain_area_va)

# Eliminate effects of watershed size----
# Calculate daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

pca_input <- pca_input %>% 
  as.tibble() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  
 
# Transform using BoxCox to approach normality---- 
# Daily flow data are highly skewed.  BoxCox transformation utilizes 
# a lambda value to transform a dataset to an ~ normal distribution.

# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

lambda_q1  <- BoxCox.lambda(pca_input$q1_depth)   
lambda_q7  <- BoxCox.lambda(pca_input$q7_depth)   
lambda_q30 <- BoxCox.lambda(pca_input$q30_depth)    

# Standardize data by z-score---- 
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

pca_input <- pca_input %>% 
  mutate(q1_tr = BoxCox(.$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(.$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(.$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>%   # means
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>%   
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>%       # sds
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>%  
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>%    # transfrom data
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  dplyr::select(sta, date, q1_depth, q7_depth, q30_depth, 
                everything())

# Calculate PCA matrix & summary info---- 
# prcomp() requires a dataset with only the variables, split data 
# into PCA input & names to connect to result 

pca_check <- pca_input %>% 
  as.tibble() %>%
  select(sta:q30_depth) %>% 
  gather(key = flow_type, val = cms, -sta, -date) %>% 
  filter(is.na(cms)) %>% 
  mutate(year = year(date)) %>% 
  group_by(sta, year) %>%  
  summarise(count = n()) 

pca_matrix <- pca_input %>%  
  dplyr::select(q1_depth, q7_depth, q30_depth) %>%
  prcomp(., scale = TRUE)      
# note that . is passing select(q1_depth, q7_depth, q30_depth)
  
# Gather & summarize PCA results----
# Eigenvectors--results about PC axes
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
# & drop the 3rd PC-axis because it's not useful 

pca_vars <-  tidy(pca_matrix, matrix = "variables") %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix - summarize & clean up
gage_pca <- augment(pca_matrix, data = pca_input) %>% 
  select(-c(.rownames, q1_mean:q30_sd, .fittedPC3)) %>% 
#  select(-c(.rownames, date_2, q1_mean:q30_sd, .fittedPC3)) %>% 
      mutate(q1_q30_diff = q1_depth - q30_depth)      

gage_pca_sum <- gage_pca %>% 
  group_by(sta) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean) %>% 
  mutate(eigen_dist = sqrt(PC1_mean^2 + PC2_mean^2)) %>% 
  arrange(eigen_dist) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the series of station runs are as follows: 

# num  min_yr max_yr  n_obs  lambda_q1  lambda_q7  lambda_q30
#  01   1990   2002 111,031    0.014      0.112      0.264 
#  02   2003   2017 128,939   -0.007      0.081      0.244
#  03   1990   2017 227,917   -0.051     -0.009     -0.17
#  04   1990   2017 279,414   -0.020     -0.030     -0.032

pca_eigen 
# num   PC1perc PC2perc PC1cum PC2cum 
#  01    0.944   0.049  0.944   0.993     
#  02    0.946   0.047  0.946   0.994   
#  03    0.954   0.049  0.963   0.994   
#  04    0.964   0.031  0.964   0.994   

pca_vars 
# run  PC  q1_depth  q7_depth  q30_depth       
#   01  1    0.577      0.589     0.566 
#   01  2    0.588      0.182    -0.788  
#   02  1    0.577      0.588     0.566       
#   02  2    0.579      0.194    -0.792      
#   03  1    0.579      0.586     0.567           
#   03  2    0.543      0.242    -0.804       
#   04  1    0.578      0.584     0.570           
#   04  2    0.548      0.240    -0.801       

# eigenvecter:
#   95% of covarience is explained by PCA1 & 4% of varience by PCA2
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30;
#   PC2 - large opposite loadings of Q1 & Q30  
#   PC1 is about the hydrologic export of the system with larger
#     values exporting a greater amount of water 
#   PC2 is about the contribution of baseflow vs event-flow
# Find nearest neighbors to extend stations

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Preparing to cluster by the central tendancy
gage_pca_sum <- gage_pca_sum %>% 
  arrange(sta)

gage_clust <- gage_pca_sum %>% 
  select(q1_mean:q30_mean)

gage_clust_meta <- gage_pca_sum %>% 
  select(sta)

# apply model-based-clustering, extract results & add metadata
gage_clust_l <- Mclust(gage_clust) 

gage_clust <- as.tibble(gage_clust_l$z) 
gage_clust <- bind_cols(gage_clust_meta, gage_clust) 

# drop low probs - join & rearrange 
gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.5) 

gage_clust <- full_join(gage_pca_sum, gage_clust, 
                             by = "sta")

gage_clust <- gage_clust %>% 
  dplyr::select(group, sta, everything()) %>% 
  arrange(sta) %>%  
  arrange(group)

# join cluster to PCA data & arrange
gage_mod <- full_join(gage_pca, gage_clust, by = "sta") 

gage_mod <- gage_mod %>% 
  select(group, sta, everything()) %>% 
  select(-c(q:q30_tr)) %>% 
  select(-eigen_dist) 

summary(gage_clust_l) # Print a summary 

#clean up environment 
rm(pca_input, pca_matrix, pca_check, pca_eigen, pca_vars) 
rm(gage_clust_meta, gage_pca_sum, gage_pca, gage_input) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Mclust model with 3 components:

# num  model  log.likeli  n   df   BIC    ICL     purpose 
#  01  EEV    132.66     27  30  166.4   165.5   estimate NA 
#  02  EEV    124.44     24  44  109.1   109.0   estimate NA 
#  03  EEE    112.38     25  19  163.6   163.5   estimate NA   
#  04  VVV    135.80     27  19  209.0   208.9   estimate NA    

# Clustering table:
# training runs#
#  01 cluster val:     1        2        3        4
#  01 num members:     6        5       13        3
#  01 cluster type: mixed   precip   gw-dom   ephemeral 
#  02 cluster val:     1        2        3       4      5      6
#  02 num members:     5        4        3       5      6      1     
#  02 cluster type: mixed      gw   ephemeral  mixed    gw   >>gw
#  03 cluster val:     1 2 3 4 5 6 7 8 
#  03 num members:     2 9 4 1 4 2 1 1    
#  03 cluster type: 
#  04 cluster val:   1        2         
#  04 num members:   14       13 
#  04 cluster type: mixed     gw

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_out <- gage_clust %>% 
  select(1:2)
gage_out

#export(gage_out, "data/gage_clust_9017.csv")

#         sta       run
# V1    bat_bhr
# V1    blp_bel
# V1    brsf_co
# V1    che_buf
# V1    che_red
# V1    hat_edg
# V1    hor_oel
# V1    spr_her
# V1    whi_int
# V1    whi_kad
# V1    whi_ogl
# V1    whi_slm
# V1    whi_sta
# V1    whi_whi
# V2    bat_her
# V2    bev_abf
# V2    bev_buf
# V2    bev_pri
# V2    fal_hot
# V2    frn_fai
# V2    lcr_abv
# V2    lcr_bel
# V2    lcr_vet
# V2    lwr_abv
# V2    lwr_mar
# V2    lwr_ros
# V2    lwr_whi
# V2    rap_far
# V2    ros_ros
# V2    wcc_ogl
# V2    wkc_wok



# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Model-based clustering selected a model with two-three components 
# (i.e. clusters). The optimal selected model name is VEV model. 
# That is the five components are ellipsoidal with varying volume, 
# and orientation, and equal shape. The summary contains also the 
# clustering table specifying the number of observations in each 
# clusters.

```

```{r fill_missing_data_wet_years_run01, eval=FALSE} 

# name check 
# gage            - complete daily flow data [1990-2018] 
# gage_clust      - station groups & PCA_means 
# gage_incomp     - daily flows from partial years  
# gage_mod        - daily flows + PCA + cluster group 

# vars to add together
# gage_group        - by-group est daily flows for missing-year data 
# gage_group_part   - by-group est daily flows for partial-year data 
# gage_incomp_run02 - measured daily flows for partial year data

# temporary variables:
# gage_meta       - complete set of metadata for 26 stations ??remove?
# gage_part_miss  - station groups + days of record 
# gage_summary    - stations, water_year, days record to assist 

# run 01 has one misclassified item: rosros
gage_clust <- gage_clust %>% 
  mutate(group = case_when(
    sta == "ros_ros" ~ "V2", 
    TRUE ~ as.character(group)
    )) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run01_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                 # n = 10 x 13 obs

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       # n = 47 obs
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) # n = 27 = gage_clust

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
#  filter(group_sta == group_lead) %>% # drops obs of different groups
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) # 28 obs

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
#  filter(group_sta == group_lag) %>% # drops obs of different groups 
  rename(group = group_sta) %>%                              # 29 obs
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              # 54 obs

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 94 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(date, sta_orig, q:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%                # n = 32,870 obs
  summarise(                                   
    q          = mean(q), 
    log_q      = mean(log_q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              # n = 17,165 obs 
  ) %>% 
  ungroup() %>% 
  rename(sta = sta_orig)

# replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run01_incomp, 
                  by = c("date", "sta"))        # n = 15,168 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run01_incomp)
                                                # n =  17,165 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id)

export(gage_est_miss, file = "data/gage_est_9002.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage <- import(file = "data/gage_inputs.csv")       # n = 249,247 

# import estimates from prior runs 
gage_est_9002 <- import(file = "data/gage_est_9002.csv") %>%  # run01
  select(-contrib_drain_area_va)                    # n =  12,864 obs

# bind the gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage, gage_est_9002) %>% 
  group_by(sta) %>%                        # orig  n = 249,247       
  mutate(min_yr = min(water_year)) %>%     # run02 n = 262,111 
  mutate(max_yr = max(water_year)) %>%     # run03 n =    
           ungroup()                       # run04 n = 

# find actual count of years - post1990  
gage_summary <- gage %>%                               # N = 44 sta
  group_by(sta) %>%                              # run01 n = 32 sta 
  summarise(min_yr = min(water_year),            # run02 n = 
            max_yr = max(water_year)             # nun03 n = 
            ) %>% 
  ungroup() %>% 
  mutate(count_years_act = max_yr - min_yr) 
 
gage <- semi_join(gage, gage_summary, 
                           by = "sta")                   

# update gage summary & gage
gage_summary <- gage %>%                            # orig n = 689 obs
  group_by(sta, water_year) %>%                     # n02    = 717 obs
  summarise(days_record = n()) %>%                  
  ungroup() %>% 
  filter(days_record > 366) 

gage_dups <- semi_join(gage_est_9002, gage_summary) %>% 
  filter(qualifier != "est")                           # 1,997 obs

gage_est_9002a <- anti_join(gage_est_9002, gage_dups, 
                            by = c("date", "sta"))    # n = 15,168

export(gage_est_9002a, file = "data/gage_est_9002.csv") 




# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

rm(gage_est_miss, gage_run01_incomp, gage_run01_map, gage_run01_sum)

```

```{r fill_missing_data_wet_years_run02, eval=FALSE} 
 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run02_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                 # n = 2 x 8  obs 

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       # n = 7 obs
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) # n = 24 = gage_clust

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) # 25 obs

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              # 26 obs
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              # 48 obs

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 24 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(date, sta_orig, q:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    log_q      = mean(log_q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>% 
  rename(sta = sta_orig)

# replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run02_incomp, 
                  by = c("date", "sta"))        # n = 4,037 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run02_incomp)
                                                # n =  4,383 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_0317.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")       # n = 249,247 

# import estimates from prior runs 
gage_est_9002 <- import(file = "data/gage_est_9002.csv")  # run01
                                                     # n =  15,168 obs

gage_est_0317 <- import(file = "data/gage_est_0317.csv")    # run02 
                                                     # n =   2,557 obs

# bind the new gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002, gage_est_0317) %>% 
  group_by(sta) %>%                               # n = 268,798 obs     
  mutate(min_yr = min(water_year)) %>%     
  mutate(max_yr = max(water_year)) %>%      
           ungroup()                     

# update gage summary & gage
gage_summary <- gage %>%                          #  orig n = 689 obs
  group_by(sta, water_year) %>%                   # n run02 = 723 obs 
  summarise(days_record = n()) %>%                  
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002) 

gage_dups <- semi_join(gage_est_0317, gage_test, 
                       by = c("date", "sta")) # 192 obs

gage_est_0317a <- anti_join(gage_est_0317, gage_dups, 
                            by = c("date", "sta"))    # n = 2,365 obs 

export(gage_est_0317a, file = "data/gage_est_0317.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

```

```{r fill_missing_data_wet_years_run03, eval=FALSE} 
 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run03_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                 # n = 4 x 14  obs 

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       # n = 48 obs
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) # n = 24 = gage_clust

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) # 25 obs

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              # 26 obs
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              # 48 obs

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 96 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:q30) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    log_q      = mean(log_q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n = 17,531 obs
  rename(sta = sta_orig)

# replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run03_incomp, 
                  by = c("date", "sta"))            # n = 17,083 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run03_incomp) 
                                                    # n = 17,531 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9017a.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")       # n = 245,440 

# import estimates from prior runs 
gage_est_9002  <- import(file = "data/gage_est_9002.csv")  # run01
                                                     # n =  15,168 obs

gage_est_0317  <- import(file = "data/gage_est_0317.csv")    # run02 
                                                     # n =   2,365 obs

gage_est_9017a <- import(file = "data/gage_est_9017a.csv")   # run03 
                                                     # n =  17,531 obs

# bind the new gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002, 
                  gage_est_0317, gage_est_9017a) %>% 
  group_by(sta) %>%                                  # n = 280,504 obs
  mutate(min_yr = min(water_year)) %>%     
  mutate(max_yr = max(water_year)) %>%      
           ungroup()                     

# update gage summary & gage
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 769 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002, gage_est_0317) 

gage_dups <- semi_join(gage_est_9017a, gage_test, 
                       by = c("date", "sta")) # 448 obs

gage_est_9017aa <- anti_join(gage_est_9017a, gage_dups, 
                            by = c("date", "sta"))  # n =  17,083 obs 

export(gage_est_9017aa, file = "data/gage_est_9017a.csv") 
```

```{r fill_missing_data_wet_years_run04, eval=FALSE} 
 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 1. create a df of missing & partial year-station pairs 
gage_part_miss <- gage_run04_sum %>% 
  filter(days_record < 365) %>% 
    spread(water_year, days_record)                 # n = 3 x 16  obs 

gage_part_miss <- inner_join(gage_clust, gage_part_miss, 
                               by = "sta") %>% 
  select(-c(PC1_mean:prob)) %>%                       # n = 43 obs
  gather(key = water_year, value = days_record, -sta, -group) %>% 
  filter(!is.na(days_record))  %>% 
  mutate(water_year = as.integer(water_year)) %>% 
  rename(sta_orig = sta)

# 2. identify nearest station pairs based on pc1 
gage_pairs <- gage_clust %>% 
  mutate(sta_lead = lead(sta, 
                     order_by = PC1_mean)) %>%   # creates a lead col
  mutate(sta_lag = lag(sta, 
                     order_by = PC1_mean)) %>%   # creates a  lag col 
  mutate(sta_lead = case_when(                   
    is.na(sta_lead) ~ lag(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lead))) %>%          # fills the NA
  mutate(sta_lag = case_when(
    is.na(sta_lag) ~ lead(sta, order_by = PC1_mean),
    TRUE ~ as.character(sta_lag))) %>% 
  rename(group_sta = group) %>% 
  rename(sta_orig = sta) %>% 
  select(group_sta, sta_orig, sta_lead, sta_lag) # n = 27 = gage_clust

# 3. create a group ids for sta_lead & sta_lag to check group id
group_lead_id <- gage_clust %>% 
  rename(sta_lead = sta) %>% 
  rename(group_lead = group) %>%
  select(sta_lead, group_lead) 

group_lag_id <- gage_clust %>% 
  rename(sta_lag = sta) %>% 
  rename(group_lag = group) %>%
  select(sta_lag, group_lag) 

# 4. join the group_lead id to stations 
gage_pairs <- full_join(gage_pairs, group_lead_id,  
                         by = "sta_lead") %>%              
  select(group_sta, group_lead, sta_orig, sta_lead, sta_lag) # 28 obs

# 5. join the group__lag id to stations 
gage_pairs <- full_join(gage_pairs, group_lag_id,  
                         by = "sta_lag") %>% 
  rename(group = group_sta) %>%                              # 29 obs
  select(-c(group_lead, group_lag)) 

# 6. tidy the df to prepare for a join
gage_pairs <- gage_pairs %>% 
  filter(!is.na(sta_orig)) %>% 
  gather(key = type, val = sta, -group, -sta_orig) %>% 
  select(-type)                                              # 54 obs

# 7. join the missing water-year pairs 
gage_part_miss <- inner_join(gage_part_miss, gage_pairs,
                        by = c("sta_orig", "group"))  # n = 86 pairs 

# 8. average the available duplicated values
gage_est_miss <- inner_join(gage, gage_part_miss, 
                        by = c("sta", "water_year"))  %>% 
  select(sta_orig, everything()) %>% 
  select(-sta) %>% 
select(date, sta_orig:qualifier) %>% 
  mutate(qualifier = "est") %>% 
  group_by(date, sta_orig) %>%               
  summarise(                                   
    q          = mean(q), 
    log_q      = mean(log_q), 
    q7         = mean(q7), 
    q30        = mean(q30), 
    water_year = mean(water_year), 
    qualifier  = first(qualifier)              
  ) %>% 
  ungroup() %>%                                     # n = 15,706 obs
  rename(sta = sta_orig)

# replace the estimated observations with actual observations
gage_est_miss <- anti_join(gage_est_miss, gage_run04_incomp, 
                  by = c("date", "sta"))            # n = 15,064 obs

gage_est_miss <- bind_rows(gage_est_miss, gage_run04_incomp) 
                                                    # n = 15,706 obs

rm(gage_pairs, gage_part_miss, group_lead_id, group_lag_id) 

export(gage_est_miss, file = "data/gage_est_9017b.csv") 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 9. test the data for duplicates & remove duplicate vals

# import and append daily flow values > 1990 
gage_orig <- import(file = "data/gage_inputs.csv")       # n = 245,440 

# import estimates from prior runs 
gage_est_9002  <- import(file = "data/gage_est_9002.csv")  # run01
                                                     # n =  15,168 obs

gage_est_0317  <- import(file = "data/gage_est_0317.csv")    # run02 
                                                     # n =   2,365 obs

gage_est_9017a <- import(file = "data/gage_est_9017a.csv")   # run03 
                                                     # n =  17,083 obs

gage_est_9017b <- import(file = "data/gage_est_9017b.csv")   # run04 
                                                     # n =  15,706 obs 

# bind the new gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002, gage_est_0317, 
                  gage_est_9017a,gage_est_9017b) %>% 
  group_by(sta) %>%                                  # n = 295,762 obs
  mutate(min_yr = min(water_year)) %>%     
  mutate(max_yr = max(water_year)) %>%      
           ungroup()                     

# update gage summary & gage
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                   
  summarise(days_record = n()) %>%                # n run03 = 808 obs   
  ungroup()     

# make a df of prior observations, test for dups & update export df
gage_test <- bind_rows(gage_orig, gage_est_9002, gage_est_0317,
                       gage_est_9017a)  

gage_dups <- semi_join(gage_est_9017b, gage_test, 
                       by = c("date", "sta")) # 642 obs

gage_est_9017bb <- anti_join(gage_est_9017b, gage_dups, 
                            by = c("date", "sta"))  # n =  15,064 obs 

export(gage_est_9017bb, file = "data/gage_est_9017b.csv") 
```

```{r Clustering_visualization} 


# plot clusters in PCA space 
ggplot(gage_clust,  
       aes(q7_mean, 
           q1_q30_mean,   
           color = as.factor(group))) +  
  geom_density2d(na.rm = TRUE, 
                 contour = TRUE, 
                 aes(color = as.factor(group)))  +  
  geom_jitter() + 
  geom_text(aes(label = sta, 
                color = "blk"), 
            check_overlap = TRUE,  
            nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2.5, 1.5)) + 
  scale_y_continuous(limits = c(-1.0, 0.6)) + 
  theme_classic() 

 #ggplot2::ggsave(path = "figure/", filename = "strm_clust.png", 
#                width = 6, height = 6, units = "in")  

gage_clust_plot <- gage_pca %>% 
  select(sta, group, q7_mean, q1_q30_mean) %>% 
  rename(hydro_exp_coeff = q7_mean) %>% 
  rename(stability_coeff = q1_q30_mean) %>% 
  gather(key = parameter, value = value, -sta, -group)

ggplot(gage_clust_plot, aes(group, value)) + 
  facet_grid(rows = vars(parameter)) + 
         geom_boxplot() + 
  theme_bw() 

#ggplot2::ggsave(path = "figure/", filename = #"strm_clust_box.png", 
#                width = 6, height = 3, units = "in")  

#?gage_clust <- gage_clust %>% 
  select(group, sta, station_nm, everything()) %>% 
  arrange(group)

# plotting options from factominer---- 
# BIC values used for choosing the number of clusters  
fviz_mclust(gage_clust_l, "BIC", palette = "jco") 
 
# Classification uncertainty 
# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations 
fviz_mclust(gage_clust_l, "uncertainty", palette = "jco")

# Classification: plot showing the clustering 
fviz_mclust(gage_clust_l, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 



```

```{r plot_PCA} 
 
# mean eigenvector plot
ggplot(gage_clust, aes(PC1_mean, PC2_mean)) + 
#  scale_y_reverse() + 
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
            nudge_y = 0.02) + 
  theme_classic() + 
#  scale_x_continuous(limits = c(-3.5, 2)) + 
  xlab("PC axis 1 mean") +
  ylab("PC axis 2 mean") 

# transformed variable plot 
ggplot(gage_pca, aes(q7_mean, q1_q30_mean)) + 
  geom_jitter() +  
  geom_text(aes(label = sta), check_overlap = TRUE,  
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2.0, 1.5)) + 
  scale_y_continuous(limits = c(-1, 0.75)) + 
  geom_density2d() + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  xlab("hydrologic export index") +
  ylab("stability index") 

# PC1 explanatory plot 
ggplot(gage_aug, 
       aes(q7_depth, .fittedPC1, color = factor(sta))) + 
  geom_jitter() + 
  theme_classic() +
  xlab("Q7 depth") +
  ylab("Fitted PC1") +  
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = "strm_pca1.png", 
#                width = 6, height = 6, units = "in")  
  
# PC2 explanatory plot
ggplot(gage_aug, aes(q1_q30_diff, .fittedPC2, 
                          color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q1 depth minus Q30 depth") +
  ylab("Fitted PC2") +
  theme(legend.position = "bottom") 

#ggplot2::ggsave(path = "figure/", filename = #"strm_pca1.png", 
#                width = 6, height = 6, units = "in")  



#ggplot2::ggsave(path = "figure/", filename = "strm_dens_mean.png", 
#                width = 6, height = 6, units = "in")  

# create density plot of all points
ggplot(gage_aug, aes(q7_depth, q1_q30_diff)) +
  geom_jitter(aes(color = sta, shape = ".")) + 
  geom_density2d(aes(color = NULL)) + 
  geom_point(data = gage_pca, mapping = aes(q7_mean, q1_q30_mean)) + 
  geom_text(data = gage_pca, 
           mapping = aes(q7_mean, q1_q30_mean, label = sta), 
            check_overlap = TRUE) + 
  scale_x_continuous(limits = c(-2, 1.5)) + 
  scale_y_continuous(limits = c(-1.0, 0.75)) + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  theme(legend.position = "none") 

#ggplot2::ggsave(path = "figure/", filename = "strm_dens_all.png", 
#                width = 6, height = 6, units = "in")  
```

