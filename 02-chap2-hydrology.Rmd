
Title (13-words or less): Relating Precipitation and Hydrologic Drought in a Semi-arid Climate with gSSURO Data  
Subtitle: Pine Ridge Reservation and surrounding Areas in Southwestern South Dakota 


## Introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

Intro p1: drought is a "creeping disaster"
Intro p2: what makes a drought? Intensity & duration 
Intro p3: different kinds of drought 
Intro p4: working with drought - skewed distribution -> how to fix?

## What are the gaps this paper hopes to address? 
Evaluate a multi-component regional drought history for the region
How to define minimum precipip record set & distributional characteristics 
Identify "pure" streamflow realizations that incorporate short streamflow gage records to characterize hydrologic drought
Identify spatial vars in gSSURGO data to relate lags to a physical process

# Specific Aims
1. Refine an approach for relating precipitation drought to hydrological drought in gaged watersheds
2. Develop a method to estimate regional hydrologic group membership for ungaged watersheds using gSSURGO data 

An example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

## Methods


## Results
Regional monthly precipitation fits a Pearson type-III distribution.
The mean and deviation of monthly precipitation depth among stations are similiar (e.g little spatial difference).  
However, the distribution of wet and dry periods is temporally dependant 
Station distance is a substantial predictor of covarience
=> Next STEP use distance from centroid to create a predictor variable to test significance..
Regional streamflow clusters into 7 groups.
Hydrologic export coefficient describes ~95% of varience and flashiness (e.g Q1 -Q3) describing 5% of varience.

```{r spi_overview eval=FALSE}
# overview of steps---- 
# 2.1    Standardized Precipitation Index 
# note: SPI package not working; created a test case to send to author
# 2.1.1  Download & munge precip data: complete; _04_prco-data_munging
# 2.1.2  Identify distribution: complete;  _05_L-moment_diagram
# 2.1.3   Calculate SPI: completed spi1, spi6

# 2.2    Stream Drought Index
# 2.2.1  Download & mungeStream data: finished - July 16
# 2.2.2  Learn purrr::map(): ok.finished 
#          http://r4ds.had.co.nz/iteration.html#the-map-functions
# 2.2.3 Cluster time series - in progress
#         see Kassambara, "Practical Guide to Cluster Analysis in R
# 2.2.4   Calculate SDI
# 2.2.5   Delineate watersheds
# 2.2.6   Cluster ungaged stations
# 2.2.7 Learn 'rf' function
# 2.3   Disseminate results
# 2.3.1 Identify journal

## Analysis Steps & progress
# 1.    Identify precipitation records for drought analysis
#         I imported Global Historical Climatology Network (GHCN) daily 
#         precipitation records for candidate "WEATHER STATIONS" 
#         into R-Studio (REF1) using the "rnoaa" package.
# 1.2   Cleaned data (see 04_prcp-data_munging)  
# 1.2.1   I used Theissen polygons and the length and continuity of 
#         precipitation records for initial station selection 
# 1.2.2  'dplyr' to fill daily NA values with data from nearest station
# 1.2.3   create monthly vals from daily vals.
# 1.2.4 I removed short record: Long Valley after checking covariance.
# 1.3   Exploratory EDA
# 1.3.1 Applied sqrt & log10 transform to explore effects on skew 
# 1.3.2 Explored the data with box plots, violin plot.
#        Sqrt trans vs plotting vals look slightly sinusoidal
#        Log-tranformation is mirror of orig depth vs plotting
# 1.3.3 Applied Weibull plotting position & graphed on sqrt plot 
# 1.3.4 Completed exploratory PCA => covarience by zero months?
# 1.4.  Calculated L-moments and L-moment ratios => Pearson-type III
# 1.4.1 Calculated 1 & 6 month SPI 
# 1.5   Clustered streamflows during wet period 
# 1.5.1 Identify streamflow records for hydrology analysis (list) 
# 1.5.2 Exploratory EDA of streamflow to identify membership of 
#       short records 


## Thoughts - the orig depth vs plotting vals look j-shaped. 

# Next steps: 
#2. find drought years & wet years for precip
#3. Examine clusters for dry years
#4. Supervised classification - random forest should cross-validate?

# Someday - Maybe
# 1. Map the variable as a function - might put off, but ugly and long 
#   code below!
# 2. figure out how to reference stuff with - grateful package
```

```{r rules, eval=FALSE, include=FALSE}
<!---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ten simple rules for structuring papers

Citation: Mensh B, Kording K (2017) Ten simple rules for structuring papers. PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619

Principles (Rules 1–4)
Rule 1: Focus your paper on a central contribution, which you communicate in the title

Your communication efforts are successful if readers can still describe the main contribution of your paper to their colleagues a year after reading it. Although it is clear that a paper often needs to communicate a number of innovations on the way to its final message, it does not pay to be greedy. Focus on a single message; papers that simultaneously focus on multiple contributions tend to be less convincing about each and are therefore less memorable.

The most important element of a paper is the title—think of the ratio of the number of titles you read to the number of papers you read. The title is typically the first element a reader encounters, so its quality [3] determines whether the reader will invest time in reading the abstract.

The title not only transmits the paper’s central contribution but can also serve as a constant reminder (to you) to focus the text on transmitting that idea. Science is, after all, the abstraction of simple principles from complex data. The title is the ultimate refinement of the paper’s contribution. Thinking about the title early—and regularly returning to hone it—can help not only the writing of the paper but also the process of designing experiments or developing theories.

This Rule of One is the most difficult rule to optimally implement because it comes face-to-face with the key challenge of science, which is to make the claim and/or model as simple as the data and logic can support but no simpler. In the end, your struggle to find this balance may appropriately result in “one contribution” that is multifaceted. For example, a technology paper may describe both its new technology and a biological result using it; the bridge that unifies these two facets is a clear description of how the new technology can be used to do new biology.

Rule 2: Write for flesh-and-blood human beings who do not know your work

Because you are the world’s leading expert at exactly what you are doing, you are also the world’s least qualified person to judge your writing from the perspective of the naïve reader. The majority of writing mistakes stem from this predicament. Think like a designer—for each element, determine the impact that you want to have on people and then strive to achieve that objective [4]. Try to think through the paper like a naïve reader who must first be made to care about the problem you are addressing (see Rule 6) and then will want to understand your answer with minimal effort.

Define technical terms clearly because readers can become frustrated when they encounter a word that they don’t understand. Avoid abbreviations and acronyms so that readers do not have to go back to earlier sections to identify them.

The vast knowledge base of human psychology is useful in paper writing. For example, people have working memory constraints in that they can only remember a small number of items and are better at remembering the beginning and the end of a list than the middle [5]. Do your best to minimize the number of loose threads that the reader has to keep in mind at any one time.

Rule 3: Stick to the context-content-conclusion (C-C-C) scheme

The vast majority of popular (i.e., memorable and re-tellable) stories have a structure with a discernible beginning, a well-defined body, and an end. The beginning sets up the context for the story, while the body (content) advances the story towards an ending in which the problems find their conclusions. This structure reduces the chance that the reader will wonder “Why was I told that?” (if the context is missing) or “So what?” (if the conclusion is missing).

There are many ways of telling a story. Mostly, they differ in how well they serve a patient reader versus an impatient one [6]. The impatient reader needs to be engaged quickly; this can be accomplished by presenting the most exciting content first (e.g., as seen in news articles). The C-C-C scheme that we advocate serves a more patient reader who is willing to spend the time to get oriented with the context. A consequent disadvantage of C-C-C is that it may not optimally engage the impatient reader. This disadvantage is mitigated by the fact that the structure of scientific articles, specifically the primacy of the title and abstract, already forces the content to be revealed quickly. Thus, a reader who proceeds to the introduction is likely engaged enough to have the patience to absorb the context. Furthermore, one hazard of excessive “content first” story structures in science is that you may generate skepticism in the reader because they may be missing an important piece of context that makes your claim more credible. For these reasons, we advocate C-C-C as a “default” scientific story structure.

The C-C-C scheme defines the structure of the paper on multiple scales. At the whole-paper scale, the introduction sets the context, the results are the content, and the discussion brings home the conclusion. Applying C-C-C at the paragraph scale, the first sentence defines the topic or context, the body hosts the novel content put forth for the reader’s consideration, and the last sentence provides the conclusion to be remembered.

Deviating from the C-C-C structure often leads to papers that are hard to read, but writers often do so because of their own autobiographical context. During our everyday lives as scientists, we spend a majority of our time producing content and a minority amidst a flurry of other activities. We run experiments, develop the exposition of available literature, and combine thoughts using the magic of human cognition. It is natural to want to record these efforts on paper and structure a paper chronologically. But for our readers, most details of our activities are extraneous. They do not care about the chronological path by which you reached a result; they just care about the ultimate claim and the logic supporting it (see Rule 7). Thus, all our work must be reformatted to provide a context that makes our material meaningful and a conclusion that helps the reader to understand and remember it.

Rule 4: Optimize your logical flow by avoiding zig-zag and using parallelism
Avoiding zig-zag.

Only the central idea of the paper should be touched upon multiple times. Otherwise, each subject should be covered in only one place in order to minimize the number of subject changes. Related sentences or paragraphs should be strung together rather than interrupted by unrelated material. Ideas that are similar, such as two reasons why we should believe something, should come one immediately after the other.
Using parallelism.

Similarly, across consecutive paragraphs or sentences, parallel messages should be communicated with parallel form. Parallelism makes it easier to read the text because the reader is familiar with the structure. For example, if we have three independent reasons why we prefer one interpretation of a result over another, it is helpful to communicate them with the same syntax so that this syntax becomes transparent to the reader, which allows them to focus on the content. There is nothing wrong with using the same word multiple times in a sentence or paragraph. Resist the temptation to use a different word to refer to the same concept—doing so makes readers wonder if the second word has a slightly different meaning.
The components of a paper (Rules 5–8)

The individual parts of a paper—abstract, introduction, results, and discussion—have different objectives, and thus they each apply the C-C-C structure a little differently in order to achieve their objectives. We will discuss these specialized structures in this section and summarize them in Fig 1.
thumbnail

Fig 1. Summary of a paper’s structural elements at three spatial scales: Across sections, across paragraphs, and within paragraphs.

Note that the abstract is special in that it contains all three elements (Context, Content, and Conclusion), thus comprising all three colors.

https://doi.org/10.1371/journal.pcbi.1005619.g001
Rule 5: Tell a complete story in the abstract

The abstract is, for most readers, the only part of the paper that will be read. This means that the abstract must convey the entire message of the paper effectively. To serve this purpose, the abstract’s structure is highly conserved. Each of the C-C-C elements is detailed below.

The context must communicate to the reader what gap the paper will fill. The first sentence orients the reader by introducing the broader field in which the particular research is situated. Then, this context is narrowed until it lands on the open question that the research answered. A successful context section sets the stage for distinguishing the paper’s contributions from the current state of the art by communicating what is missing in the literature (i.e., the specific gap) and why that matters (i.e., the connection between the specific gap and the broader context that the paper opened with).

The content (“Here we”) first describes the novel method or approach that you used to fill the gap or question. Then you present the meat—your executive summary of the results.

Finally, the conclusion interprets the results to answer the question that was posed at the end of the context section. There is often a second part to the conclusion section that highlights how this conclusion moves the broader field forward (i.e., “broader significance”). This is particularly true for more “general” journals with a broad readership.

This structure helps you avoid the most common mistake with the abstract, which is to talk about results before the reader is ready to understand them. Good abstracts usually take many iterations of refinement to make sure the results fill the gap like a key fits its lock. The broad-narrow-broad structure allows you to communicate with a wider readership (through breadth) while maintaining the credibility of your claim (which is always based on a finite or narrow set of results).

Rule 6: Communicate why the paper matters in the introduction
The introduction highlights the gap that exists in current knowledge or methods and why it is important. This is usually done by a set of progressively more specific paragraphs that culminate in a clear exposition of what is lacking in the literature, followed by a paragraph summarizing what the paper does to fill that gap.

As an example of the progression of gaps, a first paragraph may explain why understanding cell differentiation is an important topic and that the field has not yet solved what triggers it (a field gap). A second paragraph may explain what is unknown about the differentiation of a specific cell type, such as astrocytes (a subfield gap). A third may provide clues that a particular gene might drive astrocytic differentiation and then state that this hypothesis is untested (the gap within the subfield that you will fill). The gap statement sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal of developing the gap. Each paragraph first orients the reader to the topic (a context sentence or two) and then explains the “knowns” in the relevant literature (content) before landing on the critical “unknown” (conclusion) that makes the paper matter at the relevant scale. Along the path, there are often clues given about the mystery behind the gaps; these clues lead to the untested hypothesis or undeveloped method of the paper and give the reader hope that the mystery is solvable. The introduction should not contain a broad literature review beyond the motivation of the paper. This gap-focused structure makes it easy for experienced readers to evaluate the potential importance of a paper—they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the results, which fill the gap you just established. It differs from the abstract in the following ways: it does not need to present the context (which has just been given), it is somewhat more specific about the results, and it only briefly previews the conclusion of the paper, if at all.

Rule 7-Results: Deliver the results as a sequence of statements, supported by figures, that connect logically to support the central contribution

The results section needs to convince the reader that the central claim is supported by data and logic. Every scientific argument has its own particular logical structure, which dictates the sequence in which its elements should be presented.

For example, a paper may set up a hypothesis, verify that a method for measurement is valid in the system under study, and then use the measurement to disprove the hypothesis. Alternatively, a paper may set up multiple alternative (and mutually exclusive) hypotheses and then disprove all but one to provide evidence for the remaining interpretation. The fabric of the argument will contain controls and methods where they are needed for the overall logic.

In the outlining phase of paper preparation (see Rule 9), sketch out the logical structure of how your results support your claim and convert this into a sequence of declarative statements that become the headers of subsections within the results section (and/or the titles of figures). Most journals allow this type of formatting, but if your chosen journal does not, these headers are still useful during the writing phase and can either be adapted to serve as introductory sentences to your paragraphs or deleted before submission. Such a clear progression of logical steps makes the paper easy to follow.

Figures, their titles, and legends are particularly important because they show the most objective support (data) of the steps that culminate in the paper’s claim. Moreover, figures are often viewed by readers who skip directly from the abstract in order to save time. Thus, the title of the figure should communicate the conclusion of the analysis, and the legend should explain how it was done. Figure making is an art unto itself; the Edward Tufte books remain the gold standard for learning this craft [7,8].

The first results paragraph is special in that it typically summarizes the overall approach to the problem outlined in the introduction, along with any key innovative methods that were developed. Most readers do not read the methods, so this paragraph gives them the gist of the methods that were used.

Each subsequent paragraph in the results section starts with a sentence or two that set up the question that the paragraph answers, such as the following: “To verify that there are no artifacts…,” “What is the test-retest reliability of our measure?,” or “We next tested whether Ca2+ flux through L-type Ca2+ channels was involved.” The middle of the paragraph presents data and logic that pertain to the question, and the paragraph ends with a sentence that answers the question. For example, it may conclude that none of the potential artifacts were detected. This structure makes it easy for experienced readers to fact-check a paper. Each paragraph convinces the reader of the answer given in its last sentence. This makes it easy to find the paragraph in which a suspicious conclusion is drawn and to check the logic of that paragraph. The result of each paragraph is a logical statement, and paragraphs farther down in the text rely on the logical conclusions of previous paragraphs, much as theorems are built in mathematical literature.
Rule 8: Discuss how the gap was filled, the limitations of the interpretation, and the relevance to the field

The discussion section explains how the results have filled the gap that was identified in the introduction, provides caveats to the interpretation, and describes how the paper advances the field by providing new opportunities. This is typically done by recapitulating the results, discussing the limitations, and then revealing how the central contribution may catalyze future progress. The first discussion paragraph is special in that it generally summarizes the important findings from the results section. Some readers skip over substantial parts of the results, so this paragraph at least gives them the gist of that section.

Each of the following paragraphs in the discussion section starts by describing an area of weakness or strength of the paper. It then evaluates the strength or weakness by linking it to the relevant literature. Discussion paragraphs often conclude by describing a clever, informal way of perceiving the contribution or by discussing future directions that can extend the contribution.

For example, the first paragraph may summarize the results, focusing on their meaning. The second through fourth paragraphs may deal with potential weaknesses and with how the literature alleviates concerns or how future experiments can deal with these weaknesses. The fifth paragraph may then culminate in a description of how the paper moves the field forward. Step by step, the reader thus learns to put the paper’s conclusions into the right context.
Process (Rules 9 and 10)

To produce a good paper, authors can use helpful processes and habits. Some aspects of a paper affect its impact more than others, which suggests that your investment of time should be weighted towards the issues that matter most. Moreover, iteratively using feedback from colleagues allows authors to improve the story at all levels to produce a powerful manuscript. Choosing the right process makes writing papers easier and more effective.
Rule 9: Allocate time where it matters: Title, abstract, figures, and outlining

The central logic that underlies a scientific claim is paramount. It is also the bridge that connects the experimental phase of a research effort with the paper-writing phase. Thus, it is useful to formalize the logic of ongoing experimental efforts (e.g., during lab meetings) into an evolving document of some sort that will ultimately steer the outline of the paper.

You should also allocate your time according to the importance of each section. The title, abstract, and figures are viewed by far more people than the rest of the paper, and the methods section is read least of all. Budget accordingly.

The time that we do spend on each section can be used efficiently by planning text before producing it. Make an outline. We like to write one informal sentence for each planned paragraph. It is often useful to start the process around descriptions of each result—these may become the section headers in the results section. Because the story has an overall arc, each paragraph should have a defined role in advancing this story. This role is best scrutinized at the outline stage in order to reduce wasting time on wordsmithing paragraphs that don’t end up fitting within the overall story.
Rule 10: Get feedback to reduce, reuse, and recycle the story

Writing can be considered an optimization problem in which you simultaneously improve the story, the outline, and all the component sentences. In this context, it is important not to get too attached to one’s writing. In many cases, trashing entire paragraphs and rewriting is a faster way to produce good text than incremental editing.

There are multiple signs that further work is necessary on a manuscript (see Table 1). For example, if you, as the writer, cannot describe the entire outline of a paper to a colleague in a few minutes, then clearly a reader will not be able to. You need to further distill your story. Finding such violations of good writing helps to improve the paper at all levels.
thumbnail

https://doi.org/10.1371/journal.pcbi.1005619.t001

Successfully writing a paper typically requires input from multiple people. Test readers are necessary to make sure that the overall story works. They can also give valuable input on where the story appears to move too quickly or too slowly. They can clarify when it is best to go back to the drawing board and retell the entire story. Reviewers are also extremely useful. Non-specific feedback and unenthusiastic reviews often imply that the reviewers did not “get” the big picture story line. Very specific feedback usually points out places where the logic within a paragraph was not sufficient. It is vital to accept this feedback in a positive way. Because input from others is essential, a network of helpful colleagues is fundamental to making a story memorable. To keep this network working, make sure to pay back your colleagues by reading their manuscripts.
Discussion

This paper focused on the structure, or “anatomy,” of manuscripts. We had to gloss over many finer points of writing, including word choice and grammar, the creative process, and collaboration. A paper about writing can never be complete; as such, there is a large body of literature dealing with issues of scientific writing [9,10,11,12,13,14,15,16,17].

Personal style often leads writers to deviate from a rigid, conserved structure, and it can be a delight to read a paper that creatively bends the rules. However, as with many other things in life, a thorough mastery of the standard rules is necessary to successfully bend them [18]. In following these guidelines, scientists will be able to address a broad audience, bridge disciplines, and more effectively enable integrative science.
Acknowledgments

We took our own advice and sought feedback from a large number of colleagues throughout the process of preparing this paper. We would like to especially thank the following people who gave particularly detailed and useful feedback:

Sandra Aamodt, Misha Ahrens, Vanessa Bender, Erik Bloss, Davi Bock, Shelly Buffington, Xing Chen, Frances Cho, Gabrielle Edgerton, multiple generations of the COSMO summer school, Jason Perry, Jermyn See, Nelson Spruston, David Stern, Alice Ting, Joshua Vogelstein, Ronald Weber.
References

    1. Hirsch JE (2005) An index to quantify an individual's scientific research output. Proc Natl Acad Sci U S A. 102: 16569–16572. pmid:16275915
    2. Acuna DE, Allesina S, Kording KP (2012) Future impact: Predicting scientific success. Nature. 489: 201–202. pmid:22972278
    3. Paiva CE, Lima JPSN, Paiva BSR (2012) Articles with short titles describing the results are cited more often. Clinics. 67: 509–513. pmid:22666797
    4. Carter M (2012) Designing Science Presentations: A Visual Guide to Figures, Papers, Slides, Posters, and More: Academic Press.
    5. Murdock BB Jr (1968) Serial order effects in short-term memory. J Exp Psychol. 76: Suppl:1–15.
    6. Schimel J (2012) Writing science: how to write papers that get cited and proposals that get funded. USA: OUP.
    7. Tufte ER (1990) Envisioning information. Graphics Press.
    8. Tufte ER The Visual Display of Quantitative Information. Graphics Press.
    9. Lisberger SG (2011) From Science to Citation: How to Publish a Successful Scientific Paper. Stephen Lisberger.
    10. Simons D (2012) Dan's writing and revising guide. http://www.dansimons.com/resources/Simons_on_writing.pdf [cited 2017 Sep 9].
    11. Sørensen C (1994) This is Not an Article—Just Some Thoughts on How to Write One. Syöte, Finland: Oulu University, 46–59.
    12. Day R (1988) How to write and publish a scientific paper. Phoenix: Oryx.
    13. Lester JD, Lester J (1967) Writing research papers. Scott, Foresman.
    14. Dumont J-L (2009) Trees, Maps, and Theorems. Principiae. http://www.treesmapsandtheorems.com/ [cited 2017 Sep 9].
    15. Pinker S (2014) The Sense of Style: The Thinking Person’s Guide to Writing in the 21st Century. Viking Adult.
    16. Bern D (1987) Writing the empirical journal. The compleat academic: A practical guide for the beginning social scientist. 171.
    17. George GD, Swan JA (1990) The science of scientific writing. Am Sci. 78: 550–558.
    18. Strunk W (2007) The elements of style. Penguin. 
```

```{r PCA_approach, eval=FALSE, include=FALSE}
# Exploratory PCA--------------------------------------------------- 
# PCA is related to eigenvectors and eigenvalues.  The variance or 
# spread of the observations is measured as the average squared 
# distance from the center of the point cloud to each observation (c).  
# The total reconstruction error is measured as the average squared 
# length of the errors (b), and distance along the principal axis (a) 
# can also be measured.  Therefore the sum of the square of the errors 
# plus the sum of the square distance along the principal axis equals 
# the average squared distance between the center of the point cloud 
# each observation; this is precisely Pythagoras theorem. 

# You can imagine that the PC axis is a solid rod and each error 
# is a spring. The energy of the spring is proportional to its squared 
# length (this is known in physics as the Hooke's law), so the rod 
# will orient itself such as to minimize the sum of these squared 
# distances. 

# Regarding eigenvectors and eigenvalues. A 2×2 matrix given by: 
#   (1.07     0.63)
#   (0.63     0.64)

# The variance of the x variable is 1.07, 
# the variance of the y variable is 0.64, 
# and the covariance between them is 0.63. 

# As it is a square symmetric matrix, it can be diagonalized by 
# choosing a new orthogonal coordinate system, given by its 
# eigenvectors (incidentally, this is called spectral theorem); 
# corresponding eigenvalues will then be located on the diagonal. 
# In this new coordinate system, the covariance matrix is diagonal 
# and looks like this:
#   (1.52     0) 
#   (0     0.19)

# The correlation between points is now zero. It becomes clear that 
# the variance of any projection will be given by a weighted average 
# of the eigenvalues.  Consequently, the maximum possible variance 
# (1.52) will be achieved if we simply take the projection on the 
# first coordinate axis. It follows that the direction of the first 
# principal component is given by the first eigenvector of the 
# covariance matrix. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-->
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Sets up the library of packages 

# dplyr::select -- this is a potential issue!


library("here") # identifies where to save work 
library("rio") # more robust I/O - to import and clean data 
library("janitor") # tools to clean dirty data 

library("SCI")
library("munsell") # https://github.com/cwickham/munsell

library("forecast") # using the BoxCox function

library("standardize")
library("cluster")
library("factoextra") 
library("broom") # tidies up objects 
library("ggpubr") 
#library("DataExplorer")

library("clValid")

# library("grateful") - not yet ready for R 3.5.0
#library("magrittr") # contains easier ways to say things about lists
#library("lintr")
#library("test_that")
#library("jsonlite") # Convert between JSON data and R objects
#library("curl") # Drop-in replacement for base url
#library("listviewer") # htmlwidget for interactive views of R lists

#lmomco <- citation("lmomco")
#toBibtex(lmomco)
library("mclust") 
library("corrplot")
library("lubridate") # fixes dates 
library("tidyverse") 

# library("SPEI") # Calculates SPI-index # having some issues...


# Session Info
a_session <- devtools::session_info()
```

# Variable naming convention:   
a_session    list variable of session information
sta          precipitation station  
spi          Standardized Precipitation Index vals  
gage         streamflow gage 

# general modifiers:
_meta        metadata  
_raw         original imported data in wide format
_gath        wide data changed to long data format
_na          exploratory na values 
_outlier     split outlier data 
_typical     split not outlier data 
_replace     intermediate value used to replace outlier value 

# intermediate variables 
abbrev       abbreviation; a look-up table 
_mean        intermediate variable; mean value
_sum         summary data 
_prep        intermediate variable 

# regression variables:
_lm          linear model 
_aug         Added prediction info about each dataset observation 
_fit         Fitted regression model information 
_tidy        Summarizes information about model components 

# precipitation station identifiers:
_cot        Cottonwood precipitation station
_oel        Oelrichs precipitation station 
_rap        Rapid City precipitation station 
_int        Interior precipitation station 
_ora        Oral precipitation station

# SCI input & intermediate variables:
time_scale   sets the length of the averaging period 
distrib      sets the distribution type 
p_zero       sets a function to reduce zero-precip bias
scale        scales input by subtract mean & divide by sd
warn_me      sets explicit warning
first_mon    sets the first month for SCI; based on the data

.cot        Cottonwood precipitation station
.oel        Oelrichs precipitation station 
.rap        Rapid City precipitation station 
.int        Interior precipitation station 
.ora        Oral precipitation station
.l          a variable as a list 
_M          Correlation matrix 
_check      Temporary variable used to check data

# SCI output variables
zero_prob   probability of a zero depth month
_att        SPI function attributes: scale, location, shape 
_corr       final correlation dataframe
_index      SPI index

# Gage PCA input & intermediate variables:
_depth      streamflow depth; depth = Q/A 
_lambda     BoxCox lambda values defined by a ML estimator 
  _q1       one-day average streamflow volume or depth 
  _q7       seven-day average streamflow volume or depth 
  _q30      thirty-day average streamflow volume or depth 

# pca variables
_input      matrix with only observations
_meta       matrix with info about observations 
_matrix     result of prcomp analysis as prcomp object 
_eigen      tidy eigenvecter summary 
_vars       tidy summary about pc loadings on variables 
_aug        added prediction info about each dataset observation 
_sum        summary of results

```{r testing, include=FALSE, eval=FALSE}
# Mini-library
library("tidyverse") 
library("lubridate") 
library("rio") 
library("SPEI")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# load data
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  arrange(date)
  
# prepare date for joining later
Date <- sta_mon %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use a single station - cot
sta_cot_ts <- sta_mon %>% 
  select(cot) %>%
  ts(end = c(2018, 05), frequency = 12)

spi_list  <- spi(sta_cot_ts, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_cot <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_ts <- as.tibble(spi_list$fitted)  
spi1_cot_ts <- bind_cols(Date, spi1_cot_ts) 

# Results
 as.tibble(summary(spi1_cot_ts))
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data for testing Lubridate object
sta_cot_lub <- sta_mon %>% 
  select(cot) 

spi_list  <- spi(sta_cot_lub, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 
 
spi1_coeff_cot_lub <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_cot_lub <- as.tibble(spi_list$fitted) 
spi1_cot_lub <- bind_cols(Date, spi1_cot_lub) 
rm(sta_cot_lub)

# Results
as.tibble(summary(spi1_cot_lub))
# A tibble: 12 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  "
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     cot"  "Min.   :-2.40240  "  
# 8 ""    "     cot"  "1st Qu.:-0.69324  "  
# 9 ""    "     cot"  "Median : 0.02341  "  
#10 ""    "     cot"  "Mean   : 0.01383  "  
#11 ""    "     cot"  "3rd Qu.: 0.68317  "  
#12 ""    "     cot"  "Max.   : 3.30822  "   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use lubridate-created object for date for a batch process
# results are as above for 'cot' but have -Inf for 'rap' 

# prepare data for batch
sta_all <- sta_mon %>% 
  select(-c(date, year, month))  
 
spi_list  <- spi(sta_all, 1,  
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_all <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble() 

spi1_all <- as.tibble(spi_list$fitted) 
spi1_all <- bind_cols(Date, spi1_all) 

# Selected Results
summary(spi1_all$cot)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -2.40240 -0.69324  0.02341  0.01383  0.68317  3.30822 

summary(spi1_all$date)
# Min.      1st Qu.       Median     Mean      3rd Qu.       Max.
# "1909-06-01" "36-08-24" "63-11-16" "63-11-16" "91-02-08" "2018-05-01" 
summary(spi1_all$rap)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#   -Inf -0.6472  0.0022    -Inf  0.6657  2.9000     467 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use only 'rap' including NA vals 
# Summary: It looks like the "bug" is in handling the na.rm?
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare data for 'rap' only
sta_rap <- sta_mon %>% 
  select(rap) 

spi_list  <- spi(sta_rap, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble()  

spi1_rap <- as.tibble(spi_list$fitted) 
spi1_rap <- bind_cols(Date, spi1_rap) 

# Results
as.tibble(summary(spi1_rap))
# A tibble: 14 x 3
#   Var1  Var2        n                     
#   <chr> <chr>       <fct>                 
# 1 ""    "     date" "Min.   :1909-06-01  "
# 2 ""    "     date" "1st Qu.:1936-08-24  " 
# 3 ""    "     date" "Median :1963-11-16  "
# 4 ""    "     date" "Mean   :1963-11-16  "
# 5 ""    "     date" "3rd Qu.:1991-02-08  "
# 6 ""    "     date" "Max.   :2018-05-01  "
# 7 ""    "     date" NA                    
# 8 ""    "     rap"  "Min.   :   -Inf  "   
# 9 ""    "     rap"  "1st Qu.:-0.6472  "   
#10 ""    "     rap"  "Median : 0.0022  "   
#11 ""    "     rap"  "Mean   :   -Inf  "   
#12 ""    "     rap"  "3rd Qu.: 0.6657  "   
#13 ""    "     rap"  "Max.   : 2.9000  "   
#14 ""    "     rap"  "NA's   :467  "  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use only 'rap' without NA vals 
# Summary: I'm confused about the bug!

# prepare data for 'rap' without NA only
sta_rap <- sta_mon %>% 
  select(rap, date) %>%
  filter(rap != is.na(rap))

# update date for joining later
Date_rap <- sta_rap %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)

# drop date
sta_rap <- sta_rap %>% select(-date)

spi_list  <- spi(sta_rap, 1, 
                 distribution = 'PearsonIII', na.rm = TRUE) 

spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
  t() %>% as.tibble()  

spi1_rap <- as.tibble(spi_list$fitted) 
spi1_rap <- bind_cols(Date_rap, spi1_rap) 

spi1_rap <- spi1_rap %>%
  rename(spi1 = rap) %>%
  arrange(spi1)

spi1_rap <- bind_cols(sta_rap, spi1_rap) 
spi_rap_test <- spi1_rap
```

```{r testing2, include=FALSE, eval=FALSE}
# this is a new test to check on why differences between two ways of 
# calculating code

sta_rap3 <- sta_mon %>%
  arrange(date) %>%
  slice(468:1308) %>%
  select(date, rap)

spi_list  <- spi(sta_rap3[, 'rap'], 1, 
                 distribution = 'PearsonIII') 

spi1_rap3 <- as.tibble(spi_list$fitted) 

spi1_rap3 <- bind_cols(spi1_rap3, sta_rap3)

ggplot(spi1_rap3, aes(rap, rap1)) +
  geom_point()


# prepare data for 'rap' without NA only
sta_rap1 <- sta_mon %>% 
  select(rap, date) %>%
  filter(rap != is.na(rap)) %>%
  arrange(date) # This might be the issue!

# update date for joining later
Date_rap1 <- sta_rap1 %>% 
  select(date) %>%
  mutate(date = ymd(date)) %>%
  arrange(date)

# drop date
#sta_rap1 <- sta_rap1 %>% select(-date)

# calculate SPI
spi_list  <- spi(sta_rap1[, 'rap'], 1, 
                 distribution = 'PearsonIII') 

#spi1_coeff_rap <- as.tibble(spi_list$coefficients) %>% 
#  t() %>% as.tibble()  

spi1_rap1 <- as.tibble(spi_list$fitted) 
spi1_rap1 <- bind_cols(Date_rap1, spi1_rap1) 

spi1_rap1 <- spi1_rap1 %>%
  rename(spi1 = rap) %>%
  arrange(spi1)

spi1_rap1 <- bind_cols(sta_rap1, spi1_rap1) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Second approach 
sta_trans <- sta_mon %>% 
  select(date, year, month, rap) %>% 
  arrange(date) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Pull apart the NA and non-NA vals
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   1. Split the raw data into two parts at 1948-05-01
  sta_ge49      <- sta_trans %>% filter(year >= 1949) # yr above
  sta_48m05     <- sta_trans %>% filter(year == 1948 & month >= 5)
sta_rap2         <- bind_rows(sta_ge49, sta_48m05) # this is active
  rm(sta_ge49, sta_48m05)
# remove year and month
sta_rap2 <- sta_rap2 %>%
  select(date, rap)
#  2. Save NA observations  
  sta_lt48      <- sta_trans %>% filter(year < 1948)
  sta_48m01_m05 <- sta_trans %>% filter(year == 1948 & month < 5)
sta_NA          <- bind_rows(sta_lt48, sta_48m01_m05) # this is not
  rm(sta_lt48, sta_48m01_m05, sta_trans)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Calculate Rapid City SPI 
# THIS CODE COULD BE DONE MUCH BETTER! 
spi1_rap2  <- spi(sta_rap2[,'rap'],  1, distribution = 'PearsonIII') 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save & rename coefficients 
# THIS CODE COULD BE DONE MUCH BETTER! 
#spi_1rap_coeff <- as.tibble(spi_1rap$coefficients) %>% 
 # t() %>% as.tibble() 

#spi_1rap_coeff <- rownames_to_column(spi_1rap_coeff, "month")
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# save SPI values as a Tibble
spi1_rap2 <- as.tibble(spi1_rap2$fitted) %>% 
  mutate(duration = 1)



# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




# Why does sta_rap1 have 839 $ sta_rap2 have 841???




summary(sta_rap1$date)
#  Min.      1st Qu.       Median         Mean      3rd Qu. 
#"1948-05-01" "1965-11-16" "1983-05-01" "1983-05-03" "2000-10-16" 
#        Max. 
#"2018-05-01" 
summary(sta_rap2$date)
#        Min.      1st Qu.       Median         Mean      3rd Qu. 
#"1948-05-01" "1965-11-01" "1983-05-01" "1983-05-02" "2000-11-01" 
#        Max. 
#"2018-05-01" 
sta_rap1_a <- sta_rap1 %>% 
  arrange(date) %>%
  slice(131:150)
sta_rap2_a <- sta_rap2 %>%
  arrange(date) %>%
  slice(131:150)

test_a <- full_join(sta_rap1_a, sta_rap2_a, by = "date")
test_a <- test_a %>%
  mutate(dif = rap.y - rap.x)

```

```{r import-precip-data} 
# Purpose: prepare data for drought index & EDA check 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) %>% 
  mutate(min_date = ymd(mindate)) %>% 
  mutate(max_date = ymd(maxdate)) %>% 
  select(-mindate, -maxdate) %>% 
  mutate(dur_year = max_date - min_date) %>% 
  mutate(dur_year = days(dur_year)) %>% 
  mutate(dur_year = time_length(dur_year, unit = "year"))

sta_raw     <- as.tibble(import("data/stations_monthly.csv")) %>% 
  mutate(date = ymd(date)) %>%  
  arrange(desc(date)) 
 
# make the wide data long, remove NA vals 
sta_gath <- sta_raw %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) %>% 
  drop_na(depth)  %>% 
  mutate(sta = as.factor(sta))

# not sure why these zeros are thought of as NA values 
sta_na <- sta_gath %>% 
    filter(depth == is.na(depth)) 

sta_gath$depth <- ifelse(sta_gath$depth == is.na(sta_gath$depth), 0, sta_gath$depth)  

sta_na <- sta_gath %>% 
    filter(depth == is.na(depth)) 

rm(sta_gath) 
```

```{r precip-EDA}
# Purpose: EDA of precipitation data.
# Outcome: Differences among stations 1971-2018 are small.
#          less than +/-5 mm on average.

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time?  
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.

# drop rows with missing data - this organizes the stations to same 
# time series length
sta_prep <- sta_raw %>%  
  drop_na %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) 

# create abreviated column for plotting & join
abbrev <- data.frame("sta" = c("cot", "int", "oel", "ora", "rap"),
                     "sta_abrev" = c("C", "I", "E", "O", "R")) 

sta_prep <- full_join(sta_prep, abbrev, by = "sta")

# create a year_month column
sta_prep <- sta_prep %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) 

# find mean values by month & join
sta_mean <- sta_prep %>% 
  group_by(month, year) %>% 
  summarize(mean_depth = mean(depth)) %>% 
  ungroup() %>% 
  mutate(mean_depth = round(mean_depth, digits = 2)) %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) %>% 
  select(-year, -month)
 
sta_prep <- full_join(sta_prep, sta_mean, by = "yr_mon") 
rm(abbrev, sta_mean)

# create a column of deviation from the mean by year & month
sta_prep <- sta_prep %>%
  mutate(deviation = depth - mean_depth) 

# plot the precip data as an overall boxplot 
ggplot(sta_prep, aes(as.factor(sta), depth)) + 
  geom_violin() + 
  geom_boxplot() + 
  scale_y_sqrt() + 
  theme_bw() + 
  ggtitle("Weather stations near Pine Ridge Reservation, SD",  
          subtitle = "1971-2018") + 
  xlab("") +  
  ylab("Monthly depth in mm") + 
  NULL 

ggplot2::ggsave(filename = "prcp_boxplot.png", 
                width = 6, height = 6, units = "in") 

# plot the precip depth as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, depth)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# plot the precip deviations as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, deviation)) + 
  geom_boxplot() +
#  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# prepare summary of station data & clean up
sta_sum <- sta_prep %>% 
  group_by(sta, month) %>% 
  summarise(mean_depth = mean(depth),
            sd_depth = sd(depth),
            mean_dev = mean(deviation)) 

rm(sta_prep)
```

```{r spi-with-SCI-1mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
#   SCI is the Standardized Precipitation Index 
#   SRI is the Standardized Runoff Index (SRI) 
# The other SCI package index is the Standardized Precipitation 
#   Evapotranspiration Index (SPEI).

# SCI function discussion----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Initial values for SCI calculations 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
time_scale <- 1  # sets the length of the averaging period 
distrib <- "pe3" # sets the distribution type 
p_zero <- TRUE   # sets a function to reduce zero-precip bias
p_zero_cm <- TRUE # ?????
scale <- "sd"    # scales input by subtract mean & divide by sd
warn_me <- TRUE  # sets explicit warning
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# The two SCI functions used below are fitSCI & tranformSCI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   fitSCI estimates the parameters for transforming a meteorological 
#   & environmental time series to a Standardized Climate Index (SCI).  

#       fitSCI(x, first.mon, time.scale, distr, p0, 
#         p0.center.mass = FALSE, scaling = c("no","max","sd"), 
#         mledist.par =  list(), start.fun = dist.start, 
#         start.fun.fix = FALSE, warn = TRUE, ...) 

#   transformSCI applies the transformation 
#       transformSCI(x, first.mon, obj, sci.limit = Inf,
#         warn = TRUE, ...) 

# description of arguments for the SCI functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# x	- numeric vector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A monthly univariate time series for SCI input 

# first.mon 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Value in [1:12] indicating month of the first element of x 

# time.scale 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The time scale is the window length of an backward-looking running 
# mean.  Time scale is an integer value. 

# distr	
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A character string "name" naming a distribution for which the 
# corresponding density function (dname), the corresponding 
# distribution function (pname) and the quantile function (qname) must 
# be defined (see for example GammaDist) 

# dist.para 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A column matrix containing the parameters of distribution distr for 
# each month. Row names correspond to the distribution parameters. 

# p0 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated separately.

# If p0=TRUE an additional row named P0 is introduced, indicating the 
# probability of zero (precipitation) events.

# If TRUE, model Probability of zero (precipitation) months is 
# modeled with a mixed distribution as D(x) = p0 + (1-p0)G(x), 
# where G(x) > 0 is the reference distribution (e.g. Gamma) p0 is the 
# probability of a zero (precipitation) month. 

# Following Stagge et al. (2014) the probability of zero events is 
# then estimated as p0 = (n_p)/(n + 1), where np refers to the number 
# of zero events and n is the sample size. 
# The resulting mixed distribution for SCI transformation is then: 

# g(x) = if(x > 0) p0 + (1 - p0) G(x) 
#   else if(x == 0) (np + 1)/(2(n + 1))
#    where G(x) > 0 is a model (e.g. gamma) distribution.

# p0.center.mass 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated using the "centre of mass" estimator 
# (see Stagge et al. (2014) for details).

# The probability of zero precipitation events (p0) can be estimated 
# using a "center of mass" estimate based on the Weibull plotting 
# position function to reduce biases in the presence of many zero 
# precipitation events by 'p0.center.mass = TRUE' 

# If TRUE, the Probability of zero (precipitation) is 
# estimated using a "center of mass" estimate based on the Weibull 
# plotting position function (see details). Only applies if p0=TRUE.

# scaling 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Indicates whether to do some scaling of x prior to parameter 
# identification. Scaling can stabilize parameter estimation. 
# "no" (the default) indicates no scaling. 
# "max" indicates scaling by the maximum of x, such that 
#      x <- x/max(x,na.rm=TRUE). 
# "sd" stands for scaling by the standard deviation. 
# warn	- Issue warnings if problems in parameter estimation occur. 

# mledist.par 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# named list that can be used to pass parameters to mledist in package 
# fitdistrplus.  # STILL NOT SURE ABOUT THIS!!!

# start.fun 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Function with arguments x and distr estimating initial 
# parameters of the function distr for each month. The function should 
# return a named list corresponding to the parameters of distr. 
# (See also dist.start)

# start.fun.fix 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical argument, indicating if parameter estimates 
# of start.fun should be used if maximum likelihood estimation breaks 
# down. This stabilizes the implementation but can introduce biases in 
# the resulting SCI.  Should look at this for Feb Interior.

# obj 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# an object of class fitSCI, output from fitSCI.

# sci.limit 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Truncate absolute values of SCI that are larger than sci.limit. 
# transformSCI allows for a truncation of the SCI series such that 
#   abs(sci) <= sci.limit. Uncertainty in distribution parameters can 
# cause unrealistically large or small SCI values if values in x 
# exceed the values used for parameter estimation. 
# The truncation can be disabled by setting sci.limit = Inf.

# Output flags 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# fitSCI returns an object of class "fitSCI".  Some flags are:
# dist.para.flag
# an vector indicating possible issues occurring throughout parameter 
# estimation. Possible values are: 
#     0. no problems occurred; 
#     1. starting values could not be estimated; 
#     2. mledist crashed with unknown error; 
#     3. mledist did not converge; 
#     4. all values in this month are NA; 
#     5. all values in this mon are constant, distribution not defined

# scaling
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# numeric value that has been used to scale x (see argument scaling). 
# A value of 1 results from scaling="no", other values are the maximum 
# value or the standard deviation of x, depending on the choice of the 
# parameter scaling. 

# call 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the function call transform SCI returns a numeric vector containing 
# the SCI, having values of the standard normal distribution. 


# SCI Analysis----


# Cottonwoods SPI
#~~~~~~~~~~~~~~~~~~~ 
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~ 
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~ 
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~ 
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double


# calculate SPI as a list & extract results 
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI
#~~~~~~~~~~~~~~~~~~~ 
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings from SPI----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) # M2 (feb) flag => 3
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 

# the flag means ML values did not converge
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# checking into the values for ML not coverging
#sta_check <- sta_raw %>% 
#  filter(month == 2) %>% 
#  drop_na() %>% 
#  arrange(int)

#summary(sta_check$int) 
#summary(sta_check$cot) 
#summary(sta_check$oel) 
#summary(sta_check$ora) 
#summary(sta_check$rap) 
#rm(sta_check)

# Sta  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# Int  0.30    4.40    9.20   12.53   17.10   70.30 * is the max why?
# Cot  0.30    4.80   10.40   13.54   18.30   62.00 
# Oel  0.00    5.85   11.50   12.08   16.40   37.80 
# Ora  0.00    5.05   10.10   12.35   17.50   42.70 
# Rap  0.60    5.10    8.90   11.56   15.95   43.40  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 


# Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att1     <- spi_att 
spi_gath1    <- spi_gath
spi_M1       <- spi_M 
spi_check1   <- spi_check

#spi_att_skew1 <- spi_att %>% 
#  filter(property == "shape") %>% 
#  select(-property) 

rm(att_names, spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_att_skew)
rm(spi_att, spi_check, spi_gath) 
```

```{r spi-with-SCI-2mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.   
# Explanation is above in spi-with-SCI-1mo

# set SCI state variables
time_scale <- 2  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings from SCI----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()
 
# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 


# Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att2     <- spi_att 
spi_gath2    <- spi_gath
spi_M2       <- spi_M 
spi_check2   <- spi_check

rm(att_names, spi_check)  
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-3mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo

# set SCI state variables
time_scale <- 3  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib,  
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()
 
# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 


# Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att3     <- spi_att 
spi_gath3    <- spi_gath
spi_M3       <- spi_M 
spi_check3   <- spi_check 

rm(att_names, spi_check) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-4mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo

# ORAL HAS ONE SPI VAL OF -8

# set state variables for SCI
time_scale <- 4  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()
 
# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 


# Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att4     <- spi_att 
spi_gath4    <- spi_gath
spi_M4       <- spi_M 
spi_check4   <- spi_check

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-5mo} 
# The following code chunks calculate SPI & SRI using the Standardized  
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo

# ora has an extreme low value & really high skew 

# set state variables for SCI
time_scale <- 5  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment---- 
# NOTE: BE CAREFUL WITH THE VALUES FOR THE SAVING OF THE PLOTS 
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att5     <- spi_att 
spi_gath5    <- spi_gath
spi_M5       <- spi_M 
spi_check5   <- spi_check 

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-6mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo 

# ora has an extreme low value & really high skew 

# set SPI state variables 
time_scale <- 6  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 

ora <- as.double(sta_ora$ora) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att6     <- spi_att 
spi_gath6    <- spi_gath
spi_M6       <- spi_M 
spi_check6   <- spi_check

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-9mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo

# ora has an extreme low value & really high skew 

# set SCI state variables 
time_scale <- 9  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na() 
 
ora <- as.double(sta_ora$ora) #changes tibble to a vector as double 

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att9     <- spi_att 
spi_gath9    <- spi_gath
spi_M9       <- spi_M 
spi_check9   <- spi_check

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-12mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo 

# ora has an extreme low value & really high skew 

# set SCI state variables 
time_scale <- 12  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>%  
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)  

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results  
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na()  
 
ora <- as.double(sta_ora$ora) #changes tibble to a vector as double 

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att12     <- spi_att 
spi_gath12    <- spi_gath
spi_M12       <- spi_M 
spi_check12   <- spi_check 

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath)
```

```{r spi-with-SCI-18mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo 

# ora has an extreme low value & really high skew 

# set SCI state variables
time_scale <- 18  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na()  
 
ora <- as.double(sta_ora$ora) #changes tibble to a vector as double 

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att18     <- spi_att 
spi_gath18    <- spi_gath
spi_M18       <- spi_M 
spi_check18     <- spi_check 

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath) 
```

```{r spi-with-SCI-24mo} 
# The following code chunks calculate SPI & SRI using the Standardized 
# Climate Index (SCI) package.  
# Explanation is above in spi-with-SCI-1mo 

# ora has an extreme low value & really high skew 

# set SCI state variables 
time_scale <- 24  # sets the length of the averaging period 

# SCI analysis----
# Cottonwoods SPI 
# prepare cottonwoods precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_cot <- sta_raw %>% 
  arrange(date) %>% 
  select(date, cot) 

cot <- as.double(sta_cot$cot) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
# Note on variable naming convention; the SCI package doesn't like 
#   snake_case variables
spi.cot.l <- fitSCI(cot, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

spi.cot <- transformSCI(cot, first.mon = first_mon, obj = spi.cot.l) 
# create a matrix with SPI values

spi_cot <- as.tibble(spi.cot) # change the vector into a tibble

spi_cot <- spi_cot %>% 
  rename(spi_cot = value) # prepare for a later join

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value
rm(sta_cot)

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation
first_mon <- 11 # sets the first month 

sta_int <- sta_raw %>% 
  arrange(date) %>% 
  select(date, int) %>% 
  drop_na()

int <- as.double(sta_int$int) #changes tibble to a vector as double

# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value
rm(sta_int)

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare oelrichs precip for SPI transformation
first_mon <- 6 # sets the first month 

sta_oel <- sta_raw %>% 
  arrange(date) %>% 
  select(date, oel) %>% 
  drop_na()

oel <- as.double(sta_oel$oel) #changes tibble to a vector as double

# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.oel <- transformSCI(oel, first.mon = first_mon, obj = spi.oel.l) 

spi_oel <- as.tibble(spi.oel) # change the vector into a tibble

spi_oel <- spi_oel %>% 
  rename(spi_oel = value) # prepare for a later join

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value
rm(sta_oel)

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Oral precip for SPI transformation
first_mon <- 5 # sets the first month 

sta_ora <- sta_raw %>% 
  arrange(date) %>% 
  select(date, ora) %>% 
  drop_na()  
 
ora <- as.double(sta_ora$ora) #changes tibble to a vector as double 

# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.ora <- transformSCI(ora, first.mon = first_mon, obj = spi.ora.l) 

spi_ora <- as.tibble(spi.ora) # change the vector into a tibble

spi_ora <- spi_ora %>% 
  rename(spi_ora = value) # prepare for a later join

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value
rm(sta_ora)

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Rapid City precip for SPI transformation 
first_mon <- 5 # sets the first month 

sta_rap <- sta_raw %>% 
  arrange(date) %>% 
  select(date, rap) %>% 
  drop_na() 

rap <- as.double(sta_rap$rap) #changes tibble to a vector as double

# calculate SPI as a list & extract results
# It seems that the SCI package doesn't like snake_case variables
spi.rap.l <- fitSCI(rap, first.mon = first_mon, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.rap <- transformSCI(rap, first.mon = first_mon, obj = spi.rap.l) 

spi_rap <- as.tibble(spi.rap) 

spi_rap <- spi_rap %>% 
  rename(spi_rap = value) # prepare for a later join

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value
rm(sta_rap)
 

# check for warnings----
flag_cot <- as.tibble(spi.cot.l$dist.para.flag) 
flag_int <- as.tibble(spi.int.l$dist.para.flag) 
flag_oel <- as.tibble(spi.oel.l$dist.para.flag) 
flag_ora <- as.tibble(spi.ora.l$dist.para.flag) 
flag_rap <- as.tibble(spi.rap.l$dist.para.flag) 

flag_spi <- bind_rows(flag_cot, flag_int, flag_oel, 
                      flag_ora, flag_rap) 
rm(flag_cot, flag_int, flag_oel, flag_ora, flag_rap) 


# clean up & combine the SPI variables---- 
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap) 
rm(cot, int, oel, ora, rap) 

spi <- full_join(spi_cot, spi_int, by = "date") 
spi <- full_join(spi, spi_oel, by = "date") 
spi <- full_join(spi, spi_ora, by = "date") 
spi <- full_join(spi, spi_rap, by = "date") 
# could probably use map_df here

rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 

# gather SPI variables 
spi_gath <- spi %>% 
  select(-cot, -int, -oel, -ora, -rap) %>% 
  rename(cot = spi_cot) %>% 
  rename(int = spi_int) %>% 
  rename(oel = spi_oel) %>%  
  rename(ora = spi_ora) %>%  
  rename(rap = spi_rap) %>% 
  gather(key = sta, value = spi_index, -date) %>% 
  drop_na()

# check covariance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a correlation matrix
spi_M <- spi %>% 
    select(-cot, -int, -oel, -ora, -rap, -date) %>% 
  drop_na()

spi_M <- cor(spi_M)
 
# Pearson Type-III distribution properties---- 
# get names for Pearson-III distribution 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
att_names <- spi.cot.l$dist.para 
att_names <- rownames(att_names) 
att_names <- as_data_frame(att_names) %>% 
  rename(property = value)

# cottonwood PE3 ~~~~~~~~~~~~~~~~~~
att_cot <- spi.cot.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "cot")

att_cot <- bind_cols(att_names, att_cot)

# interior PE3 ~~~~~~~~~~~~~~~~~~~ 
att_int <- spi.int.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "int")

att_int <- bind_cols(att_names, att_int)

# oelrichs PE3 ~~~~~~~~~~~~~~~~~~~~
att_oel <- spi.oel.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "oel")

att_oel <- bind_cols(att_names, att_oel)

# oral PE3 ~~~~~~~~~~~~~~~~~~~~~~~
att_ora <- spi.ora.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "ora")

att_ora <- bind_cols(att_names, att_ora)

# rapid city PE3 ~~~~~~~~~~~~~~~~~~
att_rap <- spi.rap.l$dist.para %>% 
  as_data_frame() %>% 
  mutate(sta = "rap")

att_rap <- bind_cols(att_names, att_rap)

# join the rows for PE3 & prepare for gather  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
spi_att <- bind_rows(att_cot, att_int, att_oel, att_ora, att_rap) %>% 
  rename("1" = M1) %>% 
  rename("2" = M2) %>% 
  rename("3" = M3) %>% 
  rename("4" = M4) %>% 
  rename("5" = M5) %>% 
  rename("6" = M6) %>% 
  rename("7" = M7) %>% 
  rename("8" = M8) %>% 
  rename("9" = M9) %>% 
  rename("10" = M10) %>% 
  rename("11" = M11) %>% 
  rename("12" = M12) 

rm(att_cot, att_int, att_oel, att_ora, att_rap)

# gather months for PE3
spi_att <- spi_att %>% 
  select(sta, property, everything()) %>% 
  gather(key = month, value = value, -sta, -property) 

# split properties from check vars 
spi_check <- spi_att %>% 
  filter(property == "N" |
           property == "P0" |
           property == "N.P0") %>% 
  arrange(property)

spi_att <- spi_att %>% 
  filter(property != "N") %>% 
  filter(property != "P0") %>% 
  filter(property != "N.P0") %>% 
  mutate(month = as.integer(month)) 

 # Clean up Global Environment----
#rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap)
spi_att24     <- spi_att 
spi_gath24    <- spi_gath
spi_M24       <- spi_M 
spi_check24   <- spi_check 

rm(att_names) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
rm(spi, spi_M, flag_spi, spi_check)
rm(spi_att, spi_gath)
rm(distrib, first_mon, p_zero, p_zero_cm, scale, time_scale, warn_me)
```

```{r join_spi} 
# this code chunk joins the various [spi1, spi24] data slices

# join spi attributes---- 
# prepare for join- create length
spi_att1 <-  mutate(spi_att1, spi_length = 1) 
spi_att2 <-  mutate(spi_att2, spi_length = 2) 
spi_att3 <-  mutate(spi_att3, spi_length = 3) 
spi_att4 <-  mutate(spi_att4, spi_length = 4) 
spi_att5 <-  mutate(spi_att5, spi_length = 5) 
spi_att6 <-  mutate(spi_att6, spi_length = 6) 
spi_att9 <-  mutate(spi_att9, spi_length = 9)  
spi_att12 <- mutate(spi_att12, spi_length = 12)  
spi_att18 <- mutate(spi_att18, spi_length = 18) 
spi_att24 <- mutate(spi_att24, spi_length = 24) 

# prepare for join- differenciate values
spi_att1 <-  mutate(spi_att1, spi1 = value) 
spi_att2 <-  mutate(spi_att2, spi2 = value) 
spi_att3 <-  mutate(spi_att3, spi3 = value) 
spi_att4 <-  mutate(spi_att4, spi4 = value) 
spi_att5 <-  mutate(spi_att5, spi5 = value) 
spi_att6 <-  mutate(spi_att6, spi6 = value) 
spi_att9 <-  mutate(spi_att9, spi9 = value)  
spi_att12 <- mutate(spi_att12, spi12 = value)  
spi_att18 <- mutate(spi_att18, spi18 = value) 
spi_att24 <- mutate(spi_att24, spi24 = value) 

# create a list of the spi_att dataframes
att_list <- list(spi_att1, spi_att2, spi_att3, 
                            spi_att4, spi_att5, spi_att6, 
                            spi_att9, spi_att12, spi_att18, 
                            spi_att24)

# flatten the list to a dataframe & select values
spi_att <- flatten_dfc(att_list) %>% 
  select(sta, spi_length, property, month, spi1, spi2, spi3, spi4, 
         spi5, spi6, spi9, spi12, spi18, spi24) %>% 
  gather(key = length, value = spi_value, -spi_length, 
         -property, -sta, -month) %>% 
  select(-length)
 

# join spi check values----
# prepare for join- create length
spi_check1 <- mutate(spi_check1, spi_length = 1) 
spi_check2 <- mutate(spi_check2, spi_length = 2) 
spi_check3 <- mutate(spi_check3, spi_length = 3) 
spi_check4 <- mutate(spi_check4, spi_length = 4) 
spi_check5 <- mutate(spi_check5, spi_length = 5) 
spi_check6 <- mutate(spi_check6, spi_length = 6) 
spi_check9 <- mutate(spi_check9, spi_length = 9)  
spi_check12 <- mutate(spi_check12, spi_length = 12)  
spi_check18 <- mutate(spi_check18, spi_length = 18) 
spi_check24 <- mutate(spi_check24, spi_length = 24) 

# bind rows of spi correlations for different averaging periods
spi_check <- bind_rows(spi_check1, spi_check2) 
spi_check <- bind_rows(spi_check, spi_check3) 
spi_check <- bind_rows(spi_check, spi_check4) 
spi_check <- bind_rows(spi_check, spi_check5) 
spi_check <- bind_rows(spi_check, spi_check6) 
spi_check <- bind_rows(spi_check, spi_check9) 
spi_check <- bind_rows(spi_check, spi_check12) 
spi_check <- bind_rows(spi_check, spi_check18) 
spi_check <- bind_rows(spi_check, spi_check24) 


# check non-zero precip vals
zero_prob_prcp <- spi_check %>% 
  filter(property != "N") %>% 
  mutate(value = as.numeric(value)) %>% 
  mutate(month = as.integer(month)) %>% 
  drop_na() %>% 
  filter(property == "P0") %>% 
  group_by(month, spi_length) %>% 
  summarize(mean_val_perc = 100 * mean(value)) %>% 
  filter(mean_val_perc > 0)


# join spi correlation coefficients----
# prepare for join
spi_M1 <- spi_M1 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 1)

spi_M2 <- spi_M2 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 2) 

spi_M3 <- spi_M3 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 3) 

spi_M4 <- spi_M4 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 4)

spi_M5 <- spi_M5 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 5)

spi_M6 <- spi_M6 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 6)

spi_M9 <- spi_M9 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 9)

spi_M12 <- spi_M12 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 12)

spi_M18 <- spi_M18 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 18)

spi_M24 <- spi_M24 %>% 
  as.tibble() %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  gather(key = sta2, value = pears_r, -sta) %>% 
  mutate(spi_length = 24)

# bind rows of spi correlations for different averaging periods
spi_corr <- bind_rows(spi_M1, spi_M2) 
spi_corr <- bind_rows(spi_corr, spi_M3) 
spi_corr <- bind_rows(spi_corr, spi_M4) 
spi_corr <- bind_rows(spi_corr, spi_M5) 
spi_corr <- bind_rows(spi_corr, spi_M6) 
spi_corr <- bind_rows(spi_corr, spi_M9) 
spi_corr <- bind_rows(spi_corr, spi_M12) 
spi_corr <- bind_rows(spi_corr, spi_M18) 
spi_corr <- bind_rows(spi_corr, spi_M24) 

# prepare to join spi_corr 
spi_corr <- spi_corr %>% 
  filter(pears_r < .99) %>% 
  mutate(sta2 = str_remove_all(sta2, "spi_")) %>% 
  mutate(stations = str_c(sta, sta2, sep = "-")) 

# remove duplicates from dataframe
spi_corr_cot <- spi_corr %>% 
  filter(sta == "cot")

spi_corr_int <- spi_corr %>% 
  filter(sta == "int") %>% 
  filter(sta2 != "cot")

spi_corr_oel <- spi_corr %>% 
  filter(sta == "oel") %>% 
  filter(sta2 != "cot" &
           sta2 != "int")

spi_corr_ora <- spi_corr %>% 
  filter(sta == "ora") %>% 
  filter(sta2 != "cot" &
           sta2 != "int" & 
           sta2 != "oel")

spi_corr <- bind_rows(spi_corr_cot, spi_corr_int, 
                      spi_corr_oel, spi_corr_ora)

# prepare lookup table of sta_meta for a join
sta_meta <- sta_meta %>% 
  arrange(name) %>%
  mutate(sta = c("cot", "int", "ora", "oel", "rap")) 

sta_loc <- sta_meta %>% 
  select(sta, lat, lon)

# join the lookup table to create from lat-lons
spi_corr <- spi_corr %>% 
  gather(key, val, -sta, -sta2) %>% 
  group_by(key) %>% 
  mutate(id = row_number()) %>% 
  left_join(sta_loc, by = "sta") %>%
  spread(key, val, convert = TRUE) %>% 
  ungroup() %>%
  rename(from_lat = lat) %>% 
  rename(from_lon = lon) %>% 
  select(-id)

# join the lookup table to create to lat-lons
spi_corr <- spi_corr %>% 
  select(-sta) %>% 
  rename(sta = sta2) %>% 
  gather(key, val, -sta) %>% 
  group_by(key) %>% 
  mutate(id = row_number()) %>% 
  left_join(sta_loc, by = "sta") %>% 
  spread(key, val, convert = TRUE) %>% 
  ungroup() %>% 
  rename(to_lat = lat) %>% 
  rename(to_lon = lon) %>% 
  select(-sta, -id) %>% 
  select(stations, everything()) 

# convert the to and from lat-lons to northings & eastings & distance
lat_to_km <- 111.03 # 1 degree lat to km @ lat 40-degrees 
lon_to_km <- 85.39  # 1 degree lon to km @ lat 40-degrees 
  
spi_corr <- spi_corr %>% 
  mutate(northing = (to_lat - from_lat) * lat_to_km) %>% 
  mutate(easting = (to_lon - from_lon) * lon_to_km)  %>%
  mutate(distance = sqrt(northing^2 + easting^2)) 

spi_corr <- spi_corr %>% 
  select(-to_lat, -to_lon, -from_lat, -from_lon)

# join spi index values----
# prepare for join -add lengths & get dataframes to same length
spi_gath1 <- spi_gath1 %>% 
  mutate(spi_length = 1) 

spi_gath2 <- spi_gath2 %>% 
  mutate(spi_length = 2) 

spi_gath3 <- spi_gath3 %>% 
  mutate(spi_length = 3) 

spi_gath4 <- spi_gath4 %>% 
  mutate(spi_length = 4) 

spi_gath5 <- spi_gath5 %>% 
  mutate(spi_length = 5) 

spi_gath6 <- spi_gath6 %>% 
  mutate(spi_length = 6) 

spi_gath9 <- spi_gath9 %>% 
  mutate(spi_length = 9) 

spi_gath12 <- spi_gath12 %>% 
  mutate(spi_length = 12) 

spi_gath18 <- spi_gath18 %>% 
  mutate(spi_length = 18) 

spi_gath24 <- spi_gath24 %>% 
  mutate(spi_length = 24)  

# bind rows is probably the easiest approach here to get the 
# individual dataframes into a single dataframe.  The dataframes are 
# of different lengths because of the averaging periods

spi_index <- bind_rows(spi_gath1, spi_gath2) 
spi_index <- bind_rows(spi_index, spi_gath3)  
spi_index <- bind_rows(spi_index, spi_gath4)  
spi_index <- bind_rows(spi_index, spi_gath5)  
spi_index <- bind_rows(spi_index, spi_gath6)  
spi_index <- bind_rows(spi_index, spi_gath9)  
spi_index <- bind_rows(spi_index, spi_gath12)  
spi_index <- bind_rows(spi_index, spi_gath18)  
spi_index <- bind_rows(spi_index, spi_gath24)  

# clean up Global Environment----
rm(spi_att1, spi_att2, spi_att3, spi_att4, spi_att5, 
   spi_att6, spi_att9, spi_att12, spi_att18, spi_att24)

rm(spi_check1, spi_check2, spi_check3, spi_check4, 
   spi_check5, spi_check6, spi_check9, spi_check12, spi_check18, 
   spi_check24) 

rm(spi_M1, spi_M2, spi_M3, spi_M4, spi_M5, spi_M6, spi_M9, 
   spi_M12, spi_M18, spi_M24)

rm(spi_corr_cot, spi_corr_int, spi_corr_oel, spi_corr_ora, 
   lat_to_km, lon_to_km, sta_loc)

rm(spi_gath1, spi_gath2, spi_gath3, spi_gath4, spi_gath5, 
   spi_gath6, spi_gath9, spi_gath12, spi_gath18, spi_gath24) 

rm(spi_check, att_list)
```

```{r spi-correlation-analysis}
# model effect of averaging time & distance on correlation----
# fit a linear model
spi_lm <- lm(pears_r ~ distance + spi_length, 
             data = spi_corr)

# augment the original data
spi_corr_aug <- augment(spi_lm, spi_corr)

# gather the data
spi_corr_gath <- spi_corr_aug %>% 
  select(stations:.fitted) %>% 
  select(-northing, -easting) %>% 
  gather(key = factor, val, -stations, -pears_r)

# plot the original data and fitted model
ggplot(spi_corr_gath, aes(val, pears_r)) + 
  geom_point(aes(color = factor(stations))) +
  facet_wrap(ncol = 1, vars(factor), scales = "free") +
    geom_smooth(method = lm) + 
  theme_classic()

spi_lm_fit <- glance(spi_lm) 

spi_lm_tidy <- tidy(spi_lm) %>% 
  mutate(
    low = estimate - std.error,
    high = estimate + std.error
  )

# Distance model only
# model is pears_corr = 0.78 -0.001 dist_km + 0.005 length_mo
# worst is 0.78 - 0.001*145 + 0.005
# best is 0.78 - 0.001*24 + 0.005*24
# range of dist: [0.024, 17.6]; plus_minus 8.79
# range of length: [0.005, 12]; plus_minus 6.0 

# So, slightly better fit on average by incorporating distance

# clean-up Global Environment----
spi_corr <- spi_corr_aug 
rm(spi_corr_aug, spi_corr_gath) 
rm(spi_lm,spi_lm_fit, spi_lm_tidy)

```

```{r fix-SCI-values}
# This code chunk fixes the really large negative values &
# the NA values by non-closure of  ML estimator for 
# Interior 1-month SPI for Feb

# Examine the initial SPI values for outliers----
ggplot(spi_index, aes(spi_length, spi_index)) + 
  geom_jitter() + 
  facet_grid(rows = vars(sta)) +
  theme_bw() +
  ggtitle("Initial SPI Plot", 
          subtitle = "Typical values are between -4 to 4") 

# save intial plot
#ggplot2::ggsave(filename = "figure/initial_spi.png", 
#                width = 6, height = 6, units = "in")

# split outliers from typical values----
spi_index_outlier <- spi_index %>% 
  filter(spi_index < -4) %>% 
  as.tibble()

spi_index_typical <- spi_index %>% 
  filter(spi_index > -4) %>%
  arrange(spi_index)

# spi_index_outlier 
# date       sta   spi_index spi_length
#  <date>     <chr>     <dbl>      <dbl>
#1 2012-09-01 int       -8.03          1
#2 2007-01-01 ora       -8.01          4
#3 1977-02-01 ora       -8.03          5
#4 2005-01-01 ora       -7.51         12
#5 2012-11-01 ora       -7.71         12 

# spi_index_typical 
# these are the min SPI vals
#  date       sta   spi_index spi_length
#   <date>     <chr>     <dbl>      <dbl>
# 1 1936-10-01 cot       -3.31          6
# 2 1937-01-01 cot       -3.27          9
# 3 1936-09-01 cot       -3.26          4
# 4 1936-09-01 cot       -3.25          5
# 5 1936-10-01 cot       -3.25          9

# plot outlier years----
sta_outlier <- sta_raw %>% 
  gather(key = sta, value = depth, -date, -year, -month) %>% 
  filter(date == "1977-02-01" | 
    date == "2005-01-01" | 
    date == "2007-01-01" | 
    date == "2012-11-01") 

ggplot(sta_outlier, aes(date, depth, color = factor(sta))) + 
  geom_point() + 
  theme_bw() +
  ggtitle("Initial SPI Plot", 
          subtitle = "Typical values are between -4 to 4") 

# Outlier Oral values chould be approximated by the mean SPI 
# or the nearest neighbor (Oelrichs) - used mean SPI

# create a tibble to replace the SPI outliers----
spi_index_replace1 <- spi_index_typical %>% 
  filter(date == "1977-02-01" & 
           spi_length == 5) 

spi_index_replace2 <- spi_index_typical %>% 
  filter(date == "2005-01-01" & 
           spi_length == 12) 

spi_index_replace3 <- spi_index_typical %>% 
  filter(date == "2007-01-01" & 
           spi_length == 4) 

spi_index_replace4 <- spi_index_typical %>% 
  filter(date == "2012-11-01" & 
           spi_length == 12) 

spi_index_replace5 <- spi_index_typical %>% 
  filter(date == "2012-09-01" & 
           spi_length == 1) 

spi_index_replace <- bind_rows(spi_index_replace1, spi_index_replace2) 
spi_index_replace <- bind_rows(spi_index_replace, spi_index_replace3) 
spi_index_replace <- bind_rows(spi_index_replace, spi_index_replace4) 
spi_index_replace <- bind_rows(spi_index_replace, spi_index_replace5) 

# calculate mean values & prepare for join 
spi_index_replace <- spi_index_replace %>% 
    group_by(date, spi_length) %>% 
  summarize(spi_index = mean(spi_index)) %>%
  ungroup() 

spi_replace_sta <- spi_index_outlier %>% 
  select(date, sta)

# join replace values to typical values & bind rows
spi_index_replace <- full_join(spi_index_replace, 
                               spi_replace_sta, by = "date")

spi_index2 <- bind_rows(spi_index_typical, spi_index_replace) 

# check the values after the fix
# spi_index2
# A tibble: 48,011 x 4
#   date       sta   spi_index spi_length
#   <date>     <chr>     <dbl>      <dbl>
# 1 1936-10-01 cot       -3.31          6
# 2 1937-01-01 cot       -3.27          9
# 3 1936-09-01 cot       -3.26          4
# 4 1936-09-01 cot       -3.25          5
# 5 1936-10-01 cot       -3.25          9
# the outlier values are now fixed 

spi_index <- spi_index2 %>% 
  arrange(spi_index)

# fix the NA values for Interior----
# get mean values for all the years & # split at the start of 
# Interior weather station 

spi_index_na <- spi_index %>% 
  mutate(month = month(date)) %>% 
  filter(month == 2)  %>% 
  filter(spi_length == 1) %>% 
  group_by(date, spi_length) %>% 
  summarize(spi_index = mean(spi_index)) %>%
  ungroup() %>%
  filter(date > "1948-11-01") %>%  
  mutate(sta = "int")
  
spi_index <- bind_rows(spi_index, spi_index_na) 


# visually check results
spi_index_plot <- spi_index %>% 
  filter(spi_length == 12)
  
ggplot(spi_index_plot, aes(date, spi_index)) + 
  geom_line() +
  facet_wrap(vars(sta)) + 
  theme_classic() +
  geom_hline(yintercept = 0, aes)

#ggplot2::ggsave(filename = "figure/spi_1mo.png",  
#                width = 6, height = 6, units = "in")  

# clean up Global Environment----
rm(spi_index_replace1, spi_index_replace2, spi_index_replace3, 
   spi_index_replace4, spi_index_replace5, spi_replace_sta, 
   spi_index2, spi_index_outlier, spi_index_replace, 
   spi_index_typical, spi_index_na, sta_na, sta_outlier, 
   spi_index_plot)
```

```{r SCI-diagnostic-plots, eval=FALSE}
# Diagnostic plots of SPI transforms 

# PE3 scale plot---- 
spi_scale <- spi_att %>% 
  filter(property == "scale")

ggplot(spi_scale, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_scale.png", 
                width = 6, height = 6, units = "in") 

# PE3 location plot---- 
spi_location <- spi_att %>% 
  filter(property == "location")

ggplot(spi_location, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_location.png", 
                width = 6, height = 6, units = "in") 

# PE3 shape plot---- 
spi_shape <- spi_att %>% 
  filter(property == "shape")

ggplot(spi_shape, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_shape.png", 
                width = 6, height = 6, units = "in") 

# clean up Global Environment----
rm(spi_location, spi_shape, spi_scale) 
```

```{r final-plot}
# should do a final SPI plot 
```

```{r precip-clustering, include=FALSE, eval=FALSE}
# Didn't work well...

# Prepare data for exploratory PCA -----------------------------------

# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  

# The steps are as follows :
# 1. Transform using BoxCox transformation to approach normality & 
#    standardize.
# 2. Drop rows with missing observations from wide data
#    (e.g. the station records are of different lengths)
# 3. Apply transformation and the scale data after making the data 
#    long.
# 4. Drop intermediate variables & rows with missing observations 
#    

# BoxCox transformation utilizes a lamda value to transform a dataset 
# to approach a normal distribution.
#   lambda = 1 is normal distribution (no change), 
#   lambda = 0.5 is a square-root transformation, 
#   lamda = 2 is a square transformation,
#   lambda = 0 is a logrithmic transformation.
# find lambda values for BoxCox transformation  
lambda_prcp <- BoxCox.lambda(sta_grp$depth) 

# lambda vals
#  prcp ==> 0.108 

# glimpse(sta_scaled) 
# sta          <chr> "oel", "oel", "oel", "oel", "oel", "oel", "oel"
# date         <date> 2018-05-01, 2018-04-01, 2018-03-01, 2018-02-01
# depth        <dbl> 139.8, 33.5, 24.0, 18.3, 13.3, 14.8, 16.5, 19.0.
# depth_tr     <dbl> 6.528617, 4.270760, 3.792030, 3.415306, 2.985829.
# depth_scaled <dbl> 1.320188955, 0.340849965, 0.133202243 

sta_mon_wide <- sta_int %>% 
  mutate(depth_tr = BoxCox(sta_int$depth, lambda_prcp)) %>% 
  mutate(depth_mean = mean(depth_tr, na.rm = TRUE)) %>% 
  mutate(depth_sd = sd(depth_tr, na.rm = TRUE)) %>% 
  mutate(depth_scaled = (depth_tr - depth_mean)/depth_sd) %>% 
  dplyr::select(-date) %>% 
  select(-sta_abrev, -mean_depth, -deviation)

sta_mon <- sta_mon_wide %>%  
  mutate(sta = as.factor(sta)) %>%  
  select(-depth, -yr_mon, -depth_mean, -depth_sd, -depth_tr, ) %>%
  spread(month, depth_scaled, convert = TRUE) %>% 
  drop_na() %>% 
  rename(jan = "1") %>% 
  rename(feb = "2") %>% 
  rename(mar = "3") %>% 
  rename(apr = "4") %>% 
  rename(may = "5") %>% 
  rename(jun = "6") %>% 
  rename(jul = "7") %>% 
  rename(aug = "8") %>% 
  rename(sep = "9") %>% 
  rename(oct = "10") %>% 
  rename(nov = "11") %>% 
  rename(dec = "12") 

# glimpse(sta_mon) 
# Observations: 230
# Variables: 14
# year <int> 1972, 1972, 1972, 1972, 1972, 1973, 1973, 1973, 1973, 
# sta  <fct> cot, int, oel, ora, rap, cot, int, oel, ora, rap, cot, 
# jan  <dbl> -1.1705655, -1.8359887, -1.0710896, -0.5540255, -0.781 
# feb  <dbl> -0.5138541, -1.7302116, -1.1024868, -1.3120703, -0.400 
# mar  <dbl> -0.49461927, -0.43412364, -0.01338585, -0.13223232, -0 
# apr  <dbl> -0.1172139, 1.0387562, 0.6274294, 0.3555747, 0.8642143 
# may  <dbl> 1.37710556, 0.70367637, 0.96166778, 0.84463790, 0.9909 
# jun  <dbl> 0.63876170, 0.36154511, 1.03170176, 1.35395955, 1.1668 
# jul  <dbl> 1.1087770, 0.7467736, 0.2690885, 0.3312313, 0.4877920, 
# aug  <dbl> 0.32709997, 0.79595891, 0.29970391, 0.47757034, 0.7806 
# sep  <dbl> -0.53364445, -0.99892436, -0.25600144, -0.10249695, -0 
# oct  <dbl> -0.25151491, -0.25151491, -0.01338585, -0.10981856, -0 
# nov  <dbl> -0.39471011, -0.39471011, -0.78171073, -0.32692017, -0 
# dec  <dbl> -0.84271133, -6.16974428, -0.83213761, -1.11880814, -0 

# Select numeric data for PCA input &  names to connect to result 
# prcomp() requires a dataset with only the variables for the 
# unsupervised classification 

pca_meta <- sta_mon %>% 
  select(year, sta)
pca_input <- sta_mon %>% 
  select(-year, -sta)

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE) 

# Gather PCA results----
# The broom package is used to gather results into tibbles.

# this plots the PCA biplot 
biplot <- biplot(pca_matrix, scale = 0) 

# Eigenvectors
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# A tibble: 12 x 4
#      PC std.dev percent cumulative
#   <dbl>   <dbl>   <dbl>      <dbl>
# 1     1   1.25   0.130       0.130
# 2     2   1.20   0.120       0.250
# 3     3   1.14   0.108       0.357
# 4     4   1.11   0.103       0.460
# 5     5   1.07   0.0955      0.556
# 6     6   1.03   0.0877      0.643
# 7     7   0.984  0.0807      0.724
# 8     8   0.924  0.0711      0.795
# 9     9   0.885  0.0652      0.860
#10    10   0.846  0.0597      0.92 
#11    11   0.728  0.0441      0.964
#12    12   0.656  0.0359      1  
# This means that 13% of covarience is explained by PCA1 & 
# 12% of varience by PCA2

# PCA variables 
# these are the loadings on the PCA axes.  
pca_vars <-  tidy(pca_matrix, matrix = "variables") 
pca_vars <- pca_vars %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) %>% 
  arrange(value) %>% 
  arrange(PC)

pca_rotation <- pca_matrix$rotation

# Bind sample vals to PCA matrix 
pca_au <- augment(pca_matrix, data = pca_input) %>% 
  select(jan:.fittedPC3)
pca_au <- bind_cols(pca_meta, pca_au) 
rm(pca_meta)

# Calculate mean eigenvectors 
# this summarizes the individual points by station - used in plots
pca_summary <- pca_au %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            PC3_mean = mean(.fittedPC3), 
            mar_mean = mean(mar), 
            apr_mean = mean(apr), 
            may_mean = mean(may),
            jun_mean = mean(jun), 
            jul_mean = mean(jul), 
            aug_mean = mean(aug),
            sep_mean = mean(sep), 
            oct_mean = mean(oct), 
            nov_mean = mean(nov)) %>% 
  ungroup()    


# PCA Interpretation-------------------------- 
pca_expl <- pca_vars %>%
  filter(abs(value) > 0.3) %>% 
  filter(PC < 4) %>% 
  filter(var != "dec") %>% 
  filter(var != "jan") %>% 
  arrange(value) %>% 
  arrange(PC)

pca_expl
# A tibble: 9 x 3
#  var      PC  value
#  <fct> <dbl>  <dbl>
#1 jun       1 -0.612
#2 mar       1  0.357
#3 sep       1  0.368
#4 may       2 -0.485
#5 aug       2 -0.438
#6 apr       2  0.330
#7 mar       2  0.438
#8 oct       3 -0.540
#9 may       3 -0.409

# thus PC1 is var in jun vs mar & sep
#      PC2 is var in may & aug vs apr
#      PC3 is var in oct & may

#PCA1:PCA2
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC1_mean, PC2_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

# PC1:: 
# Interior has some unusually a dry March & Jun records
# Cottonwood has some unusually dry Jun & Sept records
pca_pc1 <- pca_au %>% 
  select(sta, mar, jun, sep, .fittedPC1) %>% 
  gather(key = month, value = z_depth, -.fittedPC1, -sta)

ggplot(pca_pc1, aes(.fittedPC1, z_depth, color = sta)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(cols = vars(month))

# PC2:: 
# Oral & Interior have a slightly wetter May & August
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC2_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC2_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 


pca_pc2 <- pca_au %>% 
  select(sta, apr, may, aug, .fittedPC2) %>% 
  gather(key = month, value = z_depth, -.fittedPC2, -sta)

ggplot(pca_pc2, aes(.fittedPC2, z_depth, color = sta)) + 
  geom_point() + 
  geom_smooth() + 
  facet_grid(rows = vars(month)) + 
  theme_classic() 


#PCA1:PCA3
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC1_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC1_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

#PCA2:PCA3
ggplot() +
  geom_point(data = pca_summary, 
             mapping = aes(PC2_mean, PC3_mean)) + 
    geom_text(data = pca_summary,
              mapping = aes(PC2_mean, PC3_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

#fviz_pca_ind(prcomp(pca_input), 
#             title = "PCA - Iris data", 
#             habillage = pca_meta$sta, 
#             palette = "jco", geom = "point", 
#             ggtheme = theme_classic(), 
#             legend = "bottom")  
```

```{r EDA-PCA-gages}
# Examines data from 1992-1997 - wet years with Wounded Knee station 

# Data preparation steps: import data, normalize by watershed area, 
#     remove missing values, calculate z-scores & standardize data,

# PCA method discussion----
# Overview 
# ~~~~~~~~ 
# PCA is a method to summarize data using fewer variables by 
# constructing new linear descriptors from variables. The new 
# linear discriptors describe maximum variation across observations. 
# The new discriptors can be used to predict, or "reconstruct",
# observations as well as possible.

# PCA finds the axis the describes the maximum covariance and 
# projecting all of the observations points onto this line. The line 
# called the "first principal component" simultanesous describes 
# maximal variation of values along the line & minimizes the 
# reconstruction error 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Prepare data for exploratory PCA----

# The original data is saved in a long format with rows as daily 
# observations and columns as variables
# 1. Import data: 
#      Drop Cheyenne River at Angustora with ~50% NA vals 

gage_meta <- import("data/gage_meta_92_97.csv") %>%  
  filter(sta != "chr_ang")
gage_raw <- import("data/gage_92_97.csv") %>%  
  filter(sta != "chr_ang")

# glimpse(gage_raw)
# Variables: 26
# count_yr              <int> 67, 67, 67... count of years of record
# sta                   <chr> "bat_bhr"...  station name
# site_no               <int> 6406500...    USGS site number 
# station_nm            <chr> "BATTLE CR BELOW HERMOSA,SD"... 
# dec_lat_va            <dbl> 43.72543...   latitude 
# dec_long_va           <dbl> -102.9061...  longitude 
# state_cd              <int> 46, 46...     fips? code for State  
# county_cd             <int> 103, 103...   fips? code for County  
# alt_va                <dbl> 2800.32...    altitude in feet 
# drain_area_va         <dbl> 284, 284...   drainage area in sq mi. 
# contrib_drain_area_va <dbl> 284, 284...   contrib. da in sq mi.
# min_yr                <int> 1951...       first year of record 
# max_yr                <int> 2017...       last year of record 
# Date                  <chr> "1991-10-01"  date 
# i                     <int> 2191, 2192... count from 1st day of rec. 
# Q                     <dbl> 0.12518933... discharge in cms
# Julian                <int> 51772...      Julian calendar date  
# Month                 <int> 10, 10...     month 
# Day                   <int> 275, 276...   day  
# DecYear               <dbl> 1991.749...   decimal year  
# MonthSeq              <int> 1702...       NA 
# waterYear             <int> 1992...       water year 
# Qualifier             <chr> "A", "A"...   data quality code  
# LogQ                  <dbl> -2.077928...  log of zero-adj discharge 
# Q7                    <dbl> 0.14420207... 7-day moving average  
# Q30                   <dbl> 0.1342507...  30-day moving average 

# 2. Eliminate the effects of different sizes of watersheds:  
#    I calculated daily flow depths by dividing flow (cms) by watershed 
#    area (sq-km) and multiplying the resultant by the number of 
#    seconds in a day.  The result is cu-m-d per sq-km.

# 3. Remove missing values created by rolling averages
#    The NA vals created by calculating Q7 & Q30 need to be removed or 
#    estimated.  Removal seems the more conservative approach - 
#    especially as the NA values are very small given the number of 
#    observations of 365 obs/yr * 7 years. 

gage_depth <- gage_raw %>% 
  as.tibble() %>% 
  clean_names() %>% 
  mutate(contrib_drain_area_km = contrib_drain_area_va * 2.59) %>% 
  mutate(q1_depth = q * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q7_depth = q7 * (60*60*24) / contrib_drain_area_km) %>% 
  mutate(q30_depth = q30 * (60*60*24) / contrib_drain_area_km)  %>%
  drop_na() 

# 4. Transform using BoxCox to approach normality 
# daily flow data are highly skewed & tend to approach a logrithmic 
# distribution.  BoxCox transformation utilizes a lamda value to 
# transform a dataset to approach a normal distribution.
# Lambda = 1 is normal distribution (no change), 
# lambda = 0.5 is a square-root transformation, 
# lamda = 2 is a square transformation,
# lambda = 0 is a logrithmic transformation.

# find lambda values for BoxCox transformation  
lambda_q1 <- BoxCox.lambda(gage_depth$q1_depth)   #  q1 ==> 0.013 
lambda_q7 <- BoxCox.lambda(gage_depth$q7_depth)   #  q7 ==> 0.109  
lambda_q30 <- BoxCox.lambda(gage_depth$q30_depth) # q30 ==> 0.202 

# 5.Standardize data by z-score:
# PCA requires that data are scaled with a mean of zero and standard 
# deviation of unity, e.g. a z-score.  This could be done within the 
# PCA, but I did it manually.

# Apply transformation and scale data
gage_depth <- gage_depth %>% 
  mutate(q1_tr = BoxCox(gage_depth$q1_depth,lambda_q1)) %>%
  mutate(q7_tr = BoxCox(gage_depth$q7_depth,lambda_q7)) %>% 
  mutate(q30_tr = BoxCox(gage_depth$q30_depth,lambda_q30)) %>%
  mutate(q1_mean = mean(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_sd = sd(q1_tr, na.rm = TRUE)) %>% 
  mutate(q1_depth = (q1_tr - q1_mean)/q1_sd) %>% 
  mutate(q7_mean = mean(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_sd = sd(q7_tr, na.rm = TRUE)) %>% 
  mutate(q7_depth = (q7_tr - q7_mean)/q7_sd) %>% 
  mutate(q30_mean = mean(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_sd = sd(q30_tr, na.rm = TRUE)) %>% 
  mutate(q30_depth = (q30_tr - q30_mean)/q30_sd) %>% 
  select(sta, date, q1_depth, q7_depth, q30_depth, 
         q1_depth, q7_depth, q30_depth) 

# glimpse(gage_depth) 
# sta        <chr> "bat_bhr", "bat_bhr", "bat_bhr", "bat_bhr", ... 
# date       <chr> "1991-10-01", "1991-10-02", "1991-10-03", "1... 
# q1_depth   <dbl> 14.704930, 14.704930, 16.700614, 18.696298, ... 
# q7_depth   <dbl> 16.93820, 16.46303, 16.32048, 16.46303, 16.4... 
# q30_depth  <dbl> 15.76929, 15.81364, 15.93560, 16.12408, 16.2... 
# q1_scaled  <dbl> -0.4163661, -0.4163661, -0.3542800, -0.29912... 
# q7_scaled  <dbl> -0.5215705, -0.5363362, -0.5408399, -0.53633... 
# q30_scaled <dbl> -0.7615155, -0.7600352, -0.7559816, -0.74976... 


# Conduct exploratory PCA---- 

# Select numeric data 
# ~~~~~~~~~~~~~~~~~~~
# prcomp() requires a dataset with only the variables.  
# Below splits data into PCA input & names to connect to result 

pca_input <- gage_depth %>% 
  select(q1_depth, q7_depth, q30_depth) 

pca_meta <- gage_depth %>% 
  select(sta, date, q1_depth, q7_depth, q30_depth) 

# Calculate PCA matrix & summary info
pca_matrix <- prcomp(pca_input, scale = TRUE) 

# this plots the PCA biplot--takes a LONG time
# biplot(gage_pc, scale = 0)

# Gather & summarize PCA results----
# The broom package is used to gather results into tibbles.

# Eigenvectors--results about PC axes
pca_eigen <-  tidy(pca_matrix, matrix = "pcs") 

# PCA variables--the loadings on the PCA axes.  
pca_vars <-  tidy(pca_matrix, matrix = "variables") 

# Drop the 3rd PC-axis because it's not useful 
pca_vars <- pca_vars %>% 
  filter(PC != 3) %>% 
  rename(var = column) %>% 
  mutate(var = as.factor(var)) 

# Bind sample vals to PCA matrix 
gage_aug <- augment(pca_matrix, data = pca_input) 
gage_aug <- bind_cols(pca_meta, gage_aug) 

gage_aug <- gage_aug %>% 
  select(-.fittedPC3, -.rownames, 
         -q1_depth1, -q7_depth1, -q30_depth1) %>% 
  mutate(q1_q30_diff = q1_depth - q30_depth) 

# summarize pca of gages
gage_sum <- gage_aug %>% 
  group_by(sta) %>% 
  summarize(PC1_mean = mean(.fittedPC1),
            PC2_mean = mean(.fittedPC2), 
            q1_mean = mean(q1_depth), 
            q7_mean = mean(q7_depth),
            q30_mean = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff)) %>% 
  ungroup() %>% 
  arrange(q1_q30_mean) %>% 
  arrange(q7_mean)
  
# PCA results & interpretation
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#gage_pc_eigen
#    PC std.dev percent cumulative
#  <dbl>   <dbl>   <dbl>      <dbl>
#1     1   1.68  0.941        0.941
#2     2   0.385 0.0494       0.990
#3     3   0.170 0.00958      1  

#gage_pc_vars
#var           PC  value
#  <fct>      <dbl>  <dbl>
#1 q1_scaled      1  0.576
#2 q7_scaled      1  0.589
#3 q30_scaled     1  0.567
#4 q1_scaled      2  0.598
#5 q7_scaled      2  0.169
#6 q30_scaled     2 -0.784 

# The PCA explains the following: 
# eigenvecter:
#   94% of covarience is explained by PCA1 & 5% of varience by PCA2
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30;
#   PC2 - large positive loading of Q1 & large negative loading of Q30  
#   PC1 is about the hydrologic export of the system with larger
#     values exporting a greater amount of water 
#   PC2 is about the contribution of baseflow vs event-flow
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# clean up Global Environment----
gage_sum <- full_join(gage_meta, gage_sum)

rm(lambda_q1, lambda_q7, lambda_q30, gage_depth, gage_meta, 
   pca_eigen, pca_input, pca_matrix, pca_meta, pca_vars)


# plot PCA----
# PC1 explanatory plot
ggplot(gage_aug, aes(q7_depth, .fittedPC1, color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q7 depth") +
  ylab("Fitted PC1") +
  theme(legend.position = "bottom")

# PC2 explanatory plot
ggplot(gage_aug, aes(q1_q30_diff, .fittedPC2, color = factor(sta))) +
  geom_jitter() + 
  theme_classic() +
  xlab("Q1 depth minus Q30 depth") +
  ylab("Fitted PC2") +
  theme(legend.position = "bottom") 

# mean eigenvector plot
ggplot(gage_sum, aes(PC1_mean, PC2_mean)) +
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
            nudge_y = 0.02) + 
  theme_classic() + 
  scale_x_continuous(limits = c(-3.5, 2)) + 
  xlab("PC axis 1 mean") +
  ylab("PC axis 2 mean") 

# transformed variable plot
ggplot(gage_sum, aes(q7_mean, q1_q30_mean)) +
  geom_jitter() + 
  geom_text(aes(label = sta), check_overlap = TRUE, 
              nudge_y = 0.02) + 
  scale_x_continuous(limits = c(-2, 1)) + 
  geom_vline(xintercept = 0, aes(size = 2)) + 
  geom_hline(yintercept = 0, aes(size = 1)) + 
  theme_classic() + 
  xlab("Average transformed Q7 depth") +
  ylab("Transformed Q1 and Q30 average difference") 
```

```{r Model-Based_Clustering}
# The traditional clustering methods, such as hierarchical clustering 
# and k-means clustering, are heuristic and are not based on formal  
# models. An alternative is model-based clustering, which consider the 
# data as coming from a distribution that is mixture of two or more 
# clusters (Fraley and Raftery 2002, Fraley #et al. (2012)).

# Model-based clustering uses a soft assignment, where each data point 
# has a probability of belonging to each cluster.  In model-based 
# clustering, the data is considered as coming from a mixture of 
# density.  Each component (i.e. cluster) k is modeled by the normal 
# or Gaussian distribution which is characterized by the parameters:
#   μk\mu_k: mean vector, 
#   ∑k\sum_k: covariance matrix, 
#   An associated probability in the mixture. Each point has a 
#     probability of belonging to each cluster.

# "Old faithful geyser data" Example 

# Load the data 
#library("MASS") 
#data("geyser")

# Scatter plot 
library("ggpubr") 
ggscatter(geyser, x = "duration", y = "waiting") +
geom_density2d()  # Add 2D density 
  

# The plot suggests at least 3 clusters in the mixture. 
# The shape of each of the 3 clusters appears to be approximately 
# elliptical suggesting three bivariate normal distributions. As the 
# 3 ellipses seems to be similar in terms of volume, shape and 
# orientation, we might anticipate that the three components of this 
# mixture might have homogeneous covariance matrices.  

# gage-data - first a summary then the whole set
ggscatter(pca_summary, x = "PC1_mean", y = "PC2_mean") + 
geom_density2d()  # Add 2D density 
    geom_text(label = "sta") 

ggscatter(pca_au, x = ".fittedPC1", y = ".fittedPC2") +
geom_density2d() # Add 2D density 

# The plot suggests 3 clusters in the mixture. 
# The shape of the 3 clusters appears to be approximately 
# elliptical suggesting three bivariate normal distributions. 
# As the ellipses are dissimilar in terms of volume
# Shape and orientation are more homogeneous covariance matrices. 
# Possibly, a VEE?

# Estimating model parameters------
# The model parameters can be estimated using the Expectation-
# Maximization (EM) algorithm initialized by hierarchical model-based 
# clustering. Each cluster k is centered at the means μk\mu_k, with 
# increased density for points near the mean.
# Geometric features (shape, volume, orientation) of each cluster are 
# determined by the covariance matrix ∑k\sum_k.

# Different possible parameterizations of ∑k\sum_k are available in 
# the R package mclust (see ?mclustModelNames).
# The available model options, in mclust package, are represented by 
# identifiers including: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV 
# and VVV. 

# The first identifier refers to volume, the second to shape and the 
# third to orientation. E stands for "equal", V for "variable" and I 
# for "coordinate axes".

# For example:
# EVI denotes a model in which the volumes of all clusters are 
# equal (E), the shapes of the clusters may vary (V), and the 
# orientation is the identity (I) or "coordinate axes. EEE means that 
# the clusters have the same volume, shape and orientation in 
# p-dimensional space. VEI means that the clusters have variable 
# volume, the same shape and orientation equal to coordinate axes.


# Choosing the best model
# The Mclust package uses maximum likelihood to fit all these models, 
# with different covariance matrix parameterizations, for a range of 
# k components.
# The best model is selected using the Bayesian Information Criterion 
# or BIC. A large BIC score indicates strong evidence for the 
# corresponding model.

# Computing model-based clustering in R 
# Model-based clustering can be applied on univariate or multivariate 
# data.

# M-clust Example 
# Model-based clustering on the diabetes data set [mclust package] 
# giving three measurements and the diagnosis for 145 subjects 
# described as follow:
library("mclust") 
data("diabetes") 
head(diabetes, 3)

## class glucose insulin sspg 
## 1 Normal 80 356 124 
## 2 Normal 97 289 117 
## 3 Normal 105 319 143

# class: the diagnosis: normal, chemically diabetic, and overtly 
# diabetic. Excluded from the cluster analysis. 
# glucose: plasma glucose response to oral glucose 
# insulin: plasma insulin response to oral glucose 
# sspg: steady-state plasma glucose (measures insulin resistance)

# Model-based clustering can be computed using the function Mclust() 
library(mclust) 
df <- scale(diabetes[, -1]) # Standardize the data 

mc <- Mclust(df) # Model-based-clustering 

summary(mc) # Print a summary 
## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VVV (ellipsoidal, varying volume, shape, and orientation) 
## log.likelihood   n df  BIC  ICL 
##           -169 145 29 -483 -501 
## 
## Clustering table: 
##  1  2  3  
## 81 36 28 

# You can access the results by:
mc$modelName # Optimal selected model ==> "VVV" 

mc$G # Optimal number of cluster => 3 

head(mc$z, 10) # Probability to belong to a given cluster 
#        [,1]        [,2]         [,3]
#1  0.9906745 0.008991332 3.341728e-04
#2  0.9822128 0.017783229 3.974744e-06
#3  0.9777871 0.022157665 5.527579e-05
#4  0.9774763 0.022312280 2.113743e-04
#5  0.9208978 0.079034264 6.789759e-05
#6  0.9863472 0.012977950 6.748263e-04
#7  0.9429761 0.056900375 1.235066e-04
#8  0.9768888 0.022939764 1.714394e-04
#9  0.9739547 0.025861249 1.840708e-04
#10 0.9841312 0.015858549 1.028654e-05

mc_gage <- Mclust(pca_input) # Model-based-clustering 
summary(mc_gage) # Print a summary 



#---------------------------------------------------- 
#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 
#Mclust VVV (ellipsoidal, varying volume, shape, and orientation) 
#model with 8 components: 

# log.likelihood     n df       BIC       ICL
#      -32617.22 60881 79 -66104.76 -98845.13

#Clustering table:
#    1     2     3     4     5     6     7     8 
#   2669  2287 10456  4485 11405 15100  1158 13321 

# Model-based clustering selected a model with eight components 
# (i.e. clusters). The optimal selected model name is VVV model. 
# That is the eight components are ellipsoidal with varying volume, 
# shape, and orientation. The summary contains also the clustering 
# table specifying the number of observations in each clusters.

# create an input of mean values
mc_input <- pca_summary %>% 
  dplyr::select(q1_mean, q7_mean, q30_mean)

mc_mean <- Mclust(mc_input) # Model-based-clustering 
summary(mc_mean) # Print a summary 


#---------------------------------------------------- 
#Gaussian finite mixture model fitted by EM algorithm 
#---------------------------------------------------- 

#Mclust EEE (ellipsoidal, equal volume, shape and orientation) 
# model with 7 components: 

# log.likelihood  n df      BIC      ICL
#       182.2984 28 33 254.6341 253.5411

#Clustering table:
# 1 2 3 4 5 6 7 
# 7 9 7 1 1 2 1 


# You can access the results by:
mc_mean$modelName # Optimal selected model ==> "EEE" 

mc_mean$G # Optimal number of cluster => 7 

mc_mean_prob <- as.tibble(mc_mean$z)
mc_mean_prob <- bind_cols(pca_summary, mc_mean_prob) 
 

mc_mean_class <- as.tibble(mc_mean$classification) 
mc_mean_class <- bind_cols(pca_summary, mc_mean_class) %>% 
  dplyr::select(sta, value, everything()) %>% 
  rename(classification = value)


ggplot() +
  geom_point(data = mc_mean_class, 
             mapping = aes(PC1_mean, PC2_mean, 
                           color = factor(classification))) + 
    geom_text(data = mc_mean_class,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic() 

  geom_jitter(data = pca_au_small, 
             mapping = aes(x = .fittedPC1, y = .fittedPC2, 
                           color = factor(sta))) +




head(mc_class, 10) 
# Cluster assignment of each observation ==> 6 for
# 1  2  3  4  5  6  7  8  9 10  
# 1  1  1  1  1  1  1  1  1  1  

# Visualizing model-based clustering
# Model-based clustering results can be drawn using the base 
# function plot.Mclust() [in mclust package]. We'll use the 
# function fviz_mclust() [in factoextra package] to create beautiful 
# plots based on ggplot2.

# Where the data contain more than two variables, fviz_mclust() 
# uses a principal component analysis to reduce the dimensionnality 
# of the data. The first two principal components are used to produce 
# a scatter plot of the data. However, if you want to plot the data 
# using only two variables of interest, c("insulin", "sspg"), 
# you can specify that in the fviz_mclust() function using the 
# argument choose.vars = c("insulin", "sspg").

library(factoextra) 

# BIC values used for choosing the number of clusters 
fviz_mclust(mc, "BIC", palette = "jco") 

# Classification: plot showing the clustering 
fviz_mclust(mc_mean, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco") 

# Classification uncertainty 
fviz_mclust(mc_mean, "uncertainty", palette = "jco")

# Note: in the uncertainty plot, larger symbols indicate the 
# more uncertain observations.


ggplot() +
  geom_point(data = mc_mean_class, 
             mapping = aes(PC1_mean, PC2_mean, 
                           color = factor(classification))) + 
    geom_text(data = mc_mean_class,
              mapping = aes(PC1_mean, PC2_mean, label = sta), 
              vjust = 0, hjust = 0) + 
  theme_classic()  + 
  xlim(2, -5) +
  ylim(-0.6, 0.3) 



```

```{r CLUSTERING_INFO, eval=FALSE, INCLUDE=FALSE}




Several R packages available from CRAN or Bioconducto perform cluster validation, including: 

|    Package   |    Function(s)   |      Author       |      Notes    |
|:------------:|:----------------:|:-----------------:|:-------------:|
|   cclust     | clustIndex()     |    Dimitriadou    | No user guide |
|     fpc      | cluster.stats()  |       Hennig      | No user guide |
|              | clusterboot()    |                   |               |
| clusterRepro |                  | Kapp & Tibshirani | not general   |
|  clusterSim  |                  | Walesiak & Dudek  | poor user guide |
|  clusterStab |                  | MacDonald et al.  | narrow vignette |
|     clue     | cl_validity() +  | Hornik, September |     maybe...    |
|     e1071    | fclustIndex() ++ | Dimitriadou et al.| 2006  | unk.

+ validation for both paritioning methods (“dissimilarity accounted for”) and hierarchical methods (“variance accounted for”) 
++ fuzzy cluster validation measures.

pam() in recommended pack- age cluster (Rousseeuw, Struyf, Hubert, and Maechler, 2005; Struyf, Hubert, and Rousseeuw, 1996), and Mclust() in package mclust (Fraley, Raftery, and Wehrens, 2005; Fraley and Raftery, 2003), are available as components named cluster, clustering, and classification, 

RWeka (Hornik, Hothorn, and Karatzoglou, 2006), cba (Buchta and Hahsler, 2005), cclust (Dimitriadou, 2005), cluster, e1071 (Dimitriadou, Hornik, Leisch, Meyer, and Weingessel, 2005), flexclust (Leisch, 2006), flexmix (Leisch, 2004), kernlab (Karatzoglou, Smola, Hornik, and Zeileis, 2004), and mclust (and of course, clue itself).
```

```{r PROBABLY-GARBABE}
# this is the simplist case approach
#gage_scale_l <- gage_prep %>% 
#  select(sta, q_depth, date) %>% 
#  mutate(q_mean = mean(q_depth)) %>% 
#  mutate(q_sd = sd(q_depth)) %>% 
#  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
#  select(sta, q_scaled, date)

# calculate summary values
gage_sum <- gage_scale_l %>% 
  group_by(sta) %>% 
  summarise(mean = mean(q_scaled),
            sd = sd(q_scaled)) %>%
    ungroup()

# standardize the variables
gage_scale1_l <- gage_prep %>% 
  select(sta, q_depth, date) %>% 
  mutate(q_mean = mean(q_depth)) %>% 
  mutate(q_sd = sd(q_depth)) %>% 
  mutate(q_scaled = (q_depth - q_mean)/q_sd) %>% 
  mutate(avg_period = "1") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q_scaled, date)

gage_scale7_l <- gage_prep %>%  
 filter(!is.na(q7_depth)) %>% 
  mutate(q7_mean = mean(q7_depth)) %>% 
  mutate(q7_sd = sd(q7_depth)) %>% 
  mutate(q7_scaled = (q7_depth - q7_mean)/q7_sd) %>% 
  mutate(avg_period = "7") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q7_scaled, date)
    
gage_scale30_l <- gage_prep %>% 
  filter(!is.na(q30_depth)) %>% 
  mutate(q30_mean = mean(q30_depth)) %>% 
  mutate(q30_sd = sd(q30_depth)) %>% 
  mutate(q30_scaled = (q30_depth - q30_mean)/q30_sd) %>% 
  mutate(avg_period = "30") %>% 
  mutate(sta_type = paste(sta, avg_period, sep = "")) %>% 
  select(sta_type, q30_scaled, date)

# spread the long variables
gage_scale1 <- gage_scale_l %>% 
  spread(sta_type, q_scaled) 

gage_scale7 <- gage_scale7_l %>% 
  spread(sta_type, q7_scaled) 

gage_scale30 <- gage_scale30_l %>% 
  spread(sta_type, q30_scaled) 

gage_scale <- full_join(gage_scale1, gage_scale7)
gage_scale <- full_join(gage_scale, gage_scale30)

rm(gage_scale_l, gage_scale1, gage_scale1_l, gage_scale7, 
   gage_scale7_l, gage_scale30, gage_scale30_l)

# A check on the data finds Wounded Knee Creek has ~11% NA values
#plot_missing(gage_scale)

check_sta <- gage_scale %>% 
  select(wkc_wok30, date) %>% 
  filter(is.na(wkc_wok30))

# drop the NA values (num obs drops from 2192 to 1916)
gage_scale <- gage_scale %>% 
  drop_na() 

gage_scale_l <- gage_scale %>%
  gather(key = )


$xi−center(x)scale(x) \frac{x_i - center(x)}{scale(x)}$
$xi−mean(x)sd(x) \frac{x_i - mean(x)}{(x)}$

Where center(x) can be the mean or the median of x values, and scale(x) can be the standard deviation (SD), the interquartile range, or the MAD (median absolute deviation).
```

