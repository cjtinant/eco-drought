---
title: "Meta-analysis"
author: "Charles Jason Tinant"
date: "4/11/2018"
output: pdf_document
---
<!--
# Purpose
This worksheet contains meta-analytical ideas and resources to improve the efficiency of dissertation writing.

# Next Steps
* Finish reading project structure
* check out 'packrat'
* IO: odbc readxl httr 
* EDA: DataExplorer
* Sampling: rsample modelr
* Feature Engineering: recipes
* Modelling: glmnet h2o FFTrees
* Evaluation: broom yardstick
* Deployment: sqlrutils AzureML opencpu
* Monitoring: flexdashboard
* Identifying packages used: grateful
-->

```{r setup, include=FALSE}
# sets the path and what to do with chunks----
knitr::opts_chunk$set(echo = TRUE)
#opts_chunk$set(fig.width=10,
#               fig.height=7,
#               out.width = "600px",
#               out.height = "420px",
#               fig.path = "lecture_figs/making-maps-")
```

```{r library}
# Sets up the package library
library("DiagrammeR") # used to call 'mermaid' for a Gantt chart
library("thesisdown") # used to knit a thesis follows bookdown
library("bookdown") # 
# library(knitr) # not sure if this needs to be loaded??

# To import and clean data
library(rio) # more robust I/O
# library(DataExplorer)
# library("DataRetreval")
library("tidyverse")

# to make a map and plot figures

library(maps) # outlines of continents, countries, states & counties 
library(mapdata) # higher-resolution outlines.
library(ggplot2) # used to plot maps in the `maps` package.
library(ggmap)
library(here)
here()
```

```{r example-on-lists, include=FALSE, eval=FALSE}
n = c(2, 3, 5) 
s = c("aa", "bb", "cc", "dd", "ee") 
b = c(TRUE, FALSE, TRUE, FALSE, FALSE) 
x = list(n, s, b, 3)   # x contains copies of n, s, b

# List Slicing
#We retrieve a list slice with the single square bracket "[]" operator. # The following is a slice containing the second member of x, which is # a copy of s.

b <- as.data.frame(x[2])
> x[2] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee"

With an index vector, we can retrieve a slice with multiple members. Here a slice containing the second and fourth members of x.
> x[c(2, 4)] 
[[1]] 
[1] "aa" "bb" "cc" "dd" "ee" 
  
[[2]]   
[1] 3  
```

```{r example-on-tidy-lists, include=FALSE, eval=FALSE}
 
# Simplifying lists with purrr 
library(jsonlite) # Convert between JSON data and R objects.
library(curl) # Drop-in replacement for base url
library(repurrrsive) # Recursive lists for teaching & examples. 
library(listviewer) # htmlwidget for interactive views of R lists

# Not all lists are easily coerced into data frames by simply calling 
# fromJSON() %>% as_tibble(). Unless your list is perfectly 
# structured, this will not work.

#Inspecting and exploring lists

#Before you can apply functions to a list, you should understand it. 
#Especially when dealing with poorly documented APIs, you may not know 
# in advance the structure of your list, or it may not be the same as 
# the documentation. str() is the base R method for inspecting a list 
# by printing the structure of the list to the console. If you have a 
# large list, this will be a lot of output. max.levels and list.len 
# can be used to print only a partial structure for this list. 
# Alternatively, you can use jsonedit() to interactively view the list # within RStudio.

# Let’s look at got_chars, which is a list of information on the 29 
# point-of-view characters from the first five books in A Song of Ice 
# and Fire by George R.R. Martin.

#Each element corresponds to one character and contains 18 sub-elements
# which are named atomic vectors of various lengths and types.

got_chars <- got_chars
str(got_chars, list.len = 3)
jsonedit(got_chars, mode = "view", elementId = "got_chars")

# Extract elements
# Quick review of purrr::map()

#Map functions in R for Data Science
#    Notes on map functions

# We can use purrr::map() to extract elements from lists.
# Name and position shortcuts

# Let’s extract the name element for each Game of Thrones character. 
# To do this, we can use map() and extract list elements based on their name:

chars_1to4 <- map(got_chars[1:4], "name")

# A companion shortcut is to extract elements by their integer 
# position in the list. For example, extract the 3rd element of each 
# character’s list like so:

chars_5to8 <- map(got_chars[5:8], 3)

# To recap, here are two shortcuts for making the .f function that 
# map() will apply:
#    Provide “TEXT” to extract the element named “TEXT”
#        Equivalent to function(x) x[["TEXT"]]
#    Provide i to extract the i-th element
#        Equivalent to function(x) x[[i]]

# And as always, we can use map() with the pipe %>%:

chars_name <- got_chars %>% map("name")
chars_name <- got_chars %>% map(3)

# Type-specific map
# map() always returns a list, but if you know that the elements are 
# all the same type (e.g. numeric, character, boolean) and are each 
# of length one, you can use the map_() function appropriate for that # type of vector.

chars_chr_9to12  <- as.tibble(map_chr(got_chars[9:12], "name"))
chars_chr_13to16 <- as.tibble(map_chr(got_chars[13:16], 3))

# Extract multiple values
# What if you want to retrieve elements? What if you want to know the 
# character’s name and gender? For a single user, we can use  
# traditional subsetting:

# Victarion element
chars_element3 <- got_chars[[3]]

# specific elements for Victarion
chars_ele3_sub <- got_chars[[3]][c("name", 
                                   "culture", "gender", "born")]

# We use a single square bracket indexing and a character vector to 
# index by name. To adapt this to the map() framework, recall:

# map(.x, .f, ...)

#The function .f will be [ and ... will be the character vector #identifying the names of the elements to extract.

char_map <- map(got_chars, `[`, c("name", "culture", "gender", "born"))
str(char_map[16:17])

# The result is a list of 40 with fewer elements (e.g. name, culture,
#   gender, born)

# Alternatively, we can use magrittr::extract() to do the same thing. 
# It looks a bit more clean:

library(magrittr)

char_map <- map(got_chars, extract, c("name", "culture", "gender", "born"))
str(char_map[18:19])

#Data frame output

# Notice that even by extracting multiple elements at once, we are 
# still left with a list. But we want a simplified data frame! 
# Remember that the output of map() is always a list. To force the 
# output to be a data frame, use map_df():

char_df <- map_df(got_chars, extract, c("name", "culture", "gender", "id", "born", "alive"))

# Now we have an automatically type converted data frame. It was 
# quite simple to perform, however it is not very robust. 

#It takes more code, but it is generally better to explicitly specify 
# the type of each column to ensure the output is as you would expect:

char_tibble <- got_chars %>% {
  tibble(
    name = map_chr(., "name"),
    culture = map_chr(., "culture"),
    gender = map_chr(., "gender"),       
    id = map_int(., "id"),
    born = map_chr(., "born"),
    alive = map_lgl(., "alive")
  )
}

# The dot . above is the placeholder for the primary input: got_chars 
# in this case. The curly braces {} surrounding the tibble() call 
# prevent got_chars from being passed in as the first argument of 
# tibble().

#Exercise: simplify gh_users

# repurrsive provides information on 6 GitHub users in a list named 
# gh_users. It is a recursive list:

#   One element for each of the 6 GitHub users
#   Each element is, in turn, a list with information on the user
#   What is in the list? Let’s take a look:

str(gh_users, list.len = 3)
jsonedit(gh_users, mode = "view", elementId = "gh_users") 

# Extract each user’s real name, username, GitHub ID, location, date 
# of account creation, and number of public repositories. Store this 
# information in a tidy data frame.

get_exam <- gh_users %>% { 
  tibble(
    name = map_chr(., "name"),
    username = map_chr(., "login"), 
    create_date = map_chr(., "created_at") %>%
      lubridate::ymd_hms(),
    num_repos = map_int(., "public_repos")
  )
}

# List inside a data frame

# gh_users has a single primary level of nesting, but you regularly 
# will encounter even more levels. gh_repos is a list with:

#    One element for each of the 6 GitHub users
#    Each element is another list of that user’s repositories (or the 
#      first 30 if the user has more)
#     Several of the list elements are also a list

str(gh_repos, list.len = 2)
jsonedit(gh_repos, mode = "view", elementId = "gh_repos")

# Vector input to extraction shortcuts

# Now we use the indexing shortcuts in a more complicated setting. 
# Instead of providing a single name or position, we use a vector:
#    the j-th element addresses the j-th level of the hierarchy

# Here we get the full name (element 3) of the first repository 
#   listed for each user.

gh_repos %>%
  map_chr(c(1, 3))

# Note that this does NOT give elements 1 and 3 of gh_repos. It 
# extracts the first repo for each user and, within that, the 3rd 
# piece of information for the repo.

# Get it into a data frame

# Our objective is to get a data frame with one row per repository, 
# with variables identifying which GitHub user owns it, the 
# repository name, etc.

# Create a data frame with usernames and gh_repos

# First let’s create a data frame with gh_repos as a list-column 
# along with identifying GitHub usernames. To do this, we extract 
# the user names using the approach outlined above, set them as the 
# names on gh_repos, then convert the named list into a tibble:

(unames <- map_chr(gh_repos, c(1, 4, 1)))

(udf <- gh_repos %>%
  set_names(unames) %>% 
    enframe("username", "gh_repos"))

# Next let’s extract some basic piece of information from gh_repos. 
#  For instance, how many repos are associated with each user?

udf <- udf %>% 
  mutate(n_repos = map_int(gh_repos, length))

# Practice on a single user
# Before attempting to map() functions to the entire data frame, 
# let’s first practice on a single user.

# one_user is a list of repos for one user
one_user <- udf$gh_repos[[1]]

# one_user[[1]] is a list of info for one repo
one_repo <- one_user[[1]]
str(one_repo, max.level = 1, list.len = 5)

# a highly selective list of tibble-worthy info for one repo
one_repo[c("name", "fork", "open_issues")]


# make a data frame of that info for all a user's repos
map_df(one_user, `[`, c("name", "fork", "open_issues"))
map_df(one_user, extract, c("name", "fork", "open_issues"))

# Scale up to all users
# Next let’s scale this up to all the users in the data frame by 
# executing a map() inside of a map():

udf %>% 
  mutate(repo_info = gh_repos %>%
           map(. %>%
                 map_df(extract, c("name", "fork", "open_issues"))))

# Tidy the data frame
# Now that we extracted our user-specific information, we want to 
# make this a tidy data frame. All the info we want is in repo_info, 
# so we can remove gh_repos and unnest() the data frame:

(rdf <- udf %>% 
   mutate(
     repo_info = gh_repos %>%
       map(. %>%
             map_df(extract, c("name", "fork", "open_issues")))
   ) %>% 
   select(-gh_repos) %>% 
   tidyr::unnest())

# Acknowledgments
# Examples and data files drawn from Jenny Bryan’s purrr tutorial
# This work is licensed under the CC BY-NC 4.0 Creative 
#   Commons License.
```

```{r gantt-example, fig.align='left', fig.width = 50}
# Gantt project with all of the steps for completion----
# https://mermaidjs.github.io/gantt.html

# Ask yourself if each of the objective meet the: 
# SMART criteria for objectives
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Specific: is the objective clearly defined and self-contained?
# Measurable: is there a clear indication of its completion?
# Attainable: can the target be achieved?
# Realistic: have sufficient resources been allocated to the task?
# Time-bound: is there an associated completion date or milestone?


## 0. Preproposal tasks 
### 0.1.0  Outline Proposal 
### 0.2.0 Document ThesisDown
# 0.2.1 Read/summarize manual
#0.2.2 Pull request in GITHUB 

mermaid("gantt
       dateFormat  YYYY-MM-DD
       title Dissertation overview
       section Preparation
       Sent draft proposal        :done,    can1, 2018-04-10, 1d
       Committee review           :active,  can2, after can1, 21d
       Schedule defense           :         can3, 2018-05-02, 1d
       Incorporate edits          :         can4, after can2, 5d
       Send final draft           :         can5, after can4, 1d
       Oral defense               :         can6, 2018-05-05, 1d

       section Characterize drought
       Outline project            :active,   dro1, 2018-04-10, 4d
       Calculate SPI & SPEI       :          dro2, after dro1, 21d
       Develop SDI                :          dro3, after dro2, 30d 
       Cluster time series        :          dro4, after dro3, 10d  
       Watershed delineation      :          dro5, after dro4, 7d 
       Cluster ungaged sta. by RF :          dro6, after dro5, 21d
       Write results              :          dro7, after dro6, 14d 
       NSF dissemination          : crit,    dro8, 2018-06-21, 3d 
       Identify journal           : crit,    dro9, after can6, 21d 

       section Taxa patterns
       Collect macros with OST    :        tax1, 2018-06-14, 5d
       Munge macro data           :        tax2, after dro7, 14d
       NMS ordination             :        tax3, after tax2, 14d
       PERMANOVA                  :        tax4, after tax3, 14d
       Write results              :        tax5, after tax4, 30d
       OST dissemination          : crit,  tax6, 2018-09-15, 1d 
       Identify journal           : crit,  tax7, after dro7, 21d 

     section Drought indicators
       ISA analysis               :        ind1, after tax5, 30d
       IBI metrics                :        ind2, after ind1, 30d
       Update OST WPP             :        ind3, after ind2, 30d
       Write results              :        ind4, after ind3, 30d
       SDSMT dissemination        : crit,  ind5, 2018-10-30, 1d 
       Identify journal           : crit,  ind6, after tax5, 21d 
")
```


```{r gantt-small, fig.align='left', fig.width = 12}
# Fine-scale Gantt project over next three months----
# Short-term Gantt chart for PhD
mermaid("gantt
       dateFormat  YYYY-MM-DD
       title Gantt chart short-term
       section PhD Candidacy
       Sent draft proposal        :done,    can1, 2018-04-10, 1d
       Committee review           :active,  can2, after can1, 10d
       Schedule defense           :         can3, 2018-04-17, 1d
       Incorporate edits          :         can4, after can2, 5d
       Send final draft           :         can5, after can4, 1d
       Oral defense               :         can6, 2018-05-05, 1d

       section Characterize drought
       Outline project            :active,   dro1, 2018-04-10, 4d
       Calculate SPI & SPEI       :          dro2, after dro1, 21d
       Develop SDI                :          dro3, after dro2, 30d 
       Cluster time series        :          dro4, after dro3, 10d  
       Watershed delineation      :          dro5, after dro4, 7d 
       Cluster ungaged sta. by RF :          dro6, after dro5, 21d
       Write results              :          dro7, after dro6, 14d 
       NSF dissemination          : crit,    dro8, 2018-06-21, 3d 
       Identify journal           : crit,    dro9, after can6, 21d 
")
```

```{r import-data}
# use RIO or dataRetreval----
```

# Packages and Tutorials
## Reproducible Research
[archivist](https://raw.githubusercontent.com/pbiecek/archivist/master/cheatsheets/archivistCheatsheet.png)

# Plotting
[patchwork](https://github.com/thomasp85/patchwork/tree/master/R])

# Analysis
[Decision Trees tutorial1](https://github.com/GeostatsGuy/geostatsr)
[Decision Trees tutorial2](https://heartbeat.fritz.ai/introduction-to-decision-tree-learning-cd604f85e236)


# Project structure
This outlines project as package by 'Efficient R Programming'----

Improve project management and workflow by treating projects as R packages.  Creating packages teaches correct code documentation, storage of example data, and reproducibility via vignettes. But it can take a lot of extra time.

The R/ folder contains all the R code that defines package functions. Placing code in a single place and keeping code modular can greatly reduce code duplication. Furthermore, *documentation of R packages through Roxygen tags such as: 'This function does this...' makes it easy for others to use your work.* This form of efficient documentation is facilitated by the **roxygen2** package. 

The data/ folder contains example code for demonstrating to others how the functions work and transporting datasets that will be frequently used in your workflow. Data can be added automatically using the devtools package, with devtools::use_data(). This can increase efficiency by providing a way of distributing small to medium sized datasets and making them available when the package is loaded with the function data('data_set_name').

The package **testthat** makes it easier than ever to test your R code as you go, ensuring that nothing breaks.
An intermediate approach is to use a ‘dummy package’ that includes a DESCRIPTION file in the root directory to tell which packages must be installed for the code to work.  Example is 'Efficient R Programming' based on a dummy package so that they can easily keep dependencies up-to-date (see DESCRIPTION file)

The DESCRIPTION file contains key information about the package, including which packages are required for the code contained in your package to work, e.g. using Imports:. This is efficient because it means that anyone who installs your package will automatically install the other packages that it depends on.

For more on R packages see (Wickham 2015): the online version
provides all you need to know about writing R packages for free (see r-pkgs.had.co.nz/).


# Introduction to roxygen2
*Introduction to roxygen2 vignette*
Documentation is one of the most important aspects of good code. Without it, users won’t know how to use your package, and are unlikely to do so. Documentation is also useful for you in the future (so you remember what the heck you were thinking!), and for other developers working on your package. The goal of roxygen2 is to make documenting your code as easy as possible. R provides a standard way of documenting packages: you write .Rd files in the man/ directory. These files use a custom syntax, loosely based on latex. Roxygen2 provides a number of advantages over writing .Rd files by hand:

Code and documentation are adjacent so when you modify your code, it’s easy to remember that you need to update the documentation.  Roxygen2 dynamically inspects the objects that it’s documenting, so it can automatically add data that you’d otherwise have to write by hand.  It abstracts over the differences in documenting S3 and S4 methods, generics and classes so you need to learn fewer details.

As well as generating .Rd files, roxygen will also create a NAMESPACE for you, and will manage the Collate field in DESCRIPTION.

This vignette provides a high-level description of roxygen2 and how the three main components work. The other vignettes provide more detail on the individual components:

Generating .Rd files and text formatting describe how to generate function documentation via .Rd files

Managing your NAMESPACE describes how to generate a NAMESPACE file, how namespacing works in R, and how you can use Roxygen2 to be specific about what your package needs and supplies.

Controlling collation order describes how roxygen2 controls file loading order if you need to make sure one file is loaded before another.

Running roxygen

There are three main ways to run roxygen:

roxygen2::roxygenise(), or

devtools::document(), if you’re using devtools, or

Ctrl + Shift + D, if you’re using RStudio.

As of version 4.0.0, roxygen2 will never overwrite a file it didn’t create. It does this by labelling every file it creates with a comment: “Generated by roxygen2: do not edit by hand”.

# thoughts-on-thesisdown
1. look into making a vingette for thesisdown  

```{r tidy-data}
# Tidyr and dplyr example using the fivethirtyeight package ----
# library(fivethirtyeight)

# See names of columns 

#names(murder_2015_final)

# Load murder_2015_final package and gather into a tibble

# murders_gathered <- murder_2015_final %>% 
#  gather(
#    murder_year,
#    murders,
#    murders_2014:murders_2015,
#    na.rm = TRUE)
# murders_gathered

# Arrange alphabetically by state and city

#murders_arranged <- murders_gathered %>% 
#  arrange(
#    state, 
#    city)
#murders_arranged

# Separate murder_year into murder and year

#murders_separate <- murders_arranged %>%
#  separate(
#    murder_year,
#    into = c("text", 
#             "year")
#  )
#murders_separate

# Use spread and arrange to put year into two columns

#murders_spread <- murders_separate %>% 
 # spread(
 #   year,
 #   murders
 # ) %>% 
#  arrange(
#    state,
#    city)
#murders_spread

# Use unite to paste one column into another

#murders_final <- murders_spread %>%
#  unite(
#    city_state, 
#    city, 
#    state) %>% 
#  arrange(
#    city_state
#  ) %>% 
#  select(
#    -(text)
#  )
#murders_final

# Write this tibble to a csv

#write.csv(murders_final, file = "murders_final.csv",row.names=FALSE, na=""
```

```{r some-tips}
# useful books and packages for efficient programming----
# Unless otherwise indicated ## headings are pdfs in Library3

## Efficient R Programming 
# Becoming an Efficient R Programmer
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Some keystroke tools
# Ctrl+Z/Shift+Z: Undo/Redo.
# Ctrl+Alt+R: Execute all R code in the currently open file. 
# Ctrl+Left/Right: Navigate code quickly, word by word.
# Home/End: Navigate to the beginning/end of the current line.
# Alt+Shift+Up/Down: Duplicate the current line up or down.
# Ctrl+D: Delete the current line
# Ctrl+I: Automatically indent selected code 
# Ctrl+Shift+A: add spaces for maximum readability.

# Tips for efficient workflow 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Start without writing code but with a clear mind to ensure 
#    objectives are top of mind, without getting lost in technology.
# 2. Make a plan. Time-lines, resources and ‘chunking’ the work 
#    will make you more effective when you start.
# 3. Select packages for early implementation. 
# 4. Document your work with comments. -> check roxygen2 package
# 5. Make entire workflow as reproducible as possible. 
# knitr can help with documentation. <- check on knitr

# Tips for efficient data I/O
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Don't change names of local files downloaded from the internet or 
#    copied onto your computer. Helps to trace the provenance of data 
# 2. R’s native file format is .Rds. These files can imported and 
#    exported using readRDS and saveRDS for fast and space efficient 
#    data storage.
# 3. Use import() from the rio package to efficiently import data from
#    many formats.
# 4. Use readr or data.table equivalents of read.table() to 
#    efficiently import large text files. 75
# 5. Use file.size() and object.size() to keep track of the size of 
#    files and R objects and take action if they get too big.

# Tips for efficient data carpentry
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Give data processing the proper time and attention 
# 2. Tidy data carefully at the earliest stage of analysis by tidyr
# 3. Use the tbl class defined by the tibble package and the default 
#    object type of dplyr to make data frames efficient to print
# 4. Use the %>% ‘pipe’ operator to clarify data processing workflows
# 5. Use efficient data frames with tibble

# Open source project management software assendancy steps
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Github - described in Chapter 9)
# DiagrammeR has a Gantt chart builder
# ProjectLibre - Dedicated desktop project management software



# Tidy the data with tidyr and friends
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Understand dataset structure & ‘reshape’ them to reduce 
#    reformatting time, and increase computational efficiency.
# 2a. Data cleaning: re-format and label messy data by stringr package
#     for messy character strings 
# 2b. Perform diagnostic checks for data integrity at the outset of a
#     data analysis project by assertive and assertr packages
# 2c. Convert non-standard text strings into date formats by lubridate 
# 2d. Reshape data tidyr::gather, tidyr::spread, tidyr::separate
# 2e. Hoover up model output with the broom package.
# 2f. Process data with dplyr

# Tips for efficient performance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Use a code profiler to find slow-downs; profvis package
# 2. The ifelse function is optimal for vectorised comparisons.
# 3. If the data in your data frame is all of the same type, 
#    consider converting it to a matrix for a speed boost.
# 4.  The parallel package is ideal for Monte-Carlo simulations.

# Other thoughts
# ~~~~~~~~~~~~~~
# R package development is easier with devtools package
# Look forJenny Byran’s book, “Happy Git and Github for the useR”.

## Bookdown 
# Authoring Books and Technical Documents with R Markdown

# other Yihui packages to look into:
# library("tufte")
# library("blogdown")
# library("animation")
# library("tinytex")
# library("shiny")
# library("leaflet")
```

```{r map-tutorial}
# I think this is to set for bookdown?
#title: Making Maps With R
#author: "Eric C. Anderson"
#output:
#  html_document:
#    toc: yes
#  bookdown::html_chapter:
#    toc: no
#layout: default_with_disqus

# Making Maps with R {#map-making-in-R} 
# https://raw.githubusercontent.com/eriqande/rep-res-course/master/lectures/making-maps-with-R.rmd

## Intro {#map-making-intro}
# old package ->  `maps` package for making simple outlines
# of maps and plotting lat-long points and paths on them.

# New packages -> `sp`, `rgdal`, and `rgeos` functionality of 
# traditional GIS packages (like ArcGIS, etc).  *Not covered*
  
# Middle way -> `ggmap` to tile detailed base maps from Google Earth 
# or Open Street Maps, upon which spatial data may be plotted.
# all the power of ggplot.  Goes out to different map servers and
# grabs base maps to plot things on, then it sets up the coordinate 
# system and writes it out as the base layer for further ggplotting.  
# *but does not support different projections*

# some standard map packages.
#install.packages(c("maps", "mapdata"))
#devtools::install_github("dkahle/ggmap")

# You need to translate the `maps` data into a data frame format for 
# `ggplot`.  `ggplot2` provides the `map_data()` function. 

# Syntax: `map_data("name")` where "name" is a quoted string of the 
# name of a map in the `maps` or `mapdata` package.

# Data Structure for ggplot & maps
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# `long` is longitude.  W of prime meridian = negative
# `lat` is latitude.
# `order` gives order that `ggplot` should "connect the dots"
# `region` and `subregion` tell what region or subregion a set of 
#    points surrounds.
# `group`  *Very_important!*  Controls whether adjacent points should 
#    be connected by lines.  Members of the same group get connected.
#    "having points in diff. groups means `ggplot` "lifts the pen" 

# A simple US map
# ~~~~~~~~~~~~~~~
# Plot using `geom_polygon()` draw lines between points & "close them 
#    up" (i.e. draws a line from the last point back to the 1st point)
# Map the `group` aesthetic to the `group` column
#    `x = long` and `y = lat` are the other aesthetics.
# 'coord_fixed(x)' is important when drawing maps to fix the relationship 
#    between one unit in the $y$ direction and one unit in the 
#    $x$ direction. If you change the window size or the size of the 
#    pdf file then the _aspect ratio_ remains unchanged.
#    the 'x' above is by visual inspection
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# gg1 <- map_data("usa") # prepare data as data frame for ggplot
# ggplot() + 
# geom_polygon(data = usa, aes(x=long, y = lat, group = group)) + 
# fill = NA, color = "red") + 
# coord_fixed(1.3)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Adding points to the map
# ~~~~~~~~~~~~~~~~~~~~~~~~
# To add black and yellow points to the map

# labs <- data.frame(
#   long = c(-122.064873, -122.306417),
#   lat = c(36.951968, 47.644855),
#   names = c("SWFSC-FED", "NWFSC"),
#   stringsAsFactors = FALSE
# )  
#
# gg1 + 
#   geom_point(data = labs, aes(x = long, y = lat), 
#   color = "black", size = 5) +
#   geom_point(data = labs, aes(x = long, y = lat), 
#   color = "yellow", size = 4)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# State maps
# ~~~~~~~~~~
# states <- map_data("state")
# dim(states)
# head(states)
# tail(states)

# Plot all the states, all colored a little differently
# As above, but map fill to `region` and state border lines are white
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ggplot(data = states) + 
#  geom_polygon(aes(x = long, y = lat, fill = region, group = group),
#               color = "white") + 
#  coord_fixed(1.3) +
#  guides(fill=FALSE)  # do this to leave off the color legend

# Plot a subset of states with the `subset` command
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# west_coast <- subset(states, region %in% 
#   c("california", "oregon", # "washington"))
#    
# ggplot(data = west_coast) + 
#   geom_polygon(aes(x = long, y = lat, group = group), 
#     fill = "palegreen", color = "black") +
# coord_fixed(1.3)

#### Zoom in on California and look at counties
# ca_df <- subset(states, region == "california")
#    head(ca_df)

# Now, let's also get the county lines there
 #   counties <- map_data("county")
 #   ca_county <- subset(counties, region == "california")

 #   head(ca_county)

# Plot the state first 
# but let's ditch the axes gridlines, and gray background by
# using the super-wonderful `theme_nothing()`.

#    ca_base <- ggplot(data = ca_df, mapping = aes(x = long, y = lat, 
# group = group)) + 
#      coord_fixed(1.3) + 
 #     geom_polygon(color = "black", fill = "gray")
#    ca_base + theme_nothing()

# Now plot the county boundaries in white:
#
#    ca_base + theme_nothing() + 
#      geom_polygon(data = ca_county, fill = NA, color = "white") +
# get the state border back on top
#      geom_polygon(color = "black", fill = NA)  
```

```{r Regex_example, eval=FALSE}

#### Regex example to include some facts about the counties
# I can go to Wikipedia or http://www.california-demographics.com/counties_by_population
# and grab population and area data for each county.
# In fact, I copied their little table on Wikipedia and saved it into #`data/ca-counties-wikipedia.txt`. In
#full disclosure I also edited the name of San Francisco from "City and County #of San Francisco" to 
#"San Francisco County" to be like the others (and not break my regex!)
# Watch this regex fun:

library(stringr)
library(dplyr)

# make a data frame
x <- readLines("data/ca-counties-wikipedia.txt")
pop_and_area <- str_match(x, "^([a-zA-Z ]+)County\t.*\t([0-9,]{2,10})\t([0-9,]{2,10}) sq mi$")[, -1] %>%
na.omit() %>%
str_replace_all(",", "") %>% 
str_trim() %>%
tolower() %>%
as.data.frame(stringsAsFactors = FALSE)

# make a data frame
x <- readLines("data/ca-counties-wikipedia.txt")
pop_and_area <- str_match(x, "^([a-zA-Z ]+)County\t.*\t([0-9,]{2,10})\t([0-9,]{2,10}) sq mi$")[, -1] %>%
      na.omit() %>%
      str_replace_all(",", "") %>% 
      str_trim() %>%
      tolower() %>%
      as.data.frame(stringsAsFactors = FALSE)

# Need to add a section to break the [174:1] to [58:3] dataframe
      
# give names and make population and area numeric
names(pop_and_area) <- c("subregion", "population", "area")
pop_and_area$population <- as.numeric(pop_and_area$population)
    pop_and_area$area <- as.numeric(pop_and_area$area)
  
    head(pop_and_area)

# We now have the numbers that we want, but we need to attach those to 
# every point on polygons of the counties.  This is a job for 
# `inner_join` from the `dplyr` package

cacopa <- inner_join(ca_county, pop_and_area, by = "subregion")

# And finally, add a column of `people_per_mile`:
cacopa$people_per_mile <- cacopa$population / cacopa$area

head(cacopa)


#### Now plot population density by county

# prepare to drop the axes and ticks but leave the guides and legends
# We can't just throw down a theme_nothing()!
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )

elbow_room1 <- ca_base + 
      geom_polygon(data = cacopa, aes(fill = people_per_mile), 
                   color = "white") +
      geom_polygon(color = "black", fill = NA) +
      theme_bw() +
      ditch_the_axes

elbow_room1 

#### Lame!

# The population density in San Francisco is so great that it makes it
# hard to discern differences between other areas.
# This is a job for a scale transformation.  Let's take the 
# log-base-10 of the population density. Instead of making a new 
# column which is log10 of the `people_per_mile` we can just apply the 
# transformation in the gradient using the `trans` argument

elbow_room1 + scale_fill_gradient(trans = "log10")

#### Still not great
# I personally like more color than ggplot uses in its default 
# gradient.  In that respect I gravitate more toward Matlab's default 
# color gradient.  Can we do something similar with `ggplot`?

eb2 <- elbow_room1 + 
    scale_fill_gradientn(colours = rev(rainbow(7)),
                         breaks = c(2, 4, 10, 100, 1000, 10000),
                         trans = "log10")
eb2

### zoom in?
# Note that the scale of these maps from package `maps` are not great. 
# We can zoom in to the Bay region, and it sort of works scale-wise, 
# but if we wanted to zoom in more, it would be tough.  

eb2 + xlim(-123, -121.0) + ylim(36, 38)

# Whoa! That is an epic fail. Why?
# Recall that `geom_polygon()` connects the end point of a `group` to 
# its starting point. And the kicker: the `xlim` and `ylim` functions 
# in `ggplot2` discard all the data that is not within the plot area.  
# Hence there are new starting points and ending points for some 
# groups (or in this case the black-line perimeter of California) and 
# those points get connected.  Not good.

### True zoom.

# If you want to keep all the data the same but just zoom in, you can 
# use the `xlim` and `ylim` arguments to `coord_cartesian()`.  Though, 
# to keep the aspect ratio correct we must use `coord_fixed()` instead 
# of `coord_cartesian()`.
# This chops stuff off but doesn't discard it from the data set:

eb2 + coord_fixed(xlim = c(-123, -121.0),  ylim = c(36, 38), ratio = 1.3)
```

```{r ggmap}
## ggmap {#ggmap-hooray}----

# The `ggmap` package is the most exciting R mapping tool in a long 
# time!  You might be able to get better looking maps at some 
# resolutions by using shapefiles and rasters from 
# naturalearthdata.com
# but `ggmap` will get you 95% of the way there with only 5% of the 
# work!

### Three examples
# 1. Named "sampling" points on the Sisquoc River from the 
#    "Sisquoctober Adventure"
# 2. A GPS track from a short bike ride in Wilder Ranch.
# 3. Fish sampling locations from the coded wire tag data base.
    
### How ggmap works

# ggmap simplifies the process of downloading base maps from Google or 
# Open Street Maps or Stamen Maps to use in the background of your 
# plots. It also sets the axis scales, etc, in a nice way.  
# Once you have gotten your maps, you make a call with `ggmap()` much 
# as you would with `ggplot()`
# Let's do by example.

### Sisquoctober

# Here is a small data frame of points from the Sisquoc River.
sisquoc <- read.table("data/sisquoc-points.txt", sep = "\t", header = TRUE)
# notes: ggmap tends to use "lon" instead of "long" for longitude.

# make a bounding box
# ~~~~~~~~~~~~~~~~~~~
# `ggmap` typically asks you for a zoom level, 
# But we can try using `ggmap`'s `make_bbox` function to make a boun

sbbox <- make_bbox(lon = sisquoc$lon, lat = sisquoc$lat, f = .1)

# Now, when we grab the map ggmap will try to fit it into that 
# bounding box by first getting the map. 
# By default it gets it from Google.  I want it to be a satellite map

# Wrong way!
# ~~~~~~~~~~
# sq_map <- get_map(location = sbbox, maptype = "satellite", 
#                  source = "google")
# ggmap(sq_map) + geom_point(data = sisquoc, 
#                          mapping = aes(x = lon, y = lat), 
#                           color = "red")

# Nope! That was a fail, but we got a warning about it too. 
# (Actually it is a little better than before because I hacked `ggmap` 
# a bit...) 

# Using the zoom level in ggmap 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#  Zoom levels go from 3 (world scale to 20 (house scale)).
#    compute the mean lat and lon

```

```{r ggmap-satelite}
library(ggmap)
ll_means <- sapply(sisquoc[2:3], mean)

sq_map2 <- get_map(location = ll_means,  maptype = "satellite", source = "google", zoom = 15)

sq_map2 <- get_map(location = ll_means,  maptype = "terrain", source = "google", zoom = 15)

ggmap(sq_map2) + 
      geom_point(data = sisquoc, color = "red", size = 4) +
      geom_text(data = sisquoc, aes(label = paste("  ", 
      as.character(name), sep = "")), angle = 60, hjust = 0, 
      color = "yellow")

write.table(sq_map2,file = "data/sq_map2")

sq_map2 <- read.table(file = "data/sq_map2")
ggmap(data/sq_map2) + 
      geom_point(data = sisquoc, color = "red", size = 4) +
      geom_text(data = sisquoc, aes(label = paste("  ", 
      as.character(name), sep = "")), angle = 60, hjust = 0, 
      color = "yellow")
```

```{r ggmap-terrain}
# Use the "terrain" type of ggmap - with bad lettering

sq_map3 <- get_map(location = ll_means,  
                   maptype = "terrain", source = "google", 
                   zoom = 15)
ggmap(sq_map3) + 
      geom_point(data = sisquoc, color = "red", size = 4) +
      geom_text(data = sisquoc, 
                aes(label = paste("  ", as.character(name), 
                                  sep = "")), angle = 60, 
                hjust = 0, color = "yellow")
    ```

```{r ggmap-gps}
# plot a gps route----
# Map elevation to the color of the path using rainbow colors.

# The right zoom and position for the map is trial and error.  
# Go to Google maps to figure out where the center should be (right click and choose "What's here?" to get the lat-long of any point. )
# The `make_bbox` function has never really worked for me.
bike <- read.csv("data/bike-ride.csv")

bikemap1 <- get_map(location = c(-122.080954, 36.971709), 
                    maptype = "terrain", source = "google", zoom = 14)

ggmap(bikemap1) + 
      geom_path(data = bike, aes(color = elevation), size = 3, 
                lineend = "round") + 
      scale_color_gradientn(colours = rainbow(7), 
                            breaks = seq(25, 200, by = 25))
    ```

```{r fish-sampling}
### Fish sampling locations
# Example - coded wire tag data base to georeferenced marine locations 

# I did all that you can check out 
# [this](https://github.com/eriqande/pbt-feasibility/blob/4ea2fc960f74f66b5ec3a11c107cdc52bfb346dc/Rmd/02-02-explore-recovery-and-catch-sample-data.Rmd#looking-at-locations-of-location-codes)

# The data are hierarchically structured.  Author is interested in how 
# close together sites in the same "region" or "area" or "sector" are, 
# and pondering whether it is OK to aggregate fish recoveries at a 
# certain level for the purposes of getting a better overall estimate 
# of the proportion of fish from different hatcheries in these areas.

# So, pretty simple stuff.  I just want to plot these points on a map, 
# and paint them a different color according to their sector, region, 
# area, etc.

# bc <- readRDS("data/bc_sites.rds")

# look at some wide georeferenced data (1,113 observations)
# bc %>% select(state_or_province:sub_location, longitude, latitude)

# Enumerate using `dplyr`:
# bc %>% 
# group_by(sector, region, area) %>%
# tally()

# That looks good.  It appears like we could probably color code over 
# the whole area down to region, and then down to area within 
# subregions.

#### Makin' a map.

# Let us try again to use `make_bbox()` to see if it will work better 
# when used on a large scale.

# compute the bounding box
bc_bbox <- make_bbox(lat = latitude, lon = longitude, data = bc)
bc_bbox

# grab the maps from google
bc_big <- get_map(location = bc_bbox, source = "google", 
                  maptype = "terrain")

# * Cool! That was about as easy as could be.  North is in the north, 
# south is in the south, and  the three reddish points are clearly 
# aberrant ones at the mouths of rivers.

# Plot the points and color them by sector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ggmap(bc_big) + 
  geom_point(data = bc, mapping = aes(x = longitude, y = latitude, 
                                      color = sector))
```

```{r ggmap-coloring}
#### Coloring it by region

# We should be able to color these all by region to some extent 
# (it might get overwhelming), but let us have a go with it.
# Notice that region names are unique overall (not just within N or S) so we can just color by region name.

ggmap(bc_big) + 
  geom_point(data = bc, mapping = aes(x = longitude, y = latitude, color = region))

# Once again that was dirt easy, though at this scale with all the different regions, it is hard to resolve all the colors.
```

```{r ggmap-zoom-in}
#### Zooming in on each region and coloring by area

# Use a function to make a series of maps: one for each region, in 
# which the areas in that region are colored differently.
# Approach: you pass it the region and it makes the plot.

region_plot <- function(MyRegion) {
  tmp <- bc %>% filter(region == MyRegion)
  bbox <- make_bbox(lon = longitude, lat = latitude, data = tmp)
  mymap <- get_map(location = bbox, source = "google", 
                   maptype = "terrain")
  
# now we want to count up how many areas there are
NumAreas <- tmp %>% summarise(n_distinct(area))
NumPoints <- nrow(tmp)
  
the_map <- ggmap(mymap) +
    geom_point(data = tmp, mapping = aes(x = longitude, y = latitude), 
               size = 4, color = "black") +
    geom_point(data = tmp, mapping = aes(x = longitude, y = latitude, 
                                         color = area), size = 3) +
    ggtitle(
      paste("BC Region: ", MyRegion, " with ", NumPoints, 
            " locations in ", NumAreas, " area(s)", sep = "")
      )

ggsave(paste("bc_region", MyRegion, ".pdf", sep = ""), the_map, 
       width = 9, height = 9)
}

dump <- lapply(unique(bc$region), region_plot)
```







