---
title: "02-chapt2-5_study-area"
author: "CJ Tinant"
date: "8/06/2019"
output: html_document 
---

<!--
This R markdown file uses cleaned data from 02_streamflow_cleaning.  See this 
  Rmd for metadata on cleaning the data and variable conventions.

1) construct bootstrap confidence intervals of PC axes to identify a difference 
     in means. We obtained a 95% confidence interval (95% CI) around the our  
     estimate of the mean difference. The 95% indicates that a confidence 
     interval will capture the population mean difference 95% of the time. 
     So, we can be 95% confident the interval contains the true mean
     of the population.  We can calculated the 95% CI of the mean difference by 
     performing bootstrap resampling (Bradley Efron).

     The bootstrap creates multiple resamples (with replacement) from a single 
     set of observations, and computes the effect size of interest on each of 
     these resamples. The bootstrap resamples of the effect size can then be 
     used to determine the 95% CI.

     The resampling distribution of the difference in means approaches a normal 
     distribution. This is due to the Central Limit Theorem: a large number of 
     independent random samples will approach a normal distribution even if the 
     underlying population is not normally distributed.

Bootstrap resampling gives us two important benefits:
1.1) Non-parametric statistical analysis. There is no need to assume that our   
     observations, or the underlying populations, are normally distributed. 
     Thanks to the Central Limit Theorem, the resampling distribution of the 
     effect size will approach a normality.
1.2) Easy construction of the 95% CI from the resampling distribution. 
     For 1000 bootstrap resamples of the mean difference, one can use the 25th  
     value and the 75th value of the ranked differences as boundaries of the 
     95% confidence interval -- captures the central 95% of the distribution.
     The interval construction is known as a percentile interval.

Adjusting for asymmetrical resampling distributions
  While resampling distributions of the difference in means often have a normal 
  distribution, it is not uncommon to encounter a skewed distribution. Thus, 
  Efron developed the bias-corrected and accelerated bootstrap (BCa bootstrap) 
  to account for the skew, and still obtain the central 95% of the distribution. 
  dabestr applies the BCa correction to the resampling bootstrap distributions 
  of the effect size.

Estimation plots incorporate bootstrap resampling
  The estimation plot produced by dabest presents the rawdata and the 
  bootstrap confidence # interval of the effect size (the difference in means) 
  side-by-side as a single integrated plot. It thus tightly couples visual 
  presentation of the raw data with an indication of the population mean 
  difference, and its confidence interval.

2.0) check seasonality using stl 
3.0) prepare data for glms 
4.0) Generalized linear models (glms) for all of the data 
5.0) GLMs for data by ecoregions 


Variable naming conventions   
# ~~~~~~~~~~~~~~~~~~~~~~~~~  
gage            USGS streamflow gages  
  _dep          daily mean flow depths  
  _mon          monthly dv depths 
  
ci_input_pc1    bootstrap at 95% CI for PC1 of monthly values
ci_input_pc2    bootstrap at 95% CI for PC2 of monthly values
ci_plot_pc1     plot of PC1 bootstrap  
ci_plot_pc2     plot of PC2 bootstrap  
ci_results_pc1  results of PC1 bootstrap
ci_results_pc2  results of PC2 bootstrap 
ci_table        table of PC results
ci_table2       table of PC results

mon_freq        scratch df of the average monthly frequency  
                  'anomolize' parameter to remove season from observed vals  
mon_trend_sta.  scratch df of the average monthly trend by sta  
mon_trend       scratch df of the monthly trend 
                  'anomolize' parameter to remove trend from observed vals 
decomp_input    input for decomposition function (below) 
decomp_fun      function to decompose into seasonal, trend, & remainder 
gage_mon1       scratch df-decomposed monthly data for PC1 axis 
gage_mon2       scratch df-decomposed gage_mon for PC2 axis 
gage_mon        decomposed monthly daily flow depths 

anom_PC1        anomolies of PC1 after decomposition  
pc1_obs          ggplot of PC1 observations vs time 
pc1_seas         ggplot of PC1 seasons vs time 
pc1_trend        ggplot of PC1 trend vs time 
pc1_remainder    ggplot of PC1 remainder vs time  

mon_sum           summary table of PC1 & seasonal, trend explained 
mon_sum_gath      prepares to summarize monthly data 
mon_sum_pc1       summary PC1 & seasonal, trend explained by ecoregion 
mon_sum_pc2       summary PC2 & seasonal, trend explained by ecoregion  
mon_sum_pc1_all   summary PC1 & seasonal, trend explained by station 

gpg_layers        scratch df for layers in gpg 
wsd_summary       zonal statistics of watershed environmental parameters 
env_vars          environmental variables from wsd summary 
lambda            lambda vals for normalizing data 
basic_eda_func    function for basic eda 

gaged             joined gage_dv and  env_vars  

seed              random seed for glmnet & caret   
cores             parallel processing for glmnet & caret  
reg.ctrl          trainControl for glmnet & caret  
train_index       data partition training & test data (below)  
train_dv          training dv data for glmnet 
test_dv           test dv data for glmnet  
  select(-c(sta, mon, q1_depth, .fittedPC1, .fittedPC2))     

ridge_dv <- train_dv %>%   
  select(-Date) %>%       # drop the data just prior to running the model  
  train(    
  log_q1_depth ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )    
  
# lasso -- alpha = 1====   
set.seed(seed)             # need to set a seed each time you call a rand num   
lasso_dv <- train_dv %>%   
  select(-Date) %>%        # drop the data just prior to running the model   
  train(   
  log_q1_depth ~.,         # x = 'medv', y = the rest of the columns, from    
  data = .,                #   the dataset 'train.data'    
  method = "glmnet",       #   using method "glmnet"   
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,    
  tuneGrid = expand.grid(alpha = 1, lambda = lambda) # df with tuning values  
  )   
  
# elastic net -- alpha vals [0, 1] by caret====  
set.seed(seed)   
elastic_dv <- train_dv %>%  
  select(-Date) %>%         # drop the data just prior to running the model   
  train(   
  log_q1_depth ~.,          # x = 'medv', y = the rest of the columns, from   
  data = .,                 #   the dataset 'train.data'     
  method = "glmnet",       #   using method "glmnet"   
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,   
  tuneLength = 10          # tune length    
  )    
  
# make a list of the models====   
models_dv <- list(   
  ridge = ridge_dv,  
  lasso = lasso_dv,   
  elastic = elastic_dv)    

wsd_summary
*   "_id"       unique id
*  "_sta_id"    four- or seven-digit station ID   
   "_type "     distinguishes gaged from ungaged 
   "_watshed"   describes the HUC06 watershed 
   "_HUC12"     hydrologic unit code 12
   "_sta_name"    station name derived from outlet HUC12 catchment name
   "_gage_num"    USGS site number 
   "_gage_nm"     station name from USGS gage
   "_dec_lat"     latitude in decimal degrees
   "_dec_lon"     longitude in decimal degrees
   "_cat_area"    catchment area in sq km
*  "_cat_area_l"  natural logarithm of catchment area 
   "_cat_length"  catchment length
   "_cat_width"   catchment width
*  "_lw_ratio"    catchment length divided by catchment width
   "_str_len"     stream length
*  "_drain_dens"  stream length divided by catchment area
*  "_prcp_mean"   areal mean precipitation depth 1992-2012
*  "_t07_mean"    average July temperature 1992-2012
*  "_vpd_ann"     areal mean of max vapor deficit 1992-2012
*  _"vpd_07"      areal mean of max July vapor deficit 1992-2012
*  "_cat_out"     catchment outlet elevation
*  "_cat_rel"     difference in max and min elevation
*  "_slop_med"    median percent slope
*  "_TWI_mean"    mean terrain wetness index
*  "_perc_cov"    percent forest cover from NLCD 2016
*  "_fc_mean"     mean field capacity
*  "_ksat_mean"   mean horizontal saturated hydraulic conductivity
*  "_kvert_mean"  mean vertical saturated hydrologic conductivity 

```{r eval=FALSE}

# refactor below here 
  _sta            Short name used for this project 
  _Date           Date     
  _waterYear      Year following USGS water year format
  _q1_depth       discharge depth in m/day
  _q7_depth       7-day running average of q_depth
  _q30_depth      30-day running average of q_depth 

gage_meta           cleaned metadata for USGS gaging stations in the study area 
  "site_no"                site number     
  "station_nm"             station name     
  "dec_lat_va"             latitude value in decimal degrees 
  "dec_long_va"            longitude value in decimal degrees 
  "state_cd"               State code
  "county_cd"              County code
  "alt_va"                 altitude 
  "huc_cd"                 hydrologic unit code 
  "drain_area_va"          drainage area ???in square miles???
  "contrib_drain_area_va"  contributing drainage area ???in square miles???

gage_raw          the "raw" data, after removing the NA Qvals 
  _site_no        Eight-digit USGS gage number
  _sta            Short name used for this project 
  _Date           Date 
  _Q              Discharge in m3/s
  _Julian         Num. days since January 1, 1850 
  _Month          Month of the year [1-12] 
  _Day            Day of the year [1-366] 
  _DecYear        Decimal year 
  _MonthSeq       Number of months since January 1, 1850 
  _waterYear      Year following USGS water year 
  _Qualifier      Qualifying code
  _ i             Index of days, starting with 1 
  _LogQ           Natural logarithm of Q 
  _Q7             7-day running average of Q
  _Q30            30-day running average of Q



ecoreg            shapefile data of intersection of watershed area & 
                    merged SD, NE, WY ecoregions 
eco_join          join variable for ecoreg & gage_dv 

1) Develop tables and plots for Chapter 2 


6) create metadata tables 
    - gage_table - summary of gage metadata 
    - ung_table - summary of ungaged watershed metadata 
    - soils_table - summary of geology, soils, and vegetation 
```
--> 

```{r setup, include=FALSE, message=FALSE}   
#knitr::opts_chunk$set(echo = FALSE)      
options(tibble.print_max = 70) # sets tibble output for printing      
  
# Sets up the library of packages     
library("conflicted")       # An alternative conflict resolution strategy  
library("here")             # identifies where to save work  
library("rio")              # more robust I/O - to import and clean data  
library("lubridate")        # easier dates  
library("measurements")     # eases measurement system manipulation  
library("flextable")        # construct complex table with 'kable'  
library("officer")          # facilitates '.docx' access for table export  
library("broom")            # convert statistical objs into tidy tibbles  
library("ggfortify")        # data vis tools for statistical analysis  
library("anomalize")        # detect anomalies using the tidyverse  
library("sf")               # Support for simple features, a standardized way  
                            #   to encode spatial vector data   
library("funModeling")     # EDA, data preparation and model performance  
library("mclust")           # model-based clustering & density estimation  
library("caret")            # classification and regression training  
library("doMC")             # parallelization for caret  
library("glmnet")           # fit a GLM with lasso or elasticnet regularization  
library("Metrics")          # evaluation metrics for machine learning  
library("DataExplorer")     # used for plot_correlation()  
library("assertthat")       #                    NOT SURE???
library("dabestr")          # data analysis using bootstrap estimation  
library("tidyverse")        # data munging tools  
library(rcompanion)         # for Cramer's V for association function  
library(corrr)              # correlations in R  
  
# resolve conflicted packages----  
conflict_prefer("filter", "dplyr")  
conflict_prefer("select", "dplyr")  
conflict_prefer("summarize", "dplyr")  
  
# Notes on dabest====  
# dabest for monthly scale (14,027 obs; 40,000 reps) takes ~2 hours.  
#   even with increasing memory, I ran into a memory allocation 
#    limit at 50,000 reps
# increase memory allocation for dabest() by:  
#   usethis::edit_r_environ("project")  

``` 

```{r import_streamflow_df}
  
# Import streamflow data from 'streamflow_cleaning.Rmd'====  
# gage_dep <- import("data/gage_depth.csv")  # use monthly data  ??
gage_mon_orig <- import("data/gage_mon.csv")  
  
```

# model-based clustering  
```{r Mclust-model}
  
# make monthly inputs for clustering====   
clust_input <- gage_mon_orig %>%  
  select(sta, Date, waterYear, ecoreg, q1_mon, q7_mon, q30_mon) %>%  
  group_by(sta) %>%  # check on duplicate variables  
  distinct(Date, .keep_all = TRUE) %>%  
  ungroup()  %>% 
  select(-c(sta, Date, waterYear, ecoreg)) # get rid of categorical data  
  
# Find Bayesian Information Criterion and run Mclust====  
BIC <- mclustBIC(clust_input)  
  
mod <- Mclust(clust_input, x = BIC)   
  
# get Mclust model results and use broom to tidy up the Mclust model  
mclust_fit <-glance(mod)  
mclust_aug <- augment(mod, gage_mon_orig)   
  
# get flow-type centroids===  
mclust_means <- tidy(mod) %>%  
    arrange(mean.q1_mon) %>%  
  mutate(flow_type = c("type1", "type2", "type3",  
                       "type4", "type5", "type6",  
                       "type7", "type8", "type9")  
         ) %>% 
  mutate(.class = as.factor(component)) %>%  
  select(-component)  
  
# prepare for plotting - reorganize flow by centroids====   
mclust_aug_means <- mclust_aug %>%  
  group_by(.class) %>%  
  summarize(#q1_flow = mean(q1_mon),  # these are ~equal to mclust_means  
            #q7_flow = mean(q7_mon),  
            #q30_flow = mean(q30_mon),  
            mean.PC1_mon = mean(PC1_mon),  
            mean.PC2_mon = mean(PC2_mon), 
            size = n()  
            ) %>%  
  ungroup()  
  
mclust_means <- full_join(mclust_means, mclust_aug_means,  
                  by = c("size",  
                         ".class")  
                  ) %>%  
# reorganize columns  
  select(.class,  
         flow_type,  
         size,  
         proportion,  
         everything()  
         )  
  
# join flow_type to tidy cluster model & clean up global environment====  
mclust_aug <- full_join(mclust_aug, mclust_means,  
                     by = c(".class")  
                     ) %>%  
  group_by(.class, ecoreg) %>%  
  mutate(q1 = mean(q1_mon)) %>%  
  mutate(q7_eco = mean(q7_mon)) %>%  
  mutate(q30_eco = mean(q30_mon))  %>%  
  mutate(PC1_eco = mean(PC1_mon)) %>%  
  mutate(PC2_eco = mean(PC2_mon)) %>%  
  ungroup() %>%  
  mutate(flow_type = as.factor(flow_type)) %>%  
# change the order of facets #  
  mutate(ecoreg = factor(ecoreg,  
                         levels = c('Pierre Shale Plains',  
                                    'Pine Ridge Escarpment',  
                                    'White River Badlands',  
                                    'Black Hills Plateau',  
                                    'Keya Paha Tablelands',  
                                    'Sand Hills')  
                         )  
         )  
  
# get proportions of different flow types by ecoregion  
mclust_prop <- mclust_aug %>%  
  group_by(ecoreg, flow_type) %>%  
  summarize(count = n()) %>%  
  group_by(ecoreg) %>%  
  mutate(count2 = sum(count)) %>%  
  ungroup() %>%  
  mutate(proportion_ecoreg = count/count2)  %>%  
  select(-c(count, count2))  
  
mclust_aug <- full_join(mclust_aug, mclust_prop,   
                         by = c("ecoreg", "flow_type"))  
  
# check results -- sample cluster plot====  
factoextra::fviz_mclust(mod,  
                          "classification",  
                          geom = "point",  
                          ellipse.type = "t",  
                          ellipse.level = 0.7,  
                          pointsize = 1.5,  
                          palatte = "npg"  
                          )  
  
# clean up global environment  
rm(clust_input,  
   BIC,  
   mclust_aug_means,  
   mclust_prop  
   )  
  
```

```{r Mclust-plot}
  
# get pc-vars for plotting====  
pca_vars  <- import('data/pca_vars.csv')  
pca_eigen <- import("data/pca_eigen.csv")  
  
# plot clusters by ecoregion====  
mclust_eco_plot  <- mclust_aug %>%  
  ggplot(aes(PC1_mon, PC2_mon)) +  
  # set theme and facets #  
  theme_bw() +  
  facet_wrap(vars(ecoreg)) +  
  geom_jitter(  
    aes(color = flow_type,  
        shape = ecoreg,  
        alpha = .uncertainty),  
    size = 2,  
    show.legend = FALSE) +  
  scale_alpha(".uncertainty") +  
  # plot centroids for each flow type -- commented out to reduce plot clutter  
  geom_point(aes(PC1_eco, PC2_eco,  
                 size = proportion_ecoreg,  
  ),  
  show.legend = FALSE,  
  shape = 3,                   # cross shape  
  # size = 2.5,
  stroke = .5,  
  color = "gray50"
  ) + 
  # plot pc eigenvectors as arrows #  
  geom_segment(data = pca_vars,  
               aes(x = 0, y = 0,  
                   xend = PC1, yend = PC2), 
               arrow = arrow(length = unit(0.03, "npc")) 
  ) +  
  # plot pc eigenvector text #  
  geom_text(data = pca_vars,  
            size = 3,  
            nudge_x = 0.5,  
            nudge_y = -0.1,  
            hjust = 'outside',  
            aes(x = PC1, y = PC2, label = labels)  
  ) +  
  # set axis text #  
  xlab("PC1 axis (95.0%)") +  # from pca_eigen  
  ylab("PC2 axis (4.3%)") +  
  theme(legend.position="bottom")  
  
# plot clusters by flow type====  
mclust_flow_plot <- mclust_aug %>%  
  ggplot(aes(PC1_mon, PC2_mon)) +  
  # set theme and facets #  
  theme_bw() + 
  facet_wrap(~ flow_type) +  
  # plot points # 
  geom_jitter(aes(shape = ecoreg,  
                  color = flow_type,  
                  alpha = .uncertainty 
  ),  
  show.legend=FALSE,  
  size = 2)  +  
  # plot centroids #  
  geom_point(aes(PC1_eco, PC2_eco,  
                 shape = ecoreg,  
                 size = proportion_ecoreg  
  ),  
  show.legend=FALSE,  
  color = "black"  
  ) +  
  # set axis text #  
  xlab("PC1 axis (95.0%)") +  # from pca_eigen  
  ylab("PC2 axis (4.3%)") +  
  theme(legend.position="bottom")  
  
# plot grid of ggplots====  
cowplot::plot_grid(  
  mclust_eco_plot,  
  mclust_flow_plot,  
  ncol = 1,  
  align = "v"  
)  
  
cowplot::ggsave2("figure/mclust_plot.png",  
        units = "in",  
        width = 7,  
        height = 9)  
  
# save results & clean up global environment  
export(mclust_aug, "data/mclust_results.csv")  
export(mclust_fit, "data/mclust_fit.csv")  
  
rm(mclust_flow_plot,  
   mclust_eco_plot,  
   mod,  
   mclust_aug,  
   mclust_fit,  
   mclust_means,  
   pca_eigen,  
   pca_vars  
   )  

#    scale_colour_brewer(palette = "Greys") + 
#ecoreg_sum <- mclust_aug %>%  
#  group_by(ecoreg) %>%  
#  summarise(q)  # what to do here???

# map the title of the geom for the legend  
#  scale_shape_discrete(name="Ecoregion") + 
#  scale_color_hue(name="Flow type") + 
```

# bootstrapping  
```{r 01-bootstrap}  
  
# reran results on 2019-12-15   
# Construct bootstrap confidence intervals of PC axes to identify  
#   a difference in means  
# remove 'leftover' attributes introduced by stl  
gage_mon <- gage_mon %>%  
  as_tibble()   
  
# note: start 11:14 & end at 1:28 -- didn't set parallelization  
# calculate 95% CI for PC1 - (n = 14,423) 
ci_input_pc1 <- gage_mon %>%      
  dabest(ecoreg, PC1_mon,   
         idx = c("Pierre Shale Plains",  # groups to test - first is control  
                 "Pine Ridge Escarpment",  
                 "White River Badlands",   
                 "Black Hills Plateau",   
                 "Keya Paha Tablelands",   
                 "Sand Hills"),   
         paired = FALSE,  
         reps = 40000                     # number of bootstraps  
         )    
  
# calculate 95% CI for PC2  
ci_input_pc2 <- gage_mon %>%  
  dabest(ecoreg, PC2_mon,  
         idx = c("Pierre Shale Plains",     
                 "Pine Ridge Escarpment",  
                 "White River Badlands",  
                 "Black Hills Plateau",  
                 "Keya Paha Tablelands",  
                 "Sand Hills"
                 ),  
         paired = FALSE,  
         reps = 40000  
         )  
  
# this is the output of the runs  
ci_input_pc1  
##  
##DABEST (Data Analysis with Bootstrap Estimation) v0.2.2  
##=======================================================  
#Variable: PC1_mon  
  
# Unpaired mean difference of Pine Ridge Escarpment (n=4339)  
#   minus Pierre Shale Plains (n=1297)  
#     0.401 [95CI  0.322; 0.481]  
  
# Unpaired mean difference of White River Badlands (n=4339)  
#   minus Pierre Shale Plains (n=977)  
#    0.766 [95CI  0.667; 0.863]  
  
# Unpaired mean difference of Black Hills Plateau (n=4339)  
#    minus Pierre Shale Plains (n=2561)  
#     1.64 [95CI  1.57; 1.7]  
  
# Unpaired mean difference of Keya Paha Tablelands (n=4339)  
#   minus Pierre Shale Plains (n=1502)  
#     1.8 [95CI  1.73; 1.86]  
  
# Unpaired mean difference of Sand Hills (n=4339)  
#   minus Pierre Shale Plains (n=3747)  
#     2.63 [95CI  2.57; 2.68]  
#  
#  40000 bootstrap resamples.
#  All confidence intervals are bias-corrected and accelerated.
  
ci_input_pc2  
#
#DABEST (Data Analysis with Bootstrap Estimation) v0.2.2
#=======================================================
# Variable: PC2_mon 
#
# Unpaired mean difference of Pine Ridge Escarpment (n=4339) 
# minus Pierre Shale Plains (n=1297)
# -0.124 [95CI  -0.14; -0.106]
#
# Unpaired mean difference of White River Badlands (n=4339) 
# minus Pierre Shale Plains (n=977)
# 0.0454 [95CI  0.0215; 0.0693]
#
# Unpaired mean difference of Black Hills Plateau (n=4339) 
# minus Pierre Shale Plains (n=2561)
# -0.135 [95CI  -0.148; -0.122]
#
# Unpaired mean difference of Keya Paha Tablelands (n=4339) 
# minus Pierre Shale Plains (n=1502)
#  -0.158 [95CI  -0.171; -0.144]
#
#Unpaired mean difference of Sand Hills (n=4339) 
# minus Pierre Shale Plains (n=3747)
# -0.0251 [95CI  -0.0382; -0.0124]
#
# 40000 bootstrap resamples.
# All confidence intervals are bias-corrected and accelerated. 
  
```

```{r 02-bootstrap-swarmplots} 
  
# reran results on 2019-12-15     
  
# 1. prepare to individual plots    
ci_plot_pc1 <- plot(ci_input_pc1,  
                    rawplot.type = "swarmplot",  
                    rawplot.ylabel = "PC1 Axis",  
                    tick.fontsize = 7,  
                    axes.title.fontsize = 11,  
                    palette = "Greys"  
)    
# plot results below 
ci_plot_pc1  
  
ci_plot_pc2 <- plot(ci_input_pc2,  
                    rawplot.type = "swarmplot",  
                    rawplot.ylabel = "PC2 Axis",  
                    tick.fontsize = 7,  
                    axes.title.fontsize = 11,  
                    palette = "Greys"  
)    
  
# plot results below 
ci_plot_pc2  
  
# 2. print the combined plot  
cowplot::plot_grid(  
  ci_plot_pc1,  
  ci_plot_pc2,  
  ncol = 1,  
  align = "v"  
)  
  
# 3. save the combined plot====  
cowplot::ggsave2("figure/ci_plot_grays.png",  
                 units = "in",  
                 width = 7,  
                 height = 9)  
  
rm(ci_plot_pc1,  
   ci_plot_pc2)  
  
```

```{r 03-ci_result_table}  
  
# reran results on 2019-12-15   
  
# 1. pluck results for summary tables & bind  
ci_results_pc1 <- ci_input_pc1 %>%   
  pluck("result") %>%  
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots))  
  
ci_results_pc2 <- ci_input_pc2 %>%  
  pluck("result") %>%  
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots))  
  
ci_table <- rbind(ci_results_pc1, ci_results_pc2)  

# 2. prepare table results for presentation  
ci_table <- ci_table %>% 
  mutate(variable =  case_when( 
    variable == "observed_PC1" ~ "PC1",  
    variable == "observed_PC2" ~ "PC2"    
  )  
  ) %>%  
  mutate(ci = as.integer(ci)) %>%  
  select(variable, everything())  
  
# 3. convert tibble to a flextable after fixing vars for presentation  
ci_table <- ci_table %>%  
  flextable() %>%  
  #  colformat_num(col_keys = col_key_num,  
  #                big.mark=",",  
  #                digits = 1, na_str = "N/A") %>%  
  set_header_labels(control_group = "Control Group",  
                    test_group = "Test Groups",  
                    control_size = "Control Size",  
                    test_size = "Test Size",  
                    func = "Test Statistic",  
                    variable = "Variable",  
                    difference = "Mean Difference",  
                    ci = "CI",  
                    bca_ci_low = "Lower Limit",  
                    bca_ci_high = "Upper Limit") %>%  
  autofit() %>%  
  align(., part = "all", align = "center") %>%  
  theme_booktabs()  
  
ci_table2 <- read_docx() %>%  
  body_add_flextable(value = ci_table)  
  
print(ci_table2, target = "output/ci_table_new.docx")  
  
rm(ci_results_pc1, ci_results_pc2, ci_table2, ci_table)  
  
```

# check seasonality 
```{r 04-deconvolute_pca_to_check_seasonality}
  
# Deconvolute PC eigens into trend, seasonal, & random comps----   
# 1.0 calculate parameters to remove season & trend from observed vals====  
  
# 1.1 gather monthly values   
gage_gath <- gage_mon_orig  %>%  
  mutate(Date = ymd(Date)) %>%  
  arrange(Date) %>%  
# gather to create axes  
  gather(key = axis,  
         val = PCval,  
         -c(sta,  
            Date,  
            log_q1_mon,  
            q1_mon,  
            q7_mon,  
            q30_mon,  
            q30_q1_mon,  
            ecoreg,  
            waterYear  
            )  
         )  
  
# 1.2 find the average monthly frequency & enframe    
mon_freq <- gage_gath  %>%  
  unite(sta_axis, c(sta, axis), sep = ".") %>%   
  split(.$sta_axis) %>%  
  map_dfc(~ time_frequency(period = "auto",  
                           data = .)  
          ) %>%  
  gather(key = sta.axis, value = freq) %>%  
  summarise(freq = mean(freq),  
            max = max(freq),  
            min = min(freq),  
            sd = sd(freq)  
            )  
  
# 1.3 find the average monthly trend by sta====  
mon_trend_sta <- gage_gath  %>%  
  unite(sta_axis, c(sta, axis), sep = ".") %>%   
  split(.$sta_axis) %>%  
  map_dfc(~ time_trend(period = "auto",  
                           data = .)  
          ) %>%  
  gather(key = sta.axis, value = trend) %>%  
  separate(sta.axis, into = c("sta", "sta2", "axis"), extra = "merge") %>%  
  unite(sta, c(sta, sta2)) %>%           
  spread(axis, trend)  
  
# 1.4 find median monthly trend====  
mon_trend <- mon_trend_sta %>%  
  summarise(trend = median(PC1_mon)) # no difference in trend betw PC1 & PC2  
  
# 1.5 join and tidy for function input====  
decomp_input <- bind_cols(mon_freq, mon_trend)  
  
# 2.0 make function to decompose into seasonal, trend, & remainder----    
decomp_fun <- function(df) {  
  arrange(df, .data$Date) %>%  
    group_by(.data$sta) %>%  
    time_decompose(  
      target  = PCval,  
      data         = .,  
      method       = "stl",  
      frequency    = decomp_input$freq,  
      trend        = decomp_input$trend,   
      merge        = TRUE,  
      message      = TRUE  
    ) %>%  
    ungroup() %>%  
    anomalize(remainder,  
              method = "gesd",  
              alpha = 0.003)  
}  
  
# 3.0 map_dfr causes indexing issues----   
# 3.1 use a manual split and combine to run decomp_fun====  
gage_mon1 <- gage_gath %>%  
  filter(axis == "PC1_mon") %>%  
  decomp_fun()  
  
gage_mon2 <- gage_gath %>%  
  filter(axis == "PC2_mon") %>%  
  decomp_fun()  
  
# 3.2 rejoin the datatables====   
gage_month <- bind_rows(gage_mon1, gage_mon2)  
  
# create station and ecoregion level summaries====    
gage_month <- gage_month %>%  
  as.data.frame() %>%   
  group_by(sta, axis) %>%  
  mutate(mean_q1 = mean(q1_mon)) %>%  
  mutate(mean_q7 = mean(q7_mon)) %>%   
  mutate(mean_q30 = mean(q30_mon)) %>%  
  mutate(mean_q30_q1 = mean(q30_q1_mon)) %>%  
  mutate(mean_PCval = mean(PCval)) %>%  
  ungroup() %>%   
  group_by(ecoreg, axis) %>%  
  mutate(ecoreg_q1 = mean(q1_mon)) %>%  
  mutate(ecoreg_q7 = mean(q7_mon)) %>%   
  mutate(ecoreg_q30 = mean(q30_mon)) %>%  
  mutate(ecoreg_q30_q1 = mean(q30_q1_mon)) %>%  
  mutate(ecoreg_PCval = mean(PCval)) %>%  
  ungroup() %>%  
  group_by(Date, ecoreg, axis) %>%  
  mutate(ecoreg_mean = mean(PCval)) %>%  
  mutate(ecoreg_seas = mean(season)) %>%  
  mutate(ecoreg_trend = mean(trend)) %>%   
  mutate(ecoreg_remain = mean(remainder)) %>%   
  ungroup()  
  
# arrange by PC1_axis  
ecoreg <- gage_month %>%  
  filter(axis == "PC1_mon") %>%  
  group_by(ecoreg) %>% 
  summarise(PC1_mean = mean(PCval)) %>%  
  ungroup() %>%  
  arrange(PC1_mean)  
  
gage_month <- gage_month %>%  
  mutate(ecoreg = fct_relevel(ecoreg,  
                              c("Pierre Shale Plains",  
                                "Pine Ridge Escarpment",  
                                "White River Badlands",  
                                "Black Hills Plateau",  
                                "Keya Paha Tablelands",  
                                "Sand Hills"))  
         )  
  
rm(gage_mon1,  
   gage_mon2,  
   decomp_input,  
   decomp_fun,  
   mon_freq,  
   mon_trend,  
   mon_trend_sta,  
   gage_gath,  
   ecoreg  
  )  
  
```  

```{r 05-plot_time-series_PC1}  
  
# 1.0) prepare to order time series plot----  
# 1.1) filter anomonies to plot below====   
anom_PC1 <- gage_month %>%  
  filter(axis == "PC1_mon") %>%  
  filter(anomaly == "Yes")  

# 2.0) plot seasonality, trend, remainder----  
# 2.1) PC1-observation plot====  
pc1_obs <- gage_month %>%  
  filter(axis == "PC1_mon") %>%  
  ggplot(aes(Date, observed)) +  
  facet_wrap(~ ecoreg) +   
  geom_line(size = 0.5,  
            colour = "grey"  
  ) +  
  geom_line(aes(Date, ecoreg_mean),  
            size = 0.3,  
            colour = "black"  
  ) +  
  scale_y_continuous(limits = c(-4.6, 5.1)) +  
  theme_bw() +  
  xlab("") +  
  ylab("PC1 axis values")  

# 2.2) PC1-seasons plot====   
pc1_seas  <- gage_month %>%  
  filter(axis == "PC1_mon") %>%  
  ggplot(aes(Date, season)) +  
  facet_wrap(~ ecoreg) +  
  geom_line(size = 0.5,  
            colour = "grey" 
  ) +  
  geom_line(aes(Date, ecoreg_seas),  
            size = 0.3,   
            colour = "black"  
  ) +  
  scale_y_continuous(limits = c(-4.6, 5.1)) +  
  theme_bw() +  
  xlab("") +  
  ylab("Season (12-months)")  

# 2.3) PC1-trend plot====   
pc1_trend  <- gage_month %>%  
  filter(axis == "PC1_mon") %>%  
  ggplot(aes(Date, trend)) +  
  facet_wrap(~ ecoreg) +  
  geom_line(size = 0.5,  
            colour = "grey"  
  ) +  
  geom_line(aes(Date, ecoreg_trend),  
            size = 0.3,  
            colour = "black"  
  ) +  
  scale_y_continuous(limits = c(-4.6, 5.1)) +  
  theme_bw() +  
  xlab("") +  
  ylab("Trend (60-months)")  

# 2.4) PC1 - remainder plot====   
pc1_remainder <- gage_month %>%   
  ggplot(aes(Date, remainder)) +  
  facet_wrap(~ ecoreg) +  
  geom_line(size =  0.5,  
            colour = "grey70"  
  ) +  
  geom_line(aes(Date, ecoreg_remain),  
            size = 0.3,  
            colour = "grey20"  
  ) +  
  geom_jitter(aes(Date, ecoreg_remain),  
              data = anom_PC1,  
              shape = 4,  
              size = 0.7) +  
  scale_y_continuous(limits = c(-4.6, 5.1)) +  
  theme_bw() +  
  xlab("") +  
  ylab("Remainder")  

# 2.5) plot the plots above as a grid & save====  
cowplot::plot_grid(  
  pc1_obs, pc1_seas, pc1_trend, pc1_remainder,  
  ncol = 1,  
  align = "v"  
)  

cowplot::ggsave2("figure/pc1-deconv-new.png",  
                 units = "in",  
                 width = 7,  
                 height = 9)  

rm(pc1_obs,  
   pc1_seas,  
   pc1_trend,  
   pc1_remainder,  
   anom_PC1  
)    

```  

```{r 06-pca_decomp_table_eval=FALSE} 
  
# not sure what to use this for....  consider deleting

# summarize time series into observations for a summary table----   
gage_sta_sum_PC1 <- gage_month %>% 
  group_by(ecoreg) %>%  
 summarize(value = round(mean(observed), digits = 2),  
            obs_eco    = mean(abs(observed)),  
            seas_eco   = mean(abs(season)),  
            trend_eco  = mean(abs(trend)),  
            remain_eco = mean(abs(remainder))  
            ) %>%   
  
  summarize(value = round(mean(observed), digits = 2),  
            obs_eco    = mean(abs(observed)),  
            seas_eco   = mean(abs(season)),  
            trend_eco  = mean(abs(trend)),  
            remain_eco = mean(abs(remainder))  
            ) %>%   
  ungroup() %>%   
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>%  
  mutate(remain_perc = remain_eco / obs_eco) %>%  
  mutate(sum = seas_perc + trend_perc + remain_perc) %>%  
  # readjust to 100%  
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>%  
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>%  
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>%  
  mutate(axis = str_remove_all(axis, "_mon")) %>%  
  arrange(axis, value) %>%  
  select(-c(obs_eco:sum))  
  
# split & mutate column names by PC====  
mon_sum_pc1 <- mon_sum_gath %>%  
  filter(axis == "PC1") %>%  
  select(-axis) %>%  
  rename_all(function(x) paste0("PC1_", x))  
  
mon_sum_pc2 <- mon_sum_gath %>%  
  filter(axis == "PC2") %>%  
  select(-axis) %>%  
  rename_all(function(x) paste0("PC2_", x))  
  
# find averages across all the stations====  
mon_sum_pc1_all <- gage_mon %>%  
  filter(axis == "PC1_mon") %>%  
  summarize(value = round(mean(observed), digits = 2),  
            obs_eco    = mean(abs(observed)),  
            seas_eco   = mean(abs(season)),  
            trend_eco  = mean(abs(trend)),  
            remain_eco = mean(abs(remainder))  
            ) %>%  
  mutate(seas_perc   = seas_eco / obs_eco) %>%  
  mutate(trend_perc  = trend_eco / obs_eco) %>%  
  mutate(remain_perc = remain_eco / obs_eco) %>%  
  mutate(sum = seas_perc + trend_perc + remain_perc)  %>%  
  # readjust to 100%  
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>%  
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>%  
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>%  
#  mutate(axis = str_remove_all(axis, "_mon")) #%>% 
#  arrange(axis, value) %>%  
  select(-c(obs_eco:sum)) %>%  
  mutate(ecoreg = "All Stations") %>%  
  rename_all(function(x) paste0("PC1_", x))  
  
mon_sum <- bind_rows(mon_sum_pc1_all, mon_sum_pc1) %>%  
  select(ecoreg = PC1_ecoreg, everything())  
  


``` 
  
# prepare data for glm models  
```{r 07-import-spatial-data_get-enviro-vars}  
  
# Add and join spatial data----     
# check layer names for the project geopackage====   
gpg_layers <- st_layers("sp_data/eco-drought.gpkg")$name[1:5] %>%   
  tibble::enframe(.) %>%   
  select(value)  
  
# get station and ecoregion for a join below====
scratch <- gage_mon_orig %>%  
  group_by(sta) %>%  
  summarise(ecoreg = first(ecoreg))  
  
# read in geopackage data of zonal summaries for watersheds====  
wsd_summary <- st_read("sp_data/eco-drought.gpkg",   
                       layer = "wbd_summary",    
                       as_tibble = TRUE) %>%  
  st_drop_geometry() %>%   
  as_tibble() %>%  
  modify_if(., is.factor, as.character) %>%   
  select(id, everything())  
  
wsd_summary <- full_join(scratch, wsd_summary,   
                         by = c("sta" = "sta_id"))  
  
export(wsd_summary, "data/wsd_summary.csv")   
  
# get environmental variables for gaged stations====  
env_vars <-  wsd_summary %>%  
  filter(type == "gaged") %>%          
  select(sta,  
         ecoreg,  
         cat_area_l,  
         lw_ratio,   
         drain_dens,  
         prcp_mean,  
         t07_mean,   
         vpd_ann,  
         vpd_07,  
         cat_out,  
         cat_rel,  
         slop_med,  
         TWI_mean,  
         perc_cov,   
         fc_mean,  
         ksat_mean,  
         kvert_mean  
  ) %>%   
  rename(cat_area_ln = cat_area_l)    
  
# use Box Cox to estimate transforms====  
lambda <- env_vars %>%  
  select(-c(sta,  
            ecoreg))     
  
lambda <- enframe(  
  sapply(lambda, forecast::BoxCox.lambda)   
) %>%   
  rename(sta = name) %>%   
  rename(lambda_val = value) %>%   
  arrange(sta)    
  
# clean up global environment  
rm(wsd_summary,  
   gpg_layers,  
   scratch  
)  
  
```   

```{r 08-visually_explore_spatial_data}  
  
# Create eda function ----    
# basic EDA function from  
# https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/   
basic_eda <- function(data)       
{                                  
  glimpse(data)  
  df_status(data)  
  freq(data)   
  profiling_num(data)  
  plot_num(data)  
  describe(data)  
}     
  
basic_eda(env_vars)    
  
```  

```{r 09-transform_spatial-data}  
  
# transform spatial data to approximate a normal distribution----  
env_vars <- env_vars %>%   
  mutate(area_linv  = 1/cat_area_ln) %>%        
  mutate(cout_sqr   = sqrt(cat_out)) %>%   
  mutate(crel_sqr   = sqrt(cat_rel)) %>%  
  mutate(dden_sqr   = sqrt(drain_dens)) %>%   
  mutate(ksat_ln    = log(ksat_mean)) %>%  
  mutate(kvert_ln   = log(kvert_mean)) %>%  
  mutate(lwrat_log  = log(lw_ratio)) %>%  
  mutate(pcov_ln    = log(1 + perc_cov)) %>%  
  mutate(prcp_sq    = prcp_mean^2) %>%  
  mutate(slop_ln    = log(slop_med)) %>%    
  mutate(t07_sq     = t07_mean^2) %>%      
  mutate(TWI_sq     = TWI_mean^2) %>%     
  mutate(vpd07_sq   = vpd_07^2) %>%  
  mutate(vpdan_sq   = vpd_ann^2)  
  
# check results of transformation====  
env_vars %>%  
  select(area_linv:vpdan_sq, fc_mean) %>%  
  basic_eda()  
  
```       

```{r 10-plot_quantiles}
  
plot_qq(env_vars)   
  
```

```{r 11-correlate_spatial-data} 
  
# select environmental variables for correlation====    
env_vars <- env_vars %>%   
  select(sta,  
         ecoreg,  
         fc_mean,  
         area_linv:vpdan_sq  
  )   
  
# plot correlations====   
env_vars %>%   
  select(-c(sta,  
            ecoreg) 
         ) %>%  
  plot_correlation()   

# drop 100% correlated variables====  

env_vars <- env_vars %>%  
 select(-c(vpd07_sq, ksat_ln))  
  



```     

```{r export_ecoreg_table}
  
# prepare a table of untransformed data====  
env_table <- env_vars %>%  
  select(-c(area_linv:vpdan_sq)) %>%  
  select(-c(vpd_07, kvert_mean)) %>%   
  select(-sta) %>%  
  gather(key = variable,  
         value = val,  
         -ecoreg  
         ) %>% 
  group_by(ecoreg, variable) %>%  
  summarise(mean   = round(mean(val), digits = 1),  
            median = round(median(val), digits = 1),  
            sd     = round(sd(val), digits = 1),  
            min    = round(min(val), digits = 1),  
            max    = round(max(val), digits = 1)  
            ) %>%  
  ungroup() %>%  
  arrange(variable) %>%  
  select(variable, ecoreg, everything())  
  
# * set the numeric columns====   
col_key_num <- env_table %>%  
#  rownames_to_column() %>%  
  names()  
  
# convert tibble to a flextable    
coef_table <- env_table %>%   
#  rownames_to_column() %>%  
  flextable() %>%   
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 1, na_str = "N/A") %>%  
#  set_header_labels(  
#  coeff  = "Explanatory variable",  
#  elastic = "Elastic net",    
#    lasso  = "Lasso",  
#  ridge = "Ridge") %>%      
  autofit() %>%  
  theme_booktabs()   
  
# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table)  
print(coef_table, target = "output/env_table.docx")   
  
rm(coef_table,  
   col_key_num,   
   env_table  
   )     
  
```

```{r 12-add-response-var_spatial-data} 
  
# join the environmental vars to the daily flow values  
gaged <- full_join(gage_month, env_vars,  
                   by = c("sta", "ecoreg")) %>%     
  mutate(Date = ymd(Date)) %>%    
# add month to calculate seasonality terms  
  mutate(mon = month(Date)) %>%   
  mutate(mon_spring = round(  
    sin(2/12* pi * mon),           # spring & fall  
    digits = 2)  
    ) %>%  
  mutate(mon_cos = round(        # summer & winter  
    cos(2/12* pi * mon),  
    digits = 2)  
    ) %>%  
  mutate(mon_summer = -mon_cos) %>% # flip winter and summer  
# arrange variables  
  select(sta,  
         ecoreg,  
         Date,  
         axis,  
         PCval,  
         everything()  
         ) %>%  
# remove variables not needed for the GLM  
  select(-c(remainder_l1:ecoreg_remain,   
            q1_mon:q30_q1_mon,  
            PCval,  
            mon_cos,  
            log_q1_mon)  
         )      
  
# save intermediate df  
export(gaged, "data/glm_input.csv")  
  
# clean up global environment   
rm(env_vars,  
   lambda,  
   gage_mon_orig,  
   gage_month  
   )    
  
``` 

# glm models - all data - hydrologic export & evenness  
```{r 13-prepare-glm-models}  
  
# import gaged data  
 gaged <- import("data/glm_input.csv")   
  
# set up random seed & parallel processing for glmnet & caret====  
seed <- 42  
cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE)   
registerDoMC(cores)    
  
# set up 'caret' training control & lambda--200 possible lambda vals [-3, 3]  
# note these values are used below for individual model fits  
set.seed(seed)   
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5,  
                          search = "grid", allowParallel = TRUE)   
  
lambda <- 10^seq(-3, 3, length = 200)  
  
# split into PC axes====  
gaged_pc1 <- gaged %>% 
  filter(axis == "PC1_mon")  # keep only PC1  
  
gaged_pc2 <- gaged %>%  
  filter(axis == "PC2_mon")  # keep only PC2   
  
# Split the data into training and test set====  
# create a data partition balanced by flow depths   
set.seed(seed)     
train_index_pc1 <-  createDataPartition(gaged_pc1$observed, p=0.8)[[1]]  
  
set.seed(seed)     
train_index_pc2 <-  createDataPartition(gaged_pc2$observed, p=0.8)[[1]]  
  
train_pc1 <- gaged_pc1[train_index_pc1,] %>%  
  select(-c(axis, season, trend, remainder, mon))   
  
test_pc1 <- gaged_pc1[-train_index_pc1,] %>%  
  select(-c(axis,  season, trend, remainder, mon))        
  
train_pc2 <- gaged_pc2[train_index_pc2,] %>%  
  select(-c(axis, season, trend, remainder, mon))   
  
test_pc2 <- gaged_pc2[-train_index_pc2,] %>%  
  select(-c(axis,  season, trend, remainder, mon))  
  
```

```{r 14-build-glm-models-all_data}
  
# build models----  
# ridge -- alpha = 0====  
set.seed(seed)            # need to set a seed each time you call a rand num   
ridge_pc1 <- train_pc1 %>%   
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'observed', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )    
  
set.seed(seed)            # need to set a seed each time you call a rand num   
ridge_pc2 <- train_pc2 %>%   
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )    
  
# lasso -- alpha = 1====   
set.seed(seed)             # need to set a seed each time you call a rand num  
lasso_pc1 <- train_pc1 %>%   
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )   
  
set.seed(seed)             # need to set a seed each time you call a rand num 
lasso_pc2 <- train_pc2 %>%   
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )    

# elastic net -- alpha vals [0, 1] by caret====  
set.seed(seed)   
elastic_pc1 <- train_pc1 %>%  
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneLength = 10          # tune length for elastic net   
  )   
  
set.seed(seed)   
elastic_pc2 <- train_pc2 %>%  
  select(-c(sta, Date)) %>%       # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
#  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values  
  tuneLength = 10          # tune length    
  )   

# make a list of the models====   
models_pc1 <- list(   
  ridge = ridge_pc1,  
  lasso = lasso_pc1,   
  elastic = elastic_pc1)    
  
models_pc2 <- list(   
  ridge = ridge_pc2,  
  lasso = lasso_pc2,   
  elastic = elastic_pc2)    
  
# clean up global environment    
rm(train_index_pc1,   
   train_index_pc2  
   )   
  
```

```{r 15-build-glm-models-without-ecoreg-data}  
  
# build models -- no ecoreg----  
# ridge -- alpha = 0====   
set.seed(seed)            # need to set a seed each time you call a rand num   
ridge_wsd <- train_pc1 %>%   
  select(-c(sta, Date, ecoreg)) %>%    # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'observed', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )    
  
# lasso -- alpha = 1====   
set.seed(seed)             # need to set a seed each time you call a rand num  
lasso_wsd <- train_pc1 %>%   
  select(-c(sta, Date, ecoreg)) %>%    # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values   
  )   
  
# elastic net -- alpha vals [0, 1] by caret====  
set.seed(seed)   
elastic_wsd <- train_pc1 %>%  
  select(-c(sta, Date, ecoreg)) %>%    # drop data prior to running the model  
  train(    
  observed ~.,        # x = 'medv', y = the rest of the columns, from     
  data = .,               #   the dataset 'train.data'   
  method = "glmnet",      #   using method "glmnet"    
  preProcess = c("center", "scale"),   
  trControl = reg.ctrl,     
  tuneLength = 10          # tune length for elastic net   
  )   
  
# make a list of the models====   
models_wsd <- list(   
  ridge = ridge_wsd,  
  lasso = lasso_wsd,   
  elastic = elastic_wsd)    
  
```

```{r 16-get-glm-coefs}
  
# get & tidy model coefficients for each of the GLMs----     
# get number of parameters for each of the GLMs     
  
# ridge coeffs====    
coef_ridge_pc1 <- coef(  
  ridge_pc1$finalModel,  
  ridge_pc1$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "ridge")  
  
coef_ridge_pc2 <- coef(  
  ridge_pc2$finalModel,  
  ridge_pc2$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "ridge")  
  
coef_ridge_wsd <- coef(  
  ridge_wsd$finalModel,  
  ridge_wsd$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "ridge")  
  
# lasso coeffs====  
coef_lasso_pc1 <- coef(  
  lasso_pc1$finalModel,  
  lasso_pc1$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "lasso")  
  
coef_lasso_pc2 <- coef(  
  lasso_pc2$finalModel,  
  lasso_pc2$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "lasso")  
  
coef_lasso_wsd <- coef(  
  lasso_wsd$finalModel,  
  lasso_wsd$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "lasso")  
  
# elastic coeffs==== 
coef_elastic_pc1 <- coef(  
  elastic_pc1$finalModel,  
  elastic_pc1$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "elastic")  
  
coef_elastic_pc2 <- coef(  
  elastic_pc2$finalModel,  
  elastic_pc2$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "elastic")  
  
coef_elastic_wsd <- coef(  
  elastic_wsd$finalModel,  
  elastic_wsd$bestTune$lambda) %>%  
  as.matrix() %>%     
  as_tibble(., rownames = "coeff") %>%  
  mutate(type = "elastic")  
  
# join model coeffs====  
coefs_pc1 <- bind_rows(coef_ridge_pc1, coef_lasso_pc1, coef_elastic_pc1)   
coefs_pc1 <- coefs_pc1 %>%  
  rename(value = 2) %>%  
  mutate(value = round(  
    value, digits = 3)  
    ) %>%  
  spread(type, value)  
  
coefs_pc2 <- bind_rows(coef_ridge_pc2, coef_lasso_pc2, coef_elastic_pc2)   
coefs_pc2 <- coefs_pc2 %>%  
  rename(value = 2) %>%  
  mutate(value = round(  
    value, digits = 3)  
    ) %>%  
  spread(type, value)  
  
coefs_wsd <- bind_rows(coef_ridge_wsd, coef_lasso_wsd, coef_elastic_wsd)   
coefs_wsd <- coefs_wsd %>%  
  rename(value = 2) %>%  
  mutate(value = round(  
    value, digits = 3)  
    ) %>%  
  spread(type, value)  
  
rm(coef_ridge_pc1,  
   coef_lasso_pc1,  
   coef_elastic_pc1,  
   ridge_pc1,  
   lasso_pc1,  
   elastic_pc1 ,  
   coef_ridge_pc2,  
   coef_lasso_pc2,  
   coef_elastic_pc2,  
   ridge_pc2,  
   lasso_pc2,  
   elastic_pc2,  
   coef_ridge_wsd,  
   coef_lasso_wsd,  
   coef_elastic_wsd,  
   ridge_wsd,  
   lasso_wsd,  
   elastic_wsd  
   )     
  
```

```{r 17-prepare-glm-model-plots}
  
# get number of parameters for each regression method====  
num_param_pc1 <- coefs_pc1 %>%  
  map_dfc(~sum(. != 0)) %>%  
  select(-coeff) %>%                   # transpose df  
  gather(model, num_param)  
  
num_param_pc2 <- coefs_pc2 %>%  
  map_dfc(~sum(. != 0)) %>%  
  select(-coeff) %>%                   # transpose df  
  gather(model, num_param)  
  
num_param_wsd <- coefs_wsd %>%  
  map_dfc(~sum(. != 0)) %>%  
  select(-coeff) %>%                   # transpose df  
  gather(model, num_param)  
  
# evaluate performance of models -- ridge, lasso and elastic net --      
#   best model is the one that minimizes prediction error.    
  
# pick out the mse from the output lists   
error_vals_pc1 <- resamples(models_pc1) %>%    
  summary(metric = c("RMSE", "MAE"))     
  
error_vals_pc2 <- resamples(models_pc2) %>%    
  summary(metric = c("RMSE", "MAE"))     
  
error_vals_wsd <- resamples(models_wsd) %>%    
  summary(metric = c("RMSE", "MAE"))    
  
# change list to tibble   
error_vals_pc1 <- error_vals_pc1 %>%    
  pluck(., 'statistics') %>%   
  as.data.frame() %>%   
  as_tibble(., rownames = "model") %>%  
  gather(key, val, -c(model))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>%   
  separate(key, c("statistic", "position"), extra = "drop") %>%  
  filter(position != "NA") %>%   
  spread(position, val)    
  
error_vals_pc2 <- error_vals_pc2 %>%    
  pluck(., 'statistics') %>%   
  as.data.frame() %>%   
  as_tibble(., rownames = "model") %>%  
  gather(key, val, -c(model))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>%   
  separate(key, c("statistic", "position"), extra = "drop") %>%  
  filter(position != "NA") %>%   
  spread(position, val)    
  
error_vals_wsd <- error_vals_wsd %>%    
  pluck(., 'statistics') %>%   
  as.data.frame() %>%   
  as_tibble(., rownames = "model") %>%  
  gather(key, val, -c(model))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>%   
  separate(key, c("statistic", "position"), extra = "drop") %>%  
  filter(position != "NA") %>%   
  spread(position, val)    
  
# add number of params to error vals & clean up   
error_vals_pc1 <- full_join(error_vals_pc1, num_param_pc1,   
                            by = "model")    
  
error_vals_pc2 <- full_join(error_vals_pc2, num_param_pc2,   
                            by = "model")  
  
error_vals_wsd <- full_join(error_vals_wsd, num_param_wsd,   
                            by = "model")  
  
# prepare for plotting   
error_vals_pc1 <- error_vals_pc1 %>%    
  mutate(num_param = as.character(num_param)) %>%   
  mutate(temp = ", p = ") %>%   
  mutate(model_text = str_c(model, temp, num_param))  %>%   
  mutate(num_param = as.integer(num_param))  
  
error_vals_pc2 <- error_vals_pc2 %>%    
  mutate(num_param = as.character(num_param)) %>%   
  mutate(temp = ", p = ") %>%   
  mutate(model_text = str_c(model, temp, num_param))  %>%   
  mutate(num_param = as.integer(num_param))  
  
error_vals_wsd <- error_vals_wsd %>%    
  mutate(num_param = as.character(num_param)) %>%   
  mutate(temp = ", p = ") %>%   
  mutate(model_text = str_c(model, temp, num_param))  %>%   
  mutate(num_param = as.integer(num_param))  
  
n_train_pc1 <- train_pc1 %>%  
  mutate(n_count = n()) %>%  
  distinct(n_count)   
  
n_train_pc2 <- train_pc2 %>%  
  mutate(n_count = n()) %>%  
  distinct(n_count)   
  
n_train_wsd <- train_pc1 %>%  # used same as pc1 except for dropping ecoreg  
  mutate(n_count = n()) %>%  
  distinct(n_count)  
  
```

```{r 18-plot-glm-models}
  
# make boxplots of the three models====     
model_plot_pc1 <- error_vals_pc1 %>%  
  ggplot() +   
  facet_grid(cols = vars(statistic)) +   
  geom_boxplot(aes(x = factor(model_text),  
                   group = model,  
                   lower = `1st`,  
                   upper = `3rd`,   
                   middle = `Median`,  
                   ymin = `Min`,  
                   ymax = `Max`),   
               stat = "identity") +    
  geom_text(data = n_train_pc1,   
            x = 3, y = 0.9,     
            aes(label= paste("n =", n_count, sep = " ")),    
            size = 3,    
            color = "gray20"   
  ) +   
  # ylim(0, 0.75) +   
  labs(x = "",    
       y = bquote('PC1 with ecoregions')) +     
  theme_bw()   
  
#model_plot_pc1  
  
model_plot_wsd <- error_vals_wsd %>%  
  ggplot() +   
  facet_grid(cols = vars(statistic)) +   
  geom_boxplot(aes(x = factor(model_text),  
                   group = model,  
                   lower = `1st`,  
                   upper = `3rd`,   
                   middle = `Median`,  
                   ymin = `Min`,  
                   ymax = `Max`),   
               stat = "identity") +    
  geom_text(data = n_train_pc1,   
            x = 3, y = 0.9,     
            aes(label= paste("n =", n_count, sep = " ")),    
            size = 3,    
            color = "gray20"   
  ) +    
  # ylim(0, 0.75) +   
  labs(x = "",    
       y = bquote('PC1 without ecoregions')) +     
  theme_bw()   
  
model_plot_wsd  
  
model_plot_pc2 <- error_vals_pc2 %>% 
  ggplot() +   
  facet_grid(cols = vars(statistic)) +   
  geom_boxplot(aes(x = factor(model_text),  
                   group = model,  
                   lower = `1st`,  
                   upper = `3rd`,   
                   middle = `Median`,  
                   ymin = `Min`,  
                   ymax = `Max`),   
               stat = "identity") +    
  geom_text(data = n_train_pc2,   
            x = 3, y = 0.2,     
            aes(label= paste("n =", n_count, sep = " ")),    
            size = 3,    
            color = "gray20"   
  ) +   
  # ylim(0, 0.75) +   
  labs(x = "",    
       y = bquote('PC2 with ecoregions')) +     
  theme_bw()   
  
# join the plots====  
cowplot::plot_grid(   
  model_plot_pc1, model_plot_wsd, model_plot_pc2,  
  ncol = 1, align = "v")   
  
cowplot::ggsave2("figure/glm_model_fit.png",   
                 units = "in",    
                 width = 7, 
                 height = 6)  
  
rm(error_vals_pc1,  
   error_vals_pc2,  
   error_vals_wsd,  
   model_plot_pc1,  
   model_plot_pc2,  
   model_plot_wsd,  
   n_train_pc1,  
   n_train_pc2,  
   n_train_wsd,  
   num_param_pc1,  
   num_param_pc2,  
   num_param_wsd  
)  
  
```

```{r 19-prepare-glm-coefs}
  
# prepare for export table====   
# arrange by absolute value of the elastic net model      
coefs_pc1 <- coefs_pc1 %>%   
  mutate(arrange_ridge = abs(ridge)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_lasso = abs(lasso)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_elastic = abs(elastic)) %>%   # temp terms to arrange coeffs  
  arrange(desc(arrange_lasso)) %>%  
  arrange(desc(arrange_ridge)) %>%    
  arrange(desc(arrange_elastic)) %>%    
  select(coeff, elastic, lasso, ridge)  
  
coefs_pc2 <- coefs_pc2 %>%   
  mutate(arrange_ridge = abs(ridge)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_lasso = abs(lasso)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_elastic = abs(elastic)) %>%   # temp terms to arrange coeffs  
  arrange(desc(arrange_lasso)) %>%  
  arrange(desc(arrange_ridge)) %>%    
  arrange(desc(arrange_elastic)) %>%    
  select(coeff, elastic, lasso, ridge)  
  
coefs_wsd <- coefs_wsd %>%   
  mutate(arrange_ridge = abs(ridge)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_lasso = abs(lasso)) %>%   # temp terms to arrange coeffs  
  mutate(arrange_elastic = abs(elastic)) %>%   # temp terms to arrange coeffs  
  arrange(desc(arrange_lasso)) %>%  
  arrange(desc(arrange_ridge)) %>%    
  arrange(desc(arrange_elastic)) %>%    
  select(coeff, elastic, lasso, ridge)  
  
# rename coefficients to fit the hydrologic coefficient table   
coefs_pc1 <- coefs_pc1 %>%   
  mutate(coeff = case_when(   
    coeff == "(Intercept)"                  ~ "Intercept",  
    coeff == "pcov_ln"                      ~ "perc_cov",                   
    coeff == "kvert_ln"                     ~ "kvert_mean",                  
    coeff == "vpdan_sq"                     ~ "vpd_ann",               
    coeff ==  "cout_log"                    ~ "cat_out",                
    coeff ==  "ecoregSand Hills"            ~ "Ecoreg::Sand Hills",           
    coeff == "ecoregKeya Paha Tablelands"   ~ "Ecoreg::Keya Paha Tablelands",  
    coeff == "lwrat_sqr"                    ~ "lw_ratio",           
    coeff == "area_linv"                    ~ "cat_area",  
    coeff == "dden_sqr"                     ~ "drain_dens",  
    coeff == "ecoregPierre Shale Plains"    ~ "Ecoreg::Pierre Shale Plains",   
    coeff ==  "ecoregPine Ridge Escarpment" ~ "Ecoreg::Pine Ridge Escarpment",  
    coeff == "crel_sqr"                     ~ "cat_rel",   
    coeff ==  "TWI_sq"                      ~ "TWI_mean",  
    coeff == "slop_ln"                      ~ "slop_med",  
    coeff == "ecoregWhite River Badlands"   ~ "Ecoreg::White River Badlands",  
    coeff == "t07_sq"                       ~ "t07_mean",    
    coeff ==  "prcp_sq"                     ~ "prcp_mean",    
    TRUE ~ coeff))    
  
coefs_pc2 <- coefs_pc2 %>%   
  mutate(coeff = case_when(   
    coeff == "(Intercept)"                  ~ "Intercept",  
    coeff == "pcov_ln"                      ~ "perc_cov",                   
    coeff == "kvert_ln"                     ~ "kvert_mean",                  
    coeff == "vpdan_sq"                     ~ "vpd_ann",               
    coeff ==  "cout_sqr"                    ~ "cat_out",                
    coeff ==  "ecoregSand Hills"            ~ "Ecoreg::Sand Hills",           
    coeff == "ecoregKeya Paha Tablelands"   ~ "Ecoreg::Keya Paha Tablelands",  
    coeff == "lwrat_sqr"                    ~ "lw_ratio",           
    coeff == "area_linv"                    ~ "cat_area",  
    coeff == "dden_sqr"                     ~ "drain_dens",  
    coeff == "ecoregPierre Shale Plains"    ~ "Ecoreg::Pierre Shale Plains",   
    coeff ==  "ecoregPine Ridge Escarpment" ~ "Ecoreg::Pine Ridge Escarpment",  
    coeff == "crel_sqr"                     ~ "cat_rel",   
    coeff ==  "TWI_sq"                      ~ "TWI_mean",  
    coeff == "slop_ln"                      ~ "slop_med",  
    coeff == "ecoregWhite River Badlands"   ~ "Ecoreg::White River Badlands",  
    coeff == "t07_sq"                       ~ "t07_mean",    
    coeff ==  "prcp_sq"                     ~ "prcp_mean",    
    TRUE ~ coeff))    
  
coefs_wsd <- coefs_wsd %>%   
  mutate(coeff = case_when(   
    coeff == "(Intercept)"                  ~ "Intercept",  
    coeff == "pcov_ln"                      ~ "perc_cov",                   
    coeff == "kvert_ln"                     ~ "kvert_mean",                  
    coeff == "vpdan_sq"                     ~ "vpd_ann",               
    coeff ==  "cout_sqr"                    ~ "cat_out",                
    coeff == "lwrat_sqr"                    ~ "lw_ratio",           
    coeff == "area_linv"                    ~ "cat_area",  
    coeff == "dden_sqr"                     ~ "drain_dens",  
    coeff == "crel_sqr"                     ~ "cat_rel",   
    coeff ==  "TWI_sq"                      ~ "TWI_mean",  
    coeff == "slop_ln"                      ~ "slop_med",  
    coeff == "t07_sq"                       ~ "t07_mean",    
    coeff ==  "prcp_sq"                     ~ "prcp_mean",    
    TRUE ~ coeff))    
  
```

```{r 20-get_glm_observations_and_predictions}
  
# get observations & predictions -- training====   
obs_train_pc1   <- tibble(observed = train_pc1$observed,   
                             Date = train_pc1$Date,   
                             ecoreg = train_pc1$ecoreg)  
  
preds_train_pc1 <- map_dfc(models_pc1,  
                           predict,  
                           newdata = train_pc1)     
  
obs_train_pc2   <- tibble(observed = train_pc2$observed,   
                             Date = train_pc2$Date,  
                             ecoreg = train_pc2$ecoreg)   
  
preds_train_pc2 <- map_dfc(models_pc2,  
                           predict,   
                           newdata = train_pc2)    
  
obs_train_wsd   <- tibble(observed = train_pc1$observed,   # check this!
                             Date = train_pc1$Date,        # ecoreg???
                             ecoreg = train_pc1$ecoreg)   
  
preds_train_wsd <- map_dfc(models_wsd,  
                           predict,   
                           newdata = train_pc1)    # check this!!!
  
# combine observations & predictions====  
fit_train_pc1 <- bind_cols(obs_train_pc1, preds_train_pc1) %>%  
  mutate(split = "train") %>%  
  as_tibble()   
  
fit_train_pc2 <- bind_cols(obs_train_pc2, preds_train_pc2) %>%   
  mutate(split = "train") %>%   
  as_tibble()  
  
fit_train_wsd <- bind_cols(obs_train_wsd, preds_train_wsd) %>%   
  mutate(split = "train") %>%   
  as_tibble()  
  
rm(obs_train_pc1,  
   obs_train_pc2,  
   obs_train_wsd,  
   preds_train_pc1,   
   preds_train_pc2,   
   preds_train_wsd,   
   train_pc1,  
   train_pc2,  
   train_wsd  
   )  
  
# get observations & predictions -- test====  
obs_test_pc1   <- tibble(observed = test_pc1$observed,   
                             Date = test_pc1$Date,   
                             ecoreg = test_pc1$ecoreg)   
  
preds_test_pc1 <- map_dfc(models_pc1,   
                         predict,  
                         newdata = test_pc1)  
  
obs_test_pc2   <- tibble(observed = test_pc2$observed,   
                             Date = test_pc2$Date,  
                             ecoreg = test_pc2$ecoreg)   
  
preds_test_pc2 <- map_dfc(models_pc2,   
                         predict,   
                         newdata = test_pc2)   
  
obs_test_wsd   <- tibble(observed = test_pc1$observed,     
                             Date = test_pc1$Date,  
                             ecoreg = test_pc1$ecoreg)   
  
preds_test_wsd <- map_dfc(models_wsd,   
                         predict,   
                         newdata = test_pc1)              
  
# combine observations & predictions====  
fit_test_pc1 <- bind_cols(obs_test_pc1, preds_test_pc1) %>%   
  mutate(split = "test") %>%   
  as_tibble()   
  
fit_test_pc2 <- bind_cols(obs_test_pc2, preds_test_pc2) %>%  
  mutate(split = "test") %>%  
  as_tibble()   
  
fit_test_wsd <- bind_cols(obs_test_wsd, preds_test_wsd) %>%  
  mutate(split = "test") %>%  
  as_tibble()   
  
rm(obs_test_pc1,   
   obs_test_pc2,   
   obs_test_wsd,   
   preds_test_pc1,   
   preds_test_pc2,  
   preds_test_wsd   
   )  
  
fit_model_pc1 <-  bind_rows(fit_test_pc1, fit_train_pc1)   
fit_model_pc2 <-  bind_rows(fit_test_pc2, fit_train_pc2)   
fit_model_wsd <-  bind_rows(fit_test_wsd, fit_train_wsd)   
  
rm(fit_test_pc1,   
   fit_train_pc1,  
   fit_test_pc2,   
   fit_train_pc2,  
   fit_test_wsd,   
   fit_train_wsd   
   )   
  
# drop the training data -- find model errors & fit a null model ------------  
fit_model_pc1 <- fit_model_pc1 %>%  
  filter(split == "test") %>%           # only want to see test data   
  select(-c(split, ridge, lasso)) %>%        # drop non-selected models   
  group_by(Date)  %>%                   # fit a null model by Date      
  mutate(null_mod = mean(observed)) %>%   
  ungroup() %>%   
  gather(model, fitted, -c(observed, Date, ecoreg)) %>%   
  group_by(model) %>%   
  mutate(MAE = round(  
    MAE(observed, fitted),              # find the model errors  
    digits = 3)  
    ) %>%  
  ungroup() %>%  
  mutate(residual = observed - fitted) %>%   
  mutate(freq = 1- percent_rank(observed)) %>%   
  mutate(freq = if_else(freq== 1, 0.9999, freq))   
  
fit_model_pc2 <- fit_model_pc2 %>%   
  filter(split == "test") %>%           # only want to see test data   
  select(-c(split, ridge, lasso)) %>%        # DROP NON-SELECTED MODELS   
  group_by(Date)  %>%                   # fit a null model by Date      
  mutate(null_mod = mean(observed)) %>%  
  ungroup() %>%   
  gather(model, fitted, -c(observed, Date, ecoreg)) %>%  
  group_by(model) %>%  
  mutate(MAE = round(  
    MAE(observed, fitted),              # find the model errors   
    digits = 3)   
    ) %>%   
  ungroup() %>%  
  mutate(residual = observed - fitted) %>%  
  mutate(freq = 1- percent_rank(observed)) %>%  
  mutate(freq = if_else(freq== 1, 0.9999, freq))  
  
fit_model_wsd <- fit_model_wsd %>%   
  filter(split == "test") %>%           # only want to see test data   
  select(-c(split, ridge, lasso)) %>%        # DROP NON-SELECTED MODELS    
  group_by(Date)  %>%                   # fit a null model by Date      
  mutate(null_mod = mean(observed)) %>%  
  ungroup() %>%   
  gather(model, fitted, -c(observed, Date, ecoreg)) %>%  
  group_by(model) %>%  
  mutate(MAE = round(  
    MAE(observed, fitted),              # find the model errors   
    digits = 3)   
    ) %>%   
  ungroup() %>%  
  mutate(residual = observed - fitted) %>%  
  mutate(freq = 1- percent_rank(observed)) %>%  
  mutate(freq = if_else(freq== 1, 0.9999, freq))  
  
```  

```{r 21-prepare_glm_observations_predictions_plot}
  
# set the facet labels, find number of test observations, get mae   
labels_pc1 <- c(elastic = "PC1 elastic net predicted values with ecoregion",  
                null_mod = "Mean of PC1 observations")   

labels_pc2 <- c(elastic = "PC2 elastic net predicted values",  
                null_mod = "Mean of daily observations - PC2")   

labels_wsd <- c(elastic = "PC1 elastic net predicted values without ecoregion",  
                null_mod = "Mean of PC1 observations")   

n_test <- test_pc1 %>%       # pc1 & pc2 are same number  
  mutate(n_count = n()) %>%  
  distinct(n_count)   

mae_pc1 <- fit_model_pc1 %>%   
  select(model, MAE) %>%  
  mutate(MAE = round(MAE, digits = 2)) %>%  
  mutate(MAE = as.character(MAE)) %>%  
  distinct(MAE, .keep_all = TRUE)   

mae_pc2 <- fit_model_pc2 %>%  
  select(model, MAE) %>%   
  mutate(MAE = round(MAE, digits = 2)) %>%  
  mutate(MAE = as.character(MAE)) %>%  
  distinct(model, .keep_all = TRUE)     # the MAE is the same so use 'model'  

mae_wsd <- fit_model_wsd %>%  
  select(model, MAE) %>%   
  mutate(MAE = round(MAE, digits = 2)) %>%  
  mutate(MAE = as.character(MAE)) %>%  
  distinct(model, .keep_all = TRUE)     # the MAE is the same so use 'model'  

rm(test_pc1,   
   test_pc2 #,  
   #   test_wsd          # check this !!!  
)   

```

```{r 22-plot_glm_observations_predictions}
  
# observations vs predictions plots----   
# PC1 observations vs predictions plot--all====   
obs_pred_plot_pc1 <- fit_model_pc1 %>%  
  # change the order of facets  
  mutate(model = factor(model, levels = c('null_mod', 'elastic'))) %>%  
  ggplot(., aes(observed, fitted)) +  
  theme_bw() +  
  # create facets  
  facet_wrap(vars(model),  
             ncol = 2,  
             labeller = labeller(model = labels_pc1)) +  # from above  
  # plot a 45-degree line   
  geom_abline(  
    intercept = 0,  
    slope = 1,  
    size = 0.25) +  
  # plot points as '.' to reduce overplotting  
  geom_point(shape = ".",  
             color = "gray80")  +  
  # plot points as a density plot to reduce overplotting  
  geom_density2d(color = "gray50")  +  
  geom_smooth(method = "lm",  
              color = "black") +  
  # add text of the MSE vals  
  geom_text(data = mae_pc1,                             # from above  
            x = -2.5,  
            y = 2,  
            aes(label= paste("MAE =", MAE, sep = " ")),  
            size = 3,  
            color = "gray20"  
  ) +  
  # set limits  
  #  scale_x_continuous(limits = c(-1.5, 4)) +  
  #  scale_y_continuous(limits = c(-1.5, 4)) +  
  # set labels and theme  
  labs(x = "",  
       y = "Fitted values "  
  )   
#  theme(legend.position = "none")  
  
obs_pred_plot_pc1  
  
# PC1 observations vs predictions plot--wsd====    
obs_pred_plot_wsd <- fit_model_wsd %>%  
  # change the order of facets  
  mutate(model = factor(model, levels = c('null_mod', 'elastic'))) %>%  
  ggplot(., aes(observed, fitted)) +  
  theme_bw() +  
  # create facets  
  facet_wrap(vars(model),  
             ncol = 2,  
             labeller = labeller(model = labels_wsd)) +  # from above  
  # plot a 45-degree line  
  geom_abline(  
    intercept = 0,  
    slope = 1,  
    size = 0.25) +  
  # plot points as '.' to reduce overplotting  
  geom_point(shape = ".",  
             color = "gray80")  +  
  # plot points as a density plot to reduce overplotting  
  geom_density2d(color = "gray50")  +  
  geom_smooth(method = "lm",  
              color = "black") +  
  # add text of the MSE vals  
  geom_text(data = mae_wsd,                             # from above  
            x = -2.5,  
            y = 2,  
            aes(label= paste("MAE =", MAE, sep = " ")),  
            size = 3,  
            color = "gray20"  
  ) +  
  # set labels and theme  
  labs(x = "",  
       y = "Fitted values "  
  )   
#  theme(legend.position = "none")  
  
obs_pred_plot_wsd  
  
# PC2 observations vs predictions plot--all====  
obs_pred_plot_pc2 <- fit_model_pc2 %>%  
  # change the order of facets  
  mutate(model = factor(model, levels = c('null_mod', 'elastic'))) %>%  
  ggplot(., aes(observed, fitted)) +  
  theme_bw() +  
  # create facets  
  facet_wrap(vars(model),  
             ncol = 2,  
             labeller = labeller(model = labels_pc2)) +  # from above  
  # plot a 45-degree line  
  geom_abline(  
    intercept = 0,  
    slope = 1,  
    size = 0.25) +  
  # plot points as '.' to reduce overplotting  
  geom_point(shape = ".",  
             color = "gray80") +  
  # plot points as a density plot to reduce overplotting  
  geom_density2d(color = "gray50") +  
  geom_smooth(method = "lm",  
              color = "black") +  
  # add text of the MSE vals  
  geom_text(data = mae_pc2,                             # from above  
            x = 0.1,  
            y = 0.8,  
            aes(label= paste("MAE =", MAE, sep = " ")),  
            size = 3,  
            color = "gray20"  
  ) +  
  # set labels and theme  
  labs(x = "",  
       y = "Fitted values "  
  )   
#  theme(legend.position = "none")  
  
obs_pred_plot_pc2  
  
# plot obs_pred_plot & resid_plot====    
cowplot::plot_grid(   
  obs_pred_plot_pc1, obs_pred_plot_wsd, obs_pred_plot_pc2,  
  ncol = 1, align = "v")   
  
cowplot::ggsave2("figure/glm_obs_pred_plot_all.png",   
                 units = "in",    
                 width = 7,  
                 height = 6)  
  
# clean up====  
rm(fit_model_pc1,  
   fit_model_pc2,  
   fit_model_wsd,  
   mae_pc1,  
   mae_pc2,  
   mae_wsd,  
   n_test,  
   obs_pred_plot_pc1,   
   obs_pred_plot_pc2,  
   obs_pred_plot_wsd,  
   labels_pc1,  
   labels_pc2,  
   labels_wsd  
)   
  
```

```{r 23-export-table_glm-coeffs_pc1}

# set the numeric columns   
col_key_num <- coefs_pc1 %>%  
  select(-coeff) %>%   
  names()  

# convert tibble to a flextable    
coef_table <- coefs_pc1 %>%   
  flextable() %>%   
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%  
  set_header_labels(  
    coeff  = "Explanatory variable",  
    elastic = "Elastic net",    
    lasso  = "Lasso",  
    ridge = "Ridge") %>%      
  autofit() %>%  
  theme_booktabs()   

# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table)  
print(coef_table, target = "output/table_coef_hydro-exp.docx")   

rm(coef_table,  
   col_key_num,   
   coefs_pc1  
)     

```

```{r 23b-export-table_glm-coeffs_wsd}
  
# set the numeric columns    
col_key_num <- coefs_wsd %>%  
  select(-coeff) %>%   
  names()  

# convert tibble to a flextable    
coef_table <- coefs_wsd %>%   
  flextable() %>%  
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%  
  set_header_labels(  
    coeff  = "Explanatory variable",  
    lasso  = "Lasso",  
    elastic = "Elastic net",    
    ridge = "Ridge") %>%      
  autofit() %>%  
  theme_booktabs()   

# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table)  
print(coef_table, target = "output/table_coef_wsd.docx")   

rm(coef_table,  
   col_key_num,   
   coefs_wsd  
)     

```  

```{r 23c-export-table_glm-coeffs_pc2} 

# set the numeric columns   
col_key_num <- coefs_pc2 %>%  
  select(-coeff) %>%   
  names()  

# convert tibble to a flextable    
coef_table <- coefs_pc2 %>%   
  flextable() %>% 
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%  
  set_header_labels(  
    coeff  = "Explanatory variable",  
    elastic = "Elastic net",    
    lasso  = "Lasso",  
    ridge = "Ridge") %>%      
  autofit() %>%  
  theme_booktabs()   

# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_coef_pc2.docx")   

rm(coef_table,  
   col_key_num,   
   coefs_pc2  
)     

```  

# network plot of variable associations 
```{r mixed-assoc-function}  

# Calculate pairwise associations====  
#  associan is between all variables in a data-frame:  
#    Chi-square          ~ nominal vs nominal, 
#    Pearson correlation ~ numeric vs numeric, 
#    ANOVA               ~ nominal vs numeric  
#   
# Function was posted on https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi
#   which was adopted from https://stackoverflow.com/a/52557631/590437

mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){  
  df_comb = expand.grid(names(df), 
                        names(df),  
                        stringsAsFactors = F) %>% 
    set_names("X1", "X2")  
  is_nominal = function(x) class(x) %in% c("factor", "character")  
  # https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559  
  # https://github.com/r-lib/rlang/issues/781  
  is_numeric <- function(x) { is.integer(x) || is_double(x)}  
  
  f = function(xName,yName) {  
    x =  pull(df, xName)  
    y =  pull(df, yName)  
    
    result = if(is_nominal(x) && is_nominal(y)){  
      # use bias corrected cramersV as described in   https://rdrr.io/cran/rcompanion/man/cramerV.html  
      cv = cramerV(as.character(x), as.character(y), 
                   bias.correct =   adjust_cramersv_bias)  
      data.frame(xName, yName, assoc=cv, type="cramersV")  
      
    }else if(is_numeric(x) && is_numeric(y)){  
      correlation = cor(x, y, method=cor_method, use="complete.obs")  
      data.frame(xName, yName, assoc=correlation, type="correlation")  
      
    }else if(is_numeric(x) && is_nominal(y)){  
      # from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618  
      r_squared = summary(lm(x ~ y))$r.squared  
      data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")  
      
    }else if(is_nominal(x) && is_numeric(y)){  
      r_squared = summary(lm(y ~x))$r.squared  
      data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")  
      
    }else {  
      warning(paste("unmatched column type combination: ",  
                    class(x), class(y)))  
    }  
    # finally add complete obs number and ratio to table  
    result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)),   complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)  
  }  
  # apply function to each variable combination  
  map2_df(df_comb$X1, df_comb$X2, f)  
}  

```

```{r mixed-assoc_network-plot}

# create an mixed-association network plot====  
# * create an association input dataframe using important variables 
#     variables used in the association network plot from PC1 regression  

assoc_input <- gaged_pc1 %>%  
  select(ecoregion = ecoreg,  
         mon_spring,  
         mon_summer,  
         field_capacity = fc_mean,  
         area = area_linv,  
         outlet_elev = cout_sqr,  
         drain_density = dden_sqr,  
         cover = pcov_ln,  
         July_temp = t07_sq,  
         vpd_annual = vpdan_sq,  
         TWI = TWI_sq  
  ) %>%  
  select(-c(mon_spring,  # 'spring' and 'summer' did not plot in the network  
            mon_summer  
  )  
  )  

# * create an association data.frame====  
assoc_df <- mixed_assoc(assoc_input) %>%  
  filter(assoc < 0.95)  # drops associations = 1.0  

# create a table from the association data.frame====   
assoc_table <- assoc_df %>%  
  select(x, y, assoc) %>%  
  spread(y, assoc) %>%  
  column_to_rownames("x")  

# create a network plot====   
assoc_table %>%  
  as.matrix %>% # as a matrix   
  as_cordf %>%  # as a correlation data.frame  
  network_plot()  

# save network plot====  
ggsave("figure/network-plot.png",   
       units = "in",    
       width = 7,  
       height = 6)  

# clean up global environment====   
rm(mixed_assoc,  
   assoc_df,  
   assoc_input  
)  

```

```{r export_mixed-assoc_table}

# * set the numeric columns====   
col_key_num <- assoc_table %>%  
  #  rownames_to_column()  
  names()  

# convert tibble to a flextable    
coef_table <- assoc_table %>%   
  rownames_to_column() %>%  
  flextable() %>%   
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%  
  autofit() %>%  
  theme_booktabs()   

# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table)  
print(coef_table, target = "output/assoc_table.docx")   

rm(coef_table,  
   col_key_num,   
   coefs_pc1  
)     

```

```{r 23d-plot_glm_residuals, eval=FALSE}

# residuals plots----  
# PC1 residuals plot====    
resid_plot_pc1 <- fit_model_pc1 %>% 
  # change the order of facets 
  mutate(model = factor(model, levels = c('elastic', 'null_mod'))) %>% 
  ggplot(., aes(observed, residual)) +  
  # plot points as a density plot to reduce overplotting 
  geom_point(  
    shape = ".", 
    color = "gray80")  + 
  # plot points as a density plot to reduce overplotting  
  geom_density2d(  
    color = "gray50")  +  
  # fit a linear model to the data  
  geom_smooth(  
    method = "lm",  
    color = "black") +  
  # plot a horizontal line  
  geom_abline( 
    intercept = 0,  
    slope = 0,  
    size = 0.25) +  
  # add text of the number of observations  
  geom_text(data = n_test,  
            x = 2, y = -2.5,  
            aes(label= paste("n =", n_count, sep = " ")),  
            size = 3,  
            color = "gray20"  
  ) +  
  # create facets  
  facet_wrap(vars(model),  
             ncol = 2,  
             labeller = labeller(model = labels_pc1)) +   # from above 
  # set labels and theme  
  labs(x = bquote('Observed values'),    
       y = bquote('Residuals')  
  ) + 
  theme_bw() + 
  theme(legend.position = "none") 

resid_plot_pc1  

resid_plot_pc2 <- fit_model_pc2 %>% 
  # change the order of facets 
  mutate(model = factor(model, levels = c('elastic', 'null_mod'))) %>% 
  ggplot(., aes(observed, residual)) +  
  # plot points as a density plot to reduce overplotting 
  geom_point(  
    shape = ".", 
    color = "gray80")  + 
  # plot points as a density plot to reduce overplotting  
  geom_density2d(  
    color = "gray50")  +  
  # fit a linear model to the data  
  geom_smooth(  
    method = "lm",  
    color = "black") +  
  # plot a horizontal line  
  geom_abline( 
    intercept = 0,  
    slope = 0,  
    size = 0.25) +  
  # add text of the number of observations  
  geom_text(data = n_test,  
            x = 2, y = -2.5,  
            aes(label= paste("n =", n_count, sep = " ")),  
            size = 3,  
            color = "gray20"  
  ) +  
  # create facets  
  facet_wrap(vars(model),  
             ncol = 2,  
             labeller = labeller(model = labels_pc2)) +   # from above 
  # set labels and theme  
  labs(x = bquote('Observed values'),    
       y = bquote('Residuals')  
  ) + 
  theme_bw() + 
  theme(legend.position = "none") 

resid_plot_pc2  

```

# glm models - ecoregion - hydrologic export  
```{r 24-prepare-for-glm-ecoreg-model}
  
# import intermediate data=====  
gaged <- import("data/glm_input.csv")  
  
# split into PC axes====  
gaged_pc1 <- gaged %>%  
  filter(axis == "PC1_mon")  # keep only PC1  
  
# set up random seed & parallel processing for glmnet & caret====   
 seed <- 42  
 cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE)   
 registerDoMC(cores)    
  
# set up 'caret' training control & lambda--200 possible lambda vals [-3, 3]  
# note these values are used below for individual model fits  
set.seed(seed)   
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5,  
                          search = "grid", allowParallel = TRUE)   
lambda <- 10^seq(-3, 3, length = 200)  
  
# ** check variable names====  
vars <- gaged_pc1 %>%  
  names() %>%  
  print()  

# change input name to 'gaged_ecoreg' and drop non-important predictors  
gaged_ecoreg <- gaged_pc1 %>%  
  select(sta,  
         ecoreg,  
         Date,  
         observed,  
         fc_mean,  
         area_linv,  
         cout_sqr,  
         dden_sqr,  
         pcov_ln,  
         t07_sq,  
         TWI_sq,  
         vpdan_sq,  
         mon_spring,  
         mon_summer  
         ) %>%  
  # create shorter variable names  
  mutate(ecoreg2 = case_when(   
    ecoreg == "Black Hills Plateau"   ~ "bhplat",  
    ecoreg == "Keya Paha Tablelands"  ~ "kptabl",   
    ecoreg == "Pierre Shale Plains"   ~ "pshale",   
    ecoreg == "Pine Ridge Escarpment" ~ "escarp",      
    ecoreg == "Sand Hills"            ~ "sndhil",    
    ecoreg == "White River Badlands"  ~ "wrbadl"   
  )  
  )  
  
# clean up model data from above  
rm(coefs_pc2,  
   gaged_pc1,    # renamed above  
   gaged_pc2,    # didn't fit better than a null model  
   models_pc1,  
   models_pc2,  
   models_wsd,  
   vars  
)  
  
``` 

```{r 25-build-glm-models-ecoreg}  
  
# Split the data into training and test sets by ecoreg====   
ecoreg_list <- gaged_ecoreg %>%  
  split(., .$ecoreg2)  

# create an index of values    
set.seed(seed)   
ecoreg_index <- lapply(ecoreg_list, function(df) {   
  return(createDataPartition(df$observed, p = .8, list = FALSE))    
})    
  
# get separate training df from the index (used in a code-chunk below)  
train_bhplat <- (ecoreg_list[[1]])[ecoreg_index[[1]],]  
train_kptabl <- (ecoreg_list[[2]])[ecoreg_index[[2]],]  
train_pshale <- (ecoreg_list[[3]])[ecoreg_index[[3]],]  
train_escarp <- (ecoreg_list[[4]])[ecoreg_index[[4]],]  
train_sndhil <- (ecoreg_list[[5]])[ecoreg_index[[5]],]  
train_wrbadl <- (ecoreg_list[[6]])[ecoreg_index[[6]],]  
  
# get test df from the index  
test_bhplat <- (ecoreg_list[[1]])[-ecoreg_index[[1]],]  
test_kptabl <- (ecoreg_list[[2]])[-ecoreg_index[[2]],]  
test_pshale <- (ecoreg_list[[3]])[-ecoreg_index[[3]],]  
test_escarp <- (ecoreg_list[[4]])[-ecoreg_index[[4]],]  
test_sndhil <- (ecoreg_list[[5]])[-ecoreg_index[[5]],]  
test_wrbadl <- (ecoreg_list[[6]])[-ecoreg_index[[6]],]  
  
# drop variables from the list====    
ecoreg_list <- lapply(ecoreg_list,  
                      function(x) select(x, 
                                         -c(sta,  
                                            ecoreg,  
                                            Date,  
                                            ecoreg2  
                                         )  
                      )  
)   
  
# run ecoregion glm====  
set.seed(seed)  
model_list <- purrr::map2(ecoreg_list, ecoreg_index,  
                          function(df, train_index)  {   
                            train(observed ~ ., df[train_index,],  
                                  method = 'glmnet',  
                                  trControl = reg.ctrl,  
                                  preProcess = c('nzv', 'center', 'scale'),  
                                  #  tuneGrid = expand.grid(  
                                  #   alpha = 1, lambda = lambda)  
                                  # for lasso  
                                  tuneLength = 10        # for elastic net   
                            )  
                          })  
  
# pluck individual models from the model list====  
pshale <- model_list %>%    
  pluck("pshale")  

bhplat <- model_list %>%   
  pluck("bhplat")   

escarp <- model_list %>%   
  pluck("escarp")  

kptabl <- model_list %>%  
  pluck("kptabl")  

wrbadl <- model_list %>%  
  pluck("wrbadl")  

sndhil <- model_list %>%  
  pluck("sndhil")  

# join the train and test sets==== 
train_ecoreg <- bind_rows(train_bhplat, train_kptabl, train_pshale,  
                          train_escarp, train_sndhil, train_wrbadl)  

test_ecoreg <- bind_rows(test_bhplat, test_kptabl, test_pshale,  
                         test_escarp, test_sndhil, test_wrbadl)  

# clean up global environment====  
rm(lambda,  
   seed,  
   cores,  
   reg.ctrl,  
   ecoreg_index,  
   ecoreg_list,  
   train_bhplat,  
   train_kptabl,  
   train_pshale,  
   train_escarp,  
   train_sndhil,  
   train_wrbadl,  
   test_bhplat,  
   test_kptabl,  
   test_pshale,  
   test_escarp,  
   test_sndhil,  
   test_wrbadl  
)    

```  

```{r 26-get-glm-coefs_ecoreg}  
  
# get coeffs====    
coef_pshale <- coef(pshale$finalModel,  
                    pshale$bestTune$lambda) %>%  
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") %>%  
  rename(pshale = 2)  

coef_escarp <- coef(  
  escarp$finalModel,  
  escarp$bestTune$lambda) %>%   
  as.matrix() %>%      
  as_tibble(., rownames = "coeff") %>%  
  rename(escarp = 2)  

coef_wrbadl <- coef(wrbadl$finalModel,   
                    wrbadl$bestTune$lambda) %>%  
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") %>%  
  rename(wrbadl = 2)  

coef_bhplat <- coef(bhplat$finalModel,  
                    bhplat$bestTune$lambda) %>%  
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") %>%   
  rename(bhplat = 2)  

coef_kptabl <- coef(kptabl$finalModel,  
                    kptabl$bestTune$lambda) %>%  
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") %>%  
  rename(kptabl = 2)  

coef_sndhil <- coef(sndhil$finalModel,  
                    sndhil$bestTune$lambda) %>%  
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") %>%  
  rename(sndhil = 2)   

# join model coefs====  
coefs_ecoreg <- bind_cols(coef_pshale,  
                          coef_escarp,  
                          coef_wrbadl,  
                          coef_bhplat,  
                          coef_kptabl,  
                          coef_sndhil) %>%  
  # remove duplicate columns  
  select(coeff, pshale, escarp, wrbadl, bhplat, kptabl, sndhil) %>%  
  gather(key = ecoreg, 
         value = val, 
         -coeff) %>%  
  mutate(val = round(val, digits = 3)  
  ) %>%  
  spread(ecoreg, val) %>%  
  # fix column order  
  select(coeff, pshale, escarp, wrbadl, bhplat, kptabl, sndhil) %>%   
  # rename coefficients to fit the hydrologic coefficient table  
  mutate(coeff = case_when(  
    coeff == "(Intercept)"                  ~ "Intercept",  
    coeff == "pcov_ln"                      ~ "perc_cov",                   
    coeff == "kvert_ln"                     ~ "kvert_mean",      
    coeff == "lwrat_log"                    ~ "lw_ratio",     
    coeff == "area_linv"                    ~ "cat_area",  
    coeff == "dden_sqr"                     ~ "drain_dens",  
    coeff ==  "cout_sqr"                    ~ "cat_out",                 
    coeff == "slop_ln"                      ~ "slop_med",  
    coeff == "vpdan_sq"                     ~ "vpd_ann",      
    coeff == "t07_sq"                       ~ "t07_mean",    
    coeff == "crel_sqr"                     ~ "cat_rel",   
    TRUE ~ coeff)) #%>%  

# arrange output====       # base the arrangement on sandhills  
coefs_ecoreg <- coefs_ecoreg %>%  
  mutate(row_arrange = abs(sndhil)) %>%  
  arrange(desc(row_arrange)) %>%  
  select(-row_arrange)  

# get importance values====  not important - only sandhills is significant 
#row_means <- coefs_ecoreg %>%  
#  gather(key = ecoreg,  
#         value = val,  
#         -coeff) %>%  
#  mutate(val = abs(val)) %>%  
#  group_by(coeff) %>%  
#  summarise(importance = round(median(val),  
#                               digits = 2)  
#            )  

#coefs_ecoreg <- full_join(coefs_ecoreg, row_means,  
#                           by = "coeff") %>%  
#  arrange(desc(importance))  

# clean up global environment====   
rm(coef_bhplat,  
   coef_escarp,  
   coef_kptabl,  
   coef_pshale,  
   coef_sndhil,  
   coef_wrbadl  
)    

```   

```{r 27-get_glm_ecoreg_obs_&_preds}

# get training observations & predictions====  
obs_train_ecoreg   <- tibble(observed = train_ecoreg$observed,  
                             Date = train_ecoreg$Date,  
                             ecoreg = train_ecoreg$ecoreg2)  

preds_train_ecoreg <- map_dfr(model_list,  
                              predict,  
                              newdata = train_ecoreg)    

fit_train_ecoreg <- bind_cols(obs_train_ecoreg, preds_train_ecoreg) %>%  
  mutate(split = "train") %>%   
  as_tibble()  

# combine test observations & predictions====  
obs_test_ecoreg   <- tibble(observed = test_ecoreg$observed,  
                            Date = test_ecoreg$Date,  
                            ecoreg = test_ecoreg$ecoreg2)  

preds_test_ecoreg <- map_dfr(model_list, predict,  
                             newdata = test_ecoreg)  

fit_test_ecoreg <- bind_cols(obs_test_ecoreg, preds_test_ecoreg) %>%  
  mutate(split = "test") %>%  
  as_tibble()  

rm(train_ecoreg,  
   test_ecoreg,  
   obs_train_ecoreg,  
   preds_train_ecoreg,  
   obs_test_ecoreg,  
   preds_test_ecoreg  
)   

rm(pshale,  
   escarp,  
   wrbadl,  
   bhplat,  
   kptabl,  
   sndhil  
)     

# fit the right models to the right ecoregions -------------------------------  
# the preds_train_ecoreg output fits models to all the model fits  # ???  

fit_train_ecoreg <- fit_train_ecoreg %>%  
  gather(ecoreg_mod, predicted, -c(observed, ecoreg, split, Date)) %>%  
  filter(ecoreg == ecoreg_mod) %>%  
  select(-ecoreg_mod)  

fit_test_ecoreg <- fit_test_ecoreg %>%  
  gather(ecoreg_mod, predicted, -c(observed, ecoreg, split, Date)) %>%  
  filter(ecoreg == ecoreg_mod) %>%  
  select(-ecoreg_mod)  

fit_ecoreg <- bind_rows(fit_train_ecoreg, fit_test_ecoreg)  

# drop the training data -- find model errors & fit a null model ------------  
fit_ecoreg <- fit_ecoreg %>%   
  filter(split == "test") %>%           # only want to see test data  
  select(-split) %>%   
  group_by(Date, ecoreg)  %>%     # fit a null model by Date & ECOREG  
  mutate(null_mod = mean(observed)) %>%  
  ungroup() %>%  
  gather(model, fitted, -c(observed, Date, ecoreg)) %>%  
  group_by(model, ecoreg) %>%  
  mutate(MAE = round(  
    MAE(observed, fitted),              # find the model errors  
    digits = 3)  
  ) %>%  
  ungroup() %>%  
  mutate(residual = observed - fitted) %>%  
  mutate(freq = 1- percent_rank(observed)) %>%  
  mutate(freq = if_else(freq== 1, 0.9999, freq))  

# check null model results with a single station====  
fit_ecoreg_check <- fit_ecoreg %>%  
  filter(ecoreg == "wrbadl") %>%  
  arrange(model)  

rm(fit_ecoreg_check,  
   fit_train_ecoreg,  
   fit_test_ecoreg  
)  

```

```{r 28-prepare_ecoreg_plots}

# set the facet labels, find number of test observations, get mae    
labels_models <- c(predicted = "Elastic net model",    
                   null_mod = "Mean of daily observations")    

labels_ecoreg <- c(pshale = "Pierre Shale",   
                   escarp = "PR. Escarp",  
                   wrbadl = "Badlands",  
                   bhplat = "BH. Plateau",  
                   kptabl = "Tablelands",  
                   sndhil = "Sandhills")   

n_test <- fit_ecoreg %>%   
  group_by(ecoreg) %>%  
  mutate(n_count = n()) %>%  
  distinct(n_count) %>%  
  ungroup() %>%  
  mutate(ecoreg = factor(ecoreg,  
                         levels = c('pshale',  
                                    'escarp',  
                                    'wrbadl',  
                                    'bhplat',  
                                    'kptabl',  
                                    'sndhil'  
                                    )  
  )  
  )   

mae_ecoreg <- fit_ecoreg %>%  
  select(ecoreg, model, MAE) %>%  
  distinct(MAE, model, .keep_all = TRUE) %>%  
  mutate(MAE = round(MAE, digits = 2)) %>%  
  mutate(ecoreg = factor(ecoreg,  
                         levels = c('pshale',  
                                    'escarp',  
                                    'wrbadl',  
                                    'bhplat',  
                                    'kptabl',  
                                    'sndhil'  
                         )  
  )  
  )   

``` 

```{r 30-plot_flow-duration-curve-density_ecoreg}
  
# flow duration curve plot-ecoreg====     
fit_ecoreg %>%  
  # change the order of facets  
  mutate(ecoreg = factor(ecoreg,  
                         levels = c('pshale',  
                                    'escarp',  
                                    'wrbadl',  
                                    'bhplat',  
                                    'kptabl',  
                                    'sndhil')   
  )  
  ) %>%  
  mutate(model = factor(model,  
                        levels = c('null_mod',  
                                   'predicted')  
  )  
  ) %>%  
  ggplot(aes(x = freq, y = fitted)) +  
  theme_bw() +  
  # create facets   
  facet_grid(ecoreg ~ model,  
             labeller = labeller(  
               model  = labels_models,  
               ecoreg = labels_ecoreg)  
  ) +  
  geom_point(shape = ".",  
             color = "gray80") +  
  geom_density2d(  
    color = "gray60")  +  
  geom_smooth(method = "gam",   
              linetype = 2,  
              size = 0.5,  
              color = "gray40") +  
  geom_line(aes(x = freq, y = observed)) +   
  # set axes and theme   
  scale_y_continuous(  
    name = bquote(  
      'Observations and fitted values in PC1 coordinates')  
  ) +   
  scale_x_continuous(name = 'Percentage of time flow exceeded',  
                     trans = 'probit',  
                     limits = c(0.0005, .999),  
                     breaks = c(0.99,  0.9,   0.75,  0.5,   
                                0.25,  0.1,   0.01),  
                     labels = c('99%', '90%', '75%', '50%',  
                                '25%', '10%', '1%')  
                     ) +  
  # add text of the number of observations    
  geom_text(data = n_test,  
            x = 2.2, y = 2.5,  
            aes(label= paste("n =", n_count, sep = " ")),  
            size = 2.5,  
            color = "gray20"  
  ) +  
  # add text of the MAE  
  geom_text(data = mae_ecoreg,  
            x = 2.2, y = 1.35,  
            aes(label= paste("MAE =", MAE, sep = " ")),  
            size = 2.5,  
            color = "gray20"  
  )   
  
ggsave("figure/freq_plot_eco.png",  
       width = 7,  
       height = 7,  
       units = "in"  
)  
  
rm(labels_models,  
   labels_ecoreg,  
   n_test,  
   mae_ecoreg  
)  
  
``` 

```{r 31-export-table_glm-coeffs_ecoreg}  

# * fix coefficient names====  
coef_table <- coefs_ecoreg %>% 
  mutate(coeff = case_when(  
    coeff == "cat_area" ~ "area",  
    coeff == "drain_dens" ~ "drain_density",  
    coeff == "fc_mean" ~ "field_capacity",  
    coeff == "t07_mean" ~ "July_temp",  
    coeff == "cat_out" ~ "outlet_elev",  
    coeff == "perc_cov" ~ "cover",  
    coeff == "vpd_ann" ~ "vpd_annual",  
    coeff == "mon_spring" ~ "summer",  
    coeff == "TWI_sq" ~ "TWI",   
    coeff == "mon_summer" ~ "summer", 
    TRUE ~ coeff  
  ))  
  
# * set the numeric columns====  
col_key_num <- coefs_ecoreg %>%  
  select(-coeff) %>%    
  names()  
  
# * convert tibble to a flextable====  
coef_table <- coef_table %>%   
  flextable() %>% 
  colformat_num(col_keys = col_key_num,   
                big.mark=",",  
                digits = 2, na_str = "N/A") %>%    
  set_header_labels(coeff  = "Explanatory variable",  
                    pshale = "Pierre Shale Plains",   
                    escarp = "Pine Ridge Escarpment",  
                    wrbadl = "White River Badlands",  
                    bhplat = "Black Hills Plateau",  
                    kptabl = "Keya Paha Tablelands",  
                    sndhil = "Sand Hills"    
  ) %>%       
  autofit() %>%   
  theme_booktabs()   
  
# export a docx of flextables     
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table)   
print(coef_table, target = "output/table_coef_ecoreg.docx")   
  
rm(coef_table,  
   col_key_num,  
   coefs_ecoreg  
   )       
  
```  

```{r clean-up-space}
  
rm(fit_ecoreg,  
   gaged,  
   gaged_ecoreg,  
   model_list)  
  
```

# prior code to move to dross
```{r 28-plot_flow-duration-curve_data_ecoreg, eval=FALSE} 

# spread ecoregion data 
mae_eco_sp <- mae_ecoreg %>% 
  spread(model, MAE) %>% 
  arrange(null_mod) %>% 
  arrange(predicted)  
  
# spread the null and fitted observations -- need to remove residual & MAE 
fit_ecomod_spread <- fit_ecomod_test %>% 
    select(-c(residual, MAE)) 

# pull out problem observations 
fit_ecomod_spread1 <- fit_ecomod_spread %>%  
  filter(ecoreg == "sndhil") %>% 
  filter(Date == "1994-12-12") %>% 
  arrange(observed) 

# remove problem observations from active tibble 
fit_ecomod_spread2 <- anti_join(fit_ecomod_spread, fit_ecomod_spread1, 
                                by = c("observed", "Date", "ecoreg", 
                                       "model", "fitted", "freq")) 

# spread active tibble 
fit_ecomod_spread2 <- fit_ecomod_spread2 %>% 
    spread(model, fitted) 
  
# remove problem observations - the 4 smallest observations are identical 
fit_ecomod_spread1 <- fit_ecomod_spread1 %>% 
  slice(-(1:4)) %>% 
    spread(model, fitted)  
  
# append the the active tibble - calculate diffs & clean up 
# -- the sign of mod_diff indicates: +: null < pred, -: null > pred 
fit_ecomod_spread <- bind_rows( 
  fit_ecomod_spread1, fit_ecomod_spread2) %>% 
  mutate(mod_diff = predicted-null_mod) 
  
rm(fit_ecomod_spread1, fit_ecomod_spread2) 

# set facet order 
mae_ecoreg <- mae_ecoreg %>% 
  mutate(ecoreg = factor(ecoreg, 
                         levels = c('pshale', 'escarp', 'wrbadl', 
                                    'bhplat', 'kptabl', 'sndhil') 
                         ))   


# plot flow duration curve -- differences between models --------------------- 
freq_plot_eco_diff <- fit_ecomod_spread %>% 
# change the order of facets 
  mutate(ecoreg = factor(ecoreg, 
                         levels = c('pshale', 'escarp', 'wrbadl', 
                                    'bhplat', 'kptabl', 'sndhil') 
                         ) 
         ) %>% 
  ggplot(aes(x = freq, y = null_mod)) +  
# create facets 
  facet_wrap(vars(ecoreg), 
             ncol = 2, 
             labeller = labeller( 
               ecoreg = labels_ecoreg) 
             ) + 
  geom_point(shape = ".",  
             color = "gray80") + 
  geom_smooth(method = "gam", 
              linetype = 2, 
              size = 0.5, 
             color = "gray60") + 
  geom_point(aes(x = freq, y = predicted), 
             shape = ".",  
             color = "gray40") +   
  geom_smooth(aes(x = freq, y = predicted), 
              method = "gam", 
              linetype = 4, 
              size = 0.5, 
             color = "gray20") + 
  geom_line(aes(x = freq, y = observed)) + 
  scale_y_continuous(
    name = bquote(
      'Observations and fitted values in log ('~m^3 / km^2 ~ 'day)')
    ) + 
# add text of the null model MAE 
  geom_text(data = mae_eco_sp, 
            x = 2, y = 3.5,  
           aes(label= paste("Null model MAE =", null_mod, sep = " ")), 
           size = 2.5, 
           color = "gray70" 
           ) +   
# add text of the predicted model MAE 
  geom_text(data = mae_eco_sp, 
            x = 2, y = 2.8,  
           aes(label= paste("Lasso model MAE =", predicted, sep = " ")), 
           size = 2.5, 
           color = "gray30" 
           ) + 
# set axes and themes   
  scale_x_continuous(name = 'Percentage of time flow exceeded',
                     trans = 'probit', 
                     limits = c(0.005, Inf), 
          breaks = c(0.99,  0.9,   0.75,  0.5,   0.25,  0.1,   0.01),
          labels = c('99%', '90%', '75%', '50%', '25%', '10%', '1%')) + 
  theme_bw() 

freq_plot_eco_diff 

ggsave("figure/freq_plot_eco_diff.png", width = 7, height = 4, units = "in")  
``` 

```{r 29-prepare_mixed_model, eval=FALSE}

# calculate mixed models 
fit_ecomod_spread <- fit_ecomod_spread %>% 
  mutate(pred_mixed = case_when( 
    ecoreg == "pshale" ~ case_when( 
      between(freq, 0.65, 0.9) ~ predicted,  
      freq > 0.65 ~ predicted, 
      TRUE ~ null_mod), 
    ecoreg == "escarp" ~ case_when( 
       between(freq, 0.55, 0.9) ~ predicted,   
      TRUE ~ null_mod),   
    ecoreg == "wrbadl" ~ case_when(  
      between(freq, 0.6, 0.9) ~ predicted,  # dropped from 1.0
      TRUE ~ null_mod),   
    ecoreg == "bhplat" ~ case_when( 
      between(freq, 0.1, 0.6)  ~ predicted, 
      TRUE ~ null_mod),  
    ecoreg == "kptabl" ~ case_when( 
      between(freq, 0.1, 0.6)  ~ predicted, 
      TRUE ~ null_mod),  
    ecoreg == "sndhil" ~ case_when( 
      freq < 0.6  ~ predicted, 
      TRUE ~ null_mod) 
    ))  
 
# calculate model fits 
mae_ecoreg <- fit_ecomod_spread %>% 
  group_by(ecoreg) %>% 
  mutate(MAE_null  = MAE(observed, null_mod)) %>%  
  mutate(MAE_lasso = MAE(observed, predicted)) %>% 
  mutate(MAE_mixed = MAE(observed, pred_mixed)) %>%  
  summarise(MAE_null = round(mean(MAE_null), digits = 3), 
            MAE_lasso = round(mean(MAE_lasso), digits = 3), 
            MAE_mixed = round(mean(MAE_mixed), digits =3) 
    ) %>% 
  ungroup()  

print(mae_ecoreg) 

# add final model fits 
ecoreg_freq <- tibble( 
  ecoreg     = mae_ecoreg$ecoreg, 
  hi_exceed  = c(0.10, 0.55, 0.10, 0.65, 0.0, 0.60), 
  low_exceed = c(0.60, 0.90, 0.60, 0.90, 0.6, 0.90)
  )    
  
ecoreg_freq <- full_join(mae_ecoreg, ecoreg_freq, 
                  by = "ecoreg") %>% 
  mutate(ecoreg = as.character(ecoreg)) %>% 
  mutate(ecoreg2 = case_when( 
    ecoreg == "pshale" ~ "Pierre Shale Plains",   
    ecoreg == "escarp" ~ "Pine Ridge Escarpment",  
    ecoreg == "wrbadl" ~ "White River Badlands",  
    ecoreg == "bhplat" ~ "Black Hills Plateau",  
    ecoreg == "kptabl" ~ "Keya Paha Tablelands",  
    ecoreg == "sndhil" ~ "Sand Hills" 
    )) %>% 
  select(ecoreg2, everything())


# final model   
#ecoreg null_mod predict mixed    frequency  (bottom - first time just down)
#pshale	  0.657	 0.554	 0.425	 65% to  90%    
#escarp	  0.441  0.445	 0.368   60% to  90%  
#wrbadl	  0.462	 0.565	 0.435   60% to  90% 
#bhplat	  0.454	 0.342	 0.299	 10% to  60%  
#kptabl	  0.365	 0.234	 0.207	 10% to  65% 
#sndhil	  0.594	 0.182	 0.168	  0% to  60%    
  
```   

```{r 30-export-mixed-model-table, eval=FALSE}
  
# set the numeric columns  
col_key_num <- ecoreg_freq %>% 
  select(-c(ecoreg)) %>%  
  names() 

top_row <- c("", "", "", "", 
             "Exceedance Probablity", "Exceedance Probability")    
  
hydro_table <- hydro_table %>%   
  flextable() %>%   
  theme_booktabs() %>%   
  colformat_num(col_keys = col_key_num,   
                big.mark=",",   
                digits = 1, na_str = "N/A") %>%   
  set_header_labels(values = header_labels) %>%  
  add_header_row(values = top_row,   
  top = TRUE) %>%   
  merge_at(i = 1, j = 7:9, part = "header") %>%   
  merge_at(i = 1, j = 10:12, part = "header") %>%   
  autofit()   

# convert tibble to a flextable   
coef_table <- ecoreg_freq %>% 
  select(-ecoreg) %>% 
  flextable() %>%  
  colformat_num(col_keys = col_key_num,  
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%   
  set_header_labels(   
    ecoreg2  = "Ecoregion", 
    MAE_null = "Null model MAE", 
    MAE_lasso = "Lasso model MAE", 
    MAE_mixed = "Mixed model MAE", 
    hi_exceed = "High value", 
    low_exceed = "Low value") %>% 
  add_header_row(values = top_row,   
  top = TRUE) %>%  
#  merge_at(i = 1, j = 5:6, part = "header") %>%     
  autofit() %>%   
  theme_booktabs()  
  
# export a docx of flextables    
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_ecoreg_freq.docx")  
  
rm(coef_table, col_key_num)     
  
```  

```{r 31-plot-mixed-model, EVAL=FALSE}

# set facet order 
mae_ecoreg <- mae_ecoreg %>% 
  mutate(ecoreg = factor(ecoreg, 
                         levels = c('pshale', 'escarp', 'wrbadl', 
                                    'bhplat', 'kptabl', 'sndhil') 
                         ))   

# set facet order 
mae_ecoreg <- mae_ecoreg %>% 
  mutate(ecoreg = factor(ecoreg, 
                         levels = c('pshale', 'escarp', 'wrbadl', 
                                    'bhplat', 'kptabl', 'sndhil') 
                         ))   
  
# plot flow duration curve -- differences between models ---------------------  

freq_plot_eco_mix <- fit_ecomod_spread %>% 
# change the order of facets 
  mutate(ecoreg = factor(ecoreg, 
                         levels = c('pshale', 'escarp', 'wrbadl', 
                                    'bhplat', 'kptabl', 'sndhil') 
                         ) 
         ) %>% 
  ggplot(aes(x = freq, y = pred_mixed)) +  
# create facets 
  facet_wrap(vars(ecoreg), 
             ncol = 2, 
             labeller = labeller( 
               ecoreg = labels_ecoreg) 
             ) + 
  geom_point(shape = ".",  
             color = "gray80") + 
  geom_density2d(  
             color = "gray60")  +   
  geom_smooth(method = "gam", 
              linetype = 2, 
              size = 0.5, 
             color = "gray40") + 
  geom_line(aes(x = freq, y = observed)) + 
# add text of the null model MAE 
  geom_text(data = mae_ecoreg, 
            x = 2, y = 3.5,  
           aes(label= paste("Mixed model MAE =", 
                            round(MAE_mixed, digits = 2), 
                                  sep = " ")), 
           size = 2.5, 
           color = "gray20" 
           ) +   
# set axes and themes   
  scale_x_continuous(name = 'Percentage of time flow exceeded',
                     trans = 'probit', 
                     limits = c(0.005, Inf), 
          breaks = c(0.99,  0.9,   0.75,  0.5,   0.25,  0.1,   0.01),
          labels = c('99%', '90%', '75%', '50%', '25%', '10%', '1%')) + 
  scale_y_continuous(
    name = bquote(
      'Observations and final values in log ('~m^3 / km^2 ~ 'day)')
    ) + 
  theme_bw() 

freq_plot_eco_mix 

ggsave("figure/freq_plot_eco_mix.png", width = 7, height = 4, units = "in")  
```

```{r 25b-select-glm-model_ecoreg, eval=FALSE}
  
# evaluate performance of models -- ridge, lasso and elastic net----      
#   best model is the one that minimizes prediction error.    
  
# pick out the mse from the output lists====   
error_call_ecoreg <- resamples(model_list) %>%    
  summary(metric = c("RMSE", "MAE"))     
  
# change list to tibble   
error_vals_mon <- error_call_ecoreg %>%    
  pluck(., 'statistics') %>%   
    as.data.frame() %>%   
  as_tibble(., rownames = "model") %>% 
  gather(key, val, -c(model))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>%   
  separate(key, c("statistic", "position"), extra = "drop") %>%  
  filter(position != "NA") %>%   
  spread(position, val)    
  
# add number of params to error vals & clean up   
error_vals_mon <- full_join(error_vals_mon, num_param,   
                           by = "model")    
  
# prepare for plotting====   
error_vals_mon <- error_vals_mon %>%    
  mutate(num_param = as.character(num_param)) %>%   
  mutate(temp = ", p = ") %>%   
  mutate(model_text = str_c(model, temp, num_param))  %>%   
  mutate(num_param = as.integer(num_param))  
  
n_train <- train_mon %>%  
  mutate(n_count = n()) %>%  
  distinct(n_count)   

# clean up global environment====  
rm(error_call_mon,  
   num_param)  
  
```   
