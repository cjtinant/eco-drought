---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!--  
These are some useful thoughts:    

lmomco <- citation("lmomco") 
a useful description of commits -- http://r-pkgs.had.co.nz/git.html  

# Some shortcuts in a code chunk styling format----  
# key-bindings====     
# insert operators####    
# pipe operator        %>% 	      Cmd+Shift+M
# assignment operator  <-    	    Option+-  

# Code chunks====  
# collapse                        Cmd+Option+L
# expand                          Cmd+Shift+Option+L
# collapse all                    Cmd+Option+O
# expand all                      Cmd+Shift+Option+O

# Navigation====
# insert section                 Cmd+Shift+R 
# jump to                        Shift+Alt+J

Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0   Formulate your question  
2.0   Read in your data  
3.0   Check the dataset 
3.1   Check the number of rows and columns.
3.2   Check the types of data
3.3   Look at the top and the bottom of your data 
3.4   Check your “n”s & NAs 
3.5   Validate with at least one external data source  
4.0   Try the easy solution first to answer question
5.0   Challenge your solution 
6.0  Follow up questions 


-->  

<!-- 
Purpose:  
This R markdown file calculates SCI 
Standardized precipitation index - SPI  
Streamflow runoff index - SRI  

Analysis steps 
1.0)  sets up the library and settings 

2.0)  downloads precipitation daily data and metadata (sta_orig); 
2.1)    creates groups by location;   
2.2)    creates short names;  
2.3)    downloads daily data for original stations (30-years);  
2.4)    removes incomplete years (sta_plus)  
2.5)    defines the final stations  (sta_fin)

3.0)  plot study area map;  
3.1)    prepare map elements: Theissen line segments, stream gages, location  
        data: states, counties, USA map-grob

4.0)  check data flags  




-- identified & filled NA
2.0)  convert daily precip to monthly precip  
2.1)  Updated unit vals - originally in tenths of mm; now in mm 
3.0)  Created plots 


Data:
Predominant datasets used are:  
1) NOAA data from NOAA GHCN database - 60-years (1959-2019)
2) USGS daily streamflow and station metadata,  
3) 

3)  prepare data for drought index 


1. Recreated analysis from the lmomco text ch 12 (author?) in Tidyverse
2. Imported cleaned precipitation data (see 04_prcp-data_munging)   

5. Applied Weibull plotting position and graphed the data on sqrt plot
6. Calculated L-moments and L-moment ratios 
7. Calculated SPI for 'cot', 'oel', 'rap', 'int', and 'ora' datasets using Pearson III. 


3. Applied sqrt & log10 transform to explore effects on skew 
4. Explored the data with box plots, violin plot.


####3) Summary data from a QGIS analysis of ungaged watersheds of interest.  

## Results: Fits a PE3 distribution
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?  

Variable naming convention----  
a_session    list variable of session information  

precipitation and spi variables====  
dateMin       minimum date for downloading data   
dateMax       maximum date for downloading data  
sta           NOAA weather station locations  
  _dv         daily values  
  _meta       metadata  
  _orig       all stations (n = 46 sations)  
  _plus       potential stations (n = 14 stations)  
  _fin        final dvs used in the analysis   
  _mon        monthly precipitation depths (60-years)  
  _pres       monthly precipitation depths (30-years)  

daily value check & fill variables==== 
sta  
  _check       used to summarize station flags
  _alt         alternate station dv data for filling missing values 
  _fil         station to be filled 
  _miss_day    checks for missing days 
  _short_name  adds a short name for the station to be filled
  _rap         Rapid City Regional Airport station (NW)  
  _cot         Cottonwood station (NC)  
  _oni         Onida station (NE)   
  _ora         Oral station (SW)   
  _gor         Gordon staion (SC)   
  _mis         Mission station (SE)  

L-moment variables====  
lmom          L-moments  
  _sta        L-moments for stations
  _nm        station name
lmrdia       list of L-moment distributions 
gev          Generalized Extreme Value Distribution
glo          Generalized Logistic Distribution
gpa          Generalized Pareto Distribution
gno          Generalized Logistic Distribution 
gov          Govindarajulu Distribution
pe3          Pearson Type III Distribution  
L1           first L-moment, similiar to mean
L_CV         first L-moment ratio, similiar to coefficient of var
L_skew       second L-moment ratio, similiar to skewness 
L_kurtosis   third L-moment ratio, similiar to kurtosis
n            number of months in a given record

dischord     the Hosking and Wallis discordancy of the first three L-moment 
             ratios according to their implementation in Hosking and Wallis 
             (1997) and earlier. Discordancy triplets of these L-moment ratios 
             is heuristically measured by locating the triplet from the mean
             center of the 3-dim. cloud of values. 
  $Dmax	     The maximum discordancy D_{max} = (n-1)/3.
  $Dalpha1   The critical value of D for α_1 = 0.10 (default) 
  $Dalpha2	 The critical value of D for α_2 = 0.01 (default) 
  $Dcrit	   The critical value of discordancy (user or tabled).
  $D	       The discordancy of the L-moment ratios used to trigger isD	
  $isD       Are the L-moment ratios discordant (if starred) 
  $signif	   A hyphen, star, or double star based on Dalpha1 and Dalpha2 vals.

# SCI calculation variables----   
spi           Standardized Precipitation Index vals from NOAA precip data  
sri           Standardized Runoff Index vals from USGS streamflow gage data  
sci           Generalized variable for SPI and SRI calculations  
  _1mo        One    month duration                           
  _2mo        Two    month duration                             
  _3mo        Three  month duration                           
  _4mo        Four   month duration  
  _6mo        Six    month duration  
  _9mo        Nine   month duration   
  _12mo       Twelve month duration  

_freq       frequency of periodicity -- used in seasonality calculations  
_trend      trend of periodicity -- used in seasonality calculations  

# individual station variables for SRI====  
bhp_         Black Hills Plateau  
  _bat       Battle Creek above Hermosa  
  _bev       Beaver Creek  
  _fal       Fall River  
  _frn       French Creek  
kpt_         Keya Paha Tablelands  
  _key       Keya Paha River at Keya Paha   
  _wew       Keya Paha River at Wewila  
psp_         Pierre Shale Plains  
  _bat       Battle Creek below Hermosa   
  _brs       South Fork of the Bad River   
  _che       Cheyenne River at Wa??   
  _elk       Elk Creek at Elm Springs   
  _hat       Hat Creek at Edgemont  
  _whi       White River at Oacoma 
pre_         Pine Ridge Escarpment  
  _ogl       White River at Oglala  
  _sta       White River at Stateline 
snd_         Sand Hills  
  _lcr       Lake Creek below Refuge?
  _lon       Long River   
  _mar       Little White River at Martin  
  _ros       Little White River at Rosebud   
  _vet       Little White River at Vetal     
  _whi       Little White River at White River   
  _nio       Niobrera River at SPA??
whi_         White River Badlands  
  _kad       White River at Kadoka
    _v       as a double vector  


gage          USGS streamgage station  

'site' is a variable for OST monitoring stations 

## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?

## Narrower question:
What is the underlying distribution of precipitation data?  

# Next STEPS  
3. Describe the precipitation seasonality
-->  

```{r setup_&_library, message=FALSE}   
  
knitr::opts_chunk$set(echo = FALSE)      
options(tibble.print_max = 70) # sets tibble output for printing         
  
# increase memory allocation for dabest()   
# usethis::edit_r_environ("project")  
  
# Get API key (aka, token) for downloading precip data at   http://www.ncdc.noaa.gov/cdo-web/token  
# token for NOAA API tied to jtinant@olc.edu -- see 'rnoaa' for details  
options(noaakey = "VpcuARumMpCfFyclKHPfvskEYnaiLJHD")  
  
# Sets up the library of packages   
library("conflicted")        # An alternative conflict resolution strategy  
library("here")              # identifies where to save work  
library("rnoaa")             # R wrapper for NOAA data inc. NCDC  
library("rio")               # more robust I/O - to import and clean data  
library("lubridate")         # easier dates   
library("lmomco")            # lmoments to find distribution   
library('deldir')            # for Vorononi tesselation - Theissen polygons  
library("SCI")               # calculates SPI & RDI   
library("forecast")          # using the BoxCox function   
library("broom")             # tidies linear models   
library("ggbeeswarm")        # plot 1D data as a violin / beeswarm plot  
library("scales")            # graphical scales map data to aesthetics,  
                             #   & methods for determining breaks and labels  
                             #   for axes and legends  
library("anomalize")         # detect anomalies using the tidyverse   
library("dabestr")           # data analysis using bootstrap estimation   
library("cowplot")           # streamlined theme & annotations for 'ggplot2'   
library("timetk")            # tool kit for working with time series in R 
library("tidyquant")         # integrate quant. analysis tools w/ tidyverse


# used in time series clustering example--
#library(gridExtra)           # merge plots  
#library(ggdendro)            # dendrograms
#library(gplots)              # heatmap
#library(tseries)             # bootstrap
#library(TSclust)             # cluster time series
#library(dtwclust)            # cluster time series with dynamic time warping  

library("tidyverse")  

# resolve conflicted packages----  
conflict_prefer("filter", "dplyr")  
conflict_prefer("select", "dplyr")  
conflict_prefer("as.dist", "stats")
  
# other packages I have thought about ---------------------------------------  
# library("ggfortify")        # data vis tools for statistical analysis  
# library("ggpubr")           # some easy ggplot wrappers for publication ready 
#                             #   'ggplot2'- based plots  
  
# library("standardize")      # tools for controlling continuous variable   
#  scaling and factor contrasts for linear models   
# library("pdftools")         # utilities for extracting text, fonts,  
#   attachments and metadata from a pdf file.  
  
# Packages to consider----  
# Spatial data====  
#library("biogeo")            # Functions for error detection & correction  
#   in point-data datasets; includes functions   
#   to parse & convert coords to decimal-degrees  
#library("maps")              # Outlines of continents, countries, states &  
#   counties  
#library("mapdata")           # higher-resolution outlines  
#library('ggmap')             # Spatial visualization with ggplot2   
#library("sf")                # Simple features--spatial geometries for R  
#library("RColorBrewer")      # Provides color schemes for maps -- see  
#   http://colorbrewer2.org  
  
# Colors====  
#library("colorspace")        # Manipulate & assess colors & palettes  
#library("munsell")           # Access & manipulate munsell system colours  
#   https://github.com/cwickham/munsell  
  
# working with lists and urls====  
#library("jsonlite")          # Convert between JSON data and R objects  
#library("curl")              # Drop-in replacement for base url  
#library("listviewer")        # htmlwidget for interactive views of R lists  
  
#library("forecast")         # for BoxCox.lambda   
#library("magrittr") # provides aliases for easier reading  
#library("workflowr") # creates a research website  
#library("bookdown") #  
#library(unpivotr) # fix nasty Excel files  
#library("friendlyeval")  
#library("mathpix")                # support for 'Mathpix' image to 'LaTeX'    
#library("grateful") - not yet ready for R 3.5.0  
#lmomco <- citation("lmomco")  
#toBibtex(lmomco)  
  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
why_to_write <- function()   
{today <- today(tzone = "")  
paper2 <- ymd("2019-11-15")  
until <- paper2 - today  
print(paste("You have", until, "days until the second paper is due"  
))   
}   
why_to_write()  
  
```  

```{r download_prcp_data, eval=FALSE} 
  
# Get possible station metadata----    
# It's also possible to check station id with the mapping tool at:    
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation   
  
# The geographical extent given as SElat, SElon, NWlat, NWlon  
sta_meta_orig <- ncdc_stations(extent = c(42.0, -104.5, 45, -99.5),  
                               limit = 1000) %>%      # n = 777  
  # get possible stations into a dataframe  
  pluck("data") %>%  
  # turn min & max date into lubridates  
  mutate(mindate = ymd(mindate)) %>%  
  mutate(maxdate = ymd(maxdate)) %>%                  
  # remove 'young' stations    
  filter(mindate < "1988-01-01") %>%                    # n = 434  
  # remove 'dead' stations    
  filter(maxdate > "2018-01-01") %>%                    # n =  66    
  # keep only the GHCND stations   
  filter(str_detect(id, "^GHCND"))  %>%                 # n =  60      
  filter(!str_detect(name, "^MAGPIE"))                  # n =  59  
  
sta_meta_orig <- sta_meta_orig %>% 
  mutate(elev_flag = case_when(  
    elevation > 1200 ~ "yes",  
    TRUE ~ "no"
      )                              # n =  46    
  )  
  
# create groups of stations by region====       
#   Notes: the string "GHCND:" doesn't appear in calls to get Global Historical  
#   Climatology Network (GHCN) Daily Data  
sta_meta_orig <- sta_meta_orig %>%   
  mutate(north_group = case_when(   
    latitude > 43.5 ~ "N",   
    TRUE ~ "S")   
  ) %>%   
  mutate(east_group = case_when(  
    longitude > -100.67 ~ "E",  
    longitude > -102.33 ~ "C",    
    TRUE ~ "W")  
  )  %>%  
  mutate(group = str_c(north_group, east_group, sep = "")) %>%  
  dplyr::select(-c(north_group, east_group)) %>%  
  separate(  
    col = id,  
    sep = ":",   
    into = c("type", "sta_id")   
  ) %>%  
  arrange(name)  
  
# add short names to metadata====    
sta_meta_orig <- sta_meta_orig %>%  
  mutate(sta = case_when(  
    str_detect(name, "^AGATE")            ~ "AGA",  
    str_detect(name, "^AINSWORTH")        ~ "AIN",  
    str_detect(name, "^BEAR")             ~ "BEA",  
    str_detect(name, "^BELLE FOURCHE 22") ~ "BE2",      
    str_detect(name, "^BELLE FOURCHE,")   ~ "BEL",       
    str_detect(name, "^CHADRON")          ~ "CHA",      
    str_detect(name, "^COTTONWOOD")       ~ "COT",  
    str_detect(name, "^CUSTER")           ~ "CUS",  
    str_detect(name, "^DUPREE")           ~ "DUP",  
    str_detect(name, "^EDGEMONT")         ~ "EDG",   
    str_detect(name, "^ELLSWORTH")        ~ "ELL",  
    str_detect(name, "^ELM SPRINGS")      ~ "ELM",  
    str_detect(name, "^ELSMERE")          ~ "ELS",  
    str_detect(name, "^FORT MEADE")       ~ "FME",      
    str_detect(name, "^FORT PIERRE")      ~ "FPI",  
    str_detect(name, "^FORT ROBINSON")    ~ "FRO",      
    str_detect(name, "^GORDON")           ~ "GOR",    
    str_detect(name, "^HARRISON")         ~ "HAI",   
    str_detect(name, "^HARROLD")          ~ "HAR",   
    str_detect(name, "^HEMINGFORD")       ~ "HEM",   
    str_detect(name, "^HAY SPRINGS")      ~ "HAY",   
    str_detect(name, "^HILL CITY")        ~ "HIL",   
    str_detect(name, "^HOT SPRINGS")      ~ "HOT",  
    str_detect(name, "^INTERIOR")         ~ "INT",  
    str_detect(name, "^KENNEBEC")         ~ "KEN",  
    str_detect(name, "^KIRLEY")           ~ "KIR",     
    str_detect(name, "^KYLE")             ~ "KYL",  
    str_detect(name, "^LEAD")             ~ "LEA",  
    str_detect(name, "^LINGLE")           ~ "LIN",  
    str_detect(name, "^KYLE")             ~ "KYL",  
    str_detect(name, "^MAURINE")          ~ "MAU",     
    str_detect(name, "^MILESVILLE")       ~ "MIL",    
    str_detect(name, "^MISSION")          ~ "MIS",  
    str_detect(name, "^MOUNT")            ~ "MTR",  
    str_detect(name, "^MULLEN")           ~ "MUL",  
    str_detect(name, "^MURDO")            ~ "MUR",   
    str_detect(name, "^NEWCASTLE")        ~ "NEC",  
    str_detect(name, "^NEWELL")           ~ "NEW",    
    str_detect(name, "^OAHE DAM")         ~ "OAH",      
    str_detect(name, "^NEWELL")           ~ "NEW",    
    str_detect(name, "^OELRICHS")         ~ "OEL",  
    str_detect(name, "^ONIDA")            ~ "ONI",  
    str_detect(name, "^ORAL")             ~ "ORA",  
    str_detect(name, "^PACTOLA")          ~ "PAC",  
    str_detect(name, "^PHILIP")           ~ "PHI",  
    str_detect(name, "^PLAINVIEW")        ~ "PLA",  
    str_detect(name, "^PIERRE")           ~ "PIE",  
    str_detect(name, "^PURDUM")           ~ "PUR",   
    str_detect(name, "^RAPID CITY 4")     ~ "RA4",        
    str_detect(name, "^RAPID CITY R")     ~ "RAP",   
    str_detect(name, "^SPEARFISH")        ~ "SPE",        
    str_detect(name, "^SPRINGVIEW")       ~ "SPR",      
    str_detect(name, "^RED OWL")          ~ "RED",  
    str_detect(name, "^REDBIRD")          ~ "REB",  
    str_detect(name, "^SUNDANCE")         ~ "SUN",  
    str_detect(name, "^VALENTINE MILLER") ~ "VAL",  
    str_detect(name, "^VALENTINE NWR")    ~ "VNW",  
    str_detect(name, "^WASTA")            ~ "WAS",  
    str_detect(name, "^WIND CAVE")        ~ "WIN",  
    str_detect(name, "^WINNER")           ~ "WIN",     
    str_detect(name, "^WOOD")             ~ "WOO",  
    TRUE ~ "ERROR"  
  )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation, group, everything())  
  
# download daily precip data from NOAA GHCN database-all_stations====       
# date function calls start one-year early for long-term drought calcs  
dateMin = "1959-01-01"      
dateMax = "2018-12-31"    
  
sta_dv_orig <- sta_meta_orig %>%   
  filter(elev_flag == "no") %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags   = TRUE,   
                            var          = "PRCP",   
                            date_min     = dateMin,   
                            date_max     = dateMax)  
  )     
  
# fix date & add year and month & add metadata====    
sta_dv_orig <- sta_dv_orig %>%   
  mutate(date  = ymd(date)) %>%   
  arrange(desc(date)) %>%  
  mutate(year  = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   
  
sta_dv_orig <- left_join(sta_dv_orig, sta_meta_orig,  
                         by = c("id" = "sta_id"))  
  
# check for missing years====            
sta_miss_yr <- sta_dv_orig  %>%   
  filter(year > 1988)       %>% 
  mutate(year = year(date)) %>%  
  group_by(name, year)      %>%   
  summarise(num_day = n())  %>%   
  filter(num_day < 345)     %>%   
  ungroup() %>%  
  group_by(name) %>%  
  summarise(num_year = n()) 

sta_miss_zero <- anti_join(sta_meta_orig, sta_miss_yr,  
                           by = "name") %>% 
  filter(elev_flag == "no") %>%  
  mutate(num_year = 0) %>%  
  select(name, num_year)  

sta_miss_yr <- bind_rows(sta_miss_yr, sta_miss_zero)  

# add missing year flag to table====    
sta_meta_orig <- left_join(sta_meta_orig, sta_miss_yr,  
                           by = "name")  
  
sta_meta_orig <- sta_meta_orig %>%  
  mutate(missing_flag = case_when(  
    num_year <= 3 ~ "no",  
    num_year >  3 ~ "yes"  
    )) %>%  
  select(-num_year) 
  
# remove stations with 3 years of > 95% completeness====      
sta_dv_orig <- left_join(sta_dv_orig, sta_meta_orig, 
                          by = c("sta", 
                                 "name", 
                                 "longitude", 
                                 "latitude", 
                                 "elevation", 
                                 "group", 
                                 "mindate", 
                                 "maxdate", 
                                 "datacoverage", 
                                 "type", 
                                 "elevationUnit", 
                                 "elev_flag")) %>%  
  select(-c(sta_id, type))  
  
# select final stations for each group====    
# this was created from the map below   
sta_meta_fin <- subset(sta_meta_orig,   
                       sta %in%  
                         c("RAP",  
                           "COT",  
                           "ONI",  
                           "OEL",  
                           "GOR",  
                           "MIS"  
                         )    
)   
  
sta_dv_fin <- semi_join(sta_dv_orig, sta_meta_fin,  
                        by = "sta")  
  
# export files and prepare for table below====    
export(sta_meta_fin, "data/sta_meta_fin.csv")    
  
sta_meta_fin <- sta_meta_fin %>% 
  select(sta) %>% 
  mutate(selected = "yes")

sta_meta_orig <- left_join(sta_meta_orig, sta_meta_fin, 
                           by = "sta")  

export(sta_meta_orig, "data/sta_meta_orig.csv")   

# clean up global environment==== 
rm(sta_miss_yr,   
   dateMin,  
   dateMax,  
   sta_meta_fin, 
   sta_miss_zero,  
   sta_dv_orig    
)     
  
```   

```{r check_prcp_data_quality, eval=FALSE}  
  
# Table of Measurement Flag/Attributes -- mflag----     
# Blank = no measurement information applicable            
# A     = precip depth is a multi-day total, accumulated since last meas         
# B     = precipitation total formed from two twelve-hour totals  
# D     = precipitation total formed from four six-hour totals           
# H     = represents TMAX or TMIN or average of hourly values (TAVG)           
# K     = converted from knots            
# L     = temperature appears lagged w/ respect to reported hr of observation  
# O     = converted from oktas  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
# W    = converted from 16-point WBAN code (for wind direction)  
  
# Table of Quality Flag/Attributes----    
# Blank = did not fail any quality assurance check   
# D     = failed duplicate check           
# G     = failed gap check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# M     = failed mega-consistency check            
# N     = failed naught check            
# O     = failed climatological outlier check            
# R     = failed lagged range check           
# S     = failed spatial consistency check            
# T     = failed temporal consistency check            
# W     = temperature too warm for snow            
# X     = failed bounds check           
# Z     = flagged as a result of an official Datzilla investigation  

# Table Source Flag/Attributes----     
# Blank = No source (i.e., data value missing)  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 6  = CDMP Cooperative Summary of the Day (NCDC DSI-3206)   
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder  
# A  = U.S. Automated Surface Observing System (ASOS) real-time data  
# a  = Australian data from the Australian Bureau of Meteorology           
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# b  = Belarus update           
# C  = Environment Canada            
# E  = European Climate Assessment and Dataset (Klein Tank et al., 2002)      
# F  = U.S. Fort data             
# G  = Off Global Climate Observing System (GCOS) or other gov-supplied data   
# H  = High Plains Regional Climate Center real-time data            
# I  = International collection (non U.S. data received thru pers. contacts   
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# M  = Monthly METAR Extract (additional ASOS data)           
# N  = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)           
# Q  = Data from African countries w/ later permission granted  
# R  = NCDC Reference Network Database  
# r  = All-Russian Research Inst. Hydromet Information-World Data Center    
# S  = Global Summary of the Day (NCDC DSI-9618)  
#        NOTE: "S" values are derived from hourly synoptic reports   
#         exchanged on the Global Telecommunications System (GTS).  
#         Daily values derived in this fashion may differ significantly from  
#        "true" daily data, particularly for precip (i.e., use with caution).  
# s  = China Met Admn/Nat Met Info/Climate Data Centr (http://cdc.cma.gov.cn)   
# T  = SNOwpack TELemtry (SNOTEL) data from Western Regional Climate Center   
# U  = Remote Automatic Weather Station (RAWS) data from West Reg Climate Centr  
# u  = Ukraine update           
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
# z  = Uzbekistan update   
  
sta_check <- sta_dv_orig %>%  
  group_by(sta, mflag_prcp) %>%  
  summarise(mflag = n()) %>% 
  filter(mflag_prcp != " ") %>%  
  group_by(mflag_prcp) %>%    
  summarise(mflag = n()) %>% 
  ungroup()    
  
# Measurement flags are T & P flags  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
  
sta_check <- sta_dv_orig %>%  
  group_by(sta, qflag_prcp) %>%  
  summarise(qflag = n()) %>% 
  filter(qflag_prcp != " ") %>%  
  group_by(qflag_prcp) %>%    
  summarise(qflag = n()) %>% 
  ungroup()    
  
# quality flags are D, I, K, L, O flags  
# D     = failed duplicate check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# O     = failed climatological outlier check            
  
sta_check <- sta_dv_orig %>%  
  group_by(sta, sflag_prcp) %>%  
  summarise(sflag = n()) %>% 
  filter(sflag_prcp != " ") %>%  
  group_by(sflag_prcp) %>%    
  summarise(sflag = n()) %>% 
  ungroup()    
  
# flags are 0, 7, B, H, K, W, X, Z  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder  
# A  = U.S. Automated Surface Observing System (ASOS) real-time data  
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# H  = High Plains Regional Climate Center real-time data            
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
  
rm(sta_check)   
  
```  

```{r fill-prcp-na, eval=FALSE}
  
# create a df of monthly precipitation values-NW-RAP---- 
station <- "RAP"  
sta_fil <- sta_dv_alt %>%  
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_dv_alt   %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     
  
sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   
  
# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^RAPID CITY 4")) %>%        
  filter(!is.na(prcp))    
  
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  
  
sta_fil <- bind_rows(sta_alt, sta_fil)   
  
sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    
  
# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ELM")) %>%        
  filter(!is.na(prcp))                             
  
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   
  
sta_fil <- bind_rows(sta_alt, sta_fil)   
  
sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   
  
# make a filled prcp df & clean up     
sta_rap <- sta_fil  
  
rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     
  
# create a df of monthly precipitation values-NC-COT----  
station <- "COT"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^MILES")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^INTERIOR")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# get alternate site data & check if sta_mis_day = 0  
# Phillip & Plainview were big zeros
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^DUPREE")) %>%        
  filter(!is.na(prcp))  

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# make a filled prcp df & clean up      
sta_cot <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-NE-ONI---- 
station <- "ONI"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^KENNE")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^PIERRE")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a summary & monthly prcp df & clean up       
sta_oni <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SW-OEL----   
station <- "OEL"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ORAL")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_oel <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SC-GOR----  
station <- "GOR"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^VALENTINE NWR")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^MULLEN")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ELLSWORTH")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_gor <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SW-MIS----  
station <- "MIS"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^WOOD")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^VALENTINE MILLER")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_mis <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# join the filled data & clean up----  
sta_dv_fill <- bind_rows(sta_rap,  
                         sta_cot,  
                         sta_oni,  
                         sta_oel,  
                         sta_gor,  
                         sta_mis  
                         )  

rm(sta_rap,  
   sta_cot,  
   sta_oni,  
   sta_oel,  
   sta_gor,  
   sta_mis, 
   station  
)  

# create monthly data from the daily data----    
sta_mon <- sta_dv_fill          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year   = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

# export and clean-up data 
export(sta_dv_fill, "data/sta_dv_fill.csv")    
export(sta_mon, "data/sta_mon.csv")  

rm(sta_dv_alt,  
   sta_dv_fill)

```  

```{r import_streamflow_data, eval=FALSE}   
  
# import full streamflow records----   
#gage_mon_full <- import("data/gage_mon_full.csv")  
#gage_meta <-  import("data/gage_meta.csv")  
  
#gage_eco <- gage_mon_full %>% 
#  select(sta, ecoreg) %>% 
#  distinct()  
  
# add ecoregions 
#lmom_gage_meta <- left_join(gage_meta, gage_eco, 
#                            by = "sta")  

#lmom_gage_meta <- right_join(gage_meta, gage_eco, 
#                            by = "sta")  
  
#rm(gage_eco)    
  
```

# use bootstrapping to check precip differences 
```{r bootstrap_precip}  
  
# Construct bootstrap confidence intervals of precipitation----   
#   to identify differences in means   
# Monthly precipitation data is from 1954 to 2018 = 65 years  
#   Split data into two groups of 30 years & test for differences  
  
# Notes on dabest and memory -- see setup for how to increase memory  
# dabest for 14,027 obs; 40,000 reps takes ~2 hours.    
#   Still had memory allocation issues @ 50,000 reps after increasing memory.   
  
# import final monthly precip data & prepare for bootstrapping====   
sta_mon <- import("data/sta_mon.csv") %>%  
  mutate(date = ymd(date)) %>%  
  mutate(yr = year(date)) %>%     
  
  # create an id variable  
  mutate(Period = case_when(   
    yr < 1989 ~ "<1989",  
    TRUE ~ ">1989")  
  ) %>%  
  # create a grouping variable   
  unite("group",   
        c("sta", "Period"),  
        sep = "",   
        remove = FALSE) %>%   
  arrange(date)   
  
# print a summary of the data====  
sta_mon %>%   
  group_by(group) %>%   
  summarise(count = n()) %>%   
  ungroup() %>%   
  mutate(num_yr = count/(12))  
  
# construct bootstrap confidence interval of prcp====   
prcp_dabest <- sta_mon %>%   
  dabest(x = group,             # grouping variable  
         y = prcp,              # measurement variable   
# list order shows control as first group on the list   
         idx = list(c("RAP<1989",  
                      "OEL<1989",  
                      "COT<1989",  
                      "GOR<1989",  
                      "ONI<1989",  
                      "MIS<1989"),   
                    c("RAP>1989",  
                      "OEL>1989",  
                      "COT>1989",   
                      "GOR>1989",  
                      "ONI>1989",  
                      "MIS>1989")  
         ),  
         paired    = TRUE,  
         reps      = 20000,  
         id.column = group   
  )  
  
# plot the dabest bootstrap====   
plot(prcp_dabest,  
     color.column = Period,  
     tick.fontsize = 5,  
     rawplot.type = "sinaplot",  
     axes.title.fontsize = 9,  
     rawplot.ylabel = "Monthly precipitation (mm)",  
     rawplot.groupwidth = 0.3,  
     palette = "Greys")  
  
# save results====   
ggplot2::ggsave(filename = "figure/prcp_dif.png",   
                width = 6.5, height = 4.5, units = "in")  
  
rm(prcp_dabest)    
```

<!--
need to finish this 
```{r ci_result_table, eval=FALSE}  

# get mean precipitation 
prcp <- spi_fin %>%  
  group_by(group) %>%  
  summarise(prcp = mean(prcp)) %>% 
  ungroup() 

rap_bef <- prcp %>% 
  filter(group == "RAP<1989") %>% 
  select(-group) %>% 
  as.double()

# pluck results for summary tables & bind====
prcp_results <- prcp_dabest %>% 
  pluck("result") %>% 
  select(-c(func,  
            paired,  
            variable,  
            bootstraps,  
            nboots,  
            pct_ci_low,  
            pct_ci_high  
  )  
  )   

prcp_results <- full_join(prcp_results, prcp, 
                  by = c("test_group" = "group")  
                  ) %>% 
  filter(test_group != "RAP<1989") %>% 
  mutate(control_group = case_when(  
    is.na(control_group) ~ "RAP<1989",  
    TRUE ~ control_group  
    )
  ) %>%  
  mutate(difference = case_when(  
    is.na(difference) ~ prcp - rap_bef,  
    TRUE ~ difference  
    )
  ) %>% 
  mutate(ci = as.integer(ci))  %>% 
  mutate_if(is.numeric, round, digits = 0)  


# convert tibble to a flextable after fixing vars for presentation==== 
prcp_table <- prcp_table %>% 
  flextable() %>% 
  #  colformat_num(col_keys = col_key_num, 
  #                big.mark=",", 
  #                digits = 1, na_str = "N/A") %>% 
  set_header_labels(control_group = "Control Group", 
                    test_group = "Test Groups", 
                    control_size = "Control Size",
                    test_size = "Test Size", 
                    func = "Test Statistic", 
                    variable = "Variable",
                    difference = "Mean Difference", 
                    ci = "CI", 
                    bca_ci_low = "Lower Limit", 
                    bca_ci_high = "Upper Limit") %>% 
  autofit() %>% 
  align(., part = "all", align = "center") %>% 
  theme_booktabs() 


ci_table2 <- read_docx() %>% 
  body_add_flextable(value = ci_table)  

print(ci_table2, target = "output/ci_table.docx") 

rm(ci_results_pc1, ci_results_pc2, ci_table2, ci_table) 

```  
-->

```{r compare_precip_locations}  
   
# the bootstrapping showed there is some difference between past & present   
sta_pres <- sta_mon %>%   
  filter(yr > 1988)   
  
export(sta_pres, "data/sta_pres.csv")  
  
# comparison of locations----   
# split out northwest & southeast to prepare for plot    
nw <- sta_pres %>%  
  filter(sta == "COT" |  
         sta == "RAP" |  
         sta == "OEL") %>%  
  select(sta, date, prcp) %>%  
  mutate(prcp = prcp + 1) %>%  
  spread(sta, prcp) %>%   
  gather(other, prcp, -c(date, COT)) %>%  
  mutate(location = "COT") %>%  
  rename(control = COT)   
  
se <- sta_pres %>%  
  filter(sta == "GOR" |  
         sta == "ONI" |  
         sta == "MIS") %>%  
  select(sta, date, prcp) %>%  
  mutate(prcp = prcp + 1) %>%  
  spread(sta, prcp) %>%   
  gather(other, prcp, -c(date, GOR)) %>%  
  mutate(location = "GOR")  %>%  
  rename(control = GOR)  
  
# join splits & create factor     
prcp_compare <- bind_rows(nw, se) %>%  
  mutate(diff = control - prcp) %>%    
  mutate(other = fct_relevel(other,  
                           "RAP",  
                           "OEL",  
                           "ONI",  
                           "MIS"  
  ))  
  
# plot location differences  
prcp_compare %>%   
ggplot(aes(date, diff)) +  
  theme_bw() +  
  xlab("") +  
  ylab("Monthly precipitation difference (mm)") +  
  facet_grid(cols    = vars(location),  
             rows    = vars(other)) +  
  geom_line(color    = "gray60") +  
  geom_smooth(method = "lm",  
              color  = "gray30",  
              size   = 0.5  
              )  
  
ggplot2::ggsave(filename = "figure/prcp_loc_dif.png",   
                width = 6.5, height = 4.5, units = "in")  

rm(nw,  
   se,  
   prcp_compare  
   )  

rm(sta_mon)  
  
```

# L-moments 
```{r lmoments_prcp, eval=FALSE}

sta_pres     <- import("data/sta_pres.csv")  
sta_meta_fin <- import("data/sta_meta_fin.csv")

# Note that we might consider Weiss 1964 bias value of 1.018 for L1   
  
# Calculate L-moment ratios   
lmom_sta <- sta_pres %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$prcp)) %>%    
  transpose() %>%  
  as_tibble() %>%  
  select(lambdas, ratios) %>%  
  mutate(lambdas = map(lambdas,   
                       ~as_tibble(t(.x))  
  )) %>%   
  mutate(lambdas = map(lambdas,    
                       ~set_names(.x,   
                                  c("L1", "L2", "L3", "L4", "L5")  
                       )  
  )) %>%   
  mutate(ratios = map(ratios,   
                      ~as_tibble(t(.x)    
                      )   
  )) %>%  
  mutate(ratios = map(ratios,  
                      ~set_names(.x,  
                                 c("T1", "T2", "T3", "T4", "T5")   
                      )   
  )) %>%  
  unnest(lambdas) %>%  
  unnest(ratios)  %>%   
  select(-T1) %>%  
  rename(L_CV = T2) %>%  
  rename(L_skew = T3) %>%  
  rename(L_kurtosis = T4) %>%  
  select(L1, L_CV, L_skew, L_kurtosis) %>%  
  mutate(L1 = round(L1,  
                    digits = 2)  
  )  
  
# get station names from the list  
lmom_sta_nm <- sta_pres %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$prcp)) %>%  
  transpose()  
  
lmom_sta_nm <- lmom_sta_nm %>%  
  pluck(1) %>%  
  enframe()  
  
lmom_sta_nm <- lmom_sta_nm %>%  
  unnest(value)  
  
lmom_sta_nm <- lmom_sta_nm %>%  
  group_by(name) %>%  
  summarize(L1 = first(value)) %>%  
  ungroup() %>%  
  mutate(L1 = round(L1,  
                    digits = 2))  
  
# join name to lmom vals & to the metadata  
lmom_sta <- full_join(lmom_sta_nm, lmom_sta,  
                      by = "L1") %>%  
  rename(sta = name) %>%  
  mutate(L_CV = round(L_CV,  
                      digits = 2)  
  ) %>%  
  mutate(L_skew = round(L_skew,  
                        digits = 2)  
  ) %>%  
  mutate(L_kurtosis = round(L_kurtosis,  
                            digits = 2)  
  )  
  
# join the lmoments to the metadata   
sta_meta_lmom <- full_join(sta_meta_fin, lmom_sta,   
                           by = "sta")   

export(sta_meta_lmom, "data/sta_meta_lmom.csv")    
   
rm(sta_meta_fin,   
   lmom_sta_nm,  
   sta_pres  
   )   
  
```

```{r lmoments_streamflow, eval=FALSE} 
  
# import full streamflow records----   
gage_mon_full <- import("data/gage_mon_full.csv")  
gage_meta <-  import("data/gage_meta.csv")  
  
# Note -- we might consider Weiss 1964 bias value of 1.018 for L1     
  
# Calculate L-moment ratios   
lmom_gage <- gage_mon_full %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$log_q1_mon)) %>%    
  transpose() %>%  
  as_tibble(rownames = sta) %>%  
  select(lambdas, ratios) %>%  
  mutate(lambdas = map(lambdas,   
                       ~as_tibble(t(.x))  
  )) %>%  
  mutate(lambdas = map(lambdas,   
                       ~set_names(.x,  
                                  c("L1", "L2", "L3", "L4", "L5")  
                       )  
  )) %>%  
  mutate(ratios = map(ratios,  
                      ~as_tibble(t(.x)   
                      )   
  )) %>%  
  mutate(ratios = map(ratios,  
                      ~set_names(.x,  
                                 c("T1", "T2", "T3", "T4", "T5")   
                      )   
  )) %>%  
  unnest(lambdas) %>%  
  unnest(ratios)  %>%   
  select(-T1) %>%  
  rename(L_CV = T2) %>%  
  rename(L_skew = T3) %>%  
  rename(L_kurtosis = T4) %>%  
  select(L1, L_CV, L_skew, L_kurtosis) %>%  
  mutate(L1 = round(L1,  
                    digits = 2)  
  )  
  
# get station names from the list  
lmom_gage_list <- gage_mon_full %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$log_q1_mon)) %>%  
  transpose()  
  
lmom_gage_nm <- lmom_gage_list %>%  
  pluck(2) %>%                     # plucks lmom-ratios  
  enframe()  
  
lmom_gage_nm <- lmom_gage_nm %>%  
  unnest(value) %>%  
  drop_na() %>%  
  rename(sta = name) %>%  
  group_by(sta) %>%  
  summarize(L_CV = first(value))  
  
# join name to lmom vals & to the metadata  
lmom_gage <- full_join(lmom_gage_nm, lmom_gage,  
                        by = ("L_CV"))  
  
# check value  
bad_fpi <- tibble(lmom_gage_list[["ratios"]][["bad_fpi"]])  
  
# clean up df & join with metadata====  
lmom_gage <- lmom_gage %>%  
  gather(type, value, -sta) %>%  
  mutate(value = round(value,  
                       digits = 2)  
         ) %>%   
  spread(type, value) %>%  
  select(sta, L1, L_CV, L_skew, L_kurtosis)  
  
lmom_gage <- full_join(gage_meta, lmom_gage,    
                            by = "sta") %>%  
  drop_na()  
  
# check stations for discordance & prepare for join====    
dischord <- lmrdiscord(site = lmom_gage$sta,  
                       #                       Dcrit = 2.5,  
                       digits = 2,  
                       t2 = lmom_gage$L_CV,  
                       t3 = lmom_gage$L_skew,   
                       t4 = lmom_gage$L_kurtosis  
) %>%  
  mutate(site = as.character(site)) %>%  
  rename(sta = site) %>%  
  rename(L_CV = t2) %>%  
  rename(L_skew = t3) %>%  
  rename(L_kurtosis = t4)  
  
lmom_gage <- full_join(lmom_gage, dischord, 
                  by = c("sta", "L_CV", "L_skew", 'L_kurtosis'))  
  
export(lmom_gage, "data/gage_meta_lmom.csv")   
  
# clean up  
rm(gage_meta,  
   bad_fpi,  
   lmom_gage_list,  
   lmom_gage_nm,  
   lmom_gage,  
   gage_mon_full,  
   dischord  
   )  
  
```

```{r Lmoment_diagram_ratios}
# extract elements from the lmrdia list to plot in ggplot2        
#   the x-value is the L-skewness and y-value is L-kurtosis    
  
# get vals from the lmrdia list   
# note that as gamma distribution is a 2-parameter dist, it is not shown  
lmrdia <- lmrdia()   
  
# extract L-skew & L-kurtosis values for several distributions   
#   note:  aep4 <- lmrdia %>% extract2(2) %>% as.tibble()    
  
gev <- lmrdia[[5]]  %>%  
  as_tibble() %>%  
  mutate(distribution = "GEV")      
  
glo <- lmrdia[[6]]  %>%   
  as_tibble() %>%          
  mutate(distribution = "GLO")  
  
gpa <- lmrdia[[7]]  %>%   
  as_tibble() %>%         
  mutate(distribution = "GPA")   
  
gno <- lmrdia[[9]]  %>%   
  as_tibble() %>%      
  mutate(distribution = "GNO")  
  
gov <- lmrdia[[10]] %>%    
  as_tibble() %>%     
  mutate(distribution = "GOV")  
  
pe3 <- lmrdia[[12]] %>%   
  as_tibble() %>%         
  mutate(distribution = "PE3")     
  
lmom_theo <- bind_rows(gev,   
                  glo,   
                  gpa,   
                  gno,   
                  gov,   
                  pe3   
                  ) %>%  
  rename(L_skew = V1) %>%  
  rename(L_kurtosis = V2)  
  
rm(gev,   
   glo,   
   gpa,   
   gno,   
   gov,   
   pe3,   
   lmrdia   
   )   
  
```  

# need to improve the plot with adding high discord gages 
```{r plot-lmoment-diagram} 
  
# prepare station data for plot====  
lmom_sta <- import("data/sta_meta_lmom.csv") %>%  
  select(sta,  
         L1,  
         L_CV,  
         L_skew,  
         L_kurtosis  
  ) %>%  
  mutate(flow_regime = "precipitation") %>%  
  mutate(ecoreg = "NA") %>%  
  mutate(type = "sta")  
  
# prepare gage data for plot====       
# drop discordant stations & calculate ecoregion vals  
lmom_gage <- import("data/gage_meta_lmom.csv") 

discord <- lmom_gage %>% 
  filter(isD == TRUE)  

lmom_gage <- lmom_gage %>%  
  select(sta,  
         flow_regime,  
         ecoreg,  
         L1,   
         L_CV,   
         L_skew,   
         L_kurtosis,  
         isD   
  ) %>%  
  filter(isD == FALSE)  %>% 
  select(-isD) %>%  
  mutate(type = "gage")  

# calculate ecoregion values  
lmom_ecoreg <- lmom_gage %>%  
  group_by(ecoreg) %>%  
  summarise(L1         = mean(L1),  
            L_CV       = mean(L_CV),  
            L_skew     = mean(L_skew),  
            L_kurtosis = mean(L_kurtosis),  
            n          = n()  
            ) %>%  
  ungroup() %>%  
  gather(key, val, -ecoreg) %>%  
  mutate(val           = round(val, digits = 2)) %>%  
  spread(key, val) %>%  
  select(ecoreg, L1, L_CV, L_skew, L_kurtosis) %>%  
  mutate(sta           = NA) %>%  
  mutate(flow_regime   = "ecoregion mean") %>%  
  mutate(type          = "gage")  
  
# join station & individual gage data  
lmom_data <- bind_rows(lmom_gage,  
                       lmom_sta,   
                       lmom_ecoreg  
)  
  

# refactor levels and provide label names====   
lmom_data <- lmom_data %>%   
  mutate(ecoreg = fct_relevel(ecoreg,   
                              "NA",   
                              "Sand Hills",   
                              "Keya Paha Tablelands",  
                              "Black Hills Plateau",  
                              "White River Badlands",  
                              "Pine Ridge Escarpment",  
                              "Pierre Shale Plains")    
  ) %>%   
  mutate(flow_regime = fct_relevel(flow_regime,   
                                   "precipitation",   
                                   "perennial",   
                                   "intermittent",   
                                   "ecoregion mean"  
  ))    
  
type_labels <- c(  
  gage = "stream gages",  
  sta = "weather stations"  
)  
  
# plot the theoretical distributions, and sample vals----    
ggplot(lmom_data,    
       aes(x = L_skew,  
           y = L_kurtosis  
       )) +  
  labs(x = "L-skew",  
       y = "L-kurtosis"  
  ) +   
  xlim(-0.4, 0.4) +  
  ylim(-0.1, 0.3) +  
  theme_bw() +    
  theme(legend.title      = element_text(size = 10),  
        legend.text       = element_text(size = 8),  
        legend.key.height = unit(0.3, 'cm'),  
        legend.spacing    = unit(0.1, "cm")) +  
  guides(linetype         = guide_legend(order = 1),  
         color            = guide_legend(order = 2),   
         shape            = guide_legend(order = 3)  
  ) +     
  facet_grid(cols     = vars(type),   
             rows     = NULL,  
             scale    = "fixed",   
             labeller = labeller(  
               type     = type_labels  
             )  
  ) +  
  # add l-moments 
  geom_line(data = lmom_theo,  
            size = 0.5, 
            color = "gray30", 
            aes(L_skew,  
                L_kurtosis,  
                group = distribution,  
                linetype = distribution
            )) + 
  scale_linetype_discrete(name = "Distribution") + 
  # add individual station data 
  geom_point(
    size = 2, 
    aes(shape = flow_regime,   
        color = ecoreg
    )) + 
  scale_shape_discrete(name = "Type") +
  scale_colour_grey(
    breaks = rev(
      levels(lmom_data$ecoreg)  
    ), 
    guide = "legend", 
    start      = 0.0,    
    end        = 0.6,    
    na.value   = "red",   
    aesthetics = "colour",  
    name       = 'Ecoregion'  
  ) 
  
# save plot & table====  
ggplot2::ggsave(filename = "figure/lmom_plot.png",  
                width = 6, height = 3.6, units = "in")  
  
export(lmom_data, "data/lmom_table.csv")  
  
# clean-up====   
rm(lmom_theo,   
   lmom_gage,   
   lmom_ecoreg,    
   lmom_sta,   
   type_labels,  
   lmom_gage_meta,  
   sta_meta_lmom,  
   lmom_data   
)        
  
```  

# calculate SCI  
```{r identify_representive_gages} 
  
# import streamflow records & metadata----     
gage_mon <- import("data/gage_mon_full.csv")  
  
# remove discordant stations & discord calcs  
gage_meta <- import("data/gage_meta_lmom.csv") %>%  
  filter(isD == FALSE) %>%  
  select(-c(Dmax:Dcrit, isD, signif))  
  
gage_mon <- semi_join(gage_mon, gage_meta,  
                      by = c("sta", "ecoreg"))  


# get mean L-statistic values from discord calculations  
gage_mean <-  import("data/lmom_table.csv") %>%  
  filter(sta == "") %>%  
  select(ecoreg, L_CV, L_skew, L_kurtosis) %>%  
  rename(L_CV_mean   = L_CV) %>%   
  rename(L_skew_mean = L_skew) %>%  
  rename(L_kurt_mean = L_kurtosis)  
  
# find distance from centers  
gage_meta <- left_join(gage_meta, gage_mean,  
                       by = "ecoreg") %>%  
  mutate(L_CV_diff   = L_CV - L_CV_mean) %>%  
  mutate(L_skew_diff = L_skew - L_skew_mean) %>%  
  mutate(L_kurt_diff = L_kurtosis - L_kurt_mean) %>%  
  mutate(D2 = round(digits = 2,  
                    sqrt(L_CV_diff^2 + L_skew_diff^2 + L_kurt_diff^2)  
                    )  
         ) %>%  
  arrange(D2)  
  
# select gages by ecoreg by distance from centroid====     
#gage_pick <- gage_meta %>%   
#  group_by(ecoreg) %>%  
#  summarise(sta    = first(sta),  
#            L1     = first(L1),  
#            L_CV   = first(L_CV),  
#            L_skew = first(L_skew),  
#            L_kurt = first(L_kurtosis),  
#            D      = first(D),  
#            D2     = first(D2)  
#  )  
#gage_pick <- semi_join(gage_mon, gage_pick,  
#                        by = "sta") %>%  
  
# create a 30-year record for input  
sri_input <- gage_mon %>% 
  mutate(yr = year(Date)) %>%  
  filter(yr > 1988)     
  
# export selection and clean-up  
export(sri_input, "data/sri_input.csv")  
  
rm(sri_input,  
   gage_mean,  
   gage_meta,  
#   gage_pick,    
   gage_mon
  )   
  
```

```{r SCI_function, eval=FALSE} 
  
# fitSCI identifies Standardized Climate Index (SCI) parameters----       
#   some notes about the SCI package:      
#     the SCI package doesn't like snake_case variables,     
#     need to change tibble to a vector as double:    
#       cot <- as.double(sta_cot$depth_mm)      
  
# Initial values for SCI calculations====       
time_scale <- 1     # sets the length of the averaging period    
distrib    <- "pe3" # sets the distribution type    
p_zero     <- TRUE  # sets a function to reduce zero-precip bias   
p_zero_cm  <- TRUE  # uses Weibull plotting position for p_zero     
scale      <- "sd"  # scales input by subtract mean & divide by sd     
warn_me    <- TRUE  # sets explicit warning    
first_mon  <- 1     # Set first month for each station   
sci.limit  <- 3.5     # Sets a limit of [-3, 3] for limit   
  
# sci function====                                            
sci.fun <- function(sta, time_scale) fitSCI( x = sta,    
                                             start.fun.fix  = TRUE,    
                                             time.scale     = time_scale,    
                                             first.mon      = first_mon,   
                                             distr          = distrib,   
                                             p0             = p_zero,     
                                             p0.center.mass = p_zero_cm,     
                                             scaling        = scale,     
                                             warn	         = warn_me     
)     
  
```  

```{r prepare_calculate_SPI, eval=FALSE}
  
# prepare sci function vector inputs (1989-01 to 2017-01)----     
# north-west (NW)   
sta_rap <- sta_pres %>%   
  arrange(date) %>%   
  filter(sta == "RAP")   
  
rap <- as.double(sta_rap$prcp)   
  
# north-central (NC)  
sta_cot <- sta_pres %>%   
  arrange(date) %>%  
  filter(sta == "COT")   
  
cot <- as.double(sta_cot$prcp)   
  
# north-east (NE)  
sta_oni <- sta_pres %>%  
  arrange(date) %>%   
  filter(sta == "ONI")    
  
oni <- as.double(sta_oni$prcp)   
  
# south-west (SW)   
sta_oel <- sta_pres %>%  
  arrange(date) %>%  
  filter(sta == "OEL")   
  
oel <- as.double(sta_oel$prcp)   
  
# south-central (SC)  
sta_gor <- sta_pres %>%  
  arrange(date) %>%  
  filter(sta == "GOR")    
  
gor <- as.double(sta_gor$prcp)   
  
# south-east (SE)  
sta_mis <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "MIS")   
  
mis <- as.double(sta_mis$prcp)   
  
# calculate SCI-rap----  
# set up the station for sci & make sci list vars====   
sta      <- rap  
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                             
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)   
  
# Apply the transformation identified by fitSCI function====   
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,    
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# bind and rename the sci vals====      
sci_rap <- bind_cols(sta_rap,     
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%      
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# clean up global environment====  
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(sta_rap, rap)   
  
# calculate SCI-cot----   
# set up the station for sci & make sci list vars   
sta      <- cot  
sci_1mo  <- sci.fun(sta, 1)                               
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)   
sci_9mo  <- sci.fun(sta, 9)   
sci_12mo <- sci.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function    
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the sci vals     
sci_cot <- bind_cols(sta_cot,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo   
) %>%    
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%   
  rename(sci_12mo = value6)  
  
# clean up global environment  
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )     
rm(sta_cot, cot)  
  
# calculate SCI-oni----   
# set up the station for sci & make sci list vars     
sta      <- oni    
sci_1mo  <- sci.fun(sta, 1)                              
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)   
sci_6mo  <- sci.fun(sta, 6)   
sci_9mo  <- sci.fun(sta, 9)   
sci_12mo <- sci.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function      
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the sci vals     
sci_oni <- bind_cols(sta_oni,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,  
                     sci_9mo,  
                     sci_12mo  
) %>%  
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# clean up global environment  
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )    
rm(sta_oni, oni)  
  
# calculate SCI-oel----  
# set up the station for sci & make sci list vars   
sta      <- oel  
sci_1mo  <- sci.fun(sta, 1)                              
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)   
sci_6mo  <- sci.fun(sta, 6)   
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function       
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_1mo) %>%  
  enframe(name = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_2mo) %>%  
  enframe(name = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_3mo) %>%  
  enframe(name = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_4mo) %>%  
  enframe(name = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_6mo) %>%  
  enframe(name = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_9mo) %>%  
  enframe(name = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = sci_12mo) %>%  
  enframe(name = NULL)  
  
# bind and rename the sci vals   
sci_oel <- bind_cols(sta_oel,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,   
                     sci_9mo,    
                     sci_12mo  
) %>%   
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)      
  
# clean up global environment        
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )    
rm(sta_oel, oel)    
  
# calculate SCI-gor----  
# set up the station for sci & make sci list vars   
sta      <- gor    
sci_1mo  <- sci.fun(sta, 1)                              
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)   
sci_6mo  <- sci.fun(sta, 6)   
sci_9mo  <- sci.fun(sta, 9)   
sci_12mo <- sci.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function        
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the sci vals      
sci_gor <- bind_cols(sta_gor,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo     
) %>%  
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)    
  
# clean up global environment    
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,   
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo   
   )    
rm(sta_gor, gor)    
  
# calculate sci-mis----  
# set up the station for sci & make sci list vars   
sta      <- mis  
sci_1mo  <- sci.fun(sta, 1)                              
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)   
sci_9mo  <- sci.fun(sta, 9)   
sci_12mo <- sci.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function     
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the sci vals     
sci_mis <- bind_cols(sta_mis,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo   
) %>%  
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6) 
  
# clean up global environment  
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo   
   )   
rm(sta_mis, mis)      
  
# join sci data----   
sci_sta <- bind_rows(  
  sci_rap,  
  sci_cot,  
  sci_oni,  
  sci_oel,  
  sci_gor,  
  sci_mis  
) %>%  
  select(-c(Period, group)) %>%  
    # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, yr, prcp)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# export sci data        
export(sci_sta, "data/sci_sta.csv")  
  
# clean up workspace    
rm(distrib,  
   first_mon,   
   p_zero,   
   p_zero_cm,   
   scale,   
   sci.limit,   
   sta,   
   time_scale,   
   warn_me,  
   sci.fun,  
   sci_cot,  
   sci_gor,  
   sci_mis,  
   sci_oel,  
   sci_oni,  
   sci_rap,  
   sta_pres  
   )         
  
```  

```{r prepare_SRI, eval=FALSE} 
  
# import and check gage data (1989-01 to 2017-01)----    
sri_input <- import("data/sri_input.csv") %>%    
  rename(date = Date) %>%   
  arrange(date) %>%  
  select(  
    sta,  
    ecoreg,  
    date,  
    log_q1_mon,  
    yr  
  )  
  
sri_check <- sri_input %>%  
  group_by(sta) %>%  
  summarise(min_yr = min(yr), 
            max_yr = max(yr)  
  )  
  
sri_check <- sri_input %>%  
  filter(yr == "1989") %>%  
  mutate(mon = month(date)) %>%   
  arrange(date) %>%              
  group_by(sta, ecoreg) %>%  
  summarise(min_mon = min(mon), 
            max_mon = max(mon)  
  ) %>%  
  ungroup() %>%  
  arrange(ecoreg)  
  
sri_input <- sri_input %>%  
  select(-yr)  
  
# prepare vector inputs as double for SCI fun----   
# Black Hills Plateau (bhp) gages====    
bhp_bat <- sri_input %>%  
  filter(sta == "bat_her")    
  
bhp_bev <- sri_input %>%  
  filter(sta == "bev_buf")  
  
bhp_fal <- sri_input %>%  
  filter(sta == "fal_hot")   
  
bhp_frn <- sri_input %>%  
  filter(sta == "frn_fai")    
  
bhp_bat_v <- as.double(bhp_bat$log_q1_mon)   
bhp_bev_v <- as.double(bhp_bev$log_q1_mon)  
bhp_fal_v <- as.double(bhp_fal$log_q1_mon)  
bhp_frn_v <- as.double(bhp_frn$log_q1_mon)  
  
# Keya Paha Tablelands (kpt) gages====   
kpt_key <- sri_input %>%   
  filter(sta == "key_key")   
  
kpt_wew <- sri_input %>%  
  filter(sta == "key_wew")   
  
kpt_key_v <- as.double(kpt_key$log_q1_mon)   
kpt_wew_v <- as.double(kpt_wew$log_q1_mon)   
  
# Pine Ridge Escarpment (pre) gages====   
pre_ogl <- sri_input %>%  
  filter(sta == "whi_ogl")   
  
pre_sta <- sri_input %>%  
  filter(sta == "whi_sta")   
  
pre_ogl_v <- as.double(pre_ogl$log_q1_mon)   
pre_sta_v <- as.double(pre_sta$log_q1_mon)   
  
# Pierre Shale Plains (psp) gages====     
psp_bat <- sri_input %>%  
  filter(sta == "bat_bhr")    
  
psp_brs <- sri_input %>%  
  filter(sta == "brsf_co")    
  
psp_che <- sri_input %>%  
  filter(sta == "che_was")    
  
psp_elk <- sri_input %>%  
  filter(sta == "elk_elm")    
  
psp_hat <- sri_input %>%  
  filter(sta == "hat_edg")    
  
psp_whi <- sri_input %>%  
  filter(sta == "whi_oac")    
  
psp_bat_v <- as.double(psp_bat$log_q1_mon)   
psp_brs_v <- as.double(psp_brs$log_q1_mon)   
psp_che_v <- as.double(psp_che$log_q1_mon)   
psp_elk_v <- as.double(psp_elk$log_q1_mon)   
psp_hat_v <- as.double(psp_hat$log_q1_mon)   
psp_whi_v <- as.double(psp_whi$log_q1_mon)  
   
# Sand Hills (snd) gages====   
snd_lcr <- sri_input %>%  
  filter(sta == "lcr_bel")     
  
snd_lon <- sri_input %>%  
  filter(sta == "lon_riv")     
  
snd_mar <- sri_input %>%  
  filter(sta == "lwr_mar")     
  
snd_ros <- sri_input %>%  
  filter(sta == "lwr_ros")     
  
snd_vet <- sri_input %>%  
  filter(sta == "lwr_vet")     
  
snd_whi <- sri_input %>%  
  filter(sta == "lwr_whi")    
  
snd_nio <- sri_input %>%  
  filter(sta == "nio_spa")     
  
snd_lcr_v <- as.double(snd_lcr$log_q1_mon)   
snd_lon_v <- as.double(snd_lon$log_q1_mon)   
snd_mar_v <- as.double(snd_mar$log_q1_mon)   
snd_ros_v <- as.double(snd_ros$log_q1_mon)   
snd_vet_v <- as.double(snd_vet$log_q1_mon)   
snd_whi_v <- as.double(snd_whi$log_q1_mon)   
snd_nio_v <- as.double(snd_nio$log_q1_mon)   
  
# White River Badlands (bad) gages====     
bad_whi <- sri_input %>%  
  filter(sta == "whi_kad")   
  
bad_whi_v <- as.double(bad_whi$log_q1_mon)   
  
# clean up====    
rm(sri_check,  
   sri_input  
)   
  
```  

```{r calculate_SRI}

# calculate SCI_blp----  
# SCI calcs - bha_bat -- set up the station for sci & make sci list vars====  
sta      <- bhp_bat_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                           
sci_4mo  <- sci.fun(sta, 4)     
sci_6mo  <- sci.fun(sta, 6)    # month 12 fail  
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)   
  
# bha_bat -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo, 
                         sci.limit = sci.limit  
                         ) %>%  
  enframe(name                     = NULL)    
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo,
                         sci.limit = sci.limit  
                         ) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo, 
                         sci.limit = sci.limit  
                         ) %>%   
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo, 
                         sci.limit = sci.limit  
                         ) %>%    
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo, 
                         sci.limit = sci.limit  
                         ) %>%    
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo, 
                         sci.limit = sci.limit  
                         ) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo, 
                         sci.limit = sci.limit  
                         ) %>%    
  enframe(name                     = NULL)   
  
# bha_bat -- bind and rename the sci vals====         
sci_bhp_bat <- bind_cols(bhp_bat,     
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,   
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)   
  
# bha_bat -- clean up global environment====  
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(bhp_bat, bhp_bat_v)   
  
# SCI calcs - bhp_bev -- set up the station for sci & make sci list vars====    
sta      <- bhp_bev_v  
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                             
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)   
  

# bha_bev -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# bha_bev -- bind and rename the sci vals====        
sci_bhp_bev <- bind_cols(bhp_bev,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%   
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# bha_bev -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(bhp_bev, bhp_bev_v)   
  
# SCI calcs - bha_fal -- set up the station for sci & make sci list vars====    
sta      <- bhp_fal_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)    # MLE fail for month 4                      
sci_3mo  <- sci.fun(sta, 3)    # MLE fail for month 1-, 5-        
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    # MLE fail for month 6-  
sci_9mo  <- sci.fun(sta, 9)    # MLE fail for month 7-, 8-  
sci_12mo <- sci.fun(sta, 12)   # MLE fail for month 8-, 9-  
  

# bha_fal -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,     
                         first.mon = first_mon,   
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# bha_fal -- bind and rename the sci vals====      
sci_bhp_fal <- bind_cols(bhp_fal,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# bha_fal -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(bhp_fal, bhp_fal_v)   
  
# SCI calcs - bha_frn -- set up the station for sci & make sci list vars====  
sta      <- bhp_frn_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)    # MLE fail for month 8-, 11-                 
sci_3mo  <- sci.fun(sta, 3)           
sci_4mo  <- sci.fun(sta, 4)    # MLE fail for month 1- 
sci_6mo  <- sci.fun(sta, 6)     
sci_9mo  <- sci.fun(sta, 9)      
sci_12mo <- sci.fun(sta, 12)   
  
# bha_frn -- apply the transformation identified by fitSCI function====    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)    
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# bha_frn -- bind and rename the sci vals====      
sci_bhp_frn <- bind_cols(bhp_frn,       
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# bha_frn -- clean up global environment====  
rm(sci_1mo,    
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(bhp_frn, bhp_frn_v)   
  
# calculate SCI_kpt----  
# SCI calcs - kpt_key -- set up the station for sci & make sci list vars====   
sta      <- kpt_key_v  
sci_1mo  <- sci.fun(sta, 1)    # MLW fail for month 11-                      
sci_2mo  <- sci.fun(sta, 2)    # MLE fail for month 10-                        
sci_3mo  <- sci.fun(sta, 3)           
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)     
sci_9mo  <- sci.fun(sta, 9)    # MLE fail for month 2-, 8-       
sci_12mo <- sci.fun(sta, 12)   # MLE fail for month 8-, 10-     
  
# kpt_key -- apply the transformation identified by fitSCI function====     
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# kpt_key -- bind and rename the sci vals====      
sci_kpt_key <- bind_cols(kpt_key,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# kpt_key -- clean up global environment====  
rm(sci_1mo,    
   sci_2mo,   
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(kpt_key, kpt_key_v)   
  
# SCI calcs - kpt_wew -- set up the station for sci & make sci list vars==== 
sta      <- kpt_wew_v  
sci_1mo  <- sci.fun(sta, 1)                       
sci_2mo  <- sci.fun(sta, 2)                        
sci_3mo  <- sci.fun(sta, 3)           
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)     
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# kpt_wew -- apply the transformation identified by fitSCI function====    
sci_1mo  <- transformSCI(sta,     
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   
  
# kpt_wew -- bind and rename the sci vals====      
sci_kpt_wew <- bind_cols(kpt_wew,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# kpt_wew -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(kpt_wew, kpt_wew_v)   
  
# calculate SCI_pre---- 
# SCI calcs - pre_ogl -- set up the station for sci & make sci list vars====   
sta      <- pre_ogl_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 9-, 11-                       
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 11-                     
sci_3mo  <- sci.fun(sta, 3)    # MLE fail month 12-     
sci_4mo  <- sci.fun(sta, 4)    # MLE fail month 2- 
sci_6mo  <- sci.fun(sta, 6)    # MLE fail month 3-, 4-, 5-   
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# pre_ogl -- apply the transformation identified by fitSCI function====     
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# pre_ogl -- bind and rename the sci vals====        
sci_pre_ogl <- bind_cols(pre_ogl,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# pre_ogl -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(pre_ogl, pre_ogl_v)   
  
# SCI calcs - pre_sta -- set up the station for sci & make sci list vars====   
sta      <- pre_sta_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 9-                    
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 10-                     
sci_3mo  <- sci.fun(sta, 3)             
sci_4mo  <- sci.fun(sta, 4)          
sci_6mo  <- sci.fun(sta, 6)            
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# pre_sta -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# pre_sta -- bind and rename the sci vals====       
sci_pre_sta <- bind_cols(pre_sta,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# pre_sta -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(pre_sta, pre_sta_v)  
  
# calculate SCI_psp---- 
# SCI calcs - psp_bat -- set up the station for sci & make sci list vars====   
sta      <- psp_bat_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 10-                    
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 10-, 11-                      
sci_3mo  <- sci.fun(sta, 3)    # MLE fail month 9-, 11-              
sci_4mo  <- sci.fun(sta, 4)    # MLE fail month 8-, 11-            
sci_6mo  <- sci.fun(sta, 6)    # MLE fail month 10-, 11-              
sci_9mo  <- sci.fun(sta, 9)    # MLE fail month 1-            
sci_12mo <- sci.fun(sta, 12)   
  
# psp_bat -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_bat -- bind and rename the sci vals====       
sci_psp_bat <- bind_cols(psp_bat,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_bat -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_bat, psp_bat_v)  
  
# SCI calcs - psp_brs -- set up the station for sci & make sci list vars====   
sta      <- psp_brs_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 4-, 6-, 12-                    
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 4-, 7-                      
sci_3mo  <- sci.fun(sta, 3)    # MLE fail month 1-, 2-               
sci_4mo  <- sci.fun(sta, 4)    # MLE fail month 3-, 8-            
sci_6mo  <- sci.fun(sta, 6)                   
sci_9mo  <- sci.fun(sta, 9)                
sci_12mo <- sci.fun(sta, 12)   # MLE fail month 1-, 11-   
  
# psp_brs -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_brs -- bind and rename the sci vals====       
sci_psp_brs <- bind_cols(psp_brs,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_brs -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_brs, psp_brs_v)  
  
# SCI calcs - psp_che -- set up the station for sci & make sci list vars====   
sta      <- psp_che_v  
sci_1mo  <- sci.fun(sta, 1)                       
sci_2mo  <- sci.fun(sta, 2)                        
sci_3mo  <- sci.fun(sta, 3)                 
sci_4mo  <- sci.fun(sta, 4)               
sci_6mo  <- sci.fun(sta, 6)                   
sci_9mo  <- sci.fun(sta, 9)                
sci_12mo <- sci.fun(sta, 12)      
  
# psp_che -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_che -- bind and rename the sci vals====       
sci_psp_che <- bind_cols(psp_che,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_che -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_che, psp_che_v)  
  
# SCI calcs - psp_elk -- set up the station for sci & make sci list vars====   
sta      <- psp_elk_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail months 1-, 2-, 3-, 6-, 7-, 10-, 12-  
sci_2mo  <- sci.fun(sta, 2)    # MLE fail months 6-, 8-, 9-, 10-, 11-, 12-  
sci_3mo  <- sci.fun(sta, 3)    # MLE fail months 4-, 7-, 9-, 11-             
sci_4mo  <- sci.fun(sta, 4)    # MLE fail months 1-, 2-, 3-, 4-, 7-, 9-, 11-  
sci_6mo  <- sci.fun(sta, 6)    # MLE fail months 1-, 2-, 4-, 6-, 7-, 12-  
sci_9mo  <- sci.fun(sta, 9)    # MLE fail months 1-, 2-, 3-, 8-, 12-                  
sci_12mo <- sci.fun(sta, 12)   # MLE fail months 3-, 5-, 6-, 11-        
  
# psp_elk -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_elk -- bind and rename the sci vals====       
sci_psp_elk <- bind_cols(psp_elk,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_elk -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_elk, psp_elk_v)  
  
# SCI calcs - psp_hat -- set up the station for sci & make sci list vars====   
sta      <- psp_hat_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail months 8-, 9-  
sci_2mo  <- sci.fun(sta, 2)    # MLE fail months 10-  
sci_3mo  <- sci.fun(sta, 3)    # MLE fail months 8-             
sci_4mo  <- sci.fun(sta, 4)    # MLE fail months 2-  
sci_6mo  <- sci.fun(sta, 6)    # MLE fail months 2-   
sci_9mo  <- sci.fun(sta, 9)                
sci_12mo <- sci.fun(sta, 12)   # MLE fail months 3-       
  
# psp_hat -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_hat -- bind and rename the sci vals====       
sci_psp_hat <- bind_cols(psp_hat,       
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_hat -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_hat, psp_hat_v)  
  
# SCI calcs - psp_whi -- set up the station for sci & make sci list vars====   
sta      <- psp_whi_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail months 7-   
sci_2mo  <- sci.fun(sta, 2)      
sci_3mo  <- sci.fun(sta, 3)                
sci_4mo  <- sci.fun(sta, 4)     
sci_6mo  <- sci.fun(sta, 6)     
sci_9mo  <- sci.fun(sta, 9)                 
sci_12mo <- sci.fun(sta, 12)        
  
# psp_whi -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# psp_whi -- bind and rename the sci vals====       
sci_psp_whi <- bind_cols(psp_whi,       
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# psp_whi -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(psp_whi, psp_whi_v)  
  
# calculate SCI_snd---- 
# SCI calcs - snd_lcr -- set up the station for sci & make sci list vars====    
sta      <- snd_lcr_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 5-                  
sci_2mo  <- sci.fun(sta, 2)                       
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    # MLE fail month 5-   
sci_9mo  <- sci.fun(sta, 9)    # MLE fail month 5-     
sci_12mo <- sci.fun(sta, 12)   # MLE fail month 5-, 6- 
  
# snd_lcr -- apply the transformation identified by fitSCI function====      
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_lcr -- bind and rename the sci vals====        
sci_snd_lcr <- bind_cols(snd_lcr,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_lcr -- clean up global environment====   
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_lcr, snd_lcr_v)   
  
# SCI calcs - snd_lon -- set up the station for sci & make sci list vars####  
sta      <- snd_lon_v  
sci_1mo  <- sci.fun(sta, 1)                      
sci_2mo  <- sci.fun(sta, 2)                       
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   # MLE fail month 1-, 6- 
  
# snd_lon -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_lon -- bind and rename the sci vals####      
sci_snd_lon <- bind_cols(snd_lon,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_lon -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_lon, snd_lon_v)   
  
# SCI calcs - snd_mar -- set up the station for sci & make sci list vars####  
sta      <- snd_mar_v  
sci_1mo  <- sci.fun(sta, 1)                      
sci_2mo  <- sci.fun(sta, 2)                       
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# snd_mar -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_mar -- bind and rename the sci vals####      
sci_snd_mar <- bind_cols(snd_mar,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_mar -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_mar, snd_mar_v)    
  
# SCI calcs - snd_nio -- set up the station for sci & make sci list vars####  
sta      <- snd_nio_v  
sci_1mo  <- sci.fun(sta, 1)                      
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 12-                        
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# snd_nio -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_nio -- bind and rename the sci vals####      
sci_snd_nio <- bind_cols(snd_nio,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_nio -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_nio, snd_nio_v)    
  
# SCI calcs - snd_ros -- set up the station for sci & make sci list vars####  
sta      <- snd_ros_v  
sci_1mo  <- sci.fun(sta, 1)    # MLE fail month 10-                     
sci_2mo  <- sci.fun(sta, 2)    # MLE fail month 5-                        
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# snd_ros -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_ros -- bind and rename the sci vals####      
sci_snd_ros <- bind_cols(snd_ros,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_ros -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_ros, snd_ros_v)    
  
# SCI calcs - snd_vet -- set up the station for sci & make sci list vars####  
sta      <- snd_vet_v  
sci_1mo  <- sci.fun(sta, 1)                      
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)     # MLE fail month 5-   
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# snd_vet -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_vet -- bind and rename the sci vals####      
sci_snd_vet <- bind_cols(snd_vet,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_vet -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_vet, snd_vet_v)    
  
# SCI calcs - snd_whi -- set up the station for sci & make sci list vars####  
sta      <- snd_whi_v  
sci_1mo  <- sci.fun(sta, 1)                      
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)       
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)      
sci_9mo  <- sci.fun(sta, 9)         
sci_12mo <- sci.fun(sta, 12)   
  
# snd_whi -- apply the transformation identified by fitSCI function####    
sci_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)   
  
sci_2mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%   
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)    
  
sci_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%   
  enframe(name                     = NULL)   

# snd_whi -- bind and rename the sci vals####      
sci_snd_whi <- bind_cols(snd_whi,      
                     sci_1mo,   
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,   
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo  
) %>%                             
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%   
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%   
  rename(sci_6mo = value4)  %>%   
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# snd_whi -- clean up global environment====  
rm(sci_1mo,   
   sci_2mo,  
   sci_3mo,  
   sci_4mo,   
   sci_6mo,  
   sci_9mo,  
   sci_12mo  
   )      
rm(snd_whi, snd_whi_v)    
  
# calculate SCI-bad----  
# SCI calcs - bad_whi -- set up the station for sci & make sci list vars####  
sta      <- bad_whi_v    
sci_1mo  <- sci.fun(sta, 1)                              
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                            
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    # MLE fail month 1-   
sci_9mo  <- sci.fun(sta, 9)    # MLE fail month 6-   
sci_12mo <- sci.fun(sta, 12)  
  
# bad_whi -- apply the transformation identified by fitSCI function####        
sci_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_1mo) %>%  
  enframe(name                     = NULL)  
  
sci_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_2mo) %>%  
  enframe(name                     = NULL)  
  
sci_3mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_3mo) %>%  
  enframe(name                     = NULL)  
  
sci_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_4mo) %>%  
  enframe(name                     = NULL)  
  
sci_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_6mo) %>%  
  enframe(name                     = NULL)  
  
sci_9mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = sci_9mo) %>%  
  enframe(name                     = NULL)  
  
sci_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = sci_12mo) %>%  
  enframe(name                     = NULL)  
  
# bad_whi -- bind and rename the sci vals####       
sci_bad_whi <- bind_cols(bad_whi,     
                     sci_1mo,  
                     sci_2mo,   
                     sci_3mo,  
                     sci_4mo,  
                     sci_6mo,  
                     sci_9mo,   
                     sci_12mo   
) %>%  
  rename(sci_1mo = value)   %>%  
  rename(sci_2mo = value1)  %>%  
  rename(sci_3mo = value2)  %>%  
  rename(sci_4mo = value3)  %>%  
  rename(sci_6mo = value4)  %>%  
  rename(sci_9mo = value5)  %>%  
  rename(sci_12mo = value6)  
  
# bad_whi -- clean up global environment====    
rm(sci_1mo,  
   sci_2mo,  
   sci_3mo,  
   sci_4mo,  
   sci_6mo,  
   sci_9mo,  
   sci_12mo   
   )   
rm(bad_whi, bad_whi_v)     
  
# clean up sci.fun vars====  
rm(  
  distrib,  
  first_mon,  
  p_zero,  
  p_zero_cm,  
  scale,  
  sci.limit,  
  sta,  
  time_scale,  
  warn_me  
  )   
  
```

```{r join_SRI}
# join sci data----  
# join black hills plateau SCI vals====  
sci_bhp <- bind_rows(sci_bhp_bat,  
                     sci_bhp_bev,  
                     sci_bhp_fal,  
                     sci_bhp_frn    
  )  

sci_bhp <- sci_bhp %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# join keya paha tablelands SCI vals====  
sci_kpt <- bind_rows(sci_kpt_key,  
                     sci_kpt_wew  
  )  

sci_kpt <- sci_kpt %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# join pine ridge excarpment SCI vals====  
sci_pre <- bind_rows(sci_pre_ogl,  
                     sci_pre_sta 
  )  
  
sci_pre <- sci_pre %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# join pierre shale plains SCI vals====  
sci_psp <- bind_rows(sci_psp_bat,  
                     sci_psp_brs,  
                     sci_psp_che,  
                     sci_psp_elk,  
                     sci_psp_hat,  
                     sci_psp_whi      
  )  
  
sci_psp <- sci_psp %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# join sand hills SCI vals====  
sci_snd <- bind_rows(sci_snd_lcr,  
                     sci_snd_lon,  
                     sci_snd_mar,  
                     sci_snd_nio,  
                     sci_snd_ros,  
                     sci_snd_vet,  
                     sci_snd_whi  
                     )  
  
sci_snd <- sci_snd %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    
  
# join white river badlands SCI values====    
sci_bad <- sci_bad_whi  
  
sci_bad <- sci_bad %>%    
  # truncate sci vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, ecoreg, log_q1_mon)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer sci-values to the end of the df    
  select(-sci_12mo, sci_12mo)    

# export sci data====    
sci_gage <- bind_rows(sci_bhp,  
                      sci_kpt,  
                      sci_pre,  
                      sci_psp,  
                      sci_snd,  
                      sci_bad  
                      )     
  
export(sci_gage, "data/sci_gage.csv")  
  
# clean up====  
rm(sci_bhp_bat,  
   sci_bhp_bev,  
   sci_bhp_fal,  
   sci_bhp_frn,  
   sci_kpt_key,  
   sci_kpt_wew,  
   sci_pre_ogl,  
   sci_pre_sta,  
   sci_psp_bat,  
   sci_psp_brs,  
   sci_psp_che,  
   sci_psp_elk,  
   sci_psp_hat,  
   sci_psp_whi,  
   sci_snd_lcr,  
   sci_snd_lon,  
   sci_snd_mar,  
   sci_snd_nio,  
   sci_snd_ros,  
   sci_snd_vet,  
   sci_snd_whi,    
   sci_bad_whi, 
   sci_bad,  
   sci_bhp,  
   sci_kpt,  
   sci_pre,  
   sci_psp,  
   sci_snd,  
   sci.fun  
)  
  
```

# Identify seasonality & trend of SCI data -- plot seasonality 
```{r make_stl-decomp_function}    
  
# make function to decompose into seasonal, trend, & remainder----  
decomp_fun <- function(df) {    
  arrange(df, .data$date) %>%  
    group_by(.data$sta) %>%  
    time_decompose(   
      target         = , TARGET,     
      data           = .,   
      method         = "stl",   
      frequency      = , FREQ,   
      trend          = , TREND,   
      merge          = TRUE,   
      message        = TRUE    
    ) %>%                    
    ungroup() %>%   
    anomalize(remainder,   
              method = "gesd",  
              alpha  = 0.003) %>%    
    select(-observed)    
}   
  
# notes: map_dfr can cause issues with indexing; use split and combine  
#   input needs to be a tibble   
  
```

```{r deconvolute_spi}  
  
# deconvolute the grouped prcp into trend, seasonal & random components----   
  
# import working data====         
#spi_sta <- import("data/spi_sta.csv") %>%    
# mutate(date = ymd(date))   
  
sta_meta_fin <- import("data/sta_meta_fin.csv")      
  
# prepare for deconvoluting the grouped precip====    
spi_sta <- spi_sta %>%   
  mutate(prcp = as.numeric(prcp)) %>%    
  arrange(date) %>%   
  as_tibble() %>%   
  # make groups  
  mutate(group = case_when(  
    sta == "RAP" ~ "NW",    
    sta == "COT" ~ "NC",     
    sta == "OEL" ~ "NE",     
    sta == "GOR" ~ "SW",   
    sta == "ONI" ~ "SC",  
    sta == "MIS" ~ "SE"   
  )   
  )  
  
# identify frequency of seasonality -- should be 12 months====      
#   the level is group rather than station - so need to combine stations  
freq <- spi_sta  %>%     
  split(.$group) %>%   
  map_dfc(~ anomalize::time_frequency(  
    period       = "auto",  
    data = .)   
  ) %>%  
  gather(key     = group,  
         value   = freq) %>%   
  summarise(freq = mean(freq),    
            max  = max(freq),   
            min  = min(freq),   
            sd   = sd(freq)  
  )   
  
#   identify trend of periodicity -- should be 60 months====  
trend <- spi_sta  %>%  
  split(.$group) %>%   
  map_dfc(~ anomalize::time_trend(  
    period       = "auto", data = .)   
  ) %>%  
  gather(key     = grpup, value = freq) %>%   
  summarise(freq = mean(freq),  
            max  = max(freq),  
            min  = min(freq),  
            sd   = sd(freq)  
  )  
  
# decompose prcp into seasonal, trend, & remainder====    
FREQ   <- "12 months"  
TREND  <- "60 months"   
TARGET <- "spi_1mo"  
  
spi_seas <- spi_sta %>%   
  decomp_fun() 
  
# clean up global environment####  
rm(freq,  
   trend,      
   FREQ,  
   TARGET,  
   TREND,  
   decomp_fun  
   )    
  
```  

```{r plot_seasonality_spi} 
# this code chunk is set up for both spi & sri seasonality plotting 
  
# check for anomonies====     
spi_anom <- spi_seas %>%   
  filter(anomaly == "Yes") 
  
sri_anom <- sri_seas %>%   
  filter(anomaly == "Yes") 
  
# vars for plotting====    
sta_size   <- 0.2  
sta_color  <- "black"  
  
# prcp observations plot====  
spi_obs <- 
spi_seas %>%  
  ggplot(aes(date, spi_1mo)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +  
  facet_wrap(vars(group)) +   
# plot station vals  
geom_line(size            = sta_size,  
          colour          = sta_color  
          )    
  
# precipitation seasons plot====   
spi_seas_plot <-    
spi_seas %>%       
  ggplot(aes(date, season)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +   
  facet_wrap(vars(group),  
             nrow = 2) +   
# plot group vals              
geom_line(size            = sta_size,  
          colour          = sta_color  
          )    
   
# precipitation trend plot====   
spi_trend <-   
spi_seas %>%   
  ggplot(aes(date, trend)) +      
  theme_bw() +   
  xlab("") +   
  ylab("SPI 1-month") +    
  scale_y_continuous(limits = c(-3, 3)) +  
  facet_wrap(vars(group),        
             nrow = 2) +    
# plot group vals   
geom_line(size            = sta_size,   
          colour          = sta_color   
          )    
  
# precipitation remainder plot====   
spi_remain <-       
spi_seas %>%                
  ggplot(aes(date, remainder)) +     
  theme_bw() +                  
  xlab("") +                     
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +  
  facet_wrap(vars(group)) +   
# plot station vals         
geom_line(size            = sta_size,  
          colour          = sta_color  
          )     
  
# plot the plots above as a grid & save====  
cowplot::plot_grid(  
  spi_obs, spi_seas_plot, spi_trend, spi_remain,  
  ncol = 1,   
  align = "v"  
)  
  
cowplot::ggsave2("figure/spi_deconv.png",  
                 units = "in",  
                 width = 7,  
                 height = 9)  
```  

```{r deconvolute_sri}  
  
# deconvolute the grouped prcp into trend, seasonal & random components----   

# import working data####    
gage_meta <-  import("data/gage_meta.csv")   

sci_gage <- import("data/sci_gage.csv") %>%    
  mutate(date = ymd(date))   
  
# prepare for deconvoluting the grouped precip====  
# the target for decomp_fun is 'group' -- RUN THIS FOR 'sci_gage' 
sci_gage <- sci_gage %>%   
    arrange(date) %>%   
  as_tibble() %>%  
  mutate(group = sta)   
  
# identify frequency of seasonality -- should be 12 months====      
freq <- sci_gage  %>%    
  split(.$sta) %>%   
  map_dfc(~ anomalize::time_frequency(  
    period       = "auto",  
    data = .)   
  ) %>%  
  gather(key     = group,  
         value   = freq) %>%   
  summarise(freq = mean(freq),   
            max  = max(freq),   
            min  = min(freq),   
            sd   = sd(freq)  
  )   
  
#   identify trend of periodicity -- should be 60 months====   
trend <- sci_gage  %>%   
  split(.$sta) %>%   
  map_dfc(~ anomalize::time_trend(   
    period       = "auto", data = .)    
  ) %>%                        
  gather(key     = grpup, value = freq) %>%   
  summarise(freq = mean(freq),   
            max  = max(freq),   
            min  = min(freq),   
            sd   = sd(freq)    
  )   
  
# decompose prcp into seasonal, trend, & remainder====     
FREQ   <- "12 months"   
TREND  <- "60 months"    
TARGET <- "sci_1mo"   
   
sri_seas <- sci_gage %>%   
  decomp_fun()   
  
# clean up global environment====     
rm(freq,  
   trend,       
   FREQ,  
   TARGET,  
   TREND,  
   decomp_fun, 
   sci_gage
   )    
  
# identify seasonally intermittant streams  
sri_intermit <- sri_seas %>%  
  group_by(sta) %>% 
  summarise(season_max = max(season)  
  ) %>% 
  ungroup() %>%  
  arrange(desc(season_max))  

gage_meta <- full_join(gage_meta, sri_intermit, 
                       by = "sta") 
gage_meta <- gage_meta %>% 
  mutate(flow_regime = case_when(  
    season_max > 1.5 ~ "strongly intermittent", 
    TRUE ~ flow_regime
    ))   

sri_intermit <- sri_intermit %>% 
  filter(season_max < 1.5)  

sri_seas <- semi_join(sri_seas, sri_intermit, 
                       by = "sta")

export(gage_meta, "data/gage_meta_seas.csv")  
export(sri_seas, "data/sri_seas.csv")  
  
```  
  
```{r prep_season_plot_sri}
  


# calculate group mean & sd====   
# note elk_elm has really high seasonality and is removed from future calcs
sri_1mo_mean <- sri_seas %>%  
  group_by(ecoreg, date) %>% 
  summarise(sci_1mo_sd = sd(sci_1mo),
            sci_1mo    = mean(sci_1mo) 
  ) %>%  
  ungroup() %>% 
  mutate(sci_1mo_hi = sci_1mo + sci_1mo_sd) %>% 
  mutate(sci_1mo_lo = sci_1mo - sci_1mo_sd) %>% 
  select(-c(sci_1mo_sd)) %>% 
  gather(key = sci_type, value = sci_hi_lo, 
         -c(ecoreg, date, sci_1mo)
         )    

sri_seas_mean <- sri_seas %>%  
  group_by(ecoreg, date) %>% 
  summarise(season_sd = sd(season),
            season    = mean(season) 
  ) %>%  
  ungroup() %>% 
  mutate(season_hi = season + season_sd) %>% 
  mutate(season_lo = season - season_sd) %>% 
  select(-c(season_sd)) %>% 
  gather(key = season_type, value = seas_hi_lo, 
         -c(ecoreg, date, season) 
         )   

sri_trend_mean <- sri_seas %>%  
  group_by(ecoreg, date) %>% 
  summarise(trend_sd = sd(trend),
            trend    = mean(trend) 
  ) %>%  
  ungroup() %>% 
  mutate(trend_hi = trend + trend_sd) %>% 
  mutate(trend_lo = trend - trend_sd) %>% 
  select(-c(trend_sd)) %>% 
  gather(key = trend_type, value = trend_hi_lo, 
         -c(ecoreg, date, trend)
  )  
  
sri_remain_mean <- sri_seas %>%  
  group_by(ecoreg, date) %>% 
  summarise(remain_sd = sd(remainder),
            remain    = mean(remainder) 
  ) %>%  
  ungroup() %>% 
  mutate(remain_hi = remain + remain_sd) %>% 
  mutate(remain_lo = remain - remain_sd) %>% 
  select(-c(remain_sd)) %>% 
  gather(key = remain_type, value = remain_hi_lo, 
         -c(ecoreg, date, remain)
  )  


sri_means <- full_join(sri_1mo_mean, sri_seas_mean, 
                       by = c("ecoreg", "date")  
                       )      
sri_means <- full_join(sri_means, sri_trend_mean, 
                       by = c("ecoreg", "date")  
                       ) 
sri_means <- full_join(sri_means, sri_remain_mean, 
                       by = c("ecoreg", "date")  
                       ) 
 
rm(sri_1mo_mean, sri_seas_mean, sri_trend_mean, sri_intermit)
```

```{r plot_seasonality_sri}

sri_anom <- sri_seas %>% 
  filter(sta != "elk_elm") %>%   
  filter(anomaly == "Yes") 

# set plotting position====     
sri_seas <- sri_seas %>% 
  mutate(ecoreg = fct_relevel(ecoreg,
                              "Pierre Shale Plains", 
                              "Pine Ridge Escarpment", 
                              "White River Badlands", 
                              "Black Hills Plateau", 
                              "Keya Paha Tablelands", 
                              "Sand Hills")     
  ) 

sri_means <- sri_means %>% 
  mutate(ecoreg = fct_relevel(ecoreg,
                              "Pierre Shale Plains", 
                              "Pine Ridge Escarpment", 
                              "White River Badlands", 
                              "Black Hills Plateau", 
                              "Keya Paha Tablelands", 
                              "Sand Hills")     
  ) 
  
# streamflow observations plot====  
sri_obs <- sri_seas %>%   
  ggplot(aes(date, sci_1mo)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SRI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +  
  facet_wrap(vars(ecoreg),   
             nrow = 2 
  ) +   
  # plot all station vals  
  geom_line(size            = 1,  
            colour          = "gray80"  
  )  + 
# plot mean +/- 1 SD 
  geom_line(data = sri_means,  
            aes(date, sci_hi_lo), 
            size            = 0.6,  
            colour          = "gray60"  
  )   +  
# plot mean 
  geom_line(data = sri_means,  
            size            = 0.3,  
            colour          = "black"  
  )  

# streamflow seasons plot====   
sri_seas_plot <- 
sri_seas %>% 
  ggplot(aes(date, season)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SRI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +   
  facet_wrap(vars(ecoreg),  
             nrow = 2) +   
  # plot all station vals  
  geom_line(size            = 1,  
            colour          = "gray80"  
  )  + 
# plot mean +/- 1 SD 
  geom_line(data = sri_means,  
            aes(date, seas_hi_lo), 
            size            = 0.3,  
            colour          = "gray60"  
  )   +  
# plot mean 
  geom_line(data = sri_means,  
            aes(date, season), 
            size            = 0.3,  
            colour          = "black"  
  )  
  
# streamflow trend plot====   
sri_trend <- sri_seas %>% 
  ggplot(aes(date, trend)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SRI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +   
  facet_wrap(vars(ecoreg),  
             nrow = 2) +   
  # plot all station vals  
  geom_line(size            = 1,  
            colour          = "gray80"  
  )  + 
# plot mean +/- 1 SD 
  geom_line(data = sri_means,  
            aes(date, trend_hi_lo), 
            size            = 0.3,  
            colour          = "gray60"  
  )   +  
# plot mean 
  geom_line(data = sri_means,  
            aes(date, trend), 
            size            = 0.3,  
            colour          = "black"  
  )  

# streamflow remainder plot====   
#sri_remain <- 
sri_seas %>% 
  ggplot(aes(date, remainder)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SRI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +   
  facet_wrap(vars(ecoreg),  
             nrow = 2) +   
  # plot all station vals  
  geom_line(size            = 1,  
            colour          = "gray80"  
  )  + 
# plot mean +/- 1 SD 
  geom_line(data = sri_means,  
            aes(date, remain_hi_lo), 
            size            = 0.3,  
            colour          = "gray60"  
  )   +  
# plot mean 
  geom_line(data = sri_means,  
            aes(date, remain), 
            size            = 0.3,  
            colour          = "black"  
  ) + 
# plot anomolies 
  geom_point(data = sri_anom,  
             aes(date, sci_1mo),  
             shape = 8)  
  
# plot the plots above as a grid & save====  
cowplot::plot_grid(  
  sri_obs, sri_seas_plot, sri_trend, sri_remain,  
  ncol = 1,   
  align = "v"  
)  
  
cowplot::ggsave2("figure/sri_deconv.png",  
                 units = "in",  
                 width = 7,  
                 height = 9)  
  
# clean-up====  
rm(spi_obs,  
   spi_seas_plot,  
   spi_trend,  
   spi_remain,  
   sri_obs,  
   sri_seas,  
   sri_seas_plot,  
   sri_trend,  
   sri_remain, 
   spi_anom,  
   sta_color,  
   sta_size   
   )         
  

 
```  

# TO DO Monthly plot PRCP_MON - check code   
```{r plot-annual-prcp}  
# plot annual precips  
sta_mon_plus %>%  
  ggplot(aes(year, depth_mm, group = year)) +  
  facet_grid(rows =vars(group)) +  
  #  geom_quasirandom(size = 0.2,  
  #                position = position_beeswarm()) +  
  geom_boxplot() +  
  theme_bw() +   
  labs(title = "Annual precipitation depths",  
       subtitle = "1990-2017") +  
  xlab("") +  
  ylab("mm")  
  
```

```{r ggplot_monthly, eval=FALSE} 
#sta_mon <- import(file = "data/stations_monthly.csv") %>%   
#  mutate(date = ymd(date))  
  
#sta_mon %>%   
#  group_by(sta) %>%   
#  summarize(count = n())  
  
sta_mon_plus$group <- factor(sta_mon_plus$group,  
                             levels = c("NW", "NC", "NE", "SW", "SC", "SE"))  
  
# plot monthly precips  
sta_mon_plus %>%   
  ggplot(aes(month, depth_mm, group = month)) +  
  facet_grid(rows = vars(group)) +   
  geom_quasirandom(size = 0.2,  
                   position = position_beeswarm()  
  ) + 
  scale_x_discrete(limits = c("1", "2", "3", "4", "5", "6",  
                              "7", "8", "9", "10", "11", "12"),    
                   labels = c("J", "F", "M", "A", "M", "J",  
                              "J", "A", "S", "O", "N", "D")   
  ) +  
  theme_bw() +  
  #  labs(title = "Monthly precipitation depths",  
  #       subtitle = "Southwestern South Dakota for 1990-2017") +   
  xlab("") +  
  ylab("mm")  
  
ggplot2::ggsave(path = "figure/", filename = "precip_mon.png",   
                width = 6, height = 6, units = "in")  
  
````  

```{r precip-EDA, include=FALSE, eval=FALSE}
# Purpose: EDA of precipitation data.
# Outcome: Differences among stations 1971-2018 are small.  
#          less than +/-5 mm on average.  
 
# the anomolously wet month series is dominated by May & June events.  
# what drives precip during this time?   
#   In May & June the area recieves low-level moisture from the Gulf   
#   of Mexico, strong cold fronts, and active upper-level pattern  
#   leading to greater chance for convection.  
```

```{r coerse-timeseries_using_timetk}

# Coercion issues w/ ts() - see vignette: Time Series Coercion Using timetk----     
# The ts object class for time series data is used by packages including  
#   'forecast()'.  The ts data structure is difficult to coerce back and  
#   forth because by default it does not contain a time-based index -- rather  
#   it uses a regularized index computed using start and frequency arguments.  
# Coercion to ts using the ts() function from the stats library results in  
#    various problems -- 1. only numeric columns get coerced, and if the user  
#    forgets to add the [,"pct"] to drop the “date” column, ts() returns  
#    dates in numeric format which is not what the user wants.  
# Calling the specific column desired presents a new issue -- the date index  
#    is lost, and a #different “regularized” index is built using the start  
#    and frequency attributes.  
# We can get the index using the index() function from the zoo package -- the  
#    index retained is a regular sequence of numeric values, but the  
#    regularized values cannot be coerced back to the original time-base  
#    because the date and date time data may contain year-month-day,  
#    hour-minute-second, and timezone attributes, may not be on a regularized  
#    interval (frequency).  

# Solution -- he timetk package contains a new function, tk_ts(), that  
#   enables maintaining the original date index as an attribute. When we   
#   repeat the tbl to ts coercion process using the new function, tk_ts():      
#  
# 1. only numeric columns get coerced, which prevents unintended consequences  
#   due to R coercion rules (e.g. dates getting unintentionally converted  
#   or characters causing the homogeneous data structure converting all  
#   numeric values to character).  
#  
# 2. the data returned has additional attributes --  
#   a numeric attribute, “index” containing orig. date information as a number  
#   along with the time zone and class.  
#   
# 3. the tk_tbl() function has an argument timetk_idx  which can be used to  
#    select which index to return.  
#    Note: ts() returns a “regularized” (numeric) index rather than a   
#    time-based index.  

# get data & coerce to time series====  
spi_sta <- import("data/spi_sta.csv") %>% 
  mutate(date = ymd(date))   
  
sta_meta_fin <- import("data/sta_meta_fin.csv")      

# starting small to look at ARIMA
spi01_cot_ts <- spi_sta %>%  
  select(sta, date, spi_1mo) %>%  
  filter(sta == "COT") %>%  
  spread(sta, spi_1mo) %>%  
  tk_ts( 
    start  = 1989, 
    freq   = 12,
    silent = FALSE)  

# The original time-based index can be retrieved using 
spi01_cot_ts %>% 
tk_index(timetk_idx = TRUE) %>%
    str()
  

```

```{r forecast-example}

# example uses data from remotes::install_github("robjhyndman/expsmooth")
data(usnetelec)
print(usnetelec) 


library(ggplot2)
ggAcf(wineind)
wineind %>% Acf(plot=FALSE) %>% autoplot

ggAcf(spi_sta_mon)
## Not run: 
wineind %>% taperedacf(plot=FALSE) %>% autoplot
ggtaperedacf(wineind)
ggtaperedpacf(wineind)
## End(Not run)
ggCcf(mdeaths, fdeaths)

ggAcf(spi_sta_mon %>% convert()) + 
  theme_light() + 
  labs(title = "ACF plot of Seattle Bikes Series")
```

```{r}
# https://tyleransom.github.io/teaching/MetricsLabs/lab15.html 
# Practice with time series forecasting. 
library(tidyverse)
library(wooldridge)
library(broom)
library(magrittr)
library(stargazer)
library(zoo)
library(dynlm)
library(pdfetch)
library(tseries)   # You may need to install this package
library(lubridate) # You may need to install this package
library(forecast)  # You will likely have to install this one

# Load the data
# 1. look at the return on a 3-month treasury bill over the period of:  
#    1960q1–1990q4; 2. read in Google’s and Apple’s stock price data from 
#    January 3, 2005 until October 31, 2018.

df1 <- as_tibble(intqrt)
df1 %<>% mutate(quarter = seq(yq('1960:Q1'), 
                              yq('1990:Q4'), 
                              by = 'quarters')) # create quarters

df1 %<>% select(r3,quarter)
df2 <- pdfetch_YAHOO(c("goog","aapl"), 
                     fields = c("adjclose"), 
                     from = as.Date("2005-01-01"),
                     to = as.Date("2018-11-01"),
                     interval = "1d") %>% 
  tk_tbl() %>% 
  rename(date = index)  

#df2 %<>% mutate(date=rownames(df2), 
#                date=ymd(date)) # create date variable -- gets a warning...

#Declare as time series objects
df1.ts <- df1 %>% 
  select(r3) %>% 
  zoo(order.by=df1$quarter)  

df2.ts <- df2 %>% 
  select(goog,aapl) %>% 
  zoo(order.by=df2$date)

# Autoplot time series data
autoplot(df1.ts) + 
  xlab("Year") + 
  ylab("T-bill return")

autoplot(df2.ts) + 
  xlab("Year") + 
  ylab("Price")

# Testing for a unit root
#   Test for a unit root in each of the time series by the 
#   Augmented Dickey-Fuller (ADF) test, which is available as adf.test()  
#   in the tseries package.
# The function tests \(H_0: \text{Unit Root}, H_a: \text{Stationary}\).

adf.test(df1.ts$r3, k=1)
adf.test(df2.ts$goog, k=1)
adf.test(df2.ts$aapl, k=1)

# Which of these time series has a unit root, according to the ADF test? 
# Explain what the consequences are of analyzing a time series  
#   that contains a unit root.

#Estimating AR(1) models  
# To alternatively examine the unit root, 
#   we can estimate AR(1) models for each series:

est.tbill <- dynlm(r3 ~ L(r3,1), data=df1.ts)
stargazer(est.tbill,type="text")

est.goog  <- dynlm(goog ~ L(goog,1), data=df2.ts)
stargazer(est.goog,type="text")

est.aapl  <- dynlm(aapl ~ L(aapl,1), data=df2.ts)
stargazer(est.aapl,type="text")

# Are the \(R^2\) values from these estimates meaningful?

# Forecasting
# Now let’s use our time series data to forecast future stock prices. 
# First, we should create a shortened version of the time series 
# so we can compare our forecast to actual data:

df2.short    <- df2 %>% filter(date<as.Date("2018-10-01"))
df2.ts.short <- df2.short %>% select(goog,aapl) %>%
                zoo(order.by=df2.short$date)

#Estimating simple AR models
# We can use the Arima function to estimate basic AR(1) models on the 
# differenced stock prices.

simple.goog <- Arima(df2.ts.short$goog,order=c(1,1,0))
simple.aapl <- Arima(df2.ts.short$aapl,order=c(1,1,0))

# This is the same thing as estimating 
#$\[ \Delta goog_t = \rho \Delta goog_{t-1} + u_t \]$#
  
#Estimating ARIMA models
# We can also use the auto.arima function to allow the computer to 
# choose the best ARIMA model:

auto.goog <- auto.arima(df2.ts.short$goog)
auto.aapl <- auto.arima(df2.ts.short$aapl)

#Plotting forecasts
# We can compare the 90-day-ahead forecasts of each model by 
# looking at their plots:

autoplot(forecast(simple.goog, h=90))
autoplot(forecast(  auto.goog, h=90))
autoplot(forecast(simple.aapl, h=90))
autoplot(forecast(  auto.aapl, h=90))
```

```{r tidy_time-series-1} 
#https://www.business-science.io/timeseries-analysis/2017/07/02/tidy-timeseries-analysis.html 

# The period apply functions from xts can be used to apply aggregations using  
#   common time series intervals such as weekly, monthly, quarterly, and  
#   yearly. The tq_transmute() function from tidyquant enables efficient and  
#   “tidy” application of the functions. We were able to use the period apply  
#   functions to visualize trends and volatility and to expose relationships 
#   between statistical measures.

library(tidyquant)  # Loads tidyverse, tidquant, financial pkgs, xts/zoo
library(cranlogs)   # For inspecting package downloads over time  
library(corrr)      # Tidy correlation tables and correlation plotting
library(cowplot)    # Multiple plots with plot_grid()

# Various tidyverse packages corresponding to my stickers :)
pkgs <- c(
    "tidyr", "lubridate", "dplyr", 
    "broom", "tidyquant", "ggplot2", "purrr", 
    "stringr", "knitr"
    )

# Get the downloads for the individual packages
tidyverse_downloads <- cran_downloads(
    packages = pkgs, 
    from     = "2017-01-01", 
    to       = "2017-06-30") %>%
    tibble::as_tibble() %>%
    group_by(package)
  
# Visualize the package downloads
tidyverse_downloads %>%
    ggplot(aes(x = date, y = count, color = package)) +
    geom_point() +
    labs(title = "tidyverse packages: Daily downloads", x = "") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# "apply" functions from xts
tq_transmute_fun_options()$xts %>%
    stringr::str_subset("^apply")

# Applying Functions By Period
# To perform the weekly aggregation, we will use tq_transmute() 
# which applies the non-tidy functions in a “tidy” way. 
# The function we want to use is apply.weekly(), 
# which takes the argument FUN (the function to be applied weekly) and 
# ... (additional args that get passed to the FUN function). 
# We’ll set FUN = mean to apply mean() on a weekly interval. 
# Last, we’ll pass the argument na.rm = TRUE to remove NA values during the 
# calculation.

mean_tidyverse_downloads_w <- tidyverse_downloads %>%
    tq_transmute(
        select     = count,
        mutate_fun = apply.weekly, 
        FUN        = mean,
        na.rm      = TRUE,
        col_rename = "mean_count"
    )

mean_tidyverse_downloads_w %>%
    ggplot(aes(x = date, y = mean_count, color = package)) +
    geom_point() +
    geom_smooth(method = "loess") + 
    labs(title = "tidyverse packages: Average daily downloads by week", x = "", 
         y = "Mean Daily Downloads by Week") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# Custom function to return mean, sd, quantiles
custom_stat_fun <- function(x, na.rm = TRUE, ...) {
    # x     = numeric vector
    # na.rm = boolean, whether or not to remove NA's
    # ...   = additional args passed to quantile
    c(mean    = mean(x, na.rm = na.rm),
      stdev   = sd(x, na.rm = na.rm),
      quantile(x, na.rm = na.rm, ...)) 
}

# Testing custom_stat_fun
options(digits = 4)
set.seed(3366)
nums  <- c(10 + 1.5*rnorm(10), NA)
probs <- c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)
custom_stat_fun(nums, na.rm = TRUE, probs = probs)

# Applying the custom function by week -- tidy 
stats_tidyverse_downloads_w <- tidyverse_downloads %>%
    tq_transmute(
        select = count,
        mutate_fun = apply.weekly, 
        FUN = custom_stat_fun,
        na.rm = TRUE,
        probs = probs
    )
stats_tidyverse_downloads_w

stats_tidyverse_downloads_w %>%
    ggplot(aes(x = date, y = `50%`, color = package)) +
    # Ribbon
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`), 
                color = palette_light()[[1]], fill = palette_light()[[1]], alpha = 0.5) +
    # Points
    geom_point() +
    geom_smooth(method = "loess", se = FALSE) + 
    # Aesthetics
    labs(title = "tidyverse packages: Median daily downloads by week", x = "",
         subtitle = "Range of 1st and 3rd quartile to show volatility",
         y = "Median Daily Downloads By Week") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq(theme = "dark") +
    theme_tq() +
    theme(legend.position="none")

stats_tidyverse_downloads_w %>%
    ggplot(aes(x = stdev, y = mean, color = package)) +
    geom_point() +
    geom_smooth(method = "lm") + 
    labs(title = "tidyverse packages: Mean vs standard deviation of daily downloads by week") +
    facet_wrap(~ package, ncol = 3, scale = "free") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")  

# Rolling Window Calculations====  
# The rollapply functions from zoo and TTR can be used to apply rolling  
#   window calculations. The tq_mutate() function from tidyquant enables  
#   efficient and “tidy” application of the functions. We were able to use  
#   the rollapply functions to visualize averages and standard deviations on  
#   a rolling basis, which gave us a better perspective of the dynamic trends.  
#   Using custom functions, we are unlimited to the statistics we can apply to  
#   rolling windows. 
#  
#   What are rolling window calculations, and why do we care? In time series  
#   analysis, nothing is static. A correlation may exist for a subset of time  
#   or an average may vary from one day to the next. Rolling calculations  
#   simply apply functions to a fixed width subset of the data (aka a window),  
#   indexing one observation each calculation.  

#   There are a few common reasons you may want to use a rolling calculation  
#   in time series analysis:
#
#    Measuring the central tendency over time (mean, median)
#    Measuring the volatility over time (sd, var)
#    Detecting changes in trend (fast vs slow moving averages)
#    Measuring a relationship between two time series over time (cor, cov)

# Sample Moving Average Calculation
# Combining a rolling mean with a rolling standard deviation can help detect  
# regions of abnormal volatility and consolidation. This is the concept behind  
# Bollinger Bands in the financial industry. The bands can be useful in  
# detecting breakouts in trend for many time series, not just financial.  

# Time Series Functions  
# "roll" functions from zoo
tq_mutate_fun_options()$zoo %>%
    stringr::str_subset("^roll")

##  [1] "rollapply"          "rollapplyr"         "rollmax"           
##  [4] "rollmax.default"    "rollmaxr"           "rollmean"          
##  [7] "rollmean.default"   "rollmeanr"          "rollmedian"        
## [10] "rollmedian.default" "rollmedianr"        "rollsum"           
## [13] "rollsum.default"    "rollsumr"

# "run" functions from TTR
tq_mutate_fun_options()$TTR %>%
    stringr::str_subset("^run")
  
##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"
  
# Tidy Implementation of Time Series Functions  
# Condensed function options... lot's of 'em
tq_mutate_fun_options() %>%
    str()

## List of 5
##  $ zoo: chr [1:14] "rollapply" "rollapplyr" "rollmax" "rollmax.default" ...
##  $ xts: chr [1:27] "apply.daily" "apply.monthly" "apply.quarterly" 
##      "apply.weekly" ...
##  $ quantmod: chr [1:25] "allReturns" "annualReturn" "ClCl" "dailyReturn" ...
##  $ TTR: chr [1:61] "adjRatios" "ADX" "ALMA" "aroon" ...
##  $ PerformanceAnalytics: chr [1:7] "Return.annualized" "
##      Return.annualized.excess" "Return.clean" "Return.cumulative" ...

# Tidy Application of Rolling Functions
# Rolling Mean: Inspecting Fast and Slow Moving Averages
#  Investigate if significant changes in trend are taking place such that  
#   future downloads are likely to continue to increase, decrease or stay the  
#   same. One way to do this is to use moving averages. Rather than try to  
#   sift through the noise, we can use a combination of a fast and slow moving  
#   average to detect momentum.

# We’ll create a fast moving average with width = 28 days (just enough to  
#   detrend the data) and a slow moving average with width = 84 days  
#   (slow window = 3X fast window). To do this we apply two calls to  
#   tq_mutate(), the first for the 28 day (fast) and the second for the 
#   84 day (slow) moving average. There are three groups of arguments we  
#   need to supply:  
# tq_mutate args -- These select the column to apply the mutation to “count”  
#   & the mutation function (mutate_fun) to apply (rollapply from zoo).
# rollapply args: These set the width, align = "right" 
#   (aligns with end of data frame), and 
#   the FUN we wish to apply (mean in this case).
# FUN args: These are arguments that get passed to the function. In this case  
#   we want to set na.rm = TRUE so NA values are skipped if present.
# Also add an optional tq_mutate arg, col_rename, at the end to rename the 
#   column. 

# Rolling mean example  
tidyverse_downloads_rollmean <- tidyverse_downloads %>%
    tq_mutate(
        # tq_mutate args
        select     = count,
        mutate_fun = rollapply, 
        # rollapply args
        width      = 28,
        align      = "right",
        FUN        = mean,
        # mean args
        na.rm      = TRUE,
        # tq_mutate args
        col_rename = "mean_28"
    ) %>%
    tq_mutate(
        # tq_mutate args
        select     = count,
        mutate_fun = rollapply,
        # rollapply args
        width      = 84,
        align      = "right",
        FUN        = mean,
        # mean args
        na.rm      = TRUE,
        # tq_mutate args
        col_rename = "mean_84"
    )

# ggplot results====  
tidyverse_downloads_rollmean %>%
    ggplot(aes(x = date, y = count, color = package)) +
    # Data
    geom_point(alpha = 0.1) +
    geom_line(aes(y = mean_28), color = palette_light()[[1]], size = 1) +
    geom_line(aes(y = mean_84), color = palette_light()[[2]], size = 1) +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    # Aesthetics
    labs(title = "tidyverse packages: Daily Downloads", x = "",
         subtitle = "28 and 84 Day Moving Average") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# Drop the “count” data from the plots and inspect just the moving averages to 
#   identify points where the fast trend is above (has momentum) 
#   or below (is slowing) the slow trend, & inspect for cross-over,  
#   which indicates shifts in trend.

tidyverse_downloads_rollmean %>%
    ggplot(aes(x = date, color = package)) +
    # Data
    # geom_point(alpha = 0.5) +  # Drop "count" from plots
    geom_line(aes(y = mean_28), 
              color = palette_light()[[1]], 
              linetype = 1, 
              size = 1) +
    geom_line(aes(y = mean_84), 
              color = palette_light()[[2]], 
              linetype = 1, 
              size = 1) +
    facet_wrap(~ package, 
               ncol = 3, 
               scale = "free_y") +
    # Aesthetics
    labs(title = "tidyverse packages: Daily downloads", x = "", y = "",
         subtitle = "Zoomed In: 28 and 84 Day Moving Average") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# The plot shows 'purrr' and 'lubridate' have strong upward momentum,   
#   'dplyr', 'knitr' and 'tidyr' seem to be cycling in a range, &   
#   'ggplot2' and 'stringr' have short term downward trends -- keep in mind 
#   these packages are getting the most downloads of the bunch.  

# Rolling Custom Functions: Useful for multiple statistics
#   Create a custom function, custom_stat_fun_2(), that returns statistics:  
#    mean
#    standard deviation
#    95% confidence interval (mean +/- 2SD)

# Custom function to return mean, sd, 95% conf interval  
custom_stat_fun_2 <- function(x, na.rm = TRUE) {  
  # x     = numeric vector  
  # na.rm = boolean, whether or not to remove NA's  
  m  <- mean(x, na.rm = na.rm)  
  s  <- sd(x, na.rm = na.rm)  
  hi <- m + 2*s  
  lo <- m - 2*s  
  ret <- c(mean = m, stdev = s, hi.95 = hi, lo.95 = lo)   
  return(ret)  
}  

# Apply the custom_stat_fun_2() to groups using tq_mutate() and the 
#   rolling function rollapply() -- The output returned is a “tidy” data frame  
#   with each statistic in its own column.
# The process is almost identical to the process of applying mean() with  
#   the main exception that we need to set by.column = FALSE to prevent a  
#   “length of dimnames [2]” error. 
  
# Roll apply using custom stat function
tidyverse_downloads_rollstats <- tidyverse_downloads %>%
    tq_mutate(
        select     = count,
        mutate_fun = rollapply, 
        # rollapply args
        width      = 28,
        align      = "right",
        by.column  = FALSE,
        FUN        = custom_stat_fun_2,
        # FUN args
        na.rm      = TRUE
    )
  
# We now have the data needed to visualize the rolling average (trend) and  
#   the 95% confidence bands (volatility) -- this is the concept of the  
#   Bollinger Bands to identify periods of consolidation and periods of high  
#   variability. 
# Many high variability periods are when the package downloads are rapidly  
#   increasing -- 'lubridate', 'purrr' and 'tidyquant' had spikes in  
#   downloads causing the 95% Confidence Interval (CI) bands to widen.

tidyverse_downloads_rollstats %>%
    ggplot(aes(x = date, color = package)) +
    # Data
    geom_point(aes(y = count), color = "grey40", alpha = 0.5) +
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), alpha = 0.4) +
    geom_point(aes(y = mean), size = 1, alpha = 0.5) +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    # Aesthetics
    labs(title = "tidyverse packages: Volatility and Trend", x = "",
         subtitle = "28-Day Moving Average with 95% Confidence Interval Bands (+/-2 Standard Deviations)") +
    scale_color_tq(theme = "light") +
    theme_tq() +
    theme(legend.position="none")
  
# The Rolling Correlation====  
#   tidyquant::tq_mutate_xy() enables “tidy” application of TTR::runCor()  
#   and other functions with x and y arguments. The corrr package is useful  
#   for computing the correlations and visualizing relationships, and it fits  
#   nicely into the “tidy” framework. 
#   The cowplot package helps with arranging multiple ggplots.  



CRAN tidyverse Downloads

We’ll be using the same “tidyverse” dataset as the last two posts. The script below gets the package downloads for the first half of 2017.

# tidyverse packages (see my laptop stickers from first post) ;)
pkgs <- c(
    "tidyr", "lubridate", "dplyr", 
    "broom", "tidyquant", "ggplot2", "purrr", 
    "stringr", "knitr"
)

# Get the downloads for the individual packages
tidyverse_downloads <- cran_downloads(
    packages = pkgs, 
    from     = "2017-01-01", 
    to       = "2017-06-30") %>%
    tibble::as_tibble() %>%
    group_by(package)

# Visualize the package downloads
tidyverse_downloads %>%
    ggplot(aes(x = date, y = count, color = package)) +
    # Data
    geom_point(alpha = 0.5) +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    # Aesthetics
    labs(title = "tidyverse packages: Daily downloads", x = "",
         subtitle = "2017-01-01 through 2017-06-30",
         caption = "Downloads data courtesy of cranlogs package") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

plot of chunk unnamed-chunk-2

We’ll also investigate correlations to the “broader market” meaning the total CRAN dowloads over time. To do this, we need to get the total downloads using cran_downloads() and leaving the package argument NULL, which is the default.

# Get data for total CRAN downloads
all_downloads <- cran_downloads(from = "2017-01-01", to = "2017-06-30") %>%
    tibble::as_tibble()

# Visualize the downloads
all_downloads %>%
    ggplot(aes(x = date, y = count)) +
    # Data
    geom_point(alpha = 0.5, color = palette_light()[[1]], size = 2) +
    # Aesthetics
    labs(title = "Total CRAN Packages: Daily downloads", x = "",
         subtitle = "2017-01-01 through 2017-06-30",
         caption = "Downloads data courtesy of cranlogs package") +
    scale_y_continuous(labels = scales::comma) +
    theme_tq() +
    theme(legend.position="none")

plot of chunk unnamed-chunk-3
Rolling Correlations

Correlations in time series are very useful because if a relationship exists, you can actually model/predict/forecast using the correlation. However, there’s one issue: a correlation is NOT static! It changes over time. Even the best models can be rendered useless during periods when correlation is low.

One of the most important calculations in time series analysis is the rolling correlation. Rolling correlations are simply applying a correlation between two time series (say sales of product x and product y) as a rolling window calculation.

Rolling Correlation Example

One major benefit of a rolling correlation is that we can visualize the change in correlation over time. The sample data (above) is charted (below). As shown, there’s a relatively high correlation between Sales of Product X and Y until a big shift in December. The question is, “What happened in December?” Just being able to ask this question can be critical to an organization.

plot of chunk unnamed-chunk-4

In addition to visualizations, the rolling correlation is great for a number of reasons. First, changes in correlation can signal events that have occurred causing two correlated time series to deviate from each other. Second, when modeling, timespans of low correlation can help in determining whether or not to trust a forecast model. Third, you can detect shifts in trend as time series become more or less correlated over time.
Time Series Functions

The xts, zoo, and TTR packages have some great functions that enable working with time series. Today, we’ll take a look at the runCor() function from the TTR package. You can see which TTR functions are integrated into tidyquant package below:

# "run" functions from TTR
tq_mutate_fun_options()$TTR %>%
    stringr::str_subset("^run")

##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"

Tidy Implementation of Time Series Functions

We’ll use the tq_mutate_xy() function to apply time series functions in a “tidy” way. Similar to tq_mutate() used in the last post, the tq_mutate_xy() function always adds columns to the existing data frame (rather than returning a new data frame like tq_transmute()). It’s well suited for tasks that result in column-wise dimension changes (not row-wise such as periodicity changes, use tq_transmute for those!).

Most running statistic functions only take one data argument, x. In these cases you can use tq_mutate(), which has an argument, select. See how runSD only takes x.

# If first arg is x (and no y) --> us tq_mutate()
args(runSD)

## function (x, n = 10, sample = TRUE, cumulative = FALSE) 
## NULL

However, functions like runCor and runCov are setup to take in two data arguments, x and y. In these cases, use tq_mutate_xy(), which takes two arguments, x and y (as opposed to select from tq_mutate()). This makes it well suited for functions that have the first two arguments being x and y. See how runCor has two arguments x and y.

# If first two arguments are x and y --> use tq_mutate_xy()
args(runCor)

## function (x, y, n = 10, use = "all.obs", sample = TRUE, cumulative = FALSE) 
## NULL

Static Correlations

Before we jump into rolling correlations, let’s examine the static correlations of our package downloads. This gives us an idea of how in sync the various packages are with each other over the entire timespan.

We’ll use the correlate() and shave() functions from the corrr package to output a tidy correlation table. We’ll hone in on the last column “all_cran”, which measures the correlation between individual packages and the broader market (i.e. total CRAN downloads).

# Correlation table
tidyverse_static_correlations <- tidyverse_downloads %>%
    # Data wrangling
    spread(key = package, value = count) %>%
    left_join(all_downloads, by = "date") %>%
    rename(all_cran = count) %>%
    select(-date) %>%
    # Correlation and formating
    correlate() 

# Pretty printing
tidyverse_static_correlations %>%
    shave(upper = F)

rowname 	broom 	dplyr 	ggplot2 	knitr 	lubridate 	purrr 	stringr 	tidyquant 	tidyr 	all_cran
broom 	  	0.63 	0.78 	0.67 	0.52 	0.40 	0.81 	0.17 	0.53 	0.74
dplyr 	  	  	0.73 	0.71 	0.59 	0.58 	0.71 	0.14 	0.64 	0.76
ggplot2 	  	  	  	0.91 	0.82 	0.67 	0.91 	0.20 	0.82 	0.94
knitr 	  	  	  	  	0.72 	0.74 	0.88 	0.21 	0.89 	0.92
lubridate 	  	  	  	  	  	0.79 	0.72 	0.29 	0.73 	0.82
purrr 	  	  	  	  	  	  	0.66 	0.35 	0.82 	0.80
stringr 	  	  	  	  	  	  	  	0.23 	0.81 	0.91
tidyquant 	  	  	  	  	  	  	  	  	0.26 	0.31
tidyr 	  	  	  	  	  	  	  	  	  	0.87
all_cran 	  	  	  	  	  	  	  	  	  	 

The correlation table is nice, but the outliers don’t exactly jump out. For instance, it’s difficult to see that tidyquant is low compared to the other packages withing the “all_cran” column.

Fortunately, the corrr package has a nice visualization called a network_plot(). It helps to identify strength of correlation. Similar to a “kmeans” analysis, we are looking for association by distance (or in this case by correlation). How well the packages correlate with each other is akin to how associated they are with each other. The network plot shows us exactly this association!

# Network plot
gg_all <- tidyverse_static_correlations %>%
    network_plot(colours = c(palette_light()[[2]], "white", palette_light()[[4]]), legend = TRUE) +
    labs(
        title = "Correlations of tidyverse Package Downloads to Total CRAN Downloads",
        subtitle = "Looking at January through June, tidyquant is a clear outlier"
        ) +
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +
    theme_tq() +
    theme(legend.position = "bottom")
gg_all

plot of chunk unnamed-chunk-10

We can see that tidyquant has a very low correlation to “all_cran” and the rest of the “tidyverse” packages. This would lead us to believe that tidyquant is trending abnormally with respect to the rest, and thus is possibly not as associated as we think. Is this really the case?
Rolling Correlations

Let’s see what happens when we incorporate time using a rolling correlation. The script below uses the runCor function from the TTR package. We apply it using tq_mutate_xy(), which is useful for applying functions such has runCor that have both an x and y input.

# Get rolling correlations
tidyverse_rolling_corr <- tidyverse_downloads %>%
    # Data wrangling
    left_join(all_downloads, by = "date") %>%
    select(date, package, count.x, count.y) %>%
    # Mutation
    tq_mutate_xy(
        x          = count.x,
        y          = count.y,
        mutate_fun = runCor, 
        # runCor args
        n          = 30,
        use        = "pairwise.complete.obs",
        # tq_mutate args
        col_rename = "rolling_corr"
    )

# Join static correlations with rolling correlations
tidyverse_static_correlations <- tidyverse_static_correlations %>%
    select(rowname, all_cran) %>%
    rename(package = rowname)

tidyverse_rolling_corr <- tidyverse_rolling_corr %>%
    left_join(tidyverse_static_correlations, by = "package") %>%
    rename(static_corr = all_cran)

# Plot
tidyverse_rolling_corr %>%
    ggplot(aes(x = date, color = package)) +
    # Data
    geom_line(aes(y = static_corr), color = "red") +
    geom_point(aes(y = rolling_corr), alpha = 0.5) +
    facet_wrap(~ package, ncol = 3, scales = "free_y") +
    # Aesthetics
    scale_color_tq() +
    labs(
        title = "tidyverse: 30-Day Rolling Download Correlations, Package vs Total CRAN",
        subtitle = "Relationships are dynamic vs static correlation (red line)",
        x = "", y = "Correlation"
    ) +
    theme_tq() +
    theme(legend.position="none")

plot of chunk unnamed-chunk-11

The rolling correlation shows the dynamic nature of the relationship. If we just went by the static correlation over the full timespan (red line), we’d be misled about the dynamic nature of these time series. Further, we can see that most packages are highly correlated with the broader market (total CRAN downloads) with the exception of various periods where the correlations dropped. The drops could indicate events or changes in user behavior that resulted in shocks to the download patterns.

Focusing on the main outlier tidyquant, we can see that once April hit tidyquant is trending closer to a 0.60 correlation meaning that the 0.31 relationship (red line) is likely too low going forward.

Last, we can redraw the network plot from April through June to investigate the shift in relationship. We can use the cowplot package to plot two ggplots (or corrr network plots) side-by-side.

# Redrawing Network Plot from April through June
gg_subset <- tidyverse_downloads %>%
    # Filter by date >= April 1, 2017
    filter(date >= ymd("2017-04-01")) %>%
    # Data wrangling
    spread(key = package, value = count) %>%
    left_join(all_downloads, by = "date") %>%
    rename(all_cran = count) %>%
    select(-date) %>%
    # Correlation and formating
    correlate() %>%
    # Network Plot
    network_plot(colours = c(palette_light()[[2]], "white", palette_light()[[4]]), legend = TRUE) +
    labs(
        title = "April through June (Last 3 Months)",
        subtitle = "tidyquant correlation is increasing"
        ) +
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +
    theme_tq() +
    theme(legend.position = "bottom")

# Modify the January through June network plot (previous plot)
gg_all <- gg_all +
    labs(
        title = "January through June (Last 6 months)",
        subtitle = "tidyquant is an outlier"
        )

# Format cowplot
cow_net_plots <- plot_grid(gg_all, gg_subset, ncol = 2)
title <- ggdraw() + 
    draw_label(label = 'tidyquant is getting "tidy"-er',
               fontface = 'bold', size = 18)
cow_out <- plot_grid(title, cow_net_plots, ncol=1, rel_heights=c(0.1, 1))
cow_out

plot of chunk unnamed-chunk-12
Conclusions


```

```{r tidy_time-series-2}

```


# forcasting with TSIBLE 
```{r things-tsible}  

spi_all2 %>% 
  ggplot(aes(date, spi_1mo)) + 
  facet_grid(rows = vars(sta)) + 
  geom_line() 




spi_1mo %>% 
  ggplot(aes(ONI, MIS)) + 
  #  facet_grid(rows = vars(sta)) + 
  geom_point() 
# time series arima 
library(tsibble)  
library(feasts) 

spi_1mo <- spi_all2 %>% 
  select(sta, date, spi_1mo) %>% 
  as_tibble()  

spi_1mot <- spi_1mo %>% 
  as_tsibble(key = spi_1mo)



tourism_melb <- tourism %>%
  filter(Region == "Melbourne")
tourism_melb %>%
  group_by(Purpose) %>%
  slice(1)

spi_1mot %>% 
  group_by(sta) %>%
  slice(1)


spi_1mot %>%
  autoplot(spi_1mo)

```

```{r spi-correlation-analysis}
# visually check results
#spi_index_plot <- spi_index %>% 
#  filter(spi_length == 12)

#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() +
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0, aes)

#ggplot2::ggsave(path = "figure/", filename = "spi_1mo.png", 
#                width = 6, height = 6, units = "in")  

# create correlation matrix inputs
spi_M <- spi_index %>% 
  select(-date) %>% 
  group_by(sta, spi_length) %>% 
  mutate(grouped_id = row_number()) %>% 
  spread(key = sta, value = spi_index) %>% 
  drop_na() %>% 
  ungroup() %>% 
  select(-grouped_id) %>% 
  select(spi_length, cot, int, oel, ora, rap) # ensure vars order 

# create correlation matrix names from the correlation matrix vars
spi_M_names <- spi_M %>% 
  filter(spi_length == 1) %>% 
  cor() %>% 
  as.tibble() %>% 
  names() %>% 
  as.tibble() %>% 
  slice(-1) %>% 
  mutate(value2 = value) %>% 
  mutate(value3 = value) %>%  
  mutate(value4 = value) %>%
  mutate(value5 = value) %>% 
  gather(key, sta2) %>% 
  select(-key) %>% 
  rownames_to_column() 

# create second station names column 
spi_M_names2 <- spi_M_names %>% 
  arrange(sta2) %>% 
  rename(sta1 = sta2) %>% 
  select(-rowname)

# bind the names columns
spi_M_names <- bind_cols(spi_M_names, spi_M_names2)  
rm(spi_M_names2)

# create a correlation matrix from SPI vals
spi_M <- spi_M %>% 
  split(.$spi_length) %>% 
  purrr::map_dfr(~ cor(.)) %>% 
  drop_na() %>% 
  slice(-1) %>% 
  rownames_to_column() 

# bind names to the correlation matrix 
spi_M <- full_join(spi_M_names, spi_M, by = "rowname")
spi_M <- spi_M %>% 
  select(sta1, sta2, everything()) %>%
  select(-rowname)

# prepare lookup table of lat-lons
sta_loc <- sta_meta %>% 
  arrange(name) %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  select(sta, lat, lon, dur_year)

# join the 'from' lat lons
spi_corr <- full_join(spi_M, sta_loc, by = c("sta1" = "sta")) %>% 
  rename(lat1 = lat) %>% 
  rename(lon1 = lon) 

# join the 'to' lat lons
spi_corr <- full_join(spi_corr, sta_loc, by = c("sta2" = "sta")) %>% 
  rename(lat2 = lat) %>% 
  rename(lon2 = lon) %>% 
  mutate(year_dif = abs(dur_year.x - dur_year.y))


# convert the 'to' and 'from' lat-lons to northings & eastings & distance
lat_to_km <- 111.03 # 1 degree lat to km @ lat 40-degrees 
lon_to_km <- 85.39  # 1 degree lon to km @ lat 40-degrees 

spi_corr <- spi_corr %>% 
  mutate(northing = abs((lat1 - lat2)) * lat_to_km) %>% 
  mutate(easting = abs((lon1 - lon2)) * lat_to_km) %>% 
  mutate(distance = sqrt(northing^2 + easting^2)) %>% 
  select(year_dif, everything()) %>%
  select(-(lat1:easting)) %>% 
  mutate(stations = paste(sta1, sta2, sep = "_")) %>% 
  gather(key = spi_length, value = pears_r, -distance, 
         -stations, -sta1, -sta2, -year_dif) %>% 
  filter(distance > 0) %>% 
  mutate(spi_length = as.double(spi_length))


# clean up the global environment 
rm(spi_M, spi_M_names, sta_loc, lat_to_km, lon_to_km)

# model effect of averaging time & distance on correlation----
# fit a linear model
spi_lm <- lm(pears_r ~ distance + spi_length + year_dif, 
             data = spi_corr)

# augment & gather the original data
spi_corr_aug <- augment(spi_lm, spi_corr) 

spi_corr_gath <- spi_corr_aug %>% 
  select(-(sta1:sta2)) %>%
  select(year_dif:.fitted) %>% 
  gather(key = factor, val, -stations, -pears_r) %>% 
  mutate(val = as.double(val))  

# plot the original data and fitted model for SPI----  
ggplot(spi_corr_gath, aes(val, pears_r)) + 
  geom_point(aes(color = factor(stations))) +
  facet_wrap(ncol = 1, vars(factor), scales = "free") +
  geom_smooth(method = lm) + 
  theme_classic()

spi_lm_fit <- glance(spi_lm) 

spi_lm_tidy <- tidy(spi_lm) %>% 
  mutate(
    low = estimate - std.error,
    high = estimate + std.error
  )

# clean-up Global Environment----
spi_corr <- spi_corr_aug 
rm(spi_corr_aug, spi_corr_gath, spi_lm ,spi_lm_fit)


rm(spi_corr, spi_lm_tidy)
```

```{r sweep-example}
#One of the most powerful benefits of sweep is that it helps forecasting at scale within the “tidyverse”. There are two common situations:

#    Applying a model to groups of time series
#    Applying multiple models to a time series

#In this vignette we’ll review how sweep can help the first situation: 
#Applying a model to groups of time series.
#Prerequisites

#Before we get started, load the following packages.

library(tidyverse)
library(tidyquant)
library(timetk)
library(sweep)
library(forecast)

#Bike Sales

#We’ll use the bike sales data set, bike_sales, provided with the sweep package #for this tutorial. The bike_sales data set is a fictional daily order history #that spans 2011 through 2015. It simulates a sales database that is typical of a #business. The customers are the “bike shops” and the products are the “models”.

bike_sales
spi_sta <- import("data/spi_sta.csv") 

#We’ll analyse the monthly sales trends for the bicycle manufacturer. Let’s #transform the data set by aggregating by month.

bike_sales_monthly <- bike_sales %>%
    mutate(month = month(order.date, label = TRUE),
           year  = year(order.date)) %>%
    group_by(year, month) %>%
    summarise(total.qty = sum(quantity)) 

spi_sta_monthly <- spi_sta %>% 
    mutate(date = ymd(date)) %>% 
    mutate(month = month(date, label = TRUE),
           year  = year(date)) %>%
    group_by(year, month) %>% 
  select(date, year, month, everything(), -prcp, prcp, -yr) %>% 
  ungroup()  

#We can visualize package with a month plot using the ggplot2 .

bike_sales_monthly %>%
    ggplot(aes(x = month, y = total.qty, group = year)) +
    geom_area(aes(fill = year), position = "stack") +
    labs(title = "Quantity Sold: Month Plot", x = "", y = "Sales",
         subtitle = "March through July tend to be most active") +
    scale_y_continuous() +
    theme_tq()

#Suppose Manufacturing wants a more granular forecast because the bike components #are related to the secondary category. In the next section we discuss how sweep #can help to perform a forecast on each sub-category.

#Performing Forecasts on Groups

#First, we need to get the data organized into groups by month of the year. We’ll #create a new “order.month” date using zoo::as.yearmon() that captures the year #and month information from the “order.date” and then passing this to lubridate::as_date() to convert to date format.

monthly_qty_by_cat2 <- bike_sales %>%
    mutate(order.month = as_date(as.yearmon(order.date))) %>%
    group_by(category.secondary, order.month) %>%
    summarise(total.qty = sum(quantity))

monthly_spi1mo <- spi_sta_monthly %>% 
  select(sta, date, spi_1mo)  
  
#Next, we use the nest() function from the tidyr package to consolidate each time series by group. The newly created list-column, “data.tbl”, contains the “order.month” and “total.qty” columns by group from the previous step. The nest() function just bundles the data together which is very useful for iterative functional programming.

monthly_qty_by_cat2_nest <- monthly_qty_by_cat2 %>%
    group_by(category.secondary) %>%
    nest() 

monthly_spi1mo_nest <- monthly_spi1mo %>% 
  group_by(sta) %>% 
  nest()  

## # A tibble: 9 x 2
## # Groups:   category.secondary [9]
##   category.secondary           data
##   <chr>              <list<df[,2]>>
## 1 Cross Country Race       [60 × 2]
## 2 Cyclocross               [60 × 2]
## 3 Elite Road               [60 × 2]
## 4 Endurance Road           [60 × 2]
## 5 Fat Bike                 [58 × 2]
## 6 Over Mountain            [60 × 2]
## 7 Sport                    [60 × 2]
## 8 Trail                    [60 × 2]
## 9 Triathalon               [60 × 2]

#Forecasting Workflow

#The forecasting workflow involves a few basic steps:

#    Step 1: Coerce to a ts object class.
#    Step 2: Apply a model (or set of models)
#    Step 3: Forecast the models (similar to predict)
#    Step 4: Tidy the forecast

#Step 1: Coerce to a ts object class

#In this step we map the tk_ts() function into a new column “data.ts”. The procedure is performed using the combination of dplyr::mutate() and purrr::map(), which works really well for the data science workflow where analyses are built progressively. As a result, this combination will be used in many of the subsequent steps in this vignette as we build the analysis.
#mutate and map

#The mutate() function adds a column, and the map() function maps the contents of a list-column (.x) to a function (.f). In our case, .x = data.tbl and .f = tk_ts. The arguments select = -order.month, start = 2011, and freq = 12 are passed to the ... parameters in map, which are passed through to the function. The select statement is used to drop the “order.month” from the final output so we don’t get a bunch of warning messages. We specify start = 2011 and freq = 12 to return a monthly frequency.

monthly_qty_by_cat2_ts <- monthly_qty_by_cat2_nest %>%
    mutate(data.ts = map(.x       = data, 
                         .f       = tk_ts, 
                         select   = -order.month, 
                         start    = 2011,
                         freq     = 12))  

monthly_spi1mo_ts <- monthly_spi1mo_nest %>%
    mutate(data.ts = map(.x       = data, 
                         .f       = tk_ts, 
                         select   = -date, 
                         start    = 1989,
                         freq     = 12))

#Step 2: Modeling a time series

#Next, we map the Exponential Smoothing ETS (Error, Trend, Seasonal) model function, ets, from the forecast package. Use the combination of mutate to add a column and map to interatively apply a function rowwise to a list-column. In this instance, the function to map the ets function and the list-column is “data.ts”. We rename the resultant column “fit.ets” indicating an ETS model was fit to the time series data.

monthly_qty_by_cat2_fit <- monthly_qty_by_cat2_ts %>%
    mutate(fit.ets = map(data.ts, ets))

monthly_spi1mo_fit <- monthly_spi1mo_ts %>%
    mutate(fit.ets = map(data.ts, ets))


#At this point, we can do some model inspection with the sweep tidiers.
#sw_tidy

#To get the model parameters for each nested list, we can combine sw_tidy within the mutate and map combo. The only real difference is now we unnest the generated column (named “tidy”). Last, because it’s easier to compare the model parameters side by side, we add one additional call to spread() from the tidyr package.

monthly_qty_by_cat2_fit %>%
    mutate(tidy = map(fit.ets, sw_tidy)) %>%
    unnest(tidy) %>%
    spread(key = category.secondary, value = estimate)

test <- monthly_spi1mo_fit %>%
    mutate(tidy = map(fit.ets, sw_tidy)) %>%
    unnest(tidy) %>%
    spread(key = sta, value = estimate)

## # A tibble: 128 x 13
##        data data.ts fit.ets term  `Cross Country … Cyclocross `Elite Road`
##    <list<d> <list>  <list>  <chr>            <dbl>      <dbl>        <dbl>
##  1 [60 × 2] <ts>    <ets>   alpha         0.0398           NA           NA
##  2 [60 × 2] <ts>    <ets>   gamma         0.000101         NA           NA
##  3 [60 × 2] <ts>    <ets>   l           321.               NA           NA
##  4 [60 × 2] <ts>    <ets>   s0            0.503            NA           NA
##  5 [60 × 2] <ts>    <ets>   s1            1.10             NA           NA
##  6 [60 × 2] <ts>    <ets>   s10           0.643            NA           NA
##  7 [60 × 2] <ts>    <ets>   s2            0.375            NA           NA
##  8 [60 × 2] <ts>    <ets>   s3            1.12             NA           NA
##  9 [60 × 2] <ts>    <ets>   s4            0.630            NA           NA
## 10 [60 × 2] <ts>    <ets>   s5            2.06             NA           NA
## # … with 118 more rows, and 6 more variables: `Endurance Road` <dbl>, `Fat
## #   Bike` <dbl>, `Over Mountain` <dbl>, Sport <dbl>, Trail <dbl>,
## #   Triathalon <dbl>

sw_glance

#We can view the model accuracies also by mapping sw_glance within the mutate and map combo.

monthly_qty_by_cat2_fit %>%
    mutate(glance = map(fit.ets, sw_glance)) %>%
    unnest(glance)

## # A tibble: 9 x 16
## # Groups:   category.secondary [9]
##   category.second…     data data.ts fit.ets model.desc sigma logLik   AIC
##   <chr>            <list<d> <list>  <list>  <chr>      <dbl>  <dbl> <dbl>
## 1 Cross Country R… [60 × 2] <ts>    <ets>   ETS(M,N,M) 1.06   -464.  957.
## 2 Cyclocross       [60 × 2] <ts>    <ets>   ETS(M,N,M) 1.12   -409.  848.
## 3 Elite Road       [60 × 2] <ts>    <ets>   ETS(M,N,M) 0.895  -471.  972.
## 4 Endurance Road   [60 × 2] <ts>    <ets>   ETS(M,N,M) 0.759  -439.  909.
## 5 Fat Bike         [58 × 2] <ts>    <ets>   ETS(M,N,M) 2.73   -343.  715.
## 6 Over Mountain    [60 × 2] <ts>    <ets>   ETS(M,N,M) 0.910  -423.  877.
## 7 Sport            [60 × 2] <ts>    <ets>   ETS(M,N,M) 0.872  -427.  884.
## 8 Trail            [60 × 2] <ts>    <ets>   ETS(M,A,M) 0.741  -411.  855.
## 9 Triathalon       [60 × 2] <ts>    <ets>   ETS(M,N,M) 1.52   -410.  850.
## # … with 8 more variables: BIC <dbl>, ME <dbl>, RMSE <dbl>, MAE <dbl>,
## #   MPE <dbl>, MAPE <dbl>, MASE <dbl>, ACF1 <dbl>

#sw_augment

#The augmented fitted and residual values can be achieved in much the same manner. This returns nine groups data. Note that we pass timetk_idx = TRUE to return the date format times as opposed to the regular (yearmon or numeric) time series.

augment_fit_ets <- monthly_qty_by_cat2_fit %>%
    mutate(augment = map(fit.ets, sw_augment, timetk_idx = TRUE, rename_index = "date")) %>%
    unnest(augment)

augment_fit_ets

## # A tibble: 538 x 8
## # Groups:   category.secondary [9]
##    category.second…     data data.ts fit.ets date       .actual .fitted
##    <chr>            <list<d> <list>  <list>  <date>       <dbl>   <dbl>
##  1 Cross Country R… [60 × 2] <ts>    <ets>   2011-01-01     122    373.
##  2 Cross Country R… [60 × 2] <ts>    <ets>   2011-02-01     489    201.
##  3 Cross Country R… [60 × 2] <ts>    <ets>   2011-03-01     505    465.
##  4 Cross Country R… [60 × 2] <ts>    <ets>   2011-04-01     343    161.
##  5 Cross Country R… [60 × 2] <ts>    <ets>   2011-05-01     263    567.
##  6 Cross Country R… [60 × 2] <ts>    <ets>   2011-06-01     735    296.
##  7 Cross Country R… [60 × 2] <ts>    <ets>   2011-07-01     183    741.
##  8 Cross Country R… [60 × 2] <ts>    <ets>   2011-08-01      66    220.
##  9 Cross Country R… [60 × 2] <ts>    <ets>   2011-09-01      97    381.
## 10 Cross Country R… [60 × 2] <ts>    <ets>   2011-10-01     189    123.
## # … with 528 more rows, and 1 more variable: .resid <dbl>

#We can plot the residuals for the nine categories like so. Unfortunately we do see some very high residuals (especially with “Fat Bike”). This is often the case with realworld data.

augment_fit_ets %>%
    ggplot(aes(x = date, y = .resid, group = category.secondary)) +
    geom_hline(yintercept = 0, color = "grey40") +
    geom_line(color = palette_light()[[2]]) +
    geom_smooth(method = "loess") +
    labs(title = "Bike Quantity Sold By Secondary Category",
         subtitle = "ETS Model Residuals", x = "") + 
    theme_tq() +
    facet_wrap(~ category.secondary, scale = "free_y", ncol = 3) +
    scale_x_date(date_labels = "%Y")

#sw_tidy_decomp

#We can create decompositions using the same procedure with sw_tidy_decomp() and the mutate and map combo.

monthly_qty_by_cat2_fit %>%
    mutate(decomp = map(fit.ets, sw_tidy_decomp, timetk_idx = TRUE, rename_index = "date")) %>%
    unnest(decomp)

## # A tibble: 538 x 9
## # Groups:   category.secondary [9]
##    category.second…     data data.ts fit.ets date       observed level
##    <chr>            <list<d> <list>  <list>  <date>        <dbl> <dbl>
##  1 Cross Country R… [60 × 2] <ts>    <ets>   2011-01-01      122  313.
##  2 Cross Country R… [60 × 2] <ts>    <ets>   2011-02-01      489  331.
##  3 Cross Country R… [60 × 2] <ts>    <ets>   2011-03-01      505  332.
##  4 Cross Country R… [60 × 2] <ts>    <ets>   2011-04-01      343  347.
##  5 Cross Country R… [60 × 2] <ts>    <ets>   2011-05-01      263  339.
##  6 Cross Country R… [60 × 2] <ts>    <ets>   2011-06-01      735  359.
##  7 Cross Country R… [60 × 2] <ts>    <ets>   2011-07-01      183  348.
##  8 Cross Country R… [60 × 2] <ts>    <ets>   2011-08-01       66  339.
##  9 Cross Country R… [60 × 2] <ts>    <ets>   2011-09-01       97  329.
## 10 Cross Country R… [60 × 2] <ts>    <ets>   2011-10-01      189  336.
## # … with 528 more rows, and 2 more variables: season <dbl>, slope <dbl>

#Step 3: Forecasting the model

#We can also forecast the multiple models again using a very similar approach with the forecast function. We want a 12 month forecast so we add the argument for the h = 12 (refer to ?forecast for all of the parameters you can add, there’s quite a few).

monthly_qty_by_cat2_fcast <- monthly_qty_by_cat2_fit %>%
    mutate(fcast.ets = map(fit.ets, forecast, h = 12))
monthly_qty_by_cat2_fcast

## # A tibble: 9 x 5
## # Groups:   category.secondary [9]
##   category.secondary           data data.ts fit.ets fcast.ets 
##   <chr>              <list<df[,2]>> <list>  <list>  <list>    
## 1 Cross Country Race       [60 × 2] <ts>    <ets>   <forecast>
## 2 Cyclocross               [60 × 2] <ts>    <ets>   <forecast>
## 3 Elite Road               [60 × 2] <ts>    <ets>   <forecast>
## 4 Endurance Road           [60 × 2] <ts>    <ets>   <forecast>
## 5 Fat Bike                 [58 × 2] <ts>    <ets>   <forecast>
## 6 Over Mountain            [60 × 2] <ts>    <ets>   <forecast>
## 7 Sport                    [60 × 2] <ts>    <ets>   <forecast>
## 8 Trail                    [60 × 2] <ts>    <ets>   <forecast>
## 9 Triathalon               [60 × 2] <ts>    <ets>   <forecast>

#Step 4: Tidy the forecast

#Next, we can apply sw_sweep to get the forecast in a nice “tidy” data frame. We use the argument fitted = FALSE to remove the fitted values from the forecast (leave off if fitted values are desired). We set timetk_idx = TRUE to use dates instead of numeric values for the index. We’ll use unnest() to drop the left over list-columns and return an unnested data frame.

monthly_qty_by_cat2_fcast_tidy <- monthly_qty_by_cat2_fcast %>%
    mutate(sweep = map(fcast.ets, sw_sweep, fitted = FALSE, timetk_idx = TRUE)) %>%
    unnest(sweep)
monthly_qty_by_cat2_fcast_tidy

## # A tibble: 646 x 12
## # Groups:   category.secondary [9]
##    category.second…     data data.ts fit.ets fcast.ets index      key  
##    <chr>            <list<d> <list>  <list>  <list>    <date>     <chr>
##  1 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-01-01 actu…
##  2 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-02-01 actu…
##  3 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-03-01 actu…
##  4 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-04-01 actu…
##  5 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-05-01 actu…
##  6 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-06-01 actu…
##  7 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-07-01 actu…
##  8 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-08-01 actu…
##  9 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-09-01 actu…
## 10 Cross Country R… [60 × 2] <ts>    <ets>   <forecas… 2011-10-01 actu…
## # … with 636 more rows, and 5 more variables: total.qty <dbl>,
## #   lo.80 <dbl>, lo.95 <dbl>, hi.80 <dbl>, hi.95 <dbl>

#Visualization is just one final step.

monthly_qty_by_cat2_fcast_tidy %>%
    ggplot(aes(x = index, y = total.qty, color = key, group = category.secondary)) +
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), 
                fill = "#D5DBFF", color = NA, size = 0) +
    geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), 
                fill = "#596DD5", color = NA, size = 0, alpha = 0.8) +
    geom_line() +
    labs(title = "Bike Quantity Sold By Secondary Category",
         subtitle = "ETS Model Forecasts",
         x = "", y = "Units") +
    scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
    scale_color_tq() +
    scale_fill_tq() +
    facet_wrap(~ category.secondary, scales = "free_y", ncol = 3) +
    theme_tq() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Recap

#The sweep package has a several tools to analyze grouped time series. In the next vignette we will review how to apply multiple models to a time series.

```

```{r}


data(AirPassengers)
 > class(AirPassengers)
 [1] "ts"

#This tells you that the data series is in a time series format
 > start(AirPassengers)
 [1] 1949 1

#This is the start of the time series

> end(AirPassengers)
 [1] 1960 12

#This is the end of the time series

> frequency(AirPassengers)
 [1] 12

#The cycle of this time series is 12months in a year
 > summary(AirPassengers)
 Min. 1st Qu. Median Mean 3rd Qu. Max.
 104.0 180.0 265.5 280.3 360.5 622.0

 
Detailed Metrics

#The number of passengers are distributed across the spectrum

> plot(AirPassengers)

#This will plot the time series

>abline(reg=lm(AirPassengers~time(AirPassengers)))

# This will fit in a line

time series r, r, plot_AP

Here are a few more operations you can do:

> cycle(AirPassengers)

#This will print the cycle across years.

>plot(aggregate(AirPassengers,FUN=mean))

#This will aggregate the cycles and display a year on year trend

> boxplot(AirPassengers~cycle(AirPassengers))

#Box plot across months will give us a sense on seasonal effect

r, plot_aggregate

r, plot_month_wise
Important Inferences

    The year on year trend clearly shows that the #passengers have been increasing without fail.
    The variance and the mean value in July and August is much higher than rest of the months.
    Even though the mean value of each month is quite different their variance is small. Hence, we have strong seasonal effect with a cycle of 12 months or less.

Exploring data becomes most important in a time series model – without this exploration, you will not know whether a series is stationary or not. As in this case we already know many details about the kind of model we are looking out for.

Let’s now take up a few time series models and their characteristics. We will also take this problem forward and make a few predictions.

 
3. Introduction to ARMA Time Series Modeling

ARMA models are commonly used in time series modeling. In ARMA model, AR stands for auto-regression and MA stands for moving average. If these words sound intimidating to you, worry not – I’ll simplify these concepts in next few minutes for you!

We will now develop a knack for these terms and understand the characteristics associated with these models. But before we start, you should remember, AR or MA are not applicable on non-stationary series.

In case you get a non stationary series, you first need to stationarize the series (by taking difference / transformation) and then choose from the available time series models.

First, I’ll explain each of these two models (AR & MA) individually. Next, we will look at the characteristics of these models.

 
Auto-Regressive Time Series Model

Let’s understanding AR models using the case below:

The current GDP of a country say x(t) is dependent on the last year’s GDP i.e. x(t – 1). The hypothesis being that the total cost of production of products & services in a country in a fiscal year (known as GDP) is dependent on the set up of manufacturing plants / services in the previous year and the newly set up industries / plants / services in the current year. But the primary component of the GDP is the former one.

Hence, we can formally write the equation of GDP as:

x(t) = alpha *  x(t – 1) + error (t)

This equation is known as AR(1) formulation. The numeral one (1) denotes that the next instance is solely dependent on the previous instance.  The alpha is a coefficient which we seek so as to minimize the error function. Notice that x(t- 1) is indeed linked to x(t-2) in the same fashion. Hence, any shock to x(t) will gradually fade off in future.

For instance, let’s say x(t) is the number of juice bottles sold in a city on a particular day. During winters, very few vendors purchased juice bottles. Suddenly, on a particular day, the temperature rose and the demand of juice bottles soared to 1000. However, after a few days, the climate became cold again. But, knowing that the people got used to drinking juice during the hot days, there were 50% of the people still drinking juice during the cold days. In following days, the proportion went down to 25% (50% of 50%) and then gradually to a small number after significant number of days. The following graph explains the inertia property of AR series:

time series, ar1 model

 
Moving Average Time Series Model

Let’s take another case to understand Moving average time series model.

A manufacturer produces a certain type of bag, which was readily available in the market. Being a competitive market, the sale of the bag stood at zero for many days. So, one day he did some experiment with the design and produced a different type of bag. This type of bag was not available anywhere in the market. Thus, he was able to sell the entire stock of 1000 bags (lets call this as x(t) ). The demand got so high that the bag ran out of stock. As a result, some 100 odd customers couldn’t purchase this bag. Lets call this gap as the error at that time point. With time, the bag had lost its woo factor. But still few customers were left who went empty handed the previous day. Following is a simple formulation to depict the scenario :

x(t) = beta *  error(t-1) + error (t)

If we try plotting this graph, it will look something like this :

time series, ma1 model

Did you notice the difference between MA and AR model? In MA model, noise / shock quickly vanishes with time. The AR model has a much lasting effect of the shock.

 
Difference between AR and MA models

The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference gets exploited irrespective of having the AR model or MA model. The correlation plot can give us the order of MA model.

 
Exploiting ACF and PACF plots

Once we have got the stationary time series, we must answer two primary questions:

Q1. Is it an AR or MA process?

Q2. What order of AR or MA process do we need to use?

The trick to solve these questions is available in the previous section. Didn’t you notice?

The first question can be answered using Total Correlation Chart (also known as Auto – correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now let’s reflect on what we have learnt above.

In a moving average series of lag n, we will not get any correlation between x(t) and x(t – n -1) . Hence, the total correlation chart cuts off at nth lag. So it becomes simple to find the lag for a MA series. For an AR series this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?

Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance,if we have a AR(1) series,  if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag (x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. Following are the examples which will clarify any doubts you have on this concept :

                            ACF                                                                      PACF

acf, gradual declinepacf, cut off

 

The blue line above shows significantly different values than zero. Clearly, the graph above has a cut off on PACF curve after 2nd lag which means this is mostly an AR(2) process.

                                      ACF                                                                 PACF

acf, cut offpacf, gradual decline

Clearly, the graph above has a cut off on ACF curve after 2nd lag which means this is mostly a MA(2) process.

Till now, we have covered on how to identify the type of stationary series using ACF & PACF plots. Now, I’ll introduce you to a comprehensive framework to build a time series model.  In addition, we’ll also discuss about the practical applications of time series modelling.

 
4. Framework and Application of ARIMA Time Series Modeling

A quick revision, Till here we’ve learnt basics of time series modeling, time series in R and ARMA modeling. Now is the time to join these pieces and make an interesting story.

 
Overview of the Framework

This framework(shown below) specifies the step by step approach on ‘How to do a Time Series Analysis‘:

time series analysis, arima, flowchart

As you would be aware, the first three steps have already been discussed above. Nevertheless, the same has been delineated briefly below:

 
Step 1: Visualize the Time Series

It is essential to analyze the trends prior to building any kind of time series model. The details we are interested in pertains to any kind of trend, seasonality or random behaviour in the series. We have covered this part in the second part of this series.

 
Step 2: Stationarize the Series

Once we know the patterns, trends, cycles and seasonality , we can check if the series is stationary or not. Dickey – Fuller is one of the popular test to check the same. We have covered this test in the first part of this article series. This doesn’t ends here! What if the series is found to be non-stationary?

There are three commonly used technique to make a time series stationary:

1.  Detrending : Here, we simply remove the trend component from the time series. For instance, the equation of my time series is:

x(t) = (mean + trend * t) + error

We’ll simply remove the part in the parentheses and build model for the rest.

 

2. Differencing : This is the commonly used technique to remove non-stationarity. Here we try to model the differences of the terms and not the actual term. For instance,

x(t) – x(t-1) = ARMA (p ,  q)

This differencing is called as the Integration part in AR(I)MA. Now, we have three parameters

p : AR

d : I

q : MA

 

3. Seasonality : Seasonality can easily be incorporated in the ARIMA model directly. More on this has been discussed in the applications part below.

 
Step 3: Find Optimal Parameters

The parameters p,d,q can be found using  ACF and PACF plots. An addition to this approach is can be, if both ACF and PACF decreases gradually, it indicates that we need to make the time series stationary and introduce a value to “d”.

 
Step 4: Build ARIMA Model

With the parameters in hand, we can now try to build ARIMA model. The value found in the previous section might be an approximate estimate and we need to explore more (p,d,q) combinations. The one with the lowest BIC and AIC should be our choice. We can also try some models with a seasonal component. Just in case, we notice any seasonality in ACF/PACF plots.

 
Step 5: Make Predictions

Once we have the final ARIMA model, we are now ready to make predictions on the future time points. We can also visualize the trends to cross validate if the model works fine.

 
Applications of Time Series Model

Now, we’ll use the same example that we have used above. Then, using time series, we’ll make future predictions. We recommend you to check out the example before proceeding further.

 
Where did we start ?

Following is the plot of the number of passengers with years. Try and make observations on this plot before moving further in the article.

time series r, plot_ap

Here are my observations :

1. There is a trend component which grows the passenger year by year.

2. There looks to be a seasonal component which has a cycle less than 12 months.

3. The variance in the data keeps on increasing with time.

We know that we need to address two issues before we test stationary series. One, we need to remove unequal variances. We do this using log of the series. Two, we need to address the trend component. We do this by taking difference of the series. Now, let’s test the resultant series.

adf.test(diff(log(AirPassengers)), alternative="stationary", k=0)

Augmented Dickey-Fuller Test

data: diff(log(AirPassengers))
 Dickey-Fuller = -9.6003, Lag order = 0,
 p-value = 0.01
 alternative hypothesis: stationary

We see that the series is stationary enough to do any kind of time series modelling.

Next step is to find the right parameters to be used in the ARIMA model. We already know that the ‘d’ component is 1 as we need 1 difference to make the series stationary. We do this using the Correlation plots. Following are the ACF plots for the series :

#ACF Plots

acf(log(AirPassengers))

time series r, acf

 
What do you see in the chart shown above?

Clearly, the decay of ACF chart is very slow, which means that the population is not stationary. We have already discussed above that we now intend to regress on the difference of logs rather than log directly. Let’s see how ACF and PACF curve come out after regressing on the difference.
[stextbox id="grey"]

acf(diff(log(AirPassengers)))

time series r, acf diff

pacf(diff(log(AirPassengers)))

time series r, pacf, diff

Clearly, ACF plot cuts off after the first lag. Hence, we understood that value of p should be 0 as the ACF is the curve getting a cut off. While value of q should be 1 or 2. After a few iterations, we found that (0,1,1) as (p,d,q) comes out to be the combination with least AIC and BIC.

Let’s fit an ARIMA model and predict the future 10 years. Also, we will try fitting in a seasonal component in the ARIMA formulation. Then, we will visualize the prediction along with the training data. You can use the following code to do the same :

(fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))

pred <- predict(fit, n.ahead = 10*12)

ts.plot(AirPassengers,2.718^pred$pred, log = "y", lty = c(1,3))
```





```{r calculate_SRI_prelim-test-DELETE, eval=FALSE}
# SCI prelim - bha_bat -- set up the station for sci & make sci list vars====  
sta      <- bhp_bat_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                             
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    # month 12 fail
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)   
  
# SCI prelim - bhp_bev -- set up the station for sci & make sci list vars====    
sta      <- bhp_bev_v  
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)                             
sci_3mo  <- sci.fun(sta, 3)                             
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    
sci_9mo  <- sci.fun(sta, 9)    
sci_12mo <- sci.fun(sta, 12)   
  
# SCI prelim - bha_fal -- set up the station for sci & make sci list vars====    
sta      <- bhp_fal_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)    # MLE fail for month 4                      
sci_3mo  <- sci.fun(sta, 3)    # MLE fail for month 1-, 5-        
sci_4mo  <- sci.fun(sta, 4)    
sci_6mo  <- sci.fun(sta, 6)    # MLE fail for month 6-  
sci_9mo  <- sci.fun(sta, 9)    # MLE fail for month 7-, 8-  
sci_12mo <- sci.fun(sta, 12)   # MLE fail for month 8-, 9-  
  
# SCI prelim - bha_frn -- set up the station for sci & make sci list vars====  
sta      <- bhp_frn_v   
sci_1mo  <- sci.fun(sta, 1)                             
sci_2mo  <- sci.fun(sta, 2)    # MLE fail for month 8-, 11-                 
sci_3mo  <- sci.fun(sta, 3)           
sci_4mo  <- sci.fun(sta, 4)    # MLE fail for month 1- 
sci_6mo  <- sci.fun(sta, 6)     
sci_9mo  <- sci.fun(sta, 9)     
sci_12mo <- sci.fun(sta, 12)   
    
# get distribution parameters====  
dist_01 <- sci_1mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "1")  

dist_02 <- sci_2mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "2")   

dist_03 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "3")   

dist_04 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "4")   

dist_06 <- sci_6mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "6")   

dist_09 <- sci_9mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "9")   

dist_12 <- sci_12mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "12")   

dist_bhp_frn <- bind_rows(dist_01,  
                          dist_02,  
                          dist_03,  
                          dist_04,  
                          dist_06,  
                          dist_09,  
                          dist_12  
                          ) %>% 
  mutate(sta = "bhp_frn")  

rm(dist_01,  
   dist_02,  
   dist_03,  
   dist_04,  
   dist_06,  
   dist_09,  
   dist_12  
)  
  
# get distribution parameters====  
dist_01 <- sci_1mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "1")  

dist_02 <- sci_2mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "2")   

dist_03 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "3")   

dist_04 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "4")   

dist_06 <- sci_6mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "6")   

dist_09 <- sci_9mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "9")   

dist_12 <- sci_12mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "12")   

dist_bhp_fal <- bind_rows(dist_01,  
                          dist_02,  
                          dist_03,  
                          dist_04,  
                          dist_06,  
                          dist_09,  
                          dist_12  
                          ) %>% 
  mutate(sta = "bhp_fal")  

rm(dist_01,  
   dist_02,  
   dist_03,  
   dist_04,  
   dist_06,  
   dist_09,  
   dist_12  
)  
  
# get distribution parameters====  
dist_01 <- sci_1mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "1")  

dist_02 <- sci_2mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "2")   

dist_03 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "3")   

dist_04 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "4")   

dist_06 <- sci_6mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "6")   

dist_09 <- sci_9mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "9")   

dist_12 <- sci_12mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "12")   

dist_bhp_bev <- bind_rows(dist_01,  
                          dist_02,  
                          dist_03,  
                          dist_04,  
                          dist_06,  
                          dist_09,  
                          dist_12  
                          ) %>% 
  mutate(sta = "bhp_bev")  

rm(dist_01,  
   dist_02,  
   dist_03,  
   dist_04,  
   dist_06,  
   dist_09,  
   dist_12  
)  
  
# get distribution parameters====  
dist_01 <- sci_1mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "1")  

dist_02 <- sci_2mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "2")   

dist_03 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "3")   

dist_04 <- sci_3mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "4")   

dist_06 <- sci_6mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "6")   

dist_09 <- sci_9mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "9")   

dist_12 <- sci_12mo[["dist.para"]] %>% 
  t() %>%  
  as.data.frame() %>% 
  rownames_to_column(var = "month") %>% 
  mutate(duration = "12")   

dist_bhp_bat <- bind_rows(dist_01,  
                          dist_02,  
                          dist_03,  
                          dist_04,  
                          dist_06,  
                          dist_09,  
                          dist_12  
                          ) %>% 
  mutate(sta = "bhp_bat")  

rm(dist_01,  
   dist_02,  
   dist_03,  
   dist_04,  
   dist_06,  
   dist_09,  
   dist_12  
)  
  
# get the stations & months with MLE non-convergence  
fail_bhp <- dist_bhp %>% 
  filter(is.na(shape)) %>% 
  select(sta, duration, month)  

# get group means 
dist_bhp_g <- dist_bhp %>%   
  filter(!is.na(shape)) %>%  
  group_by(duration, month) %>%   
  summarize(shape       = mean(shape),   
            scale       = mean(scale),  
            location    = mean(location)   
            ) %>% 
  ungroup()  

# create final group means for non-converging stations  
dist_bhp_g <- semi_join(dist_bhp_g, fail_bhp,  
                      by = c("month", "duration")) 

dist_bhp_g <- full_join(fail_bhp, dist_bhp_g, 
                      by = c("month", "duration")
                      )  

dist_bhp_fill <- dist_bhp %>% 
  select(month, duration, P0, N.P0, N) %>% 
  filter(!is.na(P0)) %>%  
  group_by(month, duration) %>% 
  summarise(P0 = mean(P0),  
            N.P0 = mean(P0),  
            N = mean(N)) %>% 
  ungroup()  

dist_bhp_g <- left_join(dist_bhp_g, dist_bhp_fill, 
                         by = c("duration", "month")
                         )   

M12_06 <- dist_bhp_g %>% 
  select(-c(1:3)) %>% 
  slice(1) %>% 
  as.double()

rm(fail_bhp,  
   dist_bhp_bat,  
   dist_bhp_bev,  
   dist_bhp_fal,  
   dist_bhp_frn  
   )    
# join distribution parameters -- blp====  
dist_bhp <- bind_rows(dist_bhp_bat,  
                      dist_bhp_bev,  
                      dist_bhp_fal,  
                      dist_bhp_frn  
                      )  
```