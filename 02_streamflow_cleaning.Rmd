---
title: "02_streamflow_cleaning"
author: "CJ Tinant"
date: "10/28/2019"
output: html_document
---

<!--
This R markdown file refactors prior code for data cleaning 

Data:
Predominant datasets used are:  
1) USGS daily streamflow and station metadata,  
2) Summary data from a QGIS analysis of ungaged watersheds of interest.  

Approach: 
1) find potential USGS gages by bounding box 
     bBox = a contiguous range of decimal latitude and longitude, starting 
     with the west longitude, then the south latitude, then the east longitude, 
     and then the north latitude with each value separated by a comma. 
# https://waterservices.usgs.gov/rest/Site-Service.html#bBox 
# the Pine Ridge Reservation boundary is c(-103.0, 43.0, -100.2, 43.8)

2) filter gages for the following reasons
     - upstream control, 
     - in crystaline or karst catchments
     - provisional data 
     - (sigh) overlooked in the first data screening from 1990-2017 

3) clean metadata by removing non-needed variables  

4) obtain USGS gage daily flow data

5) filter stations based on record length 
    - no data gages for 1980-2018 water years 
    - less than 6-years of continuous data for 1990-2018
      for filtering info see: https://help.waterdata.usgs.gov/site_tp_cd 

Next Steps: 
-- check into stylr-packag

Variable naming convention----  
gage                USGS gaging stations
  _poss             possible USGS gaging stations in the study area 
  _meta             metadata for possible gaging stations
  _int1             scratch df for pulling gages with integer fields 
  _int2             scratch df for pulling gages with integer fields 

wsd  
  _summary
names   

endDate             used for calling Egret::readNWISDaily  -- "1979-08-01"
parameter_cd        used for calling Egret::readNWISDaily  -- "2018-09-30"  
startDate           used for calling Egret::readNWISDaily  -- "00060" 

lon_riv_dv          scratch df for pulling lon_riv 
gage_most_dv        scratch df for pulling !lon_riv 
yrs_rec             calculates years of record 
gage_dv             daily flow values for waterYear 1980-2017 
gage_check          scratch variable used to check operations 

gage_meta           cleaned metadata for USGS gaging stations in the study area 
--> 

```{r setup_&_library, message=FALSE}  
  
knitr::opts_chunk$set(echo = FALSE)     
options(tibble.print_max = 70) # sets tibble output for printing        
  

# Sets up the library of packages   
library("conflicted")        # An alternative conflict resolution strategy  
library("here")              # identifies where to save work  
library("dataRetrieval")     # USGS data import  
library("EGRET")             # Exploration and Graphics for RivEr Trends  
library("rio")               # more robust I/O - to import and clean data  
library("sf")                # simple features--spatial geometries for R  
library("lubridate")         # easier dates 
library("forecast")          # for BoxCox.lambda 
library("stringi")           # character string processing facilities  
library("broom")             # convert statistical objs into tidy tibbles 
library("tidyverse")  


# resolve conflicted packages----  
conflict_prefer("filter", "dplyr")  
conflict_prefer("select", "dplyr")  
  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
why_to_write <- function()   
{today <- today(tzone = "") 
paper2 <- ymd("2019-11-15")  
until <- paper2 - today  
print(paste("You have", until, "days until the second paper is due"  
))  
}  
why_to_write()  
```  

```{r import_daily_flow_metadata}  
  
# get possible stations from NWIS website----  
gage_poss <- whatNWISsites(bBox = c(-103.8, 42.2, -99.2, 44.6),    
                           parameterCd = "00060",  
                           hasDataTypeCd = "dv") %>%  
  arrange(site_no)   
  
# 2. get stream station metadata====   
# this needs to be run in parts because some project numbers & 
# inventories are stored as integers & others are stored as characters 
  
gage_meta_int1 <- gage_poss %>% 
  slice(7, 17, 142, 148) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 
  
gage_meta_int2 <- gage_poss %>% 
  slice(94) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 

gage_meta_int3 <- gage_poss %>% 
  slice(-c(7, 17, 94, 142, 148)) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 
  
# 3. join gage metadata 
gage_meta_poss <- bind_rows(gage_meta_int1, 
                            gage_meta_int2, 
                            gage_meta_int3) %>% 
  #gage_meta_poss <- gage_meta_poss %>%  
  mutate(site_no = zeroPad(site_no, 8))  
  
# clean up 
rm(gage_meta_int1,  
   gage_meta_int2,  
   gage_meta_int3,  
   gage_poss  
)    

# 4. export metadata to data folder----  
#export(gage_meta_poss, "data/gage_meta_poss.csv")  
 
``` 
  
```{r clean_gage_metadata} 
  
gage_meta_poss <- import("data/gage_meta_poss.csv")       
  
# select final stations for consideration----  
# 1. remove gages that do not meet standards====    
gage_meta <- gage_meta_poss %>%    
  mutate(site_no = as.character(site_no)) %>%  
  mutate(site_no = zeroPad(site_no, 8)) %>%              # pad site_no   
  mutate(reliability_cd = replace_na(reliability_cd, 0))  %>% #  NA <- zero  
  filter(site_no != "06461150"&  
           site_no != "06463670"&   
           site_no != "06461595") %>%  # remove short sites w/ provisional data  
  filter(site_no != "06441000") %>%  # only active 180 days/yr   
  filter(site_no != "06438000") %>%  
  filter(site_no != "06437000") %>%  # remove northern Black Hills stations  
  filter(site_no != "06442718") %>%  # remove East River stations  
  filter(reliability_cd != "M") %>% # M is minimal data  
  filter(!str_detect(station_nm, 'DAM|DITCH|DRAIN')) %>% # upstream control  
  filter(!str_detect(station_nm,  
                     'CUSTER|KEYSTONE|HILL CITY|HAYWARD')) %>%  
  filter(site_no != "06424000") %>%  
  # crystaline catchments  
  filter(!str_detect(station_nm,  
                     'LEAD|DEADWOOD|WHITEWOOD')) %>%   
  # crystaline catchments  
  filter(!str_detect(station_nm, 'CLEGHORN')) %>% # karstic? spring  
  filter(!str_detect(station_nm, 'BOXELDER|LIME')) %>% # karstic  
  filter(!str_detect(station_nm, 'RAPID')) %>%    
  # Rapid Creek & upper Spring Creek  
  filter(!str_detect(station_nm, 'MISSOURI'))  
  
# 2. remove provisional sites====       
gage_meta <- gage_meta %>%  
  mutate(site_no = as.character(site_no)) %>%  
  select(-reliability_cd) %>%  
  mutate(length = str_length(site_no)) %>%  
  filter(length <= 8) %>%   # removes two provisional sites  
  select(-length)  
  
# 3. remove codes not needed for this project  
gage_meta <- gage_meta %>%  
  select(-agency_cd) %>%  
  select(-site_tp_cd) %>% # all streams so delete 
  select(-c(lat_va, long_va)) %>% # in DMS so delete  
  select(-c(coord_meth_cd, coord_acy_cd)) %>% # coord meth & agency, so delete  
  select(-c(coord_datum_cd, dec_coord_datum_cd)) %>% # NAD83 or NAD27  
  select(-c(district_cd, country_cd)) %>% # Congressional dist & Country  
  select(-c(land_net_ds, map_nm, map_scale_fc)) %>%  # refers to USGS maps  
  select(-c(alt_meth_cd, alt_datum_cd, alt_acy_va)) %>% #%>% # alt metadata   
  select(-c(basin_cd, topo_cd, instruments_cd)) %>%  
  select(-c(construction_dt)) %>%  
  select(-c(tz_cd, local_time_fg)) %>% # daily data, so NA  
  select(-c(gw_file_cd, nat_aqfr_cd, aqfr_type_cd, aqfr_cd)) %>%  
  select(-c(well_depth_va, hole_depth_va, depth_src_cd))  

rm(gage_meta_poss)  
  
```  
  
```{r add_names_to_gage_meta}  
  
# get data from QGIS analysis of gaged & ungaged watersheds of interest====    
wsd_summary <- st_read("sp_data/eco-drought.gpkg",   
                       layer = "wbd_summary",  
                       as_tibble = TRUE) %>%  
  st_drop_geometry() %>%  
  mutate_if(is.factor, as.character)  
  
# join names to gage_meta   
names <- wsd_summary %>%  
  select(site_no = gage_num, sta = sta_id) %>%  
  drop_na() %>%  
  mutate(site_no = zeroPad(site_no, 8))  
  
gage_meta <- full_join(names, gage_meta,   
                       by ="site_no")  
  
gage_meta <- gage_meta %>%  
  filter(sta != " ")  
  
# export metadata to data folder----   
#export(gage_meta, "data/gage_meta.csv")  
  
``` 

```{r import_daily_flow_data, message=FALSE} 
  
# this is a small amount of code duplication to reduce API calls    
#   to allow for working remotely -- set above to eval=FALSE   
  
# set parameters for Egret::readNWISDaily     
startDate    <- "1979-08-01" # pulling two months early to get Q7 & Q30    
endDate      <- "2018-09-30"    
parameter_cd <- "00060"   
  
# get daily flows----   
# note for EGRET::readNWISDailyQ the discharge is in m^3/s   
  
# Long River gage needs to be called separately or it creates an error   
gage_lonriv_dv <- gage_meta %>%   
  filter(site_no == "06463500") %>%   
  split(.$site_no)  %>%   
  map_dfr(~ readNWISDaily(   
    siteNumber = .$site_no,   
    parameter_cd,   
    startDate,   
    endDate),   
    .id = "site_no")   
   
gage_most_dv <- gage_meta %>%   
  filter(site_no != "06463500") %>%           # drop long river from call   
  split(.$site_no) %>%    
  map_dfr(~ readNWISDaily(   
    siteNumber = .$site_no,   
    parameter_cd,   
    startDate,   
    endDate),  
    .id = "site_no")   
  
# remove the 1978 data that was needed to calculate Q7 & Q30   
gage_dv <-bind_rows(gage_most_dv, gage_lonriv_dv) %>%   
  filter(waterYear > "1979")   

rm(gage_lonriv_dv, gage_most_dv)  
  
```  

```{r filter_short_flow_recs}
  
# 1. prepare to filter gages with short flow records----     
# calculate min & max years of record   
yrs_summary <- gage_dv %>% 
  group_by(site_no) %>%                  
  summarise(years_rec = n_distinct(waterYear),   
            min_year = min(waterYear),   
            max_year = max(waterYear)   
            ) %>%                     
  ungroup() %>%  
  mutate(apparent_yrs = 1 + max_year - min_year) %>% 
  mutate(yeas_rec = as.numeric(years_rec))   
  
gage_meta <- import("data/gage_meta.csv") %>% 
    mutate(site_no = zeroPad(site_no, 8))  
  
gage_meta <- full_join(yrs_summary, gage_meta,        
                       by = "site_no")  
  
rm(yrs_summary)  
  
# 2. filter gages with less than 6 years of record----    
# filter gages with no records  
gage_check <- gage_meta %>%  
  filter(is.na(years_rec))  
  
gage_meta <- gage_meta %>%  
  filter(!is.na(years_rec))                         
  
# drop stations with less than 6 years of record   
gage_check <- gage_meta %>%  
  filter(years_rec < 6)  
  
gage_meta <- gage_meta %>%  
  filter(years_rec >= 6)                        
  
# 3. check for incomplete days of record====    
yr_incomp <- gage_dv %>%                           
  group_by(site_no, waterYear) %>%                   
  summarise(days_yr = n()) %>%                     
  ungroup() %>%  
  filter(days_yr < 360) %>%  
  group_by(site_no) %>%    
  summarise(yrs_incomp = n()) %>%     
  ungroup()  
  
gage_meta <- full_join(yr_incomp, gage_meta,   
                       by = "site_no") %>%  
  filter(!is.na(years_rec))  
  
# 4. remove data with short record between 1990-2015====  
gage_check <- gage_meta %>%  
  filter(sta == "")  
  
gage_meta <- gage_meta %>%  
  filter(sta != "")  
  
## These didn't make the cut because the original data selection was  
# for 1990-2015  & it would be extremely difficult to put in post-hoc.  
# it would be good for the future!  
# Also - its possible to use these for model testing...  
  
# station   min_yr  maxyr  name  
# 06442000   1980   1990   MEDICINE KNOLL CR NEAR BLUNT SD   
# 06459175   1982   1995   SNAKE R AT DOUGHBOY, NE  
# 06459500   1980   1995   SNAKE RIVER NEAR BURGE, NEBR.  
# 06454100   1980   1992   Niobrara River at Agate, Nebr.  
# 06461000   1980   1995   MINNECHADUZA CREEK AT VALENTINE, NEBR.  
# 06462500   1980   1995   PLUM CREEK AT MEADVILLE, NE 
# 06463720   2012   2018   Niobrara River at Mariaville, Nebr.  
# 06400497   1980   1995   CASCADE SPRINGS NEAR HOT SPRINGS SD  
# 06462000   1980   1986   NIOBRARA RIVER NR NORDEN NEBR 
# 06463080   1980   1991   LONG PINE CREEK NR LONG PINE, NE 
# 06439300   1980   1994   CHEYENNE RIVER AT CHERRY CREEK,SD 
# 06442500   1980   1990   MEDICINE CR AT KENNEBEC SD 
# 06444000   1980   2004   White River at Crawford, Nebr. 
# 06454500   1980   1994   NIOBRARA RIVER ABOVE BOX BUTTE RESERVOIR, NE 
# 06455500   1980   1991   NIOBRARA RIVER BELOW BOX BUTTE RESERVOIR NEBR 
# 06457500   1980   1991   NIOBRARA RIVER NEAR GORDON, NEBR.  
  
# sync gage_dv w/ gage_meta & export results as posthoc to avoid interference  
gage_dv <- full_join(names, gage_dv,   
                     by ="site_no")    
  
gage_dv <- semi_join(gage_dv, gage_meta,   
                     by = "site_no")     
 
#export(gage_meta, "data/gage_meta.csv")   
#export(gage_dv, "data/gage_dv.csv")   
  
# clean up  
rm(startDate,  
   endDate,  
   parameter_cd,  
   wsd_summary,  
   names,   
   gage_check,  
   yr_incomp  
   )    
  
```  

```{r clean_daily_flow} 
  
# this is a small amount of code duplication to reduce API calls     
  
gage_dv <- import("data/gage_dv.csv") %>% 
    mutate(site_no = zeroPad(site_no, 8)) 

gage_meta <- import("data/gage_meta.csv") %>%  
    mutate(site_no = zeroPad(site_no, 8)) 

# filter missing records from Q1, Q7, Q30 
gage_check <- gage_dv %>% 
  filter(is.na(Q30) | 
           is.na(Q7) | 
           is.na(Q) 
  )  

gage_dv <- gage_dv %>% 
  filter(!is.na(Q30) & 
           !is.na(Q7) & 
           !is.na(Q)  
  )  

gage_check <- gage_dv %>% 
  filter(is.na(Q30) | 
           is.na(Q7) | 
           is.na(Q) 
  )  

# 4. remove provincial data - we removed WY 2018 in the prior analysis 
gage_dv <- gage_dv %>% 
  filter(Qualifier == "A"   |  
           Qualifier == "A:e" |  
           Qualifier == "A:<" 
  )   

gage_check <- gage_dv %>% 
  filter(Qualifier != "A") %>% 
  filter(Qualifier != "A:e") %>%  
  filter(Qualifier != "A:<")  

# 5. remove 2018 data 
gage_dv <- gage_dv %>% 
  filter(waterYear != "2018")  

gage_check <- gage_dv %>%  
  filter(waterYear == "2018")  

rm(gage_check)   

#export(gage_dv, "data/gage_dv.csv")   
  
``` 

```{r censor_low_flows, message=FALSE} 

gage_dv <- import("data/gage_dv.csv")  

# fix low flows ---- this step needed for the PCA    
#   EGRET calculates a "better" zero-flow value, but causes issues with results  
#   this code chunk fixes low flow values by substituting 0.01 cfs    
#   for zero-flow values   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   

# 1. gather the different flow values  
gage_dv_gath <- gage_dv %>%  # using diff var name to check length later   
  select(site_no, sta, Date, waterYear, Q, Q7, Q30) %>%  
  gather(key = q_type, val = q_val,       # prepares to censor to 0.01 cfs  
         -c(site_no, sta, waterYear, Date))   

# 2. filter & censor zero flows & non-zero flows -> join flows  
gage_dv_low <- gage_dv_gath %>%  # n ~ 123,987/3 = 41,329 obs or ~7%/yr 
  filter(q_val < 0.01) %>%                
  mutate(q_val = 0.01)  

gage_dv_high <- gage_dv_gath %>%  
  filter(q_val >= 0.01) %>%  
  mutate(q_val = round(.$q_val, digits = 2))  

gage_dv <- bind_rows(gage_dv_high, gage_dv_low) %>%  
  spread(q_type, q_val)  

# 3. check low-flow stations -- intermittant stations 
gage_low_flow <- gage_dv_low %>%  
  distinct(sta) %>% 
  arrange(sta)  

rm(gage_dv_gath, 
   gage_dv_high,  
   gage_dv_low  
   )     

# incorporate zero flow into a gage id  
gage_low_flow <- gage_low_flow %>%  
  mutate(flow_regime = "intermittent") 

gage_meta <- full_join(gage_meta, gage_low_flow) 

gage_meta <-gage_meta %>%  
  mutate(flow_regime = 
           replace_na(flow_regime, "perennial")  
         )  

# export results  
#export(gage_meta, "data/gage_meta.csv")    
#export(gage_dv, "data/gage_dv_fin.csv")  
  
``` 

```{r convert_flow_to_depth, message=FALSE} 

# 1. calculate contributing drainage area in square kilometers & sec/day     
gage_contrib_area <- gage_meta %>%   
  select(sta, drain_area_va, contrib_drain_area_va) %>%  
  mutate(contrib_drain_area_va =   
           contrib_drain_area_va %>%  
             is.na %>%   
             ifelse(drain_area_va, contrib_drain_area_va)) %>%   
  select(-drain_area_va) %>%   
  mutate(contrib_drain_km2 =  
           measurements::conv_unit(contrib_drain_area_va,  
                                   from = "mi2",   
                                   to = "km2"  
                                   )  
         ) %>%  
  mutate(contrib_drain_km2 =  
           round(  
             contrib_drain_km2,  
             digits = 1  
             )  
         ) %>%  
  mutate(sec_per_day = 60 * 60 * 24)  
  
# 2. calculate Q-depth:: q = Q/DA [m/km2 day]    
gage_depth <- left_join(gage_dv, gage_contrib_area, by = "sta") %>%  
  select(site_no, sta, Date, Q, Q7, Q30, waterYear,  
         contrib_drain_km2, sec_per_day)  %>%  
  gather(type, value,   
         -c(site_no, sta, Date, waterYear, contrib_drain_km2, sec_per_day)  
         ) %>%  
  mutate(value = value * sec_per_day / contrib_drain_km2) %>%  
  spread(type, value) %>%  
  rename(q1_depth = Q) %>%  
  rename(q7_depth = Q7) %>%  
  rename(q30_depth = Q30) %>%  
  select(-c(site_no, contrib_drain_km2, sec_per_day))   
  
rm(gage_contrib_area)    
  
#export(gage_depth, "data/gage_depth.csv")    
  
``` 

```{r PCA-calculations}  

gage_depth <- import("data/gage_depth.csv") 
  
# Prepare to approach normality using forcast::BoxCox----    
# make a dataframe of lambda values====   
lambda_q1  <- enframe(   
  BoxCox.lambda(gage_depth$q1_depth)   
  )    
 
lambda_q7  <- enframe(  
  BoxCox.lambda(gage_depth$q7_depth)  
  )   
  
lambda_q30 <- enframe(  
  BoxCox.lambda(gage_depth$q30_depth)  
  )  
  
lambda_vals <- bind_cols(lambda_q1,  
                         lambda_q7,  
                         lambda_q30  
                         ) %>%   
  select(-c(name1, name2)) %>%  
  rename(lambda_q1  = value) %>%   
  rename(lambda_q7  = value1) %>%  
  rename(lambda_q30 = value2)  
  
rm(lambda_q1, lambda_q7, lambda_q30)    
  
# create a PCA input dataframe & PCA matrix====   
pca_input <- gage_depth %>%  
  mutate(q1_tr = BoxCox(.$q1_depth,  
                        lambda_vals$lambda_q1)   
         ) %>%  
  mutate(q7_tr = BoxCox(.$q7_depth,  
                        lambda_vals$lambda_q7)  
         ) %>%  
  mutate(q30_tr = BoxCox(.$q30_depth,  
                         lambda_vals$lambda_q30)  
         ) %>%  
  select(sta,  
         Date,  
         q1_tr,  
         q7_tr,  
         q30_tr  
         )    
  
pca_matrix <- pca_input %>%  
  select(q1_tr,  
         q7_tr,  
         q30_tr  
         ) %>%   
  prcomp(.,  
         center = TRUE,  
         scale. = TRUE  
         )        

# Gather & summarize PCA results----  
# results about PC axes  
pca_eigen <- tidy(pca_matrix, 
                  matrix = "pcs"  
                  )       

# PCA loadings 
pca_vars <-  tidy(pca_matrix,  
                  matrix = "variables"  
                  ) %>%     
  filter(PC != 3) %>%              
  rename(var = column) %>%  
  mutate(PC = as.character(PC)) %>%  
  mutate(pc_stem = "PC") %>%  
  unite("pc_axis",  
        c("pc_stem",  
          "PC"  
          ),  
        sep = "",  
        remove = TRUE  
        ) %>%   
  spread(pc_axis,  
         value) %>%  
  mutate(labels = c("q1", "q30", "q7")  
         )   
  
# Bind sample vals to PCA matrix  
gage_depth <- augment(pca_matrix, 
                   data = gage_depth  
                   ) %>% 
  select(-c(.rownames, 
            .fittedPC3)
         ) %>%       
      mutate(q1_q30_diff = q1_depth - q30_depth)  
  
# Save results  
#export(gage_depth, "data/gage_depth.csv")   
#export(pca_eigen, "data/pca_eigen.csv")  
#export(pca_vars, "data/pca_vars.csv")   
  
rm(pca_eigen, pca_input, pca_matrix, pca_vars, lambda_vals)  
  
```  

```{r add_ecoreg_&_metadata}

#gage_meta <- import("data/gage_meta.csv")  

# 1. read in shapefile data of intersection of watershed area &  
# merged SD, NE, WY ecoregions----  
ecoreg <- st_read("sp_data/ecoreg_int.shp",  
                       as_tibble = TRUE) %>%  
  st_drop_geometry() %>%                       #  we only want the data table  
  mutate_if(is.factor, as.character) %>%   
  rename(ecoreg_L4 = US_L4NAME) %>%     
  rename(ecoreg_L3 = US_L3NAME) %>%  
  rename(ecoreg_L2 = NA_L2NAME) %>%  
  rename(ecoreg_L1 = NA_L1NAME) %>%  
  rename(area_km = Shape_Area) %>%   
  select(sta_id, watshed, ecoreg_L4, ecoreg_L3,  
         ecoreg_L2, ecoreg_L1, area_km) %>%  
# prep for gather -> change title case  
  mutate(grouped_id = row_number()) %>%  
  gather(key, val, -c(sta_id, watshed, grouped_id)) %>%  
  mutate(val = stri_trans_totitle(val)) %>%  
  spread(key, val) %>%  
  select(-grouped_id) %>%  
  mutate(area_km = as.numeric(area_km)  # round the areas  
         ) %>%  
  mutate(area_km = round(area_km, digits = 2)  
         )%>%  
# get percentages by ecoregion  
  group_by(sta_id) %>%  
  mutate(area_tot = sum(area_km)) %>%  
  mutate(area_perc = round(  
    area_km / area_tot,  
    digits = 2)  
  ) %>%  
  ungroup() %>%  
  filter(area_perc > 0.10) %>%   
  arrange(  
    desc(area_perc)  
    ) %>%  
  arrange(sta_id) %>%  
  select(sta_id, area_perc, ecoreg_L4,  
         ecoreg_L3, ecoreg_L2, ecoreg_L1, watshed)  
  
# 2. generalize ecoregions using max percentage & percentage near the gage  
ecoreg <- ecoreg %>%  
  group_by(sta_id) %>%   
  arrange(sta_id, desc(area_perc)) %>%  
  # make a default pick based on the max %  
  mutate(ecoreg_wtsd = head(ecoreg_L4, 1) ) %>%  
  # refine the pick based on expert knowlege & percentage near the gage  
  mutate(  
    ecoreg_wtsd =   
           case_when(  
             sta_id == "bat_bhr" ~ "Semiarid Pierre Shale Plains",  
             sta_id == "blp_bel" ~ "White River Badlands",   
             sta_id == "che_buf" ~ "Semiarid Pierre Shale Plains",  
             sta_id == "che_pla" ~ "Semiarid Pierre Shale Plains",   
             sta_id == "che_red" ~ "Semiarid Pierre Shale Plains",  
             sta_id == "che_sce" ~ "Semiarid Pierre Shale Plains",  
             sta_id == "che_was" ~ "Semiarid Pierre Shale Plains",  
             sta_id == "lcr_bel" ~ "Sand Hills",      
             sta_id == "lwr_aro" ~ "Sand Hills",  
             sta_id == "lwr_mar" ~ "Sand Hills",  
             sta_id == "lwr_ros" ~ "Sand Hills",  
             sta_id == "lwr_vet" ~ "Sand Hills",  
             sta_id == "lwr_whi" ~ "Sand Hills",  
             sta_id == "spr_her" ~ "Black Hills Plateau",   
             sta_id == "whi_int" ~ "White River Badlands",  
             sta_id == "whi_kad" ~ "White River Badlands",  
             sta_id == "whi_oac" ~ "Subhumid Pierre Shale Plains",   
                           TRUE ~ ecoreg_wtsd)  
   ) %>%   
  select(sta_id, ecoreg_wtsd, everything()) %>%   
  group_by(sta_id) %>%   
  distinct(ecoreg_wtsd, .keep_all = TRUE) %>%   
  # generalize the Pierre shale   
  mutate(  
    ecoreg_wtsd =   
           case_when(  
             ecoreg_wtsd ==  
               "Semiarid Pierre Shale Plains" ~ "Pierre Shale Plains",  
             ecoreg_wtsd ==  
               "Subhumid Pierre Shale Plains" ~ "Pierre Shale Plains",  
                           TRUE ~ ecoreg_wtsd)  
   ) %>%  
  ungroup %>%  
  arrange(ecoreg_wtsd)  
  
# 3. join ecoregions to gage_dv  
eco_join <- ecoreg %>%  
  select(sta = sta_id, ecoreg = ecoreg_wtsd)  
  
gage_depth <- full_join(gage_depth, eco_join,  
                  by = "sta")   
  
rm(eco_join, ecoreg)  

gage_ecoreg <- gage_depth %>% 
  select(sta, ecoreg) %>% 
  distinct()  

gage_meta <- full_join(gage_meta, gage_ecoreg, 
                         by = "sta")  

#export(gage_depth, "data/gage_depth.csv")    
#export(gage_meta, "data/gage_meta.csv")   
  
```   

```{r daily_to_monthly_depth}

gage_depth <- gage_depth %>% 
  mutate(log_q1_depth = log10(q1_depth))    

# average daily depths into monthly mean depths----     
gage_mon <- gage_depth %>%  
  mutate(yr = year(Date)) %>%  
  mutate(mon = month(Date)) %>%  
  mutate(q30_q1_diff = q30_depth - q1_depth) %>%  
  group_by(sta, mon, yr) %>%  
  summarize(waterYear = first(waterYear),   
            log_q1_mon = mean(log_q1_depth),  
            q1_mon     = mean(q1_depth),  
            q7_mon     = mean(q7_depth),  
            q30_mon    = mean(q30_depth),           
            PC1_mon    = mean(.fittedPC1),  
            PC2_mon    = mean(.fittedPC2),  
            q30_q1_mon = mean(abs(q30_q1_diff)),  
            ecoreg      = first(ecoreg)  
  ) %>%  
  mutate(day = 15) %>%   
  ungroup() %>%  
  unite("Date", c("yr", "mon", "day")) %>%  
  mutate(Date = ymd(Date)) %>%  
  as_tibble()   
  
#export(gage_mon, "data/gage_mon.csv")   
  
```

```{r select_complete_records}  

# select complete records for period >1989 to present (n = 29) for SRI  
#gage_mon <- import("data/gage_mon.csv") %>%  
#  mutate(Date = ymd(Date))  

gage_yrs_comp <- gage_mon %>%  
  filter(Date > "1988-12-15") %>% 
  group_by(sta, waterYear) %>% 
  summarise(years = n()   
  ) %>%  
  group_by(sta) %>%  
  summarise(years = n()  
  ) %>%  
  ungroup() %>%  
  filter(years >= 29)  
  
gage_mon_full <- semi_join(gage_mon, gage_yrs_comp,  
                           by = "sta")  
  
#export(gage_mon_full, "data/gage_mon_full.csv")   
  
rm(gage_yrs_comp)  
  
```



