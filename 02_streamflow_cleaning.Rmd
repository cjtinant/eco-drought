---
title: "02_streamflow_cleaning"
author: "CJ Tinant"
date: "10/28/2019"
output: html_document
---

<!--
This R markdown file refactors prior code for data cleaning 

1) Develop tables and plots for Chapter 2 

Data:
Predominant datasets used are:  
1) USGS daily streamflow and station metadata,  
2) Summary data from a QGIS analysis of ungaged watersheds of interest.  

Approach: 
1) find potential USGS gages by bounding box 
     bBox = a contiguous range of decimal latitude and longitude, starting 
     with the west longitude, then the south latitude, then the east longitude, 
     and then the north latitude with each value separated by a comma. 
# https://waterservices.usgs.gov/rest/Site-Service.html#bBox 
# the Pine Ridge Reservation boundary is c(-103.0, 43.0, -100.2, 43.8)

2) filter gages for the following reasons
     - upstream control, 
     - in crystaline or karst catchments
     - provisional data 
     - (sigh) overlooked in the first data screening from 1990-2017 

3) clean metadata by removing non-needed variables  

4) obtain USGS gage daily flow data

5) filter stations based on record length 
    - no data gages for 1980-2018 water years 
    - less than 6-years of continuous data for 1990-2018
      for filtering info see: https://help.waterdata.usgs.gov/site_tp_cd 

6) create metadata tables 
    - gage_table - summary of gage metadata 
    - ung_table - summary of ungaged watershed metadata 
    - soils_table - summary of geology, soils, and vegetation 

7) prepare daily flow data for PCA 
     - check missing records & filter 
     - remove provincial data 
     - eliminate watershed size effects: 
          Daily flow depth is calculated by dividing flow (cms) by watershed 
          area (sq-km) and multiply the resultant by num. of sec. in a day.  
          The resulting unit is cu-m-d per sq-km. 
     - transform the data by BoxCox transformation by forcast::BoxCox
          The daily flow data are highly skewed.  BoxCox transformation uses 
          maximum likelihood to identify a best value (lambda) that best 
          transforms a dataset to an ~ normal distribution.

          # this is table of what the lambda values (probably) mean
          # lambda = 1 is normal distribution (no change), 
          # lambda = 0.5 is a square-root transformation, 
          # lamda = 2 is a square transformation,
          # lambda = 0 is a logrithmic transformation.

8) make a PCA matrix and tidy the output 
     - used prcomp() with center & scale = TRUE to center the data with a 
         mean of zero and standard deviation of unity, e.g. a z-score.  
         Note: '.' passes select(q1_tr, q7_tr, q30_tr) %>% to prcomp() 
     - found eigenvectors-- the results about the PC axes

9) construct bootstrap confidence intervals of PC axes to identify a difference 
     in means. We obtained a 95% confidence interval (95% CI) around the our  
     estimate of the mean difference. The 95% indicates that a confidence 
     interval will capture the population mean difference 95% of the time. 
     So, we can be 95% confident the interval contains the true mean
     of the population.  We can calculated the 95% CI of the mean difference by 
     performing bootstrap resampling (Bradley Efron).

     The bootstrap creates multiple resamples (with replacement) from a single 
     set of observations, and computes the effect size of interest on each of 
     these resamples. The bootstrap resamples of the effect size can then be 
     used to determine the 95% CI.

     The resampling distribution of the difference in means approaches a normal 
     distribution. This is due to the Central Limit Theorem: a large number of 
     independent random samples will approach a normal distribution even if the 
     underlying population is not normally distributed.

# Bootstrap resampling gives us two important benefits:

# Non-parametric statistical analysis. There is no need to assume that our 
# observations, or the underlying populations, are normally distributed. 
# Thanks to the Central Limit Theorem, the resampling distribution of the 
# effect size will approach a normality.
# Easy construction of the 95% CI from the resampling distribution. 
# For 1000 bootstrap resamples of the mean difference, one can use the 25th 
# value and the 75th value of the ranked differences as boundaries of the 
# 95% confidence interval. (This captures the central 95% of the distribution.) 
# Such an interval construction is known as a percentile interval.

# Adjusting for asymmetrical resampling distributions
# While resampling distributions of the difference in means often have a normal 
# distribution, it is not uncommon to encounter a skewed distribution. Thus, 
# Efron developed the bias-corrected and accelerated bootstrap (BCa bootstrap) 
# to account for the skew, and still obtain the central 95% of the distribution. 
# dabestr applies the BCa correction to the resampling bootstrap distributions 
# of the effect size.

# Estimation plots incorporate bootstrap resampling
# The estimation plot produced by dabest presents the rawdata and the 
# bootstrap confidence # interval of the effect size (the difference in means) 
# side-by-side as a single integrated plot. It thus tightly couples visual 
# presentation of the raw data with an indication of the population mean 
# difference, and its confidence interval.


Next Steps: 
-- check into stylr-packag



Variable naming convention----  
gage                USGS gaging stations
  _poss             possible USGS gaging stations in the study area 
  _meta             metadata for possible gaging stations
  _int1             scratch df for pulling gages with integer fields 
  _int2             scratch df for pulling gages with integer fields 







gage_meta           cleaned metadata for USGS gaging stations in the study area 
  "site_no"                site number     
  "station_nm"             station name     
  "dec_lat_va"             latitude value in decimal degrees 
  "dec_long_va"            longitude value in decimal degrees 
  "state_cd"               State code
  "county_cd"              County code
  "alt_va"                 altitude 
  "huc_cd"                 hydrologic unit code 
  "drain_area_va"          drainage area in square miles 
  "contrib_drain_area_va"  contributing drainage area in square miles 

endDate             used for calling Egret::readNWISDaily 
parameter_cd        used for calling Egret::readNWISDaily 
startDate           used for calling Egret::readNWISDaily 

lon_riv_dv          scratch df for pulling lon_riv 
gage_most_dv        scratch df for pulling !lon_riv 
yrs_rec             calculates years of record 
gage_dv             daily flow values for waterYear 1980-2017 

wsd_summary     zonal statistics of watershed environmental parameters 
*   "_id"       unique id
*  "_sta_id"    four- or seven-digit station ID   
   "_type "     distinguishes gaged from ungaged 
   "_watshed"   describes the HUC06 watershed 
   "_HUC12"     hydrologic unit code 12
   "_sta_name"    station name derived from outlet HUC12 catchment name
   "_gage_num"    USGS site number 
   "_gage_nm"     station name from USGS gage
   "_dec_lat"     latitude in decimal degrees
   "_dec_lon"     longitude in decimal degrees
   "_cat_area"    catchment area in sq km
*  "_cat_area_l"  natural logarithm of catchment area 
   "_cat_length"  catchment length
   "_cat_width"   catchment width
*  "_lw_ratio"    catchment length divided by catchment width
   "_str_len"     stream length
*  "_drain_dens"  stream length divided by catchment area
*  "_prcp_mean"   areal mean precipitation depth 1992-2012
*  "_t07_mean"    average July temperature 1992-2012
*  "_vpd_ann"     areal mean of max vapor deficit 1992-2012
*  _"vpd_07"      areal mean of max July vapor deficit 1992-2012
*  "_cat_out"     catchment outlet elevation
*  "_cat_rel"     difference in max and min elevation
*  "_slop_med"    median percent slope
*  "_TWI_mean"    mean terrain wetness index
*  "_perc_cov"    percent forest cover from NLCD 2016
*  "_fc_mean"     mean field capacity
*  "_ksat_mean"   mean horizontal saturated hydraulic conductivity
*  "_kvert_mean"  mean vertical saturated hydrologic conductivity 

gage_table        summary of gage metadata for flextable 
ung_table         summary of ungaged watershed metadata for flextable  
soils_table       summary of geology, soils, and vegetation for flextable  

the tables above use flextable control keys: 
col_key_char      controls character elelements for flextables
col_key_int       controls integer elelements for flextables
col_key_num       controls numeric elelements for flextables

gage_check        scratch variable used to check operations 
gage_raw          the "raw" data, after removing the NA Qvals 
  _site_no        Eight-digit USGS gage number
  _sta            Short name used for this project 
  _Date           Date 
  _Q              Discharge in m3/s
  _Julian         Num. days since January 1, 1850 
  _Month          Month of the year [1-12] 
  _Day            Day of the year [1-366] 
  _DecYear        Decimal year 
  _MonthSeq       Number of months since January 1, 1850 
  _waterYear      Year following USGS water year 
  _Qualifier      Qualifying code
  _ i             Index of days, starting with 1 
  _LogQ           Natural logarithm of Q 
  _Q7             7-day running average of Q
  _Q30            30-day running average of Q

gage_dv_gath      scratch variable, gathers the Q, Q7, Q30 
gage_dv_low       censored zero flows (q_val < 0.01)
gage_dv_high      filtered non-zero flows -> join flows 
gage_contrib_area used for converting Q [m3/s] -> q [m/day]

gage_dv           the "main" daily mean flow values used in this .Rmd file 
  _sta            Short name used for this project 
  _Date           Date     
  _waterYear      Year following USGS water year format
  _q1_depth       discharge depth in m/day
  _q7_depth       7-day running average of q_depth
  _q30_depth      30-day running average of q_depth 

pca_input         prcomp() input using gage_dv 
pca_matrix        the matrix created by prcomp() 
lambda_q1         lambda values for BoxCox transform prior to PCA 
lambda_q7  
lambda_q30 
pca_eigen         results about PC axes -- the eigenvalues
pca_vars          results about PC rotation -- magnitude and direction of vars  

ecoreg            shapefile data of intersection of watershed area & 
                    merged SD, NE, WY ecoregions 
eco_join          join variable for ecoreg & gage_dv 

gage_mon          monthly dv depths used in decomposing PC eigens into trend, 
                    seasonal, & random comps 
mon_freq          'anomolize' parameter to remove season from observed vals  
mon_trend          'anomolize' parameter to remove trend from observed vals 
decomp_input      input for 'anomolize' decomp function 
decomp_fun        function to decompose into seasonal, trend, & remainder vals 

mon_sum           summary table of PC1 & seasonal, trend explained 
mon_sum_gath      prepares to summarize monthly data 
mon_sum_pc1       summary PC1 & seasonal, trend explained by ecoregion 
mon_sum_pc2       summary PC2 & seasonal, trend explained by ecoregion  
mon_sum_pc1_all   summary PC1 & seasonal, trend explained by station 
  
pca_plot          prepares PCA data for plotting 
pc1_pc2_plot      ggplot PC1 vs PC2 plot  
pc1_q7_plot       ggplot PC1 vs q7 plot  
pc2_q_diff_plot   ggplot PC2 vs q_diff plot  

mon_plot_pc1     prepares PC1 data for plotting 
pc1_obs          ggplot of PC1 observations vs time 
pc1_seas         ggplot of PC1 seasons vs time 
pc1_trend        ggplot of PC1 trend vs time 
pc1_remainder    ggplot of PC1 remainder vs time  


--> 

```{r setup_&_library, message=FALSE}  
  
knitr::opts_chunk$set(echo = FALSE)     
options(tibble.print_max = 70) # sets tibble output for printing        
  

# Sets up the library of packages   
library("conflicted")        # An alternative conflict resolution strategy  
library("here")              # identifies where to save work  
library("dataRetrieval")     # USGS data import  
library("rio")               # more robust I/O - to import and clean data  
library("tidyverse")  


#library("rnoaa")             # R wrapper for NOAA data inc. NCDC  
#library("lubridate")         # easier dates   
#library("lmomco")            # lmoments to find distribution   
#library('deldir')            # for Vorononi tesselation - Theissen polygons  
#library("SCI")               # calculates SPI & RDI   
#library("forecast")          # using the BoxCox function   
#library("broom")             # tidies linear models   
#library("ggbeeswarm")        # plot 1D data as a violin / beeswarm plot  
#library("scales")            # graphical scales map data to aesthetics,  
                             #   & methods for determining breaks and labels  
                             #   for axes and legends  
#library("anomalize")         # detect anomalies using the tidyverse   
#library("dabestr")           # data analysis using bootstrap estimation   
#library("cowplot")  
#library("flextable")        # construct complex table with 'kable'  
#library("officer")          # facilitates '.docx' access for table export   

# resolve conflicted packages----  
conflict_prefer("filter", "dplyr")  
conflict_prefer("select", "dplyr")  
  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
why_to_write <- function()   
{today <- today(tzone = "") 
paper2 <- ymd("2019-11-15")  
until <- paper2 - today  
print(paste("You have", until, "days until the second paper is due"  
))  
}  
why_to_write()

```  

# data cleaning 
```{r import_daily_flow_metadata, eval=FALSE}  

# get possible stations from NWIS website----  
gage_poss <- whatNWISsites(bBox = c(-103.8, 42.2, -99.2, 44.6),    
                       parameterCd = "00060",  
                       hasDataTypeCd = "dv") %>%  
  arrange(site_no)   
  
# 2. get stream station metadata====   
# this needs to be run in parts because some project numbers & 
# inventories are stored as integers & others are stored as characters 
 
gage_meta_int1 <- gage_poss %>% 
    slice(7, 17, 142, 148) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 
  
gage_meta_int2 <- gage_poss %>% 
    slice(94) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 
  
gage_meta_int3 <- gage_poss %>% 
    slice(-c(7, 17, 94, 142, 148)) %>% 
  split(.$site_no) %>% 
  map_dfr(~ readNWISsite(siteNumbers = .$site_no)) %>% 
  select(-c(project_no, inventory_dt)) 
  
# 3. join gage metadata 
gage_meta_poss <- bind_rows(gage_meta_int1, 
                            gage_meta_int2, 
                            gage_meta_int3) %>% 
#gage_meta_poss <- gage_meta_poss %>%  
  mutate(site_no = zeroPad(site_no, 8))  
 
# clean up 
rm(gage_meta_int1, gage_meta_int2, gage_meta_int3, gage_poss)  
 
# 4. export metadata to data folder----  
export(gage_meta_poss, "data/gage_meta_poss.csv")  
 
``` 

```{r clean_gage_metadata, eval=FALSE} 
#gage_meta_poss <- import("data/gage_meta_poss.csv")       

# select final stations for consideration----  
# 1. remove gages that do not meet standards====    
gage_meta <- gage_meta_poss %>%    
  mutate(site_no = as.character(site_no)) %>%  
  mutate(site_no = zeroPad(site_no, 8)) %>%              # pad site_no   
  mutate(reliability_cd = replace_na(reliability_cd, 0))  %>% #  NA <- zero  
  filter(site_no != "06461150"&  
         site_no != "06463670"&   
         site_no != "06461595") %>%  # remove short sites w/ provisional data  
  filter(site_no != "06441000") %>%  # only active 180 days/yr   
  filter(site_no != "06438000") %>%  
  filter(site_no != "06437000") %>%  # remove northern Black Hills stations  
  filter(site_no != "06442718") %>%  # remove East River stations  
  filter(reliability_cd != "M") %>% # M is minimal data  
  filter(!str_detect(station_nm, 'DAM|DITCH|DRAIN')) %>% # upstream control  
  filter(!str_detect(station_nm,  
                     'CUSTER|KEYSTONE|HILL CITY|HAYWARD')) %>%  
  filter(site_no != "06424000") %>%  
  # crystaline catchments  
  filter(!str_detect(station_nm,  
                     'LEAD|DEADWOOD|WHITEWOOD')) %>%   
  # crystaline catchments  
  filter(!str_detect(station_nm, 'CLEGHORN')) %>% # karstic? spring  
  filter(!str_detect(station_nm, 'BOXELDER|LIME')) %>% # karstic  
  filter(!str_detect(station_nm, 'RAPID')) %>%    
  # Rapid Creek & upper Spring Creek  
  filter(!str_detect(station_nm, 'MISSOURI'))  
  
# 2. remove provisional sites====       
gage_meta <- gage_meta %>%  
  mutate(site_no = as.character(site_no)) %>%  
  select(-reliability_cd) %>%  
  mutate(length = str_length(site_no)) %>%  
  filter(length <= 8) %>%   # removes two provisional sites  
  select(-length)  
   
# 3. remove codes not needed for this project  
gage_meta <- gage_meta %>%  
  select(-agency_cd) %>%  
  select(-site_tp_cd) %>% # all streams so delete 
  select(-c(lat_va, long_va)) %>% # in DMS so delete  
  select(-c(coord_meth_cd, coord_acy_cd)) %>% # coord meth & agency, so delete  
  select(-c(coord_datum_cd, dec_coord_datum_cd)) %>% # NAD83 or NAD27  
  select(-c(district_cd, country_cd)) %>% # Congressional dist & Country  
  select(-c(land_net_ds, map_nm, map_scale_fc)) %>%  # refers to USGS maps  
  select(-c(alt_meth_cd, alt_datum_cd, alt_acy_va)) %>% #%>% # alt metadata   
  select(-c(basin_cd, topo_cd, instruments_cd)) %>%  
  select(-c(construction_dt)) %>%  
  select(-c(tz_cd, local_time_fg)) %>% # daily data, so NA  
  select(-c(gw_file_cd, nat_aqfr_cd, aqfr_type_cd, aqfr_cd)) %>%  
  select(-c(well_depth_va, hole_depth_va, depth_src_cd))  
  
rm(gage_meta_poss)  
  
``` 

```{r add_names_to_gage_meta, eval=FALSE}  
  
# get data from QGIS analysis of gaged & ungaged watersheds of interest  
wsd_summary <- st_read("sp_data/eco-drought.gpkg",  
                       layer = "wbd_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry() %>% 
  mutate_if(is.factor, as.character) 
  
# join names to gage_meta  
names <- wsd_summary %>% 
  select(site_no = gage_num, sta = sta_id) %>% 
  drop_na() %>% 
  mutate(site_no = zeroPad(site_no, 8)) 
  
gage_meta <- full_join(names, gage_meta, 
                     by ="site_no")  
  
# export metadata to data folder ---------------------------------------------  
export(gage_meta, "data/gage_meta_fin.csv")  
 
``` 

```{r import_daily_flow_data, eval=FALSE} 
   
# this is a small amount of code duplication to reduce API calls   
#   to allow for working remotely -- set above to eval=FALSE  
gage_meta <- import("data/gage_meta_fin.csv") %>%  
    mutate(site_no = zeroPad(site_no, 8))  
  
wsd_summary <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "wbd_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry() %>% 
  mutate_if(is.factor, as.character) 
 
# set parameters for Egret::readNWISDaily   
startDate    <- "1979-08-01" # pulling two months early to get Q7 & Q30  
endDate      <- "2018-09-30"  
parameter_cd <- "00060" 
  
# get daily flows ------------------------------------------------------------ 
# note for EGRET::readNWISDailyQ the discharge is in m^3/s   
  
# Long River gage needs to be called separately or it creates an error  
gage_lonriv_dv <- gage_meta %>%  
  filter(site_no == "06463500") %>%  
  split(.$site_no)  %>%  
  map_dfr(~ readNWISDaily( 
    siteNumber = .$site_no, 
    parameter_cd, 
    startDate, 
    endDate), 
    .id = "site_no") 
  
gage_most_dv <- gage_meta %>% 
  filter(site_no != "06463500") %>%           # drop long river from call  
  split(.$site_no) %>%  
  map_dfr(~ readNWISDaily( 
    siteNumber = .$site_no, 
    parameter_cd, 
    startDate, 
    endDate), 
    .id = "site_no") 
  
# remove the 1978 data needed to calculate Q7 & Q30 
gage_dv <-bind_rows(gage_most_dv, gage_lonriv_dv) %>% 
  filter(waterYear > "1979") 
  
```  

```{r filter_short_flow_recs, eval=FALSE}
  
# 1. prepare to filter gages with short flow records---------------------------   
# calculate min & max years of record   
yrs_summary <- gage_dv %>% 
  group_by(site_no) %>%                  
  summarise(years_rec = n_distinct(waterYear),   
            min_year = min(waterYear),   
            max_year = max(waterYear)   
            ) %>%                     
  ungroup() %>%  
  mutate(apparent_yrs = 1 + max_year - min_year)  
  
gage_meta <- full_join(yrs_summary, gage_meta,      # N = 88  
                       by = "site_no")  
  
rm(yrs_summary)  
  
# 2. filter gages with less than 6 years of record----------------------------  
# filter gages with no records  
gage_check <- gage_meta %>%  
  filter(is.na(years_rec))  
  
gage_meta <- gage_meta %>%  
  filter(!is.na(years_rec))                           # N = 69  
  
# 06400500 CHEYENNE R NEAR HOT SPRINGS SD  
# 06403500 FRENCH CR NEAR FAIRBURN SD  
# 06404500 BATTLE CR NEAR HERMOSA SD  
# 06424500 ELK CR ABOVE PIEDMONT SD  
# 06437200 BEAR BUTTE CR NEAR GALENA,SD  
# 06437500 BEAR BUTTE CR NEAR STURGIS SD  
# 06440500 NORTH FORK BAD R NEAR PHILIP SD  
# 06445000 WHITE R BELW COTTONWOOD C N WHITNEY, NEBR.  
# 06445500 WHITE R NEAR CHADRON NEBR  
# 06446200 WHITE R NEAR ROCKYFORD SD  
# 06449250 SPRING CR NEAR ST FRANCIS SD  
# 06455900 NIOBRARA RIVER NEAR DUNLAP, NEBR.  
# 06456500 NIOBRARA RIVER NR HAY SPRINGS, NEBR.  
# 06457000 NIOBRARA RIVER NEAR COLCLESSER, NEBR.  
# 06458500 BEAR C NR ELI NEBR  
# 06459000 NIOBRARA R NEAR CODY, NEBR.  
# 06460900 MINNECHADUZA CREEK NEAR KILGORE, NEBRASKA  
# 06463000 NIOBRARA RIVER AT MEADVILLE NE  
# 06464000 KEYA PAHA R NEAR HIDDEN TIMBER SD   
  
# drop stations with less than 6 years of record   
gage_check <- gage_meta %>%  
  filter(years_rec < 6)  
  
gage_meta <- gage_meta %>%  
  filter(years_rec >= 6)                         # N = 58  
  
# The following were removed because less than 6 years of record   
# 06400870 HORSEHEAD CR NEAR OELRICHS SD  
# 06405400 GRACE COOLIDGE CR NEAR FAIRBURN SD  
# 06405500 GRACE COOLIDGE CR NEAR HERMOSA SD  
# 06437400 BEAR BUTTE CREEK AT STURGIS, SD  
# 06441400 WILLOW CREEK NEAR FORT PIERRE, SD  
# 06442130 CEDAR CREEK NR PRESHO, SD  
# 06442600 MEDICINE CREEK NR LOWER BRULE, SD  
# 06442950 CROW CR NEAR GANN VALLEY SD  
# 06445590 BIG BORDEAUX CREEK NEAR CHADRON NEBR  
# 06459200 SNAKE RIVER ABV MERRITT RESERVOIR NEBR  
# 10150005 Big Beaver Cr at Nebr Hwy 12 nr Valentine, Nebr 
  
# 3. check for incomplete days of record ------------------------------------   
yr_incomp <- gage_dv %>%                           
  group_by(site_no, waterYear) %>%                   
  summarise(days_yr = n()) %>%                     
  ungroup() %>%  
  filter(days_yr < 360) %>%  
  group_by(site_no) %>%    
  summarise(yrs_incomp = n()) %>%     
  ungroup()  
  
gage_meta <- full_join(yr_incomp, gage_meta,   
                       by = "site_no") %>%  
  filter(!is.na(years_rec))  
  
# 4. remove data with short record between 1990-2015 ------------------------- 
gage_check <- gage_meta %>%  
  filter(sta == "")  
  
gage_meta <- gage_meta %>%  
  filter(sta != "")  
  
## These didn't make the cut because the original data selection was  
# for 1990-2015  & it would be extremely difficult to put in post-hoc.  
# it would be good for the future!  
# Also - its possible to use these for model testing...  
  
# station   min_yr  maxyr  name  
# 06442000   1980   1990   MEDICINE KNOLL CR NEAR BLUNT SD   
# 06459175   1982   1995   SNAKE R AT DOUGHBOY, NE  
# 06459500   1980   1995   SNAKE RIVER NEAR BURGE, NEBR.  
# 06454100   1980   1992   Niobrara River at Agate, Nebr.  
# 06461000   1980   1995   MINNECHADUZA CREEK AT VALENTINE, NEBR.  
# 06462500   1980   1995   PLUM CREEK AT MEADVILLE, NE 
# 06463720   2012   2018   Niobrara River at Mariaville, Nebr.  
# 06400497   1980   1995   CASCADE SPRINGS NEAR HOT SPRINGS SD  
# 06462000   1980   1986   NIOBRARA RIVER NR NORDEN NEBR 
# 06463080   1980   1991   LONG PINE CREEK NR LONG PINE, NE 
# 06439300   1980   1994   CHEYENNE RIVER AT CHERRY CREEK,SD 
# 06442500   1980   1990   MEDICINE CR AT KENNEBEC SD 
# 06444000   1980   2004   White River at Crawford, Nebr. 
# 06454500   1980   1994   NIOBRARA RIVER ABOVE BOX BUTTE RESERVOIR, NE 
# 06455500   1980   1991   NIOBRARA RIVER BELOW BOX BUTTE RESERVOIR NEBR 
# 06457500   1980   1991   NIOBRARA RIVER NEAR GORDON, NEBR.  
  
# sync gage_dv w/ gage_meta & export results as posthoc to avoid interference  
names <- wsd_summary %>%  
  select(site_no = gage_num, sta = sta_id) %>%  
  drop_na() %>%  
  mutate(site_no = zeroPad(site_no, 8))  
  
gage_dv <- full_join(names, gage_dv,  
                     by ="site_no")   
  
gage_dv <- semi_join(gage_dv, gage_meta,  
                     by = "site_no")    
  
export(gage_meta, "data/gage_meta_posthoc.csv")  
  
# clean up 
rm(startDate, endDate, parameter_cd, wsd_summary, names) 
rm(gage_check, yr_incomp, gage_lonriv_dv, gage_most_dv)  
  
```  