---
title: "Meta-analysis"
author: "Charles Jason Tinant"
date: "4/11/2018"
output: pdf_document
---



```{r project-structure-notes}
#Outlines project as package by 'Efficient R Programming'----
# Improve project management and workflow by treating projects as 
# R packages. An intermediate approach is to use a ‘dummy package’ 
# that includes a DESCRIPTION file in the root directory to tell which
# packages must be installed for the code to work.  Example is 
# 'Efficient R Programming' based on a dummy package so that they
# can easily keep dependencies up-to-date (see DESCRIPTION file)

# Creating packages teaches correct code documentation,
# storage of example data, and reproducibility via vignettes.
# But it can take a lot of extra time.

# Project Structure
# ~~~~~~~~~~~~~~~~~
#   DESCRIPTION file contains key information about the package,
# including which packages are required for the code contained in your
# package to work, e.g. using Imports:. This is efficient because it
# means that anyone who installs your package will automatically
# install the other packages that it depends on.

# The R/ folder contains all the R code that defines package functions.
# Placing code in a single place and keeping code modular can greatly
# reduce code duplication. Furthermore, documentation of R packages
# through Roxygen tags such as:
#' This function does this... makes it easy for others to use your
# work. This form of efficient documentation is facilitated by the
# roxygen2 package.

# The data/ folder contains example code for demonstrating to others
# how the functions work and transporting datasets that will be
# frequently used in your workflow. Data can be added automatically
# using the devtools package, with devtools::use_data().
# This can increase efficiency by providing a way of distributing
# small to medium sized datasets and making them available when the
# package is loaded with the function data('data_set_name').

# The package testthat makes it easier than ever to test your R code
# as you go, ensuring that nothing breaks.
# For more on R packages see (Wickham 2015): the online version
# provides all you need to know about writing R packages for free
# (see r-pkgs.had.co.nz/).
```
```{r thoughts-on-thesisdown}
# 1. look into making a vingette for thesisdown  
```
```{r library}
# To set up the thesis
library("here") # sets up a home for your work
library("DiagrammeR") # used to call 'mermaid' for a Gantt chart
library("thesisdown") # used to knit a thesis follows bookdown
library("bookdown") # 
# library(knitr) # not sure if this needs to be loaded??

# To import and clean data
library(rio) # more robust I/O
# library(DataExplorer)
# library("DataRetreval")
library("tidyverse")

# to make a map and plot figures
library(ggmap)
library(maps) # outlines of continents, countries, states & counties 
library(mapdata) # higher-resolution outlines.
library(ggplot2) # used to plot maps in the `maps` package.
```
```{r gantt-big, fig.align='left', fig.width = 12}
# Gantt project with all of the steps for completion----
# https://mermaidjs.github.io/gantt.html

# Ask yourself if each of the objective meet the: 
# SMART criteria for objectives
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Specific: is the objective clearly defined and self-contained?
# Measurable: is there a clear indication of its completion?
# Attainable: can the target be achieved?
# Realistic: have sufficient resources been allocated to the task?
# Time-bound: is there an associated completion date or milestone?

mermaid("gantt
       dateFormat  YYYY-MM-DD
       title Gantt chart for PhD completion
       section PhD Candidacy
       Sent draft proposal        :done,    can1, 2018-04-10, 1d
       Committee review           :active,  can2, after can1, 10d
       Schedule defense           :         can3, 2018-04-17, 1d
       Incorporate edits          :         can4, after can2, 5d
       Send final draft           :         can5, after can4, 1d
       Oral defense               :         can6, 2018-05-05, 1d

       section Characterize drought
       Outline project            :active,   dro1, 2018-04-10, 4d
       Calculate SPI & SPEI       :          dro2, after dro1, 21d
       Develop SDI                :          dro3, after dro2, 30d 
       Cluster time series        :          dro4, after dro3, 10d  
       Watershed delineation      :          dro5, after dro4, 7d 
       Cluster ungaged sta. by RF :          dro6, after dro5, 21d
       Write results              :          dro7, after dro6, 14d 
       NSF dissemination          : crit,    dro8, 2018-06-21, 3d 
       Identify journal           : crit,    dro9, after can6, 21d 

       section Taxa patterns
       Collect macros with OST    :        tax1, 2018-06-14, 5d
       Munge macro data           :        tax2, after dro7, 14d
       NMS ordination             :        tax3, after tax2, 14d
       PERMANOVA                  :        tax4, after tax3, 14d
       Write results              :        tax5, after tax4, 30d
       OST dissemination          : crit,  tax6, 2018-09-15, 1d 
       Identify journal           : crit,  tax7, after dro7, 21d 

     section Drought indicators
       ISA analysis               :        ind1, after tax5, 30d
       IBI metrics                :        ind2, after ind1, 30d
       Update OST WPP             :        ind3, after ind2, 30d
       Write results              :        ind4, after ind3, 30d
       SDSMT dissemination        : crit,  ind5, 2018-10-30, 1d 
       Identify journal           : crit,  ind6, after tax5, 21d 
")
```

```{r gantt-small, fig.align='left', fig.width = 12}
# Fine-scale Gantt project over next three months----
# Short-term Gantt chart for PhD
mermaid("gantt
       dateFormat  YYYY-MM-DD
       title Gantt chart short-term
       section PhD Candidacy
       Sent draft proposal        :done,    can1, 2018-04-10, 1d
       Committee review           :active,  can2, after can1, 10d
       Schedule defense           :         can3, 2018-04-17, 1d
       Incorporate edits          :         can4, after can2, 5d
       Send final draft           :         can5, after can4, 1d
       Oral defense               :         can6, 2018-05-05, 1d

       section Characterize drought
       Outline project            :active,   dro1, 2018-04-10, 4d
       Calculate SPI & SPEI       :          dro2, after dro1, 21d
       Develop SDI                :          dro3, after dro2, 30d 
       Cluster time series        :          dro4, after dro3, 10d  
       Watershed delineation      :          dro5, after dro4, 7d 
       Cluster ungaged sta. by RF :          dro6, after dro5, 21d
       Write results              :          dro7, after dro6, 14d 
       NSF dissemination          : crit,    dro8, 2018-06-21, 3d 
       Identify journal           : crit,    dro9, after can6, 21d 
")
```
```{r import-data}
# use RIO or dataRetreval----
```

```{r tidy-data}
# Tidyr and dplyr example using the fivethirtyeight package ----
# library(fivethirtyeight)

# See names of columns 

#names(murder_2015_final)

# Load murder_2015_final package and gather into a tibble

# murders_gathered <- murder_2015_final %>% 
#  gather(
#    murder_year,
#    murders,
#    murders_2014:murders_2015,
#    na.rm = TRUE)
# murders_gathered

# Arrange alphabetically by state and city

#murders_arranged <- murders_gathered %>% 
#  arrange(
#    state, 
#    city)
#murders_arranged

# Separate murder_year into murder and year

#murders_separate <- murders_arranged %>%
#  separate(
#    murder_year,
#    into = c("text", 
#             "year")
#  )
#murders_separate

# Use spread and arrange to put year into two columns

#murders_spread <- murders_separate %>% 
 # spread(
 #   year,
 #   murders
 # ) %>% 
#  arrange(
#    state,
#    city)
#murders_spread

# Use unite to paste one column into another

#murders_final <- murders_spread %>%
#  unite(
#    city_state, 
#    city, 
#    state) %>% 
#  arrange(
#    city_state
#  ) %>% 
#  select(
#    -(text)
#  )
#murders_final

# Write this tibble to a csv

#write.csv(murders_final, file = "murders_final.csv",row.names=FALSE, na=""
```

```{r some-tips}
# useful books and packages for efficient programming----
# Unless otherwise indicated ## headings are pdfs in Library3

## Efficient R Programming 
# Becoming an Efficient R Programmer
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Some keystroke tools
# Ctrl+Z/Shift+Z: Undo/Redo.
# Ctrl+Alt+R: Execute all R code in the currently open file. 
# Ctrl+Left/Right: Navigate code quickly, word by word.
# Home/End: Navigate to the beginning/end of the current line.
# Alt+Shift+Up/Down: Duplicate the current line up or down.
# Ctrl+D: Delete the current line
# Ctrl+I: Automatically indent selected code 
# Ctrl+Shift+A: add spaces for maximum readability.

# Tips for efficient workflow 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Start without writing code but with a clear mind to ensure 
#    objectives are top of mind, without getting lost in technology.
# 2. Make a plan. Time-lines, resources and ‘chunking’ the work 
#    will make you more effective when you start.
# 3. Select packages for early implementation. 
# 4. Document your work with comments. -> check roxygen2 package
# 5. Make entire workflow as reproducible as possible. 
# knitr can help with documentation. <- check on knitr

# Tips for efficient data I/O
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Don't change names of local files downloaded from the internet or 
#    copied onto your computer. Helps to trace the provenance of data 
# 2. R’s native file format is .Rds. These files can imported and 
#    exported using readRDS and saveRDS for fast and space efficient 
#    data storage.
# 3. Use import() from the rio package to efficiently import data from
#    many formats.
# 4. Use readr or data.table equivalents of read.table() to 
#    efficiently import large text files. 75
# 5. Use file.size() and object.size() to keep track of the size of 
#    files and R objects and take action if they get too big.

# Tips for efficient data carpentry
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Give data processing the proper time and attention 
# 2. Tidy data carefully at the earliest stage of analysis by tidyr
# 3. Use the tbl class defined by the tibble package and the default 
#    object type of dplyr to make data frames efficient to print
# 4. Use the %>% ‘pipe’ operator to clarify data processing workflows
# 5. Use efficient data frames with tibble

# Open source project management software assendancy steps
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Github - described in Chapter 9)
# DiagrammeR has a Gantt chart builder
# ProjectLibre - Dedicated desktop project management software



# Tidy the data with tidyr and friends
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Understand dataset structure & ‘reshape’ them to reduce 
#    reformatting time, and increase computational efficiency.
# 2a. Data cleaning: re-format and label messy data by stringr package
#     for messy character strings 
# 2b. Perform diagnostic checks for data integrity at the outset of a
#     data analysis project by assertive and assertr packages
# 2c. Convert non-standard text strings into date formats by lubridate 
# 2d. Reshape data tidyr::gather, tidyr::spread, tidyr::separate
# 2e. Hoover up model output with the broom package.
# 2f. Process data with dplyr

# Tips for efficient performance
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 1. Use a code profiler to find slow-downs; profvis package
# 2. The ifelse function is optimal for vectorised comparisons.
# 3. If the data in your data frame is all of the same type, 
#    consider converting it to a matrix for a speed boost.
# 4.  The parallel package is ideal for Monte-Carlo simulations.

# Other thoughts
# ~~~~~~~~~~~~~~
# R package development is easier with devtools package
# Look forJenny Byran’s book, “Happy Git and Github for the useR”.

## Bookdown 
# Authoring Books and Technical Documents with R Markdown

# other Yihui packages to look into:
# library("tufte")
# library("blogdown")
# library("animation")
# library("tinytex")
# library("shiny")
# library("leaflet")
```

```{r Regex_example, eval=FALSE}

#### Regex example to include some facts about the counties
# I can go to Wikipedia or http://www.california-demographics.com/counties_by_population
# and grab population and area data for each county.
# In fact, I copied their little table on Wikipedia and saved it into #`data/ca-counties-wikipedia.txt`. In
#full disclosure I also edited the name of San Francisco from "City and County #of San Francisco" to 
#"San Francisco County" to be like the others (and not break my regex!)
# Watch this regex fun:

library(stringr)
library(dplyr)

# make a data frame
x <- readLines("data/ca-counties-wikipedia.txt")
pop_and_area <- str_match(x, "^([a-zA-Z ]+)County\t.*\t([0-9,]{2,10})\t([0-9,]{2,10}) sq mi$")[, -1] %>%
na.omit() %>%
str_replace_all(",", "") %>% 
str_trim() %>%
tolower() %>%
as.data.frame(stringsAsFactors = FALSE)

# make a data frame
x <- readLines("data/ca-counties-wikipedia.txt")
pop_and_area <- str_match(x, "^([a-zA-Z ]+)County\t.*\t([0-9,]{2,10})\t([0-9,]{2,10}) sq mi$")[, -1] %>%
      na.omit() %>%
      str_replace_all(",", "") %>% 
      str_trim() %>%
      tolower() %>%
      as.data.frame(stringsAsFactors = FALSE)

# Need to add a section to break the [174:1] to [58:3] dataframe
      
# give names and make population and area numeric
names(pop_and_area) <- c("subregion", "population", "area")
pop_and_area$population <- as.numeric(pop_and_area$population)
    pop_and_area$area <- as.numeric(pop_and_area$area)
  
    head(pop_and_area)

# We now have the numbers that we want, but we need to attach those to 
# every point on polygons of the counties.  This is a job for 
# `inner_join` from the `dplyr` package

cacopa <- inner_join(ca_county, pop_and_area, by = "subregion")

# And finally, add a column of `people_per_mile`:
cacopa$people_per_mile <- cacopa$population / cacopa$area

head(cacopa)


#### Now plot population density by county

# prepare to drop the axes and ticks but leave the guides and legends
# We can't just throw down a theme_nothing()!
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )

elbow_room1 <- ca_base + 
      geom_polygon(data = cacopa, aes(fill = people_per_mile), 
                   color = "white") +
      geom_polygon(color = "black", fill = NA) +
      theme_bw() +
      ditch_the_axes

elbow_room1 

#### Lame!

# The population density in San Francisco is so great that it makes it
# hard to discern differences between other areas.
# This is a job for a scale transformation.  Let's take the 
# log-base-10 of the population density. Instead of making a new 
# column which is log10 of the `people_per_mile` we can just apply the 
# transformation in the gradient using the `trans` argument

elbow_room1 + scale_fill_gradient(trans = "log10")

#### Still not great
# I personally like more color than ggplot uses in its default 
# gradient.  In that respect I gravitate more toward Matlab's default 
# color gradient.  Can we do something similar with `ggplot`?

eb2 <- elbow_room1 + 
    scale_fill_gradientn(colours = rev(rainbow(7)),
                         breaks = c(2, 4, 10, 100, 1000, 10000),
                         trans = "log10")
eb2

### zoom in?
# Note that the scale of these maps from package `maps` are not great. 
# We can zoom in to the Bay region, and it sort of works scale-wise, 
# but if we wanted to zoom in more, it would be tough.  

eb2 + xlim(-123, -121.0) + ylim(36, 38)

# Whoa! That is an epic fail. Why?
# Recall that `geom_polygon()` connects the end point of a `group` to 
# its starting point. And the kicker: the `xlim` and `ylim` functions 
# in `ggplot2` discard all the data that is not within the plot area.  
# Hence there are new starting points and ending points for some 
# groups (or in this case the black-line perimeter of California) and 
# those points get connected.  Not good.

### True zoom.

# If you want to keep all the data the same but just zoom in, you can 
# use the `xlim` and `ylim` arguments to `coord_cartesian()`.  Though, 
# to keep the aspect ratio correct we must use `coord_fixed()` instead 
# of `coord_cartesian()`.
# This chops stuff off but doesn't discard it from the data set:

eb2 + coord_fixed(xlim = c(-123, -121.0),  ylim = c(36, 38), ratio = 1.3)
```
