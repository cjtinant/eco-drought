```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r streamflow-data-organization-prior}    
# This is the prior code that was used in the analysis...
# Stream flow is analyzed over a period of 1990-2016.       
# This code chunk identifies missing gage-year combinations    
# An iterative approach is used to identify cluster memberships &  
# fill missing years with the streamflow depths of the nearest   
# neighbor in the group.   

# The 'wet-cycle' transitions to 'dry cycle' in 2002  

# import and append daily flow values > 1990 
gage_orig       <- import(file = "data/gage_inputs.csv")   # n = 245,440 obs 
  
# import estimates from prior runs 
gage_est_9002a  <- import(file = "data/gage_est_9002a.csv") %>% 
  select(-contrib_drain_area_va) 
                                                      # run01 n =  12,783 obs 

gage_est_0317a  <- import(file = "data/gage_est_0317a.csv")  %>% 
  select(-contrib_drain_area_va)                      # run02 n =     731 obs

gage_est_9002b  <- import(file = "data/gage_est_9002b.csv")  %>% 
 select(-contrib_drain_area_va)                       # run03 n =   9,972 obs

gage_est_0317b  <- import(file = "data/gage_est_0317b.csv")  %>% 
  select(-contrib_drain_area_va)                      # run04 n =   1,634 obs

gage_est_9017a  <- import(file = "data/gage_est_9017a.csv") %>% 
  select(-contrib_drain_area_va)                      # run05 n =   9,496 obs

gage_est_9017b  <- import(file = "data/gage_est_9017b.csv") 
                                                      # run06 n =  15,064 obs

gage_est_9017c  <- import(file = "data/gage_est_9017c.csv") %>% 
  select(-contrib_drain_area_va)                      # run07 n =  21,916 obs

# bind the gage dfs together, make min & max yr vars - clean up
gage <- bind_rows(gage_orig, gage_est_9002a, gage_est_0317a,  
                  gage_est_9002b, gage_est_0317b, gage_est_9017a,  
                gage_est_9017b, gage_est_9017c) 
                                               # orig  n = 249,247 obs    
                                               # run01 n = 258,281  
                                               # run02 n = 258,954 
                                               # run03 n = 268,926 
                                               # run04 n = 270,560   
                                               # run04 n = 280,056   
                                               # run06 n = 295,120      
                                               # run07 n = 317,036   

rm(gage_est_9002a, gage_est_0317a, gage_est_9002b, gage_est_0317b, 
   gage_est_9017a, gage_est_9017b, gage_est_9017c, gage_orig)

# update gage summary complete days of record 
gage_summary <- gage %>%                          
  group_by(sta, water_year) %>%                  
  summarise(days_record = n()) %>%                   
  ungroup()                                    

# find & split stations-years with incomplete days of record 
gage_incomp <- gage %>%                          
  group_by(sta, water_year) %>%                     
  summarise(days_record = n()) %>%              
  ungroup() %>%                                     
  filter(days_record < 365)                           

gage_incomp <- semi_join(gage, gage_incomp, 
                         by = c("sta", "water_year")) 
                                               # run00 n =   3,279 obs 
                                               # run01 n =   1,454 obs 
                                               # run02 n =   1,454 obs 
                                               # run03 n =     834 obs 
                                               # run04 n =     642 obs 
                                               # run05 n =     642 obs 
                                               # run06 n =       0 obs 
                                               # run07 n =       0 obs
# add area to gage & gage_incomp_year 
gage_meta <- import("data/gage_meta_full.csv") 
gage_meta <- semi_join(gage_meta, gage_summary, by = "sta")

gage <- full_join(gage, gage_meta, 
                  by = "sta") %>% 
  select(date, sta, q, q7, q30, water_year, qualifier, 
         contrib_drain_area_va)

gage_incomp <- full_join(gage_incomp, gage_meta, by = "sta") %>% 
  select(date, sta, q, q7, q30, 
         water_year, contrib_drain_area_va, 
         qualifier) %>% 
  filter(!is.na(q)) 

# check on observations 
gage_sum <- gage_summary %>% 
  spread(water_year, days_record) 

# once the missing vals are complete, then 

gage_input <- gage    

rm(gage_incomp, gage_meta)
```

```{r library, include=FALSE, message=FALSE}
# Sets up the library of packages 
library("tidyverse")
library("here") # identifies where to save work
library("rio") # more robust I/O - to import and clean data
library("janitor") # tools for examining and cleaning dirty data
library("DataExplorer")
library("lubridate")
library("SPEI")

# library("rnoaa") #  R wrapper for NOAA data inc. NCDC
# library("dataRetrieval") # USGS data import
# library("RColorBrewer") - there is a better one?
# library("workflowr") # creates a research website
# library("colorspace")
# library("bookdown") # 
# library(unpivotr) # fix nasty Excel files
# library("friendlyeval")

# a useful description of commits:
# http://r-pkgs.had.co.nz/git.html
```


```{r good-housekeeping_projects}
# Recommendations of Wilson et al. (2017):
#1. Put each project in its own directory, which is named after the project
#2. Put text documents associated with the project in the doc directory
#3. Put raw data and metadata in a data directory and files generated during
#     cleanup and analysis in a results directory
#4. Put project source code in the R/src directory
#5. Name all files to reflect their content or function

#6. For files sent to external collaborators: 
#     Use a date stamp as part of the file name

# Wilson, G., Bryan, J., Cranston, K., Kitzes, J., Nederbragt, L., 
#   and Teal, T. K. (2017). Good enough practices in scientific computing.
#   PLOS Computational Biology, 13(6):1–20. 
```

```{r good-housekeeping_setup}
#--README
#|--LICENSE
#|--data
#| |--birds_count_table-2018-12-12.csv
#| |--measurement-locations-2018-12-12.csv
#|--doc
#| |--email-collaborator-2018-10-10.txt
#| |--todo.txt
#|--results
#| |--birdlocations.RData
#| |--summarized_results.RData
#|--R
#| |--sightings_analysis.R
#| |--runall.R
#|--manuscript
#| |--howbirdsmove.Rmd
#| |--howbirdsmove.docx
#| |--submission
#|   |--howbirdsmove-nature-2018-12-13.pdf
```

```{r good-housekeeping_data-mgmt}
# Recommendations from Wilson et al. (2017):
#.1 Save the raw data
#2. Ensure that raw data are backed up in more than one location
#3. Create the data you wish to see in the world
#4. Create analysis-friendly data
#5. Record all the steps used to process data
#6. Anticipate the need to use multiple tables, 
#      and use a unique identifier for every record
# 7. Submit data to a reputable DOI-issuing repository 
#      so that others can access and cite it

```

```{r good-housekeeping_coding}
#1. keep code chunks at ~30 lines of code
#2. Most likely, you will spend 80% of the project time wrangling 
#     your data into the desired shape 
#3. Use the tidyverse and pipes for data-wrangling:  
#   - Wickham and Grolemund (2017): https://r4ds.had.co.nz 

#Recommendations from Wilson et al. (2017):
#1.  Place a brief explanatory comment at the start of every program
#2.  Decompose programs into functions
#3.  Be ruthless about eliminating duplication
#4.  Always search for well-maintained software libraries that do what you need
#5.  Test libraries before relying on them
#6.  Give functions and variables meaningful names
#7,  Make dependencies and requirements explicit
#8.  Do not comment and uncomment sections of code to control 
#      a program’s behavior
#9.  Provide a simple example or test data set
#10. Use a version control program-git
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#library(namer)   # automatically names code chunks 
#library(officer) # powerpoint export 
#libary(here)     # creates a named path 
#use packrat for project archival 
```

```{r good-housekeeping_critical-analysis}
# guarantee results are correct (according to the selected method) by:
#  Single player mode: Write unit tests
#  Pair programming - Four-eye principle as part of a code review
# Collaborate with your future self: Redo your analyses after 6 weeks...
# Once you are stable: Track any errors you find in a database
# Comparison of results is supported by the 'daff' package: 
#   calculates differences between data.frame objects 
```

```{r 10_rules_for_reproducible_analysis} 
# 10 Rules for Reproducible Analyses: Sandve et al. (2013)  
#  Following 10 rules to “foster a culture of reproducibility”: 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
#1.  For Every Result, Keep Track of How It Was Produced 
#2.  Avoid Manual Data Manipulation Steps 
#3.  Archive the Exact Versions of All External Programs Used 
#4.  Version Control All Custom Scripts 
#5.  Record All Intermediate Results, When Possible in Standardized Formats 
#6.  For Analyses That Include Randomness, Note Underlying Random Seeds
#7.  Always Store Raw Data behind Plots
#8.  Generate Hierarchical Analysis Output: 
#      Allowing Layers of Increasing Detail to Be Inspected
#9.  Connect Textual Statements to Underlying Results
#10. Provide Public Access to Scripts, Runs, and Results 

# Sandve, G. K., Nekrutenko, A., Taylor, J., and Hovig, E. (2013). 
#  Ten simple rules for reproducible computational research. 
#  PLOS Computational Biology, 9(10):1–4.

```

```{r good-housekeeping_manuscript-writing}
#0. It is possible to convert your Rmd file to Word – this is done by
#     rmarkdown::render with the word_document() renderer 
#1. Better alternative: 
#   Use GoogleDocs, Overleaf (LaTeX) or Word Online
#2. The officer package provides helpful functionality to dynamically produce
#     PowerPoint presentations
#3. Watch for the redoc package (now early stage): 
#     allows for a two-way R-Markdown <->Microsoft Word workflow 

```

```{r draft_language, include=FALSE, eval=FALSE}
#draft language----
## Introduction The introduction highlights the gap that exists
in current knowledge or methods and why it is important. This is usually done
by a set of progressively more specific paragraphs that culminate in a clear
exposition of what is lacking in the literature, followed by a paragraph
summarizing what the paper does to fill that gap.

Intro p1: drought is a "creeping disaster" Intro p2: what makes a drought?
Intensity & duration Intro p3: different kinds of drought Intro p4: working with
drought - skewed distribution -> how to fix?

## What are the gaps this paper hopes to address? Evaluate a multi-component
regional drought history for the region How to define minimum precipip record
set & distributional characteristics Identify "pure" streamflow realizations
that incorporate short streamflow gage records to characterize hydrologic
drought Identify spatial vars in gSSURGO data to relate lags to a physical
process

# Specific Aims 1. Refine an approach for relating precipitation drought
to hydrological drought in gaged watersheds 2. Develop a method to estimate
regional hydrologic group membership for ungaged watersheds using gSSURGO data

An example of the progression of gaps, a first paragraph may explain why
understanding cell differentiation is an important topic and that the field has
not yet solved what triggers it (a field gap). A second paragraph may explain
what is unknown about the differentiation of a specific cell type, such as
astrocytes (a subfield gap). A third may provide clues that a particular gene
might drive astrocytic differentiation and then state that this hypothesis is
untested (the gap within the subfield that you will fill). The gap statement
sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal
of developing the gap. Each paragraph first orients the reader to the topic
(a context sentence or two) and then explains the “knowns” in the relevant
literature (content) before landing on the critical “unknown” (conclusion) that
makes the paper matter at the relevant scale. Along the path, there are often
clues given about the mystery behind the gaps; these clues lead to the untested
hypothesis or undeveloped method of the paper and give the reader hope that
the mystery is solvable. The introduction should not contain a broad literature
review beyond the motivation of the paper. This gap-focused structure makes it
easy for experienced readers to evaluate the potential importance of a paper—
they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the
results, which fill the gap you just established. It differs from the abstract
in the following ways: it does not need to present the context (which has just
been given), it is somewhat more specific about the results, and it only briefly
previews the conclusion of the paper, if at all.

## Methods


## Results Regional monthly precipitation fits a Pearson type-III distribution.
The mean and deviation of monthly precipitation depth among stations are
similiar (e.g little spatial difference). However, the distribution of wet and
dry periods is temporally dependant Station distance is a substantial predictor
of covarience => Next STEP use distance from centroid to create a predictor
variable to test significance.. Regional streamflow clusters into 7 groups.
Hydrologic export coefficient describes ~95% of varience and flashiness (e.g Q1
-Q3) describing 5% of varience.

# overview of steps---- 
# 2.1    Standardized Precipitation Index 
# note: SPI package not working; created a test case to send to author
# 2.1.1  Download & munge precip data: complete; _04_prco-data_munging
# 2.1.2  Identify distribution: complete;  _05_L-moment_diagram
# 2.1.3   Calculate SPI: completed spi1, spi6

# 2.2    Stream Drought Index
# 2.2.1  Download & mungeStream data: finished - July 16
# 2.2.2  Learn purrr::map(): ok.finished 
#          http://r4ds.had.co.nz/iteration.html#the-map-functions
# 2.2.3 Cluster time series - in progress
#         see Kassambara, "Practical Guide to Cluster Analysis in R
# 2.2.4   Calculate SDI
# 2.2.5   Delineate watersheds
# 2.2.6   Cluster ungaged stations
# 2.2.7 Learn 'rf' function
# 2.3   Disseminate results
# 2.3.1 Identify journal

## Analysis Steps & progress
# 1.    Identify precipitation records for drought analysis
#         I imported Global Historical Climatology Network (GHCN) 
#         daily precipitation records for candidate "WEATHER STATIONS" 
#         into R-Studio (REF1) using the "rnoaa" package.
# 1.2   Cleaned data (see 04_prcp-data_munging)  
# 1.2.1   I used Theissen polygons and the length and continuity of 
#         precipitation records for initial station selection 
# 1.2.2  'dplyr' to fill daily NA values with data from nearest station
# 1.2.3   create monthly vals from daily vals.
# 1.2.4 I removed short record: Long Valley after checking covariance.
# 1.3   Exploratory EDA
# 1.3.1 Applied sqrt & log10 transform to explore effects on skew 
# 1.3.2 Explored the data with box plots, violin plot.
#        Sqrt trans vs plotting vals look slightly sinusoidal
#        Log-tranformation is mirror of orig depth vs plotting
# 1.3.3 Applied Weibull plotting position & graphed on sqrt plot 
# 1.3.4 Completed exploratory PCA => covarience by zero months?
# 1.4.  Calculated L-moments and L-moment ratios => Pearson-type III
# 1.4.1 Calculated 1 & 6 month SPI 
# 1.5   Clustered streamflows during wet period 
# 1.5.1 Identify streamflow records for hydrology analysis (list) 
# 1.5.2 Exploratory EDA of streamflow to identify membership of 
#       short records 

## Thoughts - the orig depth vs plotting vals look j-shaped. 

# Next steps: 
#2. find drought years & wet years for precip
#3. Examine clusters for dry years
#4. Supervised classification - random forest should cross-validate?

# Someday - Maybe
# 1. Map the variable as a function - might put off, but ugly and long 
#   code below!
# 2. figure out how to reference stuff with - grateful package

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
#Ten simple rules for structuring papers----

Citation: Mensh B, Kording K (2017) Ten simple rules for structuring papers.
PLoS Comput Biol 13(9): e1005619. https://doi.org/10.1371/journal.pcbi.1005619

#Principles (Rules 1–4)
##Rule 1: Focus your paper on a central contribution, which
##you communicate in the title

Your communication efforts are successful if readers can still describe the main
contribution of your paper to their colleagues a year after reading it. Although
it is clear that a paper often needs to communicate a number of innovations on
the way to its final message, it does not pay to be greedy. Focus on a single
message; papers that simultaneously focus on multiple contributions tend to be
less convincing about each and are therefore less memorable.

The most important element of a paper is the title—think of the ratio of
the number of titles you read to the number of papers you read. The title is
typically the first element a reader encounters, so its quality [3] determines
whether the reader will invest time in reading the abstract.

The title not only transmits the paper’s central contribution but can also
serve as a constant reminder (to you) to focus the text on transmitting that
idea. Science is, after all, the abstraction of simple principles from complex
data. The title is the ultimate refinement of the paper’s contribution. Thinking
about the title early—and regularly returning to hone it—can help not only the
writing of the paper but also the process of designing experiments or developing
theories.

This Rule of One is the most difficult rule to optimally implement because
it comes face-to-face with the key challenge of science, which is to make the
claim and/or model as simple as the data and logic can support but no simpler.
In the end, your struggle to find this balance may appropriately result in “one
contribution” that is multifaceted. For example, a technology paper may describe
both its new technology and a biological result using it; the bridge that
unifies these two facets is a clear description of how the new technology can be
used to do new biology.

##Rule 2: Write for flesh-and-blood human beings who do not know your work

Because you are the world’s leading expert at exactly what you are doing, you
are also the world’s least qualified person to judge your writing from the
perspective of the naïve reader. The majority of writing mistakes stem from this
predicament. Think like a designer—for each element, determine the impact that
you want to have on people and then strive to achieve that objective [4]. Try to
think through the paper like a naïve reader who must first be made to care about
the problem you are addressing (see Rule 6) and then will want to understand
your answer with minimal effort.

Define technical terms clearly because readers can become frustrated when they
encounter a word that they don’t understand. Avoid abbreviations and acronyms so
that readers do not have to go back to earlier sections to identify them.

The vast knowledge base of human psychology is useful in paper writing. For
example, people have working memory constraints in that they can only remember a
small number of items and are better at remembering the beginning and the end of
a list than the middle [5]. Do your best to minimize the number of loose threads
that the reader has to keep in mind at any one time.

##Rule 3: Stick to the context-content-conclusion (C-C-C) scheme

The vast majority of popular (i.e., memorable and re-tellable) stories have a
structure with a discernible beginning, a well-defined body, and an end. The
beginning sets up the context for the story, while the body (content) advances
the story towards an ending in which the problems find their conclusions.
This structure reduces the chance that the reader will wonder “Why was I told
that?” (if the context is missing) or “So what?” (if the conclusion is missing).

There are many ways of telling a story. Mostly, they differ in how well they
serve a patient reader versus an impatient one [6]. The impatient reader needs
to be engaged quickly; this can be accomplished by presenting the most exciting
content first (e.g., as seen in news articles). The C-C-C scheme that we
advocate serves a more patient reader who is willing to spend the time to get
oriented with the context. A consequent disadvantage of C-C-C is that it may
not optimally engage the impatient reader. This disadvantage is mitigated by
the fact that the structure of scientific articles, specifically the primacy of
the title and abstract, already forces the content to be revealed quickly. Thus,
a reader who proceeds to the introduction is likely engaged enough to have the
patience to absorb the context. Furthermore, one hazard of excessive “content
first” story structures in science is that you may generate skepticism in the
reader because they may be missing an important piece of context that makes
your claim more credible. For these reasons, we advocate C-C-C as a “default”
scientific story structure.

The C-C-C scheme defines the structure of the paper on multiple scales. At
the whole-paper scale, the introduction sets the context, the results are the
content, and the discussion brings home the conclusion. Applying C-C-C at the
paragraph scale, the first sentence defines the topic or context, the body
hosts the novel content put forth for the reader’s consideration, and the last
sentence provides the conclusion to be remembered.

Deviating from the C-C-C structure often leads to papers that are hard to read,
but writers often do so because of their own autobiographical context. During
our everyday lives as scientists, we spend a majority of our time producing
content and a minority amidst a flurry of other activities. We run experiments,
develop the exposition of available literature, and combine thoughts using
the magic of human cognition. It is natural to want to record these efforts on
paper and structure a paper chronologically. But for our readers, most details
of our activities are extraneous. They do not care about the chronological path
by which you reached a result; they just care about the ultimate claim and the
logic supporting it (see Rule 7). Thus, all our work must be reformatted to
provide a context that makes our material meaningful and a conclusion that helps
the reader to understand and remember it.

##Rule 4: Optimize your logical flow by avoiding zig-zag and using parallelism
##Avoiding zig-zag.

Only the central idea of the paper should be touched upon multiple times.
Otherwise, each subject should be covered in only one place in order to minimize
the number of subject changes. Related sentences or paragraphs should be strung
together rather than interrupted by unrelated material. Ideas that are similar,
such as two reasons why we should believe something, should come one immediately
after the other. Using parallelism.

Similarly, across consecutive paragraphs or sentences, parallel messages should
be communicated with parallel form. Parallelism makes it easier to read the text
because the reader is familiar with the structure. For example, if we have three
independent reasons why we prefer one interpretation of a result over another,
it is helpful to communicate them with the same syntax so that this syntax
becomes transparent to the reader, which allows them to focus on the content.
There is nothing wrong with using the same word multiple times in a sentence
or paragraph. Resist the temptation to use a different word to refer to the
same concept—doing so makes readers wonder if the second word has a slightly
different meaning. The components of a paper (Rules 5–8)

The individual parts of a paper—abstract, introduction, results, and discussion
—have different objectives, and thus they each apply the C-C-C structure a
little differently in order to achieve their objectives. We will discuss these
specialized structures in this section and summarize them in Fig 1. thumbnail

Fig 1. Summary of a paper’s structural elements at three spatial scales: Across
sections, across paragraphs, and within paragraphs.

Note that the abstract is special in that it contains all three elements
(Context, Content, and Conclusion), thus comprising all three colors.

https://doi.org/10.1371/journal.pcbi.1005619.g001 
##Rule 5: Tell a complete story in the abstract

The abstract is, for most readers, the only part of the paper that will be
read. This means that the abstract must convey the entire message of the
paper effectively. To serve this purpose, the abstract’s structure is highly
conserved. Each of the C-C-C elements is detailed below.

The context must communicate to the reader what gap the paper will fill. The
first sentence orients the reader by introducing the broader field in which the
particular research is situated. Then, this context is narrowed until it lands
on the open question that the research answered. A successful context section
sets the stage for distinguishing the paper’s contributions from the current
state of the art by communicating what is missing in the literature (i.e., the
specific gap) and why that matters (i.e., the connection between the specific
gap and the broader context that the paper opened with).

The content (“Here we”) first describes the novel method or approach that you
used to fill the gap or question. Then you present the meat—your executive
summary of the results.

Finally, the conclusion interprets the results to answer the question that was
posed at the end of the context section. There is often a second part to the
conclusion section that highlights how this conclusion moves the broader field
forward (i.e., “broader significance”). This is particularly true for more
“general” journals with a broad readership.

This structure helps you avoid the most common mistake with the abstract, which
is to talk about results before the reader is ready to understand them. Good
abstracts usually take many iterations of refinement to make sure the results
fill the gap like a key fits its lock. The broad-narrow-broad structure allows
you to communicate with a wider readership (through breadth) while maintaining
the credibility of your claim (which is always based on a finite or narrow set
of results).

##Rule 6: Communicate why the paper matters in the introduction 
The introduction highlights the gap that exists in current knowledge or methods 
and why it is important. This is usually done by a set of progressively more 
specific paragraphs that culminate in a clear exposition of what is lacking in
the literature, followed by a paragraph summarizing what the paper does to fill
that gap.

As an example of the progression of gaps, a first paragraph may explain why
understanding cell differentiation is an important topic and that the field has
not yet solved what triggers it (a field gap). A second paragraph may explain
what is unknown about the differentiation of a specific cell type, such as
astrocytes (a subfield gap). A third may provide clues that a particular gene
might drive astrocytic differentiation and then state that this hypothesis is
untested (the gap within the subfield that you will fill). The gap statement
sets the reader’s expectation for what the paper will deliver.

The structure of each introduction paragraph (except the last) serves the goal
of developing the gap. Each paragraph first orients the reader to the topic
(a context sentence or two) and then explains the “knowns” in the relevant
literature (content) before landing on the critical “unknown” (conclusion) that
makes the paper matter at the relevant scale. Along the path, there are often
clues given about the mystery behind the gaps; these clues lead to the untested
hypothesis or undeveloped method of the paper and give the reader hope that
the mystery is solvable. The introduction should not contain a broad literature
review beyond the motivation of the paper. This gap-focused structure makes it
easy for experienced readers to evaluate the potential importance of a paper—
they only need to assess the importance of the claimed gap.

The last paragraph of the introduction is special: it compactly summarizes the
results, which fill the gap you just established. It differs from the abstract
in the following ways: it does not need to present the context (which has just
been given), it is somewhat more specific about the results, and it only briefly
previews the conclusion of the paper, if at all.

## Rule 7-Results: Deliver the results as a sequence of statements, 
# supported by figures, that connect logically to support the central 
# contribution

The results section needs to convince the reader that the central claim is
supported by data and logic. Every scientific argument has its own particular
logical structure, which dictates the sequence in which its elements should be
presented.

For example, a paper may set up a hypothesis, verify that a method for
measurement is valid in the system under study, and then use the measurement to
disprove the hypothesis. Alternatively, a paper may set up multiple alternative
(and mutually exclusive) hypotheses and then disprove all but one to provide
evidence for the remaining interpretation. The fabric of the argument will
contain controls and methods where they are needed for the overall logic.

In the outlining phase of paper preparation (see Rule 9), sketch out the
logical structure of how your results support your claim and convert this into
a sequence of declarative statements that become the headers of subsections
within the results section (and/or the titles of figures). Most journals allow
this type of formatting, but if your chosen journal does not, these headers
are still useful during the writing phase and can either be adapted to serve as
introductory sentences to your paragraphs or deleted before submission. Such a
clear progression of logical steps makes the paper easy to follow.

Figures, their titles, and legends are particularly important because they show
the most objective support (data) of the steps that culminate in the paper’s
claim. Moreover, figures are often viewed by readers who skip directly from the
abstract in order to save time. Thus, the title of the figure should communicate
the conclusion of the analysis, and the legend should explain how it was done.
Figure making is an art unto itself; the Edward Tufte books remain the gold
standard for learning this craft [7,8].

The first results paragraph is special in that it typically summarizes the
overall approach to the problem outlined in the introduction, along with any key
innovative methods that were developed. Most readers do not read the methods, so
this paragraph gives them the gist of the methods that were used.

Each subsequent paragraph in the results section starts with a sentence or two
that set up the question that the paragraph answers, such as the following:
“To verify that there are no artifacts…,” “What is the test-retest reliability
of our measure?,” or “We next tested whether Ca2+ flux through L-type Ca2+
channels was involved.” The middle of the paragraph presents data and logic that
pertain to the question, and the paragraph ends with a sentence that answers
the question. For example, it may conclude that none of the potential artifacts
were detected. This structure makes it easy for experienced readers to fact-
check a paper. Each paragraph convinces the reader of the answer given in its
last sentence. This makes it easy to find the paragraph in which a suspicious
conclusion is drawn and to check the logic of that paragraph. The result of each
paragraph is a logical statement, and paragraphs farther down in the text rely
on the logical conclusions of previous paragraphs, much as theorems are built in
mathematical literature. 

##Rule 8: Discuss how the gap was filled, the limitations
of the interpretation, and the relevance to the field

The discussion section explains how the results have filled the gap that was
identified in the introduction, provides caveats to the interpretation, and
describes how the paper advances the field by providing new opportunities. This
is typically done by recapitulating the results, discussing the limitations,
and then revealing how the central contribution may catalyze future progress.
The first discussion paragraph is special in that it generally summarizes the
important findings from the results section. Some readers skip over substantial
parts of the results, so this paragraph at least gives them the gist of that
section.

Each of the following paragraphs in the discussion section starts by describing
an area of weakness or strength of the paper. It then evaluates the strength or
weakness by linking it to the relevant literature. Discussion paragraphs often
conclude by describing a clever, informal way of perceiving the contribution or
by discussing future directions that can extend the contribution.

For example, the first paragraph may summarize the results, focusing on their
meaning. The second through fourth paragraphs may deal with potential weaknesses
and with how the literature alleviates concerns or how future experiments
can deal with these weaknesses. The fifth paragraph may then culminate in a
description of how the paper moves the field forward. Step by step, the reader
thus learns to put the paper’s conclusions into the right context. Process
(Rules 9 and 10)

To produce a good paper, authors can use helpful processes and habits. Some
aspects of a paper affect its impact more than others, which suggests that
your investment of time should be weighted towards the issues that matter most.
Moreover, iteratively using feedback from colleagues allows authors to improve
the story at all levels to produce a powerful manuscript. Choosing the right
process makes writing papers easier and more effective. 
## Rule 9: Allocate time where it matters: 
## Title, abstract, figures, and outlining

The central logic that underlies a scientific claim is paramount. It is also the
bridge that connects the experimental phase of a research effort with the paper-
writing phase. Thus, it is useful to formalize the logic of ongoing experimental
efforts (e.g., during lab meetings) into an evolving document of some sort that
will ultimately steer the outline of the paper.

You should also allocate your time according to the importance of each section.
The title, abstract, and figures are viewed by far more people than the rest of
the paper, and the methods section is read least of all. Budget accordingly.

The time that we do spend on each section can be used efficiently by planning
text before producing it. Make an outline. We like to write one informal
sentence for each planned paragraph. It is often useful to start the process
around descriptions of each result—these may become the section headers in the
results section. Because the story has an overall arc, each paragraph should
have a defined role in advancing this story. This role is best scrutinized at
the outline stage in order to reduce wasting time on wordsmithing paragraphs
that don’t end up fitting within the overall story. 

##Rule 10: Get feedback to reduce, reuse, and recycle the story

Writing can be considered an optimization problem in which you simultaneously
improve the story, the outline, and all the component sentences. In this
context, it is important not to get too attached to one’s writing. In many
cases, trashing entire paragraphs and rewriting is a faster way to produce good
text than incremental editing.

There are multiple signs that further work is necessary on a manuscript (see
Table 1). For example, if you, as the writer, cannot describe the entire outline
of a paper to a colleague in a few minutes, then clearly a reader will not be
able to. You need to further distill your story. Finding such violations of good
writing helps to improve the paper at all levels. thumbnail

https://doi.org/10.1371/journal.pcbi.1005619.t001

Successfully writing a paper typically requires input from multiple people.
Test readers are necessary to make sure that the overall story works. They
can also give valuable input on where the story appears to move too quickly or
too slowly. They can clarify when it is best to go back to the drawing board
and retell the entire story. Reviewers are also extremely useful. Non-specific
feedback and unenthusiastic reviews often imply that the reviewers did not
“get” the big picture story line. Very specific feedback usually points out
places where the logic within a paragraph was not sufficient. It is vital to
accept this feedback in a positive way. Because input from others is essential,
a network of helpful colleagues is fundamental to making a story memorable.
To keep this network working, make sure to pay back your colleagues by reading
their manuscripts. Discussion

This paper focused on the structure, or “anatomy,” of manuscripts. We had to
gloss over many finer points of writing, including word choice and grammar,
the creative process, and collaboration. A paper about writing can never be
complete; as such, there is a large body of literature dealing with issues of
scientific writing [9,10,11,12,13,14,15,16,17].

Personal style often leads writers to deviate from a rigid, conserved
structure, and it can be a delight to read a paper that creatively bends
the rules. However, as with many other things in life, a thorough mastery of
the standard rules is necessary to successfully bend them [18]. In following
these guidelines, scientists will be able to address a broad audience, bridge
disciplines, and more effectively enable integrative science. Acknowledgments

We took our own advice and sought feedback from a large number of colleagues
throughout the process of preparing this paper. We would like to especially
thank the following people who gave particularly detailed and useful feedback:

Sandra Aamodt, Misha Ahrens, Vanessa Bender, Erik Bloss, Davi Bock, Shelly
Buffington, Xing Chen, Frances Cho, Gabrielle Edgerton, multiple generations of
the COSMO summer school, Jason Perry, Jermyn See, Nelson Spruston, David Stern,
Alice Ting, Joshua Vogelstein, Ronald Weber. References

1. Hirsch JE (2005) An index to quantify an individual's scientific research
output. Proc Natl Acad Sci U S A. 102: 16569–16572. pmid:16275915 
2. Acuna DE,Allesina S, Kording KP (2012) Future impact: Predicting scientific success.Nature. 489: 201–202. pmid:22972278 
3. Paiva CE, Lima JPSN, Paiva BSR (2012) Articles with short titles describing 
the results are cited more often. Clinics. 67: 509–513. pmid:22666797 
4. Carter M (2012) Designing Science Presentations: A Visual Guide to Figures, 
Papers, Slides, Posters, and More: Academic Press.
5. Murdock BB Jr (1968) Serial order effects in short-term memory. J Exp
Psychol. 76: Suppl:1–15. 
6. Schimel J (2012) Writing science: how to write papers that get cited and
proposals that get funded. USA: OUP. 
7. Tufte ER (1990) Envisioning information. Graphics Press. 
8. Tufte ER The Visual Display of Quantitative Information. Graphics Press. 
9. Lisberger SG (2011) From Science to Citation: How to Publish a Successful
Scientific Paper. Stephen Lisberger.
10. Simons D (2012) Dan's writing and revising guide. http://www.dansimons.com/
resources/Simons_on_writing.pdf [cited 2017 Sep 9]. 
11. Sørensen C (1994) This is Not an Article—Just Some Thoughts on How to Write 
One. Syöte, Finland: Oulu University, 46–59. 
12. Day R (1988) How to write and publish a scientific paper. Phoenix: Oryx. 
13. Lester JD, Lester J (1967) Writing research papers. Scott, Foresman. 
14. Dumont J-L (2009) Trees, Maps, and Theorems. Principiae. http://
www.treesmapsandtheorems.com/ [cited 2017 Sep 9]. 
15. Pinker S (2014) The Sense of Style: The Thinking Person’s Guide to Writing 
in the 21st Century. Viking Adult. 
16. Bern D (1987) Writing the empirical journal. The compleat academic: 
  A practical guide for the beginning social scientist. 171. 
17. George GD, Swan JA (1990) The science of scientific writing.
Am Sci. 78: 550–558. 
18. Strunk W (2007) The elements of style. Penguin.

<!--
Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0  Formulate your question  
Spend a few minutes to figure out the question you’re really interested in, and narrow it down to be as specific as possible (without becoming uninteresting).

General question:
Are air pollution levels higher on the east coast than on the west coast?
More specific question:
Are hourly ozone levels on average higher in New York City than they are in Los Angeles?

2.0   Read in your data  
Sometimes the data will need some cleaning and every dataset has its unique quirks. The dataset is a comma-separated value (CSV) file, where each row of the file contains one daily measurement of precipation depth.  The readr package can rewrite column names to remove spaces.
> names(ozone) <- make.names(names(ozone))

3.0  Check the dataset 
3.1  Check the number of rows and columns.
3.2  Check the types of data
3.3  Look at the top and the bottom of your data 
3.4  Check your “n”s & NAs 
3.5  Validate with at least one external data source  
4.0  Try the easy solution first to answer question
5.0  Challenge your solution 
6.0  Follow up questions 


## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?
What is streamflow variation across the Pine Ridge Reservation?

## Narrower questions:
What is are precipitation trends?  
What is streamflow variation across the Pine Ridge Reservation?



<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.


# R Markdown Basics {#rmd-basics}

Here is a brief introduction into using _R Markdown_. _Markdown_ is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. _R Markdown_ provides the flexibility of _Markdown_ with the implementation of **R** input and output.  For more details on using _R Markdown_ see <http://rmarkdown.rstudio.com>.  

Be careful with your spacing in _Markdown_ documents.  While whitespace largely is ignored, it does at times give _Markdown_ signals as to how to proceed.  As a habit, try to keep everything left aligned whenever possible, especially as you type a new paragraph.  In other words, there is no need to indent basic text in the Rmd document (in fact, it might cause your text to do funny things if you do).

## Lists

It's easy to create a list.  It can be unordered like

* Item 1
* Item 2

or it can be ordered like

1. Item 1
4. Item 2

Notice that I intentionally mislabeled Item 2 as number 4.  _Markdown_ automatically figures this out!  You can put any numbers in the list and it will create the list.  Check it out below.

To create a sublist, just indent the values a bit (at least four spaces or a tab).  (Here's one case where indentation is key!)

1. Item 1
1. Item 2
1. Item 3
    - Item 3a
    - Item 3b

## Line breaks

Make sure to add white space between lines if you'd like to start a new paragraph.  Look at what happens below in the outputted document if you don't:

Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph.
This should be a new paragraph.

*Now for the correct way:* 

Here is the first sentence.  Here is another sentence.  Here is the last sentence to end the paragraph.

This should be a new paragraph.

## R chunks

When you click the **Knit** button above a document will be generated that includes both content as well as the output of any embedded **R** code chunks within the document. You can embed an **R** code chunk like this (`cars` is a built-in **R** dataset):

```{r cars}
summary(cars)
```

## Inline code

If you'd like to put the results of your analysis directly into your discussion, add inline code like this:

> The `cos` of $2 \pi$ is `r cos(2*pi)`. 

Another example would be the direct calculation of the standard deviation:

> The standard deviation of `speed` in `cars` is `r sd(cars$speed)`.

One last neat feature is the use of the `ifelse` conditional statement which can be used to output text depending on the result of an **R** calculation:

> `r ifelse(sd(cars$speed) < 6, "The standard deviation is less than 6.", "The standard deviation is equal to or greater than 6.")`

Note the use of `>` here, which signifies a quotation environment that will be indented.

As you see with `$2 \pi$` above, mathematics can be added by surrounding the mathematical text with dollar signs.  More examples of this are in [Mathematics and Science] if you uncomment the code in [Math].  

## Including plots

You can also embed plots.  For example, here is a way to use the base **R** graphics package to produce a plot using the built-in `pressure` dataset:

```{r pressure, echo=FALSE, cache=TRUE}
plot(pressure)
```

Note that the `echo=FALSE` parameter was added to the code chunk to prevent printing of the **R** code that generated the plot.  There are plenty of other ways to add chunk options.  More information is available at <http://yihui.name/knitr/options/>.  

Another useful chunk option is the setting of `cache=TRUE` as you see here.  If document rendering becomes time consuming due to long computations or plots that are expensive to generate you can use knitr caching to improve performance.  Later in this file, you'll see a way to reference plots created in **R** or external figures.

## Loading and exploring data

Included in this template is a file called `flights.csv`.  This file includes a subset of the larger dataset of information about all flights that departed from Seattle and Portland in 2014.  More information about this dataset and its **R** package is available at <http://github.com/ismayc/pnwflights14>.  This subset includes only Portland flights and only rows that were complete with no missing values.  Merges were also done with the `airports` and `airlines` data sets in the `pnwflights14` package to get more descriptive airport and airline names.

We can load in this data set using the following command:

```{r load_data}
flights <- read.csv("data/flights.csv")
```

The data is now stored in the data frame called `flights` in **R**.  To get a better feel for the variables included in this dataset we can use a variety of functions.  Here we can see the dimensions (rows by columns) and also the names of the columns.

```{r str}
dim(flights)
names(flights)
```

Another good idea is to take a look at the dataset in table form.  With this dataset having more than 50,000 rows, we won't explicitly show the results of the command here.  I recommend you enter the command into the Console **_after_** you have run the **R** chunks above to load the data into **R**.

```{r view_flights, eval=FALSE}
View(flights)
```

While not required, it is highly recommended you use the `dplyr` package to manipulate and summarize your data set as needed.  It uses a syntax that is easy to understand using chaining operations.  Below I've created a few examples of using `dplyr` to get information about the Portland flights in 2014.  You will also see the use of the `ggplot2` package, which produces beautiful, high-quality academic visuals.

We begin by checking to ensure that needed packages are installed and then we load them into our current working environment:

```{r load_pkgs, message=FALSE}
# List of packages required for this analysis
pkg <- c("dplyr", "ggplot2", "knitr", "bookdown", "devtools")
# Check if packages are not installed and assign the
# names of the packages not installed to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg))
  install.packages(new.pkg, repos = "http://cran.rstudio.com")
# Load packages (thesisdown will load all of the packages as well)
library(thesisdown)
```

\clearpage

The example we show here does the following:

- Selects only the `carrier_name` and `arr_delay` from the `flights` dataset and then assigns this subset to a new variable called `flights2`. 

- Using `flights2`, we determine the largest arrival delay for each of the carriers.

```{r max_delays}
flights2 <- flights %>% 
  select(carrier_name, arr_delay)
max_delays <- flights2 %>% 
  group_by(carrier_name) %>%
  summarize(max_arr_delay = max(arr_delay, na.rm = TRUE))
```

A useful function in the `knitr` package for making nice tables in _R Markdown_ is called `kable`.  It is much easier to use than manually entering values into a table by copying and pasting values into Excel or LaTeX.  This again goes to show how nice reproducible documents can be! (Note the use of `results="asis"`, which will produce the table instead of the code to create the table.)  The `caption.short` argument is used to include a shorter title to appear in the List of Tables.

```{r maxdelays, results="asis"}
kable(max_delays, 
      col.names = c("Airline", "Max Arrival Delay"),
      caption = "Maximum Delays by Airline",
      caption.short = "Max Delays by Airline",
      longtable = TRUE,
      booktabs = TRUE)
```

The last two options make the table a little easier-to-read.

We can further look into the properties of the largest value here for American Airlines Inc.  To do so, we can isolate the row corresponding to the arrival delay of 1539 minutes for American in our original `flights` dataset.


```{r max_props}
flights %>% filter(arr_delay == 1539, 
                  carrier_name == "American Airlines Inc.") %>%
  select(-c(month, day, carrier, dest_name, hour, 
            minute, carrier_name, arr_delay))
```

We see that the flight occurred on March 3rd and departed a little after 2 PM on its way to Dallas/Fort Worth.  Lastly, we show how we can visualize the arrival delay of all departing flights from Portland on March 3rd against time of departure.

```{r march3plot, fig.height=3, fig.width=6}
flights %>% filter(month == 3, day == 3) %>%
  ggplot(aes(x = dep_time, y = arr_delay)) + geom_point()
```

## Additional resources

- _Markdown_ Cheatsheet - <https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet>

- _R Markdown_ Reference Guide - <https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf>

- Introduction to `dplyr` - <https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html>

- `ggplot2` Documentation - <http://docs.ggplot2.org/current/>

## Overview
<!--This is the chapter 1 drought review.  
Next Steps:
1. Research how to structure a review paper.
2. Research how to download a bibtex file
-->

## Introduction
Drought a complex and poorly understood natural hazard that simultaneously affects many economic sectors and people (Wilhite 2000).  Changing climatic conditions and increases in extreme climate events have resulted in increased concern about drought frequency, severity, and duration (Peterson et al. 2013). Drought can be defined as the extreme persistence of precipitation deficit over a specific region for a specific period  (Zargar et al. 2011).  However, a single definition of drought is not possible because drought effects vary geographically, temporally, and by economic and environmental sectors impacted by a reduction in the water supply (Heim 2002).  Different drought types are defined based on the system of interest with operational definitions used for parts of the hydrologic cycle that experience drought and conceptual definitions used for societal or ecological systems that depend on a water supply for which factors outside of the physical nature of drought exist (Tsakiris and Vangelis 2005).

Drought impacts available water supply to economic and social sectors, and ecosystems. Socioeconomic drought occurs when water demand is greater than water supply and results from increased water demand by growing populations or economic sector development, over-allocation among competing beneficial uses, or non-sustainable groundwater use (Frick et al. 1990; Heim 2002; Svoboda and Fuchs 2016).  Socioeconomic drought magnitudes have increased in recent years as a result of the narrowing of the gap between water supply and water demand (Hayes et al. 2011).  Ecological drought is defined as a “shortage of water causing stress on ecosystems, adversely affecting the life of plants and animals” (Lake 2011). Unlike other forms of drought, ecological drought does not currently have specific indices to quantify it (Lake 2011; Svoboda et al. n.d.). The impacts of these types of drought depend on the magnitude of the water shortage, as well as the vulnerability of a socioeconomic sector or ecosystem to a reduction in the water supply (Zargar et al. 2011).

Meteorological, agricultural, and hydrological drought are operationally defined by duration, magnitude, geographic extent, and frequency, as well as by drought impacts, indicators, and indices 
(Wilhite and Glantz 1985; Wilhite et al. 2014; Zargar et al. 2011). Indicators are used in combination to simplify complex interrelationships these parameters to derive a drought index.  A drought index is a numerical standard based on water-balance or hydrological models and professional judgment (i.e., the US Drought Index) (Svoboda et al. 2015). A drought index objectively compares cumulative effects of a prolonged and abnormal moisture deficiency and recurrence probability from region to region and historical drought to current conditions (Heim 2002; Svoboda and Fuchs 2016; Zargar et al. 2011).

Communication of climate anomalies using drought indices can facilitate the planning the development of water resources as part of drought risk management (Wilhite 2000).  Duration is the length of a drought, usually of a minimum of two to three months to become established and continuing for months to years. Magnitude is the accumulated deficit of water (e.g., precipitation, soil moisture, or runoff) below some threshold during a drought period. Drought impact magnitude of drought impacts is closely related to the timing of the onset of the precipitation shortage, its intensity, and the duration of the event.  Intensity is the ratio of drought magnitude to duration (Zargar et al. 2011).  The degree of a precipitation shortfall and the severity of associated impacts, often measured by a departure from expected or average normal precipitation for a period ranging from one to twelve or more months.  Drought intensity and duration determine the extent of drought impact (Wilhite 2000).  Geographic extent is the areal coverage of the drought which is variable during the event (Zargar et al. 2011).  Drought frequency or average return period is defined as the average time between drought events that have a severity that is equal to or greater than a threshold (Zargar et al. 2011).  Drought impacts are defined as an observable loss or change at a specific time because of drought.  Along with precipitation deficit, drought indicators are combinations of climate parameters: evapotranspiration, temperature, streamflow, groundwater and reservoir levels, soil moisture and snowpack (Svoboda and Fuchs 2016). 

Meteorological drought is an extended precipitation deficit over an extended period between the actual depth of precipitation received and the expected precipitation depth (Heim 2002).  Meteorological drought, which can develop quickly and end abruptly, leads to other types of drought.  Agricultural drought can rapidly follow a meteorological drought, particularly during a period of high temperatures and windy conditions (Heim 2002; Wilhite 2000).  Agricultural drought, defined by sustained soil moisture deficits, leads to low recharge from the soil to surface and ground waters and may result in hydrological drought.  

Agricultural drought “is typically defined as a period when soil moisture is inadequate to meet evapotranspiration demands to initiate and sustain crop growth” (Changnon 1987).  Soil moisture availability is the difference between actual and potential evapotranspiration (Tate and Gustard 2000). The potential evapotranspiration (PET) equals the actual evapotranspiration (AET) when an adequate supply of moisture in the soil is available to meet vegetation moisture demands. Otherwise, the actual evapotranspiration is less than the potential evapotranspiration.  While solar radiation is the dominant factor for evapotranspiration, direct measurements of solar radiation are often unavailable; and mean daily temperature, latitude, and time of year are used to approximate PET.   Soil infiltration rates, soil moisture capacity, and the magnitude and timing of precipitation (Heim 2002; Lake 2011).  Wilhite (2000) suggests an operational definition of agricultural drought by comparing precipitation to AET to determine the rate of soil water depletion, then expressing the relationship on soil moisture effects on plant development.  

Hydrological drought emphasizes interactions between the natural characteristics of meteorological drought and the human activities that depend on precipitation to provide adequate water supplies to meet societal and environmental demands (Wilhite et al. 2014). Hydrological drought is  “a period of abnormally dry weather sufficiently prolonged for the lack of precipitation to cause a serious hydrological imbalance” (AMS Council 2013).  A hydrological drought has two components: surface water drought and groundwater drought, “with the latter lagging well behind surface water drought in both commencing and finishing” (Bond et al. 2008). 

Hydrological drought determination is based on the effects of the precipitation shortfall on the surface or subsurface water supply, rather than the magnitude of the precipitation deficiency (Dracup et al. 1980; Wilhite 2000).  Antecedent soil moisture and aquifer conditions, hot temperatures, low relative humidity, and desiccating winds are other factors that may result in agricultural and hydrological droughts following a short-term absence of precipitation (Heim 2002). Because time elapses before precipitation deficiencies become evident as reduced streamflow, reservoir, and groundwater levels, hydrological drought, is often out of phase with the occurrence of meteorological and agricultural drought.  A hydrological drought may continue for months or years beyond the termination of other drought types because of the long time needed to recharge reservoirs or groundwater (Wilhite 2000). 

Hydrological drought designation is often based on streamflow (Heim 2002) because streamflow integrates hydrologic processes at the watershed level. Streamflow at the watershed level includes direct runoff from the ground surface to a channel, interflow of water into the stream channel from saturated soils, and base runoff from groundwater outflow.  Reduced stream flows in upstream segments of a basin may result in lower reservoir and groundwater levels at downstream locations, even though meteorological drought does not extend to lower portions.  Reduced streamflow impacts public water supply, hydroelectric power production, recreation, transportation, and agriculture, leading to conflicts between upstream and downstream water users (Wilhite 2000).  

Quantification of hydrologic drought impacts are further complicated by multiple and often competing hydrological storage system usages such as irrigation, recreation, tourism, flood control, hydroelectric power production, domestic water supply, protection of endangered species, and ecosystem preservation.  Timing between meteorological drought and hydrological drought depends on watershed storage and recharge rates.  Recovery from a hydrologic drought recovery lags the return to normal meteorological conditions with a length of the lag related to the groundwater recharge rate that varies with watershed geology and vertical connectivity (Wilhite et al. 2014). 

Ecological drought is defined as a shortage of water causing stress on ecosystems, adversely affecting the life of plants and animals (Lake 2011). Drought stress is a ramp disturbance that steadily builds in strength and spatial extent as water availability declines (Humphries and Baldwin 2003).   While hydrology fundamentally influences ecosystem dynamics, life history strategies, and diversity patterns in streams (Schriever et al. 2015), water workers have only recently identified ecological drought as a disturbance. Stream channel water levels (e.g., stream stage) drops during a hydrologic drought leading to a weakening of lateral connectivity as water recedes from the riparian and littoral zones and backwaters (Boulton, 2003). Hydrological drought results in a stage reduction caused by decreased lateral connectivity as waters recede from the riparian and littoral zones and backwaters, and groundwater flow.  Flow reduction causes several abiotic changes including reduced organic C, N, and P inputs from the riparian zone, higher water temperatures caused by reduced riparian shading, high air temperatures, increased salinity, alkalinity, pH, Mg/Ca ratios, hypoxic conditions, and decreased water clarity.  A stream may become intermittent later in a hydrologic drought, and in streams with high particulate organic matter concentrations "blackwater conditions" can increase turbidity (Lake 2011; Russell and Johnson 2007).  The abiotic effects of hydrological drought often result in stream ecosystem changes, including increases in filamentous algae and decreased heterotrophic production.  The ecologic response depends on the magnitude of the precipitation drought, streamflow permanence, and ecosystem resiliency (Bonada et al. 2007; Bond et al. 2008; Humphries and Baldwin 2003; Resh et al. 2013).  Several authors have distinguished predictable seasonal drought, allowing for evolutionary or life history adaptations to dry periods, from an unpredictable supra-seasonal drought that can result in abrupt long-term changes in community structure (Boersma et al. 2014; Bogan et al. 2013; Humphries:2003tna Lake 2011).  

States and Tribal entities are required under the federal Clean Water Act to develop standards for their waters to ensure the protection of beneficial uses protected. The United States Environmental Protection Agency (US EPA) consults on a government‐to‐government basis with federally recognized tribes under the EPA Policy on Consultation and Coordination with Indian Tribes (US Environmental Protection Agency 1995).  As discussed above, the study area is the Pine Ridge reservation and is under the jurisdiction of the Oglala Sioux Tribe (OST).  The OST currently follows South Dakota surface water quality standards for waters of the Pine Ridge Reservation (SD Dept Environment and Natural Resources 2017) for beneficial uses including Warm and Cold Water Permanent and Semi-Permanent Fisheries, Immersion Recreation, Stock Watering.  States and Tribal Entities are required by the US EPA to adopt either narrative or numeric criteria to maintain or restore biological integrity in waters under their jurisdiction (US Environmental Protection Agency 1995). Biological integrity is defined to mean “the capability of supporting and maintaining a balanced, integrated, and adaptive community of organisms having a composition and diversity comparable to that of natural habitats of the region” (US Environmental Protection Agency 2011). 

The US EPA recommends a multiple-stressor approach to identify key stressors (disturbances) and primarily taxa-based metrics such as taxa-richness,  dominance, the percentage of pollution-sensitive taxa, and percentage of pollution-tolerant taxa (Karr 1999).   "A perturbation consists of two parts: the disturbance, and the biotic responses to the disturbance" (Lake 2003).  Stressors (Bunn and Arthington 2002; Poff and Zimmerman 2010), can be naturally occurring (floods, droughts, earthquakes, fires) and human-caused (increased nutrient or sediment loading, human-induced changes in riparian or channel structure.  The OST has designated a Spiritual Use for surface waters under Tribal jurisdiction.  The Tribe has not identified criteria for the Spiritual Use designation; however, Tribal Resource Agencies indicate their interest in developing standards-based criteria describing the desired biological condition of aquatic communities.

Literature Review
The World Meteorological Organization (WMO) recommends the use of drought indices for measuring meteorological, agricultural and hydrologic drought magnitudes (Hayes et al. 2011).  The recommended drought indices are the Standardized Precipitation Index (SPI) for meteorological drought, the Standardized Precipitation Evapotranspiration Index (SPEI), another soil moisture or water balance index, or the normalized difference vegetation index (NDVI) for agricultural drought.  The WMO discussed the potential for use of the streamflow drought index (SDI) but did not come to a consensus on a single hydrological drought index.  The WMO has not developed recommendations for ecological drought.

Drought monitoring in regions where drought occurs mainly because of precipitation variability is well-represented by SPI because the index measures the total precipitation deviation from normal over a monthly or longer averaging period (Fiorillo and Guadagno 2009; Khan et al. 2008; Vicente-Serrano and López-Moreno 2005). SPI has a solid theoretical development, is robust to missing data, and is characteristic of a moving process (Guttman 1999; Heim 2002; Redmond 2002; Svoboda and Fuchs 2016).  SPI index values are normally distributed, uniquely related to the probability of occurrence and can be used to calculate a running precipitation deficit to indicate drought duration and intensity (Tsakiris et al. 2006).  Since SPI is normalized, it is comparable across wetter and drier locations, time periods and scales (Tigkas et al. 2014; Vicente-Serrano et al. 2012).  Different drought types can be identified by SPI, which is a valuable property of the index because hydrologic systems (i.e., precipitation, soil moisture, streamflow), and regions (i.e., headwaters, valley fill, karstic) can respond to drought conditions at different time scales (Vicente-Serrano and López-Moreno 2005).  Heim (2002), and Svoboda and Fuchs (2016) suggest the following time-steps for SPI calculations for different drought types: one to three months correspond with meteorological drought, three to six months correspond with agricultural drought, and time steps of twelve months or longer correspond with hydrological drought.  The SPI requires 20 to 30 years of time series data with additional years of data having more extreme wet and extreme dry observations resulting in more robust results (Guttman 1999; Svoboda and Fuchs 2016).

The SPI is a dimensionless index that relates dry and wet periods to frequency and duration. SPI transforms a long-term precipitation record to standardized series with an average of zero and a standard deviation of unity.  More negative numbers indicate high intensity and less probable dry events, and, correspondingly,  more positive numbers indicate high intensity and less probable wet events (Svoboda and Fuchs 2016). I provide the mathematical basis for SPI following Vicente-Serrano’s description fitting observations to the Pearson III distribution (Vicente-Serrano 2006).  McKee's (1993) original method used a two-parameter gamma distribution with parameters estimated by the maximum likelihood method; however, Guttman (1999) found Pearson III distribution provides the better goodness of fit. The probability density function for a Pearson III distributed variable is expressed as:
f(x)=1/αΓ(β)  ((x-γ)/α)^(β-1) e^(-((x-γ)/α) )
where α, β and γ are the shape, scale and origin parameters, respectively, for precipitation values x > 0; and (β) is the Gamma function of β. The Pearson III distribution parameters are obtained from L-moment ratios.  The first L-moment ratio, τ2 = λ2/λ1 is analogous to the coefficient of variation, and the second and third L-moment ratios, τ3 = λ3/λ2 and τ4 = λ4/λ2 are analogous to coefficients of skewness and kurtosis, respectively (Hosking 1990). L- moments are linear combinations of probability weighted moments (PWM), which can be calculated using the formulae: 
λ1 = α0(
λ2 = α0 − 2α1(
λ3 = α0 −6α1 +6α2
λ4 = α0 −12α1 +30α2 −20α3 
so PWM of order s is calculated using: 
a_s=1/N ∑_(i=1)^N▒(〖1-F〗_i )^s  x_i;       F_i=(i-0.35)/N
where xi is the data from a given precipitation series, Fi is the frequency estimator, i is the range of observations arranged in rising order, and N is the number of data points (Vicente-Serrano 2006)
If τ3 ≥ 1/3, then τm = 1 − τ3 and β can be obtained using the formula: 
β=  ((0.36067 τ_m  - 〖0.5967τ_m〗^2  + 0.25361〖τ_m〗^3 ))/(1 - 2.78861τ_m  + 2.56096〖τ_m〗^2  - 0.77045〖τ_m〗^3  )
If τ3 < 1/3, then τm = 3π τ32and β can be obtained using the following expression: 
β=  ((1+0.2906 τ_m ))/(τ_m  + 0.1882〖τ_m〗^2  - 0.0442〖τ_m〗^3  )
a=√(π ) λ_2  (Γ(β))/(Γ(β + 1/2) )
γ = λ_1-αβ
The probability distribution function of x given by and are calculated analytically: 
F(x)=1/(αΓ(β)) ∫_y^x▒〖((x-γ)/α)^(β-1) e^(-((x-γ)/α) ) 〗
Pearson III distribution is not defined for x = 0; however, precipitation series may include months in which there is no precipitation. An adapted statistic H(x) can be calculated using the following formula: 
H(x) = q + (1 - q)F(x) 
where q is the probability of zero precipitation and is calculated as m/n, where n is the total number of months and m is the number of months with no precipitation. The cumulative probability H(x), is then transformed to the standard normal random variable z with mean zero and variance of one.  Fortunately, SPI can be calculated using either a stand-alone package available through the University of Lincoln Drought Mitigation Center webpage (Svoboda and Fuchs 2016) or using the SPEI package (Beguería and Vicente-Serrano 2017) in the R statistical programming language (R Core Team 2018).
McKee et al. (1993) state that drought begins at an SPI of zero or less; however, some researchers choose a drought threshold that is less than zero, but not quite −1 (Svoboda and Fuchs 2016).  The drought event continues until SPI reaches a value greater than zero.  A potential weakness of the SPI is that the index by itself may not account for a particular region's overall water balance and water use if different temperatures occur for similar SPI values (Svoboda and Fuchs 2016).  Drought magnitude is the positive sum of the SPI for each month during the drought event (Hayes et al. 2007).   A composite table of narrative drought classes is shown below with DI (drought index) indicating SPI (meteorological drought), SPEI (soil moisture drought), and SDI (hydrological drought) (Lloyd-Hughes 2002; Tigkas et al. 2014)
Table 5: Drought states, descriptions, index values and frequencies
Drought State	Description	Criterion	Percent frequency
0	Extremely wet	DI≤2.0	2.3
0	Very wet	1.5≤DI<2.0	4.4
0	Moderately wet	1.0≤DI<1.5	9.2
0	Mildly wet	0≤DI<1.0	34.1
0	Non-drought	DI>0	50.0
1	Mild drought	-1.0≤DI<0	34.1
2	Moderate drought	-1.0≤DI<-1.5	9.2
3	Severe drought	-2.0≤DI<-1.5	4.4
4	Extreme drought	-2.0≤DI	2.3

The Standardized Precipitation Evaporation Index (SPEI) is the preferred index for the overall water balance and water use of a region, in which temperature variation is non-stationary, or potential evapotranspiration (PET) is non-negligible (Svoboda and Fuchs 2016).  The SPEI represents departures in the difference between water availability and the atmospheric water demand, the "climate water balance."  SPEI has been shown to be more effective than SPI in correlating streamflow deficits, reservoir storage, and water demand, and therefore, is assumed to provide a reasonable estimate of soil moisture(Lorenzo-Lacruz et al. 2010; Svoboda and Fuchs 2016). A potential weakness of the SPEI is that the index requires a serially complete dataset for both temperature and precipitation (Svoboda and Fuchs 2016).  The SPEI algorithm calculates effective precipitation by subtracting PET from precipitation, with PET estimated by the Thornwaite (Thornthwaite 1948), Penman-Monteith (PM) (Walter et al. 2000), or Hargreaves equation (Droogers and Allen 2002; Hargreaves and Samani 1985). 
The SPEI is mathematically similar to SPI, except that the input for the index is effective precipitation (Beguería et al. 2013; Svoboda and Fuchs 2016); and a log-logistic distribution provides the better goodness of fit than the gamma distribution in the estimation of SPEI values (Vicente-Serrano et al. 2010a).  I provide below the mathematical basis for SPI following Beguería et al. (2014); Vicente-Serrano et al. (2010a, 2010b, 2011, 2012) provide a complete description of the theory behind the SPEI, the computational details, and comparisons of SPEI with other drought indicators. The probability distribution function of a variable D according to a log-logistic distribution is given by:
F(D)=[1+(α/(D-γ))^β ]^(-1)
where andrepresent the scale, shape, and location parameters that are estimated from the sample D, which is the difference between precipitation and PET.  An unbiased plotting estimator based on probability-weighted moments (Hosking 1986) is the default plotting position method implemented in the SPEI package in R as: 
w_s=1/N ∑_(i=1)^N▒((□((N-i)/s)) D_i)/□((N-i)/s)

Nalbantis and Tsakiris (Nalbantis and Tsakiris 2009) derived the standardized streamflow index (SDI) from the SPI to identify hydrologic drought. The SDI uses monthly streamflow values and the methods of normalization associated with SPI for developing a drought index based upon streamflow data (Svoboda and Fuchs 2016).  However, it may be difficult to select the most appropriate distribution to calculate a streamflow drought index over a wide area because topography, lithology, vegetation, and human management may increase flow variability within a basin and change statistical properties of downstream reaches, (Vicente-Serrano et al. 2012).

Nalbantis and Tsakiris (2008) defined the Streamflow Drought Index (SDI) is a hydrological drought index with properties identical to the SPI. I provide the mathematical basis for SPI following Tigkas (2014).  The cumulative streamflow volume Vi k is calculated from the equation: 
V_(i,k)=∑_(j=1)^3k▒Q_(i,j)    i=1,2,⋯;  j=1,2,⋯12,k=1,2,3,4
for monthly streamflow volumes Qi j, in which i denotes the hydrological year, j the month within that hydrological year (j = 1 for October and j = 12 for September), and k the reference period, k = 1 for October-December, k = 2 for October-March, k = 3 for October-June, and k = 4 for October-September.  The Streamflow Drought Index (SDI) is defined for each reference period k of the i-th hydrological year based on the cumulative streamflow volumes Vi,k by:
〖SDI〗_(i,k)=  (V_(i,k)-V ̅_k)/s_k ,   i=1,2,⋯;   k=1,2,3,4
in which Vk and sk are the mean and the standard deviation of cumulative streamflow volumes of the reference period k.  The resulting index is equal to the standardized streamflow volume.  To reduce skewness and because streamflow tends to be well-approximated by a Gamma distribution, Nalbantis and Tsarkaris recommend logarithmic transformation of streamflow volume is before SDI calculation such that  
〖SDI〗_(i,k)=  (y_(i,k)-y ̅_k)/s_(y,k) ,   i=1,2,⋯;   k=1,2,3,4
where
y_k=lna(V_(i,k) ),   i=1,2,⋯;   k=1,2,3,4

Streams are increasingly understood as the central agent at the interface of the co-evolution of climate, geology, topography and ecology and their transient and long-term responses to change (Hrachowitz et al. 2014; Thorp 2014).  Hydrological systems respond differently at various time scales because of lithologic and geometric watershed differences (Lopez-Moreno et al. 2013).  Effective prediction of streamflow responses to drought is increasingly understood in managing for sustainable water supply (Vörösmarty et al. 2000) and water quality (Kundzewicz et al. 2008).  
Hydrology fundamentally influences ecosystem dynamics, life history strategies, and diversity patterns in streams (Belmar et al. 2013; Bonada et al. 2007; Gallart et al. 2012; Ledger et al. 2013a; Lytle and Poff 2016; Olden and Poff 2003).  Differences in streamflow dynamics result in strong habitat-filters favoring taxa adapted to particular hydrological extremes as well as habitat generalists capable of persisting in a variety of habitats (Schriever et al. 2015). Ecological community composition can remain stable for a long time and rapidly transition into an alternative stable state (Bogan and Lytle 2011; Bogan et al. 2015; Yodzis 1989). Hydrologic drought results in a progressive habitat loss, depletion of food resources, and increased predation and interspecific competition that puts aquatic biota under stress (Bond et al. 2008). 

Stream biota exhibit a variable response to seasonal and supra-seasonal drought; tending to exhibit high resistance to seasonal drought, and low resistance and variable resilience to supra-seasonal drought (Boersma et al. 2014; Bogan et al. 2015).  Native biota in drought-prone systems has evolved resistance or resilience traits that allow them to survive predictable drought (Boersma et al. 2014; Lytle and Poff 2016; Robinson et al. 1992; Stanley et al. 1994; Stubbington et al. 2016), including flow cessation resulting in >95% habitat contraction (Bogan and Lytle 2007).  Biota with resistance traits to ‘sit out the drought' either possess desiccation resistant life-history stages or utilize ‘refugia,' habitats that offer less harsh conditions in an otherwise drought-affected environment (Adams and Warren 2005; Arthington et al. 2005; Bond et al. 2008; Stubbington 2012).  Biota with resilience traits have mechanisms allowing widespread and rapid dispersal among suitable habitat patches (Bond et al. 2008; Boulton 2003; Fritz and Dodds 2004; Humphries and Baldwin 2003). Community recovery, particularly in perennial streams, tends to be rapid following seasonal drought, with resilience mechanisms responsible for a greater portion of recovery (Datry et al. 2014).  However, supra-seasonal drought may cause stream intermittency and community turnover in which short-lived (<1 year) strong dispersers replace relatively long-lived (≥1 year) weak dispersers.  The regime shift may delay or preclude recovery to pre-drought conditions, and potential for community recovery from extreme drought decreases with greater drought magnitude and duration, hydrologic connectivity, and proximity to drought refuges (Bogan et al. 2015; Robson et al. 2011). 

A literature review returned some prior work on stream community alteration and recovery from drought in Great Plains streams.  Dodds (2004) summarized macroinvertebrate recovery in the Great Plains following drought.  He found community composition immediately following a drought is dominated by taxa with drought-resistant traits with a general colonization sequence reflecting life-cycle length.  Initial recruitment following a drought is made up of upper-reach drift of resistant taxa plus resilient taxa, such as chironomid midges, with high growth and reproduction rates that aerially colonize stream reaches.   Larger invertebrates with slower lifecycles, stoneflies and caddisflies, tend to arrive one to two months post-drought (Dodds et al. 2004).

Miller and Golladay (1996) investigated effects of seasonal drought the macroinvertebrate community in an intermittent stream in southeastern Oklahoma.  Invertebrate density in pools doubled during a gradual five-month summer drying event and increased six-fold during a spring drying event.  The density increase was dominated by biting midges (ceratopogonids) and nonbiting midges (chironomids) and aquatic worms (oligochaetes) during both drying events.  Small squaregill mayfly (caenids), which were the major taxon in riffles, density in pools increased during the spring drying event, but did not increase during the summer drying event. Post-drought recovery in riffles was first dominated by black flies (simuliids), until three-months later when rolled-winged stoneflies (leuctrids) became the most dominant taxon.  Water scavenger (hydrophilid) beetle density did not substantially change during drought (Miller and Golladay 1996).
Research in semi-arid regions outside the Great Plains (Bêche et al. 2009; Bogan et al. 2015; Pace et al. 2013), arid regions (Sponseller et al. 2010), and mesocosms (Ledger et al. 2013a; b) provides other analogs for Pine Ridge reservation stream community alteration and recovery from drought.  Resh, Bêche, and others (2013) provide general description of the effects of drought stress in their synthesis of community change in Mediterranean-climate streams in California. They found severe drought resulted in macroinvertebrate community shifts, and rapid changes in age-structure from multi-cohort to single-cohort that recovered slowly (~10 years).  Bogan and Lytle (2011) found temperature and pool area, and conductivity accounted for nearly 90% of variance as a stream shifted from perennial to intermittent during a supraseasonal drought.  Pre-drought conditions were characterized by a diverse low-resilience and long-lived community of skimmer dragonflies (libellulids), giant water bugs (belostomatids), broad-shouldered water striders (veliids), and beetles (coleopterans: dryopids, dytiscids, haliplids).  The post supra-seasonal drought community was equally diverse.  However, short-lived vagrile taxa characterized the post-drought community of baetids, veliids, water striders (gerrids), diving water beetles (dytiscids), and minute moss-beetles (hydraenids) (Bogan and Lytle 2011).
Bêche and others (2009) used indicator species analysis to characterize successional communities pre-drought and following a supra-seasonal (1.3% recurrence interval) drought.  They found the dominant taxa for the pre-drought period included northern and mortar-joint casemaker caddisflies (limnephilids and odontocerids), creeping water beetles (naucorids), and stratiomyids.  Post-drought dominant early successional taxa included hydroptilids, and stratiomyids followed by mid-successional dytiscids, hydrophilids, and sandflies (ceratopogonid), and late-successional comb-mouthed mayflies (ameletids), baetids, and green stoneflies (chloroperlids) became dominant in late succession.  Crane flies (tipulids) were drought indicators in all successional stages.  They concluded there was no evidence of a complete post-drought community recovery following the supra-seasonal drought (Bêche et al. 2009)
Pace and others (2013) studied the drought response of mayflies, stoneflies, and caddisflies, collectively (EPT) in wet- and dry- Mediterranean climate streams in a Catalonia, Spain between 1995-2008.  They found the EPT compositional shifts from drought were less extensive in the wet mesoclimate streams that experienced a less-severe drought and had greater baseflow.  The EPT community in wet mesoclimate streams shifted from a pre-drought community of prong-gilled mayflies (leptophlebiids), chloroperlids, and nemourids to a post-drought community of baetids, stripetail stoneflies (perlodids), and saddle casemaker (glossosomatid), limnephilid and bushtailed (sericostomatid) caddisflies.  The EPT community in dry mesoclimate streams shifted from a pre-drought community of flat-headed mayflies (heptageniids) and hydropsychids to a post-drought community of baetids, limnephiloids, hydropsychids, and net-tube caddisflies (psychomyiids)

Bogan, Boersma and others (2014, 2015) identified drought-intolerant and drought-tolerant taxa. The seasonal-drought intolerant taxa included small minnow (baetid) mayflies, small winter (capniid) and spring (nemourid) stoneflies, net-spinning (hydropsychid) caddisflies, riffle (elmid) and water-penny (psephenid) beetles, and naucorids. Seasonal drought-adapted (e.g., resistant) taxa included calamoceratid caddisflies, diving beetles (dytiscids), giant water bugs (belostomatids), true flies (dipterans; including ceratopogonids, chironomids, corydalids, simuliids, stratiomyids and tabanids), and non-insects (copepods, amphipods, and mites).  Instream colonizers included primitive-minnow (siphlonurid) mayflies, tube-case (limnephilid) caddisflies, capniids, net-winged midges (blepharicerids). Drought-resilient early-areal colonizers following seasonal drought including backswimmer and water-boatman (notonectid and corixid) water bugs, and diving and water scavenger (dytiscid and hydrophilid) beetles.  Drought-resilient late-areal colonizers included hydroptilids, spread-winged damselflies (lestids), and drain flies (psychodids).

Bogan and others (2015) describe macroinvertebrate community trajectories following extreme supra-seasonal droughts as having little effect on aquatic invertebrate taxon richness, but significantly altering community composition from a pre-drying community dominated by relatively large, long-lived and sedentary taxa to smaller, shorter-lived and highly vagile taxa, including strong aerial dispersers post-drought.  They documented an eight-year (2001-2009) community transition in an ecologically isolated series of travertine (limestone springs) pools in south-eastern Arizona. The pre-drought community was diverse and characterized by long-lived, poor-dispersing, taxa made up of large-sized water bugs, libellulids, calamoceratid caddisflies; and mid-sized veliids, whirligig and creeping water beetles (gyrinids and halipids).  The mid-drought community was dominated by mosquito larvae (culicids).  The post-drought community was characterized by hydrophilids, dytiscids, and veliids. Three common species and three uncommon species were extirpated from the post-drought community (Bogan and Lytle 2011), potentially as a result of  low resilience (e.g. low dispersal from refugea) (Phillipsen et al. 2015). 
Bogan and others (2015) synthesized findings from geographically separated semi-arid harshly intermittent desert mountain streams to update a conceptual model of post-drought recovery trajectories developed by Boulton (2003) to include seasonal vs. supra-seasonal drought.   The model predicts that low isolation streams following a predictable, mild seasonal drought should exhibit high species richness as lotic and lentic taxa ‘time-share' between wet and dry season.  Highly isolated streams following a low severity drought should exhibit reduced species richness as weak dispersers experience stochastic extirpations.  As some sensitive taxa and weak dispersers disappear in moderately isolated streams following a moderate severity drought should experience reduced species richness.  Non-isolated streams following a high severity drought should experience reduced species richness as populations of resistant taxa recover and colonists from nearby refuges return.  Highly isolated streams experiencing a severe drought should experience substantial reductions in species as only the most resistant taxa remain (Bogan et al. 2015).  
Storey (2016) investigated macroinvertebrate community compositional differences drought in forested and pasture-land intermittent streams in New Zealand. Perennial forested streams were dominated by leptophlebiid and coloburiscid mayflies, Stony-cased (pycnocentrod) and conoesucid caddisflies, and elmids, with lesser abundances of chironomids, and austroperlid and gripopterygid stoneflies.  Perennial pastureland streams were similar to perennial forested streams with the addition of higher abundances of hydroptilids.  In contrast, forested and pastureland intermittent streams were dominated by gastropods, chironomids, simuliids, and oligachaetes, with lesser abundances of leptophbiid mayflies, polycentropodid caddisflies, ostracod and cladoceran shrimps.  Oligochaetes, chironomids, and nematodes increased during dry years, while mayflies, caddisflies and beetles decreased in intermittent streams, with ‘weeks of no flow’ as the strongest predictor variable.
Sponseller and others (2010) studied macroinvertebrate community recovery following spring floods from 1983–1999; including a supra-seasonal 1989-1990 drought in an intermittent desert stream in Arizona and found antecedent flow conditions were the best predictor of macroinvertebrate community structural changes.  They found baetid, snail-case caddisfly (helicopsychid), tipulid, and stratiomyid abundance decreased following drought and recovered during wet periods (e.g., drought resilience).  Hydropsychids, which were abundant before a supra-seasonal drought, were extirpated.  Gastropod and ceratopogonid abundance increased during drought, most likely because of their tolerance to increased water temperature and corresponding hypoxic conditions.  The early colonizers (e.g. r-selected taxa), chironomids and ceratopogonids, were dominant in early post-drought successional stages but were replaced in dominance in later successional phases by more slowly-growing hydropsychids, helicopsychids, and gastropods.

Ledger and others simulated the effect prolonged drought on the structure and functioning of complex food webs with a two-year experiment using stream mesocosms (Ledger et al. 2013b; a).  The results of their experimental study are consistent with the results of field studies summarized above: drought triggered losses of species, especially engulfing macropredators and changes in trophic interactions resulting from top-down food web erosion leading to their partial collapse.  The recurrent drying disturbances resulted in a macroinvertebrate community shift to transient communities dominated by relatively few, r-selected (e.g., multivoltine) and smaller-bodied species (>50% decline in secondary production), and functional feeding group increases in gastropod grazers. Compensatory dynamics sustained total macroinvertebrate densities (e.g., count per sampling effort), in part through increased amorphous detritus fluxes to chironomids. Ledger and others found no evidence that drought promoted trophic generalists over specialists through indirect effects on food supply.  Instead, drought increased physiological stress, with large-bodied species being most strongly affected, directly increasing consumer mortality.  Similar regime shifts following a supra-seasonal drought were reported in a UK chalk stream.  The drought reduced taxa richness and abundance, particularly baetids, ephemerellids, small squaregill mayflies (caenids) hydropsychids, sericostomatids and leptocerids (Stubbington et al. 2009).



# SCI----
# The two SCI functions used below are fitSCI & tranformSCI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#   fitSCI estimates the parameters for transforming a meteorological 
#   & environmental time series to a Standardized Climate Index (SCI).  

#       fitSCI(x, first.mon, time.scale, distr, p0, 
#         p0.center.mass = FALSE, scaling = c("no","max","sd"), 
#         mledist.par =  list(), start.fun = dist.start, 
#         start.fun.fix = FALSE, warn = TRUE, ...) 

#   transformSCI applies the transformation 
#       transformSCI(x, first.mon, obj, sci.limit = Inf,
#         warn = TRUE, ...) 

# description of arguments for the SCI functions
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# x	- numeric vector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A monthly univariate time series for SCI input 

# first.mon 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Value in [1:12] indicating month of the first element of x 

# time.scale 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The time scale is the window length of an backward-looking running 
# mean.  Time scale is an integer value. 

# distr	
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A character string "name" naming a distribution for which the 
# corresponding density function (dname), the corresponding 
# distribution function (pname) and the quantile function (qname) must 
# be defined (see for example GammaDist) 

# dist.para 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A column matrix containing the parameters of distribution distr for 
# each month. Row names correspond to the distribution parameters. 

# p0 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated separately.

# If p0=TRUE an additional row named P0 is introduced, indicating the 
# probability of zero (precipitation) events.

# If TRUE, model Probability of zero (precipitation) months is 
# modeled with a mixed distribution as D(x) = p0 + (1-p0)G(x), 
# where G(x) > 0 is the reference distribution (e.g. Gamma) p0 is the 
# probability of a zero (precipitation) month. 

# Following Stagge et al. (2014) the probability of zero events is 
# then estimated as p0 = (n_p)/(n + 1), where np refers to the number 
# of zero events and n is the sample size. 
# The resulting mixed distribution for SCI transformation is then: 

# g(x) = if(x > 0) p0 + (1 - p0) G(x) 
#   else if(x == 0) (np + 1)/(2(n + 1))
#    where G(x) > 0 is a model (e.g. gamma) distribution.

# p0.center.mass 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical indicating whether probability of zero (precipitation) 
# events is estimated using the "centre of mass" estimator 
# (see Stagge et al. (2014) for details).

# The probability of zero precipitation events (p0) can be estimated 
# using a "center of mass" estimate based on the Weibull plotting 
# position function to reduce biases in the presence of many zero 
# precipitation events by 'p0.center.mass = TRUE' 

# If TRUE, the Probability of zero (precipitation) is 
# estimated using a "center of mass" estimate based on the Weibull 
# plotting position function (see details). Only applies if p0=TRUE.

# scaling 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Indicates whether to do some scaling of x prior to parameter 
# identification. Scaling can stabilize parameter estimation. 
# "no" (the default) indicates no scaling. 
# "max" indicates scaling by the maximum of x, such that 
#      x <- x/max(x,na.rm=TRUE). 
# "sd" stands for scaling by the standard deviation. 
# warn	- Issue warnings if problems in parameter estimation occur. 

# mledist.par 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# named list that can be used to pass parameters to mledist in package 
# fitdistrplus.  # STILL NOT SURE ABOUT THIS!!!

# start.fun 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Function with arguments x and distr estimating initial 
# parameters of the function distr for each month. The function should 
# return a named list corresponding to the parameters of distr. 
# (See also dist.start)

# start.fun.fix 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# logical argument, indicating if parameter estimates 
# of start.fun should be used if maximum likelihood estimation breaks 
# down. This stabilizes the implementation but can introduce biases in 
# the resulting SCI.  Should look at this for Feb Interior.

# obj 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# an object of class fitSCI, output from fitSCI.

# sci.limit 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Truncate absolute values of SCI that are larger than sci.limit. 
# transformSCI allows for a truncation of the SCI series such that 
#   abs(sci) <= sci.limit. Uncertainty in distribution parameters can 
# cause unrealistically large or small SCI values if values in x 
# exceed the values used for parameter estimation. 
# The truncation can be disabled by setting sci.limit = Inf.

# Output flags 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# fitSCI returns an object of class "fitSCI".  Some flags are:
# dist.para.flag
# an vector indicating possible issues occurring throughout parameter 
# estimation. Possible values are: 
#     0. no problems occurred; 
#     1. starting values could not be estimated; 
#     2. mledist crashed with unknown error; 
#     3. mledist did not converge; 
#     4. all values in this month are NA; 
#     5. all values in this mon are constant, distribution not defined

# scaling
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# numeric value that has been used to scale x (see argument scaling). 
# A value of 1 results from scaling="no", other values are the maximum 
# value or the standard deviation of x, depending on the choice of the 
# parameter scaling. 

# call 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the function call transform SCI returns a numeric vector containing 
# the SCI, having values of the standard normal distribution. 


# PCA_&_clustering_approach----

# Exploratory PCA
# PCA is related to eigenvectors and eigenvalues.  The variance or 
# spread of the observations is measured as the average squared 
# distance from the center of the point cloud to each observation (c).  
# The total reconstruction error is measured as the average squared 
# length of the errors (b), and distance along the principal axis (a) 
# can also be measured.  Therefore the sum of the square of the errors 
# plus the sum of the square distance along the principal axis equals 
# the average squared distance between the center of the point cloud 
# each observation; this is precisely Pythagoras theorem. 

# You can imagine that the PC axis is a solid rod and each error 
# is a spring. The energy of the spring is proportional to its squared 
# length (this is known in physics as the Hooke's law), so the rod 
# will orient itself such as to minimize the sum of these squared 
# distances. 

# Regarding eigenvectors and eigenvalues. A 2×2 matrix given by: 
#   (1.07     0.63)
#   (0.63     0.64)

# The variance of the x variable is 1.07, 
# the variance of the y variable is 0.64, 
# and the covariance between them is 0.63. 

# As it is a square symmetric matrix, it can be diagonalized by 
# choosing a new orthogonal coordinate system, given by its 
# eigenvectors (incidentally, this is called spectral theorem); 
# corresponding eigenvalues will then be located on the diagonal. 
# In this new coordinate system, the covariance matrix is diagonal 
# and looks like this:
#   (1.52     0) 
#   (0     0.19)

# The correlation between points is now zero. It becomes clear that 
# the variance of any projection will be given by a weighted average 
# of the eigenvalues.  Consequently, the maximum possible variance 
# (1.52) will be achieved if we simply take the projection on the 
# first coordinate axis. It follows that the direction of the first 
# principal component is given by the first eigenvector of the 
# covariance matrix. 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Clustering methods----
# The traditional clustering methods, such as hierarchical clustering 
# and k-means clustering, are heuristic and are not based on formal  
# models. An alternative is model-based clustering, which consider the  
# data as coming from a distribution that is mixture of two or more  
# clusters (Fraley and Raftery 2002, Fraley #et al. (2012)). 
# Model-based clustering uses a soft assignment, where each data point 
# has a probability of belonging to each cluster.  In model-based 
# clustering, the data is considered as coming from a mixture of 
# density.  Each component (i.e. cluster) k is modeled by the normal 
# or Gaussian distribution which is characterized by the parameters: 
#   μk\mu_k: mean vector, 
#   ∑k\sum_k: covariance matrix, 
#   An associated probability in the mixture. Each point has a 
#     probability of belonging to each cluster.  

# The model parameters can be estimated using the Expectation-
# Maximization (EM) algorithm initialized by hierarchical model-based 
# clustering. Each cluster k is centered at the means μk\mu_k, with 
# increased density for points near the mean.
# Geometric features (shape, volume, orientation) of each cluster are 
# determined by the covariance matrix ∑k\sum_k. 

# Different possible parameterizations of ∑k\sum_k are available in 
# the R package mclust (see ?mclustModelNames).
# The available model options, in mclust package, are represented by 
# identifiers including: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV 
# and VVV. 

# The first identifier refers to volume, the second to shape and the 
# third to orientation. E stands for "equal", V for "variable" and I 
# for "coordinate axes".

# The Mclust package uses maximum likelihood to fit all these models, 
# with different covariance matrix parameterizations, for a range of 
# k components.

# The best model is selected using the Bayesian Information Criterion 
# or BIC. A large BIC score indicates strong evidence for the 
# corresponding model. 

# Visualizing model-based clustering
# Model-based clustering results can be drawn using the base 
# function plot.Mclust() [in mclust package]. We'll use the 
# function fviz_mclust() [in factoextra package] to create beautiful 
# plots based on ggplot2.

# Where the data contain more than two variables, fviz_mclust() 
# uses a principal component analysis to reduce the dimensionnality 
# of the data. The first two principal components are used to produce 
# a scatter plot of the data. However, if you want to plot the data 
# using only two variables of interest, c("insulin", "sspg"), 
# you can specify that in the fviz_mclust() function using the 
# argument choose.vars = c("insulin", "sspg").

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Several R packages available from CRAN or Bioconducto perform 
# cluster validation, including: 

#|    Package   |    Function(s)   |      Author       |      Notes    |
#|:------------:|:----------------:|:-----------------:|:-------------:|
#|   cclust     | clustIndex()     |    Dimitriadou    | No user guide |
#|     fpc      | cluster.stats()  |       Hennig      | No user guide |
#|              | clusterboot()    |                   |               |
#| clusterRepro |                  | Kapp & Tibshirani | not general   |
#|  clusterSim  |                  | Walesiak & Dudek  | poor user guide |
#|  clusterStab |                  | MacDonald et al.  | narrow vignette |
#|     clue     | cl_validity() +  | Hornik, September |     maybe...    |
#|     e1071    | fclustIndex() ++ | Dimitriadou et al.| 2006  | unk.

# + validation for both paritioning methods 
#   (“dissimilarity accounted for”) and hierarchical methods 
#   (“variance accounted for”) 

# ++ fuzzy cluster validation measures.

# pam() in recommended package cluster 
# (Rousseeuw, Struyf, Hubert, and Maechler, 2005; Struyf, Hubert, and 
# Rousseeuw, 1996), and Mclust() in package mclust (Fraley, Raftery, 
# and Wehrens, 2005; Fraley and Raftery, 2003), are available as 
# components named cluster, clustering, and classification, 

#RWeka (Hornik, Hothorn, and Karatzoglou, 2006), cba 
# (Buchta and Hahsler, 2005), cclust (Dimitriadou, 2005), cluster, 
# e1071 (Dimitriadou, Hornik, Leisch, Meyer, and Weingessel, 2005), 
# flexclust (Leisch, 2006), flexmix (Leisch, 2004), kernlab 
# (Karatzoglou, Smola, Hornik, and Zeileis, 2004), and mclust 
# (and of course, clue itself).

```{r sri_prepare_raw_data2}   
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Initial values for SCI calculations 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
time_scale <- 1  # sets the length of the averaging period 
distrib <- "pe3" # sets the distribution type 
p_zero <- TRUE   # sets a function to reduce zero-precip bias
p_zero_cm <- TRUE # ?????
scale <- "sd"    # scales input by subtract mean & divide by sd
warn_me <- TRUE  # sets explicit warning

# Set first month for each station 
first_mon_cot <- 6 
first_mon_int <- 11 
first_mon_oel <- 6 
first_mon_ora <- 5  
first_mon_rap <- 5 

# prepare station for analysis 
sta_cot <- sta_raw %>% arrange(date) %>% select(date, cot) 
sta_int <- sta_raw %>% arrange(date) %>% select(date, int) 
sta_ora <- sta_raw %>% arrange(date) %>% select(date, ora) 
sta_oel <- sta_raw %>% arrange(date) %>% select(date, oel) 
sta_rap <- sta_raw %>% arrange(date) %>% select(date, rap) 

 # change tibble to a vector as double
cot <- as.double(sta_cot$cot)
int <- as.double(sta_int$int) 
ora <- as.double(sta_ora$ora)
oel <- as.double(sta_oel$oel) 
rap <- as.double(sta_rap$rap) 

# notes: 
# A note on variable naming convention; the SCI package doesn't like 
#   snake_case variables 
# 1 month spi - Int did not close on february

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1mon-spi-with-SCI-1mo----
# Cottonwoods SPI
#~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - throws a month 7 NA error.
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI
#~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   1mon- combine & gather the SPI variables---- 
spi_gath01 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_01 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 2mon-spi-with-SCI-2mo----

# set SCI state variables 
time_scale <- 2  # sets the length of the averaging period 

# Cottonwoods SPI - 2 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value)

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   2mon- combine & gather the SPI variables---- 
spi_gath02 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_02 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 3mon-spi-with-SCI---- 
time_scale <- 3  # sets the length of the averaging period 

# Cottonwoods SPI - 3
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib,  
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   3mon - combine & gather the SPI variables---- 
spi_gath03 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_03 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 4mon-spi-with-SCI-4mo----
# notes:  ORAL HAS ONE SPI VAL OF -8
time_scale <- 4  # sets the length of the averaging period 

# Cottonwoods SPI - 4
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# prepare Interior precip for SPI transformation

# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results

spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   4mon - combine & gather the SPI variables---- 
spi_gath04 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_04 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#   5mon-spi-with-SCI-5mo----
# notes: ora has an extreme low value & really high skew 
time_scale <- 5  # sets the length of the averaging period 

# Cottonwoods SPI - 5 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value)

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value



#   5mon - combine & gather the SPI variables---- 
spi_gath05 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_05 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 6mon-spi-with-SCI-6mo---- 
 time_scale <- 6  # sets the length of the averaging period 

# Cottonwoods SPI - 6
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   6mon - combine & gather the SPI variables---- 
spi_gath06 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_06 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#   9mon-spi-with-SCI-9mo----
# Notes: ora has an extreme low value & really high skew ??

# set SCI state variables 
time_scale <- 9  # sets the length of the averaging period 

# Cottonwoods SPI - 9 
#~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   9mon - combine & gather the SPI variables---- 
spi_gath09 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_09 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 12mon- spi-with-SCI-12mo----
time_scale <- 12  # sets the length of the averaging period 

# Cottonwoods SPI - 12
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)  

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results  
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   12mon - combine & gather the SPI variables---- 
spi_gath12 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_12 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 18mon-spi-with-SCI-18mo----
time_scale <- 18  # sets the length of the averaging period 

# Cottonwoods SPI - 18
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

spi.int <- transformSCI(int, first.mon = first_mon_int, obj = spi.int.l) 

spi_int <- as.tibble(spi.int) # change the vector into a tibble

spi_int <- spi_int %>% 
  rename(spi_int = value) # prepare for a later join

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value 

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   18mon - combine & gather the SPI variables---- 
spi_gath18 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_18 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 24mon- spi-with-SCI-24mo---- 
time_scale <- 24  # sets the length of the averaging period 

# Cottonwoods SPI - 24
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   24mon - combine & gather the SPI variables---- 
spi_gath24 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_24 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 30mon- spi-with-SCI-30mo---- 
time_scale <- 30  # sets the length of the averaging period 

# Cottonwoods SPI - 30 
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value


#   30mon - combine & gather the SPI variables---- 
spi_gath30 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_30 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 36mon- spi-with-SCI-36mo---- 
time_scale <- 36  # sets the length of the averaging period 

# Cottonwoods SPI - 36 
#~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   36mon - combine & gather the SPI variables---- 
spi_gath36 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_36 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 42mon- spi-with-SCI-42mo---- 
time_scale <- 42  # sets the length of the averaging period 

# Cottonwoods SPI - 42
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   42mon - combine & gather the SPI variables---- 
spi_gath42 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_42 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 48mon- spi-with-SCI-48mo---- 
time_scale <- 48  # sets the length of the averaging period 

# Cottonwoods SPI -  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 


# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   48mon - combine & gather the SPI variables---- 
spi_gath48 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_48 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# 54mon- spi-with-SCI-54mo---- 
time_scale <- 54  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   54mon - combine & gather the SPI variables---- 
spi_gath54 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_54 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# 60mon- spi-with-SCI-60mo---- 
time_scale <- 60  # sets the length of the averaging period 

# Cottonwoods SPI  
#~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 

spi.cot.l <- fitSCI(cot, first.mon = first_mon_cot , distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me) 

# create a matrix with SPI values - used below
spi.cot <- transformSCI(cot, first.mon = first_mon_cot, 
                        obj = spi.cot.l) 

# change to tibble, prepare for join
spi_cot <- as.tibble(spi.cot) %>% 
  rename(spi_cot = value) 

spi_cot <- bind_cols(sta_cot, spi_cot) # bind date, depth, SPI value

# Interior SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results 
spi.int.l <- fitSCI(int, first.mon = first_mon_int, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.int <- transformSCI(int, first.mon = first_mon_int, 
                        obj = spi.int.l) 

# change to tibble, prepare for join
spi_int <- as.tibble(spi.int) %>% 
  rename(spi_int = value)

spi_int <- bind_cols(sta_int, spi_int) # bind date, depth, SPI value

# Oelrichs SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# calculate SPI as a list & extract results
spi.oel.l <- fitSCI(oel, first.mon = first_mon_oel, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.oel <- transformSCI(oel, first.mon = first_mon_oel, 
                        obj = spi.oel.l) 

# change to tibble, prepare for join
spi_oel <- as.tibble(spi.oel) %>% 
  rename(spi_oel = value) 

spi_oel <- bind_cols(sta_oel, spi_oel) # bind date, depth, SPI value 

# Oral SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.ora.l <- fitSCI(ora, first.mon = first_mon_ora, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.ora <- transformSCI(ora, first.mon = first_mon_ora, 
                        obj = spi.ora.l) 

# change to tibble, prepare for join
spi_ora <- as.tibble(spi.ora) %>% 
  rename(spi_ora = value) 

spi_ora <- bind_cols(sta_ora, spi_ora) # bind date, depth, SPI value

# Rapid SPI 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# calculate SPI as a list & extract results
spi.rap.l <- fitSCI(rap, first.mon = first_mon_rap, distr = distrib, 
                   time.scale = time_scale, p0 = p_zero, 
                   p0.center.mass = p_zero_cm, scaling = scale, 
                   warn	= warn_me)

# create a matrix with SPI values - used below
spi.rap <- transformSCI(rap, first.mon = first_mon_rap, 
                        obj = spi.rap.l) 

# change to tibble, prepare for join
spi_rap <- as.tibble(spi.rap) %>% 
  rename(spi_rap = value) 

spi_rap <- bind_cols(sta_rap, spi_rap) # bind date, depth, SPI value

#   60mon - combine & gather the SPI variables---- 
spi_gath60 <- list(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) %>% 
  reduce(left_join, by = "date") %>% 
  select(date, starts_with("spi")) %>% # selects the spi vals
  gather(key = sta, value = spi_index, -date) %>% 
  mutate(sta = str_remove_all(.$sta, "spi_")) %>% 
  drop_na() 

#   create a list of lists
spi_list_60 <- list(spi.cot.l, spi.int.l, spi.oel.l, 
             spi.ora.l, spi.rap.l) %>% 
  flatten() 

# remove the spi vectors & individual dataframes
rm(spi.cot, spi.int, spi.oel, spi.ora, spi.rap)
rm(spi_cot, spi_int, spi_oel, spi_ora, spi_rap) 
rm(spi.cot.l, spi.int.l, spi.oel.l, spi.ora.l, spi.rap.l) 
```