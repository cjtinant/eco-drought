---
title: "Untitled"
author: "CJ Tinant"
date: "11/25/2019"
output: html_document
---


```{r other-correlations-testing}  
  
# static correlations-stations====   
sta_corr <- sci_sta_simple %>%  
  select(-group) %>%  
  spread(sta, spi_1mo) %>%  
  select(-date) %>%  
  correlate()  

# static correlations-gages_NW====  
sci_corr_NW <- sci_simple %>%  
  filter(group == "NW") %>% 
  unite(scratch, c("sta", "gage")) %>%  
  unite(scratch2, c("spi_1mo", "sci_1mo")) %>%  
  spread(scratch, scratch2) %>%     
  separate(RAP_bat_her, 
           into = c("RAP", "bat_her"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_che_was, 
           into = c(NA, "che_was"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_elk_elm, 
           into = c(NA, "elk_elm"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_frn_fai, 
           into = c(NA, "frn_fai"),  
           sep = "_",  
           convert = TRUE) %>% 
  select(-c(group, date)) %>% 
# Correlation and formating  
  correlate()  


# rolling correlations====   
sci_rollcorr_NW <- sci_simple %>%  
  filter(group == "NW") %>% 
  unite(scratch, c("sta", "gage")) %>%  
  # Data wrangling  
  select(date, scratch, spi_1mo, sci_1mo) %>%  
  group_by(scratch) %>% 
#  arrange(scratch, date) #%>%  
  # Mutation  
  tq_mutate_xy(  
    x          = spi_1mo,  
    y          = sci_1mo,  
    mutate_fun = runCor,  
    # runCor args  
    n          = 12,  
    use        = "pairwise.complete.obs",  
    # tq_mutate args  
    col_rename = "rolling_corr"  
  ) %>%  
  ungroup()  
  
```

```{r other-stations}

sci_corr <- sci_simple %>% 
  select(-c("group")) %>%   
  unite(scratch, c("sta", "gage")) %>%  
  unite(scratch2, c("spi_1mo", "sci_1mo")) %>%  
  spread(scratch, scratch2) %>% 
  select(date, RAP_bat_her, RAP_che_was,  
         RAP_elk_elm, RAP_frn_fai,  
         COT_lwr_whi, COT_whi_kad,  
         ONI_whi_oac, OEL_bev_buf,  
         OEL_fal_hot, OEL_hat_edg,  
         OEL_whi_ogl, OEL_whi_sta,  
         GOR_lcr_bel, GOR_lwr_mar,  
         GOR_lwr_ros, GOR_lwr_vet,  
         MIS_key_key, MIS_key_wew,  
         MIS_lon_riv, MIS_nio_spa  
  ) %>%  
  separate(RAP_bat_her, 
           into = c("RAP", "bat_her"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_che_was, 
           into = c(NA, "che_was"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_elk_elm, 
           into = c(NA, "elk_elm"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(RAP_frn_fai, 
           into = c(NA, "frn_fai"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate(COT_lwr_whi, 
           into = c("COT", "lwr_whi"),  
           sep = "_",  
           convert = TRUE) %>% 
  separate("COT_whi_kad", 
           into = c(NA, "whi_kad"),  
           sep = "_",  
           convert = TRUE) %>% 
  separate("ONI_whi_oac", 
           into = c("ONI", "whi_oac"),  
           sep = "_",  
           convert = TRUE) %>% 
      separate("OEL_bev_buf", 
           into = c(NA, "bev_buf"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("OEL_fal_hot", 
           into = c(NA, "fal_hot"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("OEL_hat_edg", 
           into = c(NA, "bev_buf"),  
           sep = "_",  
           convert = TRUE) %>%   
  separate("OEL_whi_ogl", 
           into = c(NA, "whi_ogl"),  
           sep = "_",  
           convert = TRUE) %>% 
  separate("OEL_whi_sta", 
           into = c(NA, "whi_sta"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("GOR_lcr_bel", 
           into = c("GOR", "lcr_bel"),  
           sep = "_",  
           convert = TRUE) %>% 
  separate("GOR_lwr_mar", 
           into = c(NA, "lwr_mar"),  
           sep = "_",  
           convert = TRUE) %>% 
  separate("GOR_lwr_ros",  
           into = c(NA, "lwr_ros"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("GOR_lwr_vet",  
           into = c(NA, "lwr_vet"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("MIS_key_key",  
           into = c("MIS", "key_key"),  
           sep = "_",   
           convert = TRUE) %>%  
  separate("MIS_key_wew",  
           into = c(NA, "key_wew"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("MIS_lon_riv",  
           into = c(NA, "lon_riv"),  
           sep = "_",  
           convert = TRUE) %>%  
  separate("MIS_nio_spa",  
           into = c(NA, "nio_spa"),  
           sep = "_",  
           convert = TRUE) %>% 
  select(-date) %>% 
# Correlation and formating  
  correlate()  





rm(sci_gage_simple, sci_sta_simple)

sci_simple_ck <- sci_simple %>% 
  filter(group == "NC" | 
           group == "SC") %>%  
  select(-c(ecoreg)) %>% 
  group_by(group) %>%  
  spread(key = gage, value = sci_1mo)
  
# Static Correlations====  
# Use the correlate() and shave() functions from the corrr package to output   
#   a tidy correlation table. We’ll hone in on the last column “all_cran”,   
#   which measures the correlation between individual packages and the   
#   broader market (i.e. total CRAN downloads).  

# Correlation table -- tidy====  
tidyverse_static_correlations <- tidyverse_downloads %>%  
    # Data wrangling  
    spread(key = package, value = count) %>%  
    left_join(all_downloads, by = "date") %>%  
    rename(all_cran = count) %>%  
    select(-date) %>%  
    # Correlation and formating  
    correlate()  

# Pretty printing
tidyverse_static_correlations %>%
    shave(upper = F)

# The corrr package has a nice visualization called a network_plot() --   
#   to identify strength of correlation. Similar to a “kmeans” analysis,  
#   we are looking for association by distance (or in this case by correlation).  
# The network plot shows us how well the data correlate with each other -- 
#   akin to how associated they are with each other.  
  
# Network plot of correlations====  
# Below, 'tidyquant' has a very low correlation to “all_cran” and the rest  
#   of the “tidyverse” packages -- this would lead us to believe that  
#   tidyquant is trending abnormally with respect to the rest, and thus is  
#   possibly not as associated as we think.  
# Is this really the case?  

gg_all <- tidyverse_static_correlations %>%  
    network_plot(colours = c(palette_light()[[2]],  
                             "white",   
                             palette_light()[[4]]),  
                 legend = TRUE) +  
    labs(  
        title = "Correlations of tidyverse downloads to total CRAN downloads",  
        subtitle = "January through June, tidyquant is a clear outlier"  
        ) +  
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +  
    theme_tq() +  
    theme(legend.position = "bottom")  
gg_all  
```

```{r lag_operator_overview}
  
# Lags (Lag Operator) overview====  
# Calculate lags and analyze autocorrelation.  
# The lag operator (also known as backshift operator) is a function that  
#   shifts (offsets) a time series such that the “lagged” values are aligned   
#   with the actual time series. The lags can be shifted any number of units,  
#     which simply controls the length of the backshift.  
  
# Lags are useful in time series analysis because of a phenomenon called   
#   autocorrelation, which is a tendency for the values within a time series   
#   to be correlated with previous copies of itself. One benefit to   
#   autocorrelation is that we can identify patterns within the time series,    
#   which helps in determining seasonality, the tendency for patterns to   
#   repeat at periodic frequencies. 
  
# Lags and autocorrelation are central to forecasting models that incorporate  
#   autoregression, regressing a time series using previous values of itself.   
# Autoregression is the basis for one of the most widely used forecasting  
#   techniques, the autoregressive integrated moving average model or ARIMA  
# for short. The forecast package by Rob Hyndman, implements ARIMA and a  
# number of other forecast modeling techniques. 
# Note -- Autoregression and ARIMA not discussed below.  

# Lag and autocorrelation analysis is a good way to detect seasonality. 
#   The autocorrelation of the lagged values can be used to detect “abnormal”  
#   seasonal patterns. 
# The tq_mutate() function was used to apply lag.xts() to the daily download  
#   counts to efficiently get lags 1 through 28. Once the lags were retrieved,  
#   we use other dplyr functions such as gather() to pivot the data and  
#   summarize() to calculate the autocorrelations. Finally, we saw the power  
#   of visual analysis of the autocorrelations -- created an ACF plot that  
#   showed a visual trend. Then we used a boxplot to detect which lags had  
#   consistent outliers. Ultimately a weekly pattern was confirmed.   

# The timetk package is a toolkit for working with time series. It has  
#   functions that simplify and make consistent the process of coercion --  
#   converting to and from different time series classes. In addition,  
#   it has functions to aid the process of time series machine learning and  
#   data mining. 

# Convert to an xts object -- use tk_xts() from the timetk package  
#   to coerce from a time-based tibble -- tibble with a date or time component 
#   and xts object.

# We still need our original values so we can analyze the counts against  
#   the lags. 
# If we want to get the original values too, we can do something like this.

#  tq_mutate()====  
# The tq_mutate() function from tidyquant enables “tidy” application of the  
#   xts-based functions. The tq_mutate() function works similarly to mutate()  
#   from dplyr in the sense that it adds columns to the data frame.
 
# The tidyquant package enables a “tidy” implementation of the xts-based  
#   functions from packages such as xts, zoo, quantmod, TTR and  
# PerformanceAnalytics. 

# Quick example -- use the select = value to send the “value” column to the  
#   mutation function. In this case our mutate_fun = lag.xts. We supply k = 5  
#   as an additional argument.  
# That’s much easier -- we get the value column returned in addition to the  
#   lags, which is the benefit of using tq_mutate(). If you use tq_transmute()  
#   instead, the result would be the lags only, which is what lag.xts() returns.  

# General steps for autocorrelation====  
# 1. Convert to xts  
# 2. Get original values and lags in xts  
# 3. Convert back to tbl  
# But that’s a lot of work -- fortunately we have tq_mutate()  

# Consider a time series of ten values beginning in 2017.  
set.seed(1)  
my_time_series_tbl <- tibble(  
    date   = seq.Date(ymd("2017-01-01"),  
                      length.out = 10,  
                      by = "day"),  
    value  = 1:10 + rnorm(10)  
)  

# tq-mutate avoids coercing to xts automaticilly merges the original vals   
my_time_series_tbl %>%  
    tq_mutate(  
        select     = value,  
        mutate_fun = lag.xts,  
        k          = 1:5  
    )  
```  

```{r lag_operator_example}
  
# Analyzing tidyverse Downloads: Lag and Autocorrelation Analysis----  
  
# get data for various tidyverse packages - count of downloads====  
pkgs <- c(  
    "tidyr", "lubridate", "dplyr",  
    "broom", "tidyquant", "ggplot2", "purrr",  
    "stringr", "knitr"  
    )  
  
tidyverse_downloads <- cran_downloads(  
    packages = pkgs,  
    from     = "2017-01-01",  
    to       = "2017-06-30") %>%  
    tibble::as_tibble() %>%  
    group_by(package)  # this is key! 

# Scaling the Lag and Autocorrelation Calculation  
# Get lags 1 through 28 (4 weeks of lags):  
#   Take the tidyverse_downloads data frame, which is grouped by package,  
#     and apply tq_mutate() using the lag.xts function.  
#   We can provide column names for the new columns by prefixing “lag_” to   
#   the lag numbers, k, which the sequence from 1 to 28.   
# The output is all of the lags for each package.  

# Use tq_mutate() to get lags 1:28 using lag.xts()----  
# create lags and column names to pass to tq_mutate()  
k <- 1:28  
col_names <- paste0("lag_", k)  
  
tidyverse_lags <- tidyverse_downloads %>%  
    tq_mutate(  
        select     = count,  
        mutate_fun = lag.xts,  
        k          = 1:28,  
        col_rename = col_names  
    )  

# Next steps with lag.xts====   
# The goal is to get count and each lag side-by-side so we can do a correlation. 
# Correlating each of the lags to the “count” column involves steps  
#   strung together in a dplyr pipe (%>%):  
# 1. Use gather() to pivot each lagged column into a “tidy” long-format df,   
#     and exclude columns: “package”, “date” and “count” columns from the pivot.   
# 2. Convert the new “lag” column from a character string (e.g. “lag_1”)  
#     to numeric (e.g. 1) using mutate() to make ordering the lags easier.  
# 3. group the long data frame by package and lag to calculate subsets of  
#     package and lag.  
# 4. apply the correlation to each group of lags. The summarize() function   
#     can be used to implement cor(), which takes x = count and y = lag_value.   
#   Make sure to pass use = "pairwise.complete.obs", which is almost always   
#   desired.  
# 5. The 95% upper and lower cutoff can be approximated by: cutoff=±2 / N^0.5  
#     Where:  
#       N = number of observations.  

# Calculate the autocorrelations and 95% cutoffs  
tidyverse_count_autocorrelations <- tidyverse_lags %>%  
    gather(key = "lag",  
           value = "lag_value",  
           -c(package, date, count)) %>%  
    mutate(lag = str_sub(lag, start = 5) %>% as.numeric) %>%  
    group_by(package, lag) %>%  
    summarize(  
        cor = cor(x = count,  
                  y = lag_value,  
                  use = "pairwise.complete.obs"),  
        cutoff_upper = 2/(n())^0.5,  
        cutoff_lower = -2/(n())^0.5  
        )  

#Visualizing Autocorrelation: ACF Plot====
# Now correlations are calculated by package and lag number in a “tidy” format,  
#   we can visualize the autocorrelations with ggplot to check for patterns.  
# The plot shown below is known as an ACF plot, which is simply the  
#   autocorrelations at various lags. Initial examination of the ACF plots  
# indicate a weekly frequency.

# Visualize the autocorrelations
tidyverse_count_autocorrelations %>%
    ggplot(aes(x = lag, y = cor, color = package, group = package)) +
    # Add horizontal line a y=0
    geom_hline(yintercept = 0) +
    # Plot autocorrelations
    geom_point(size = 2) +
    geom_segment(aes(xend = lag, yend = 0), size = 1) +
    # Add cutoffs
    geom_line(aes(y = cutoff_upper), color = "blue", linetype = 2) +
    geom_line(aes(y = cutoff_lower), color = "blue", linetype = 2) +
    # Add facets
    facet_wrap(~ package, ncol = 3) +
    # Aesthetics
    expand_limits(y = c(-1, 1)) +
    scale_color_tq() +
    theme_tq() +
    labs(
        title = paste0("Tidyverse ACF Plot: Lags ", rlang::expr_text(k)),
        subtitle = "Appears to be a weekly pattern",
        x = "Lags"
    ) +
    theme(
        legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1)
    )

# We see that there appears to be a weekly pattern, but we want to be sure.   
  
# Get the absolute autocorrelations====  
# Verify the weekly pattern assessment by reviewing the absolute value of the  
#   correlations independent of package. We take the absolute autocorrelation   
#   because we use the magnitude as a proxy for how much explanatory value the   
#   lag provides.   
# Use dplyr functions to manipulate the data for visualization:  
# 1. drop the package group constraint using ungroup(),    
# 2. calculate the absolute correlation using mutate(),  
# 3. convert the lag to a factor, which helps with reordering the plot.  
# 4. Select() only the “lag” and “cor_abs” columns,    
# 5.  group by “lag” to lump all of the lags together -- to determine the  
#       trend independent of package.  
  
tidyverse_absolute_autocorrelations <- tidyverse_count_autocorrelations %>%  
    ungroup() %>%  
    mutate(  
        lag = as_factor(as.character(lag)),  
        cor_abs = abs(cor)  
        ) %>%  
    select(lag, cor_abs) %>%  
    group_by(lag)   
  
# Visualize the absolute correlations====  
# 1. Use a box plot that lumps each of the lags together. 
# 2. Add a line to indicate the presence of outliers at values above 1.5IQR  
#     If the values are consistently above the 1.5IQR limit, the lag can be  
#     considered an outlier. Note that we use the fct_reorder() function from  
#     forcats to organize the boxplot in order of decending magnitude.  

# Visualize boxplot of absolute autocorrelations   
break_point <- 1.5 * IQR(tidyverse_absolute_autocorrelations$cor_abs) %>%  
  signif(3)  

tidyverse_absolute_autocorrelations %>%    
    ggplot(aes(x = fct_reorder(lag, cor_abs, .desc = TRUE),  
               y = cor_abs)) +  
    # Add boxplot   
    geom_boxplot(color = palette_light()[[1]]) +  
    # Add horizontal line at outlier break point  
    geom_hline(yintercept = break_point, color = "red") +  
    annotate("text", label = paste0("Outlier Break Point = ", break_point),  
             x = 24.5, y = break_point + .03, color = "red") +  
    # Aesthetics  
    expand_limits(y = c(0, 1)) +  
    theme_tq() +  
    labs(  
        title = paste0("Absolute Autocorrelations: Lags ", 
                       rlang::expr_text(k)),  
        subtitle = "Weekly pattern is consistently above outlier break point",  
        x = "Lags"  
    ) +  
    theme(  
        legend.position = "none",  
        axis.text.x = element_text(angle = 45, hjust = 1)  
    )  
  
# outcome -- lags in multiples of seven have the highest autocorrelation  
#   and are consistently above the outlier break point indicating the presence  
#   of a strong weekly pattern. The autocorrelation with the seven-day lag is  
#   the highest, with a median of approximately 0.75. Lags 14, 21, and 28 are  
#   also outliers with median autocorrelations in excess of our outlier break  
#   point of 0.471.

# Note -- the median of Lag 1 is essentially at the break point indicating  
#   that half of the packages have a presence of “abnormal” autocorrelation. 
#   However, this is not part of a seasonal pattern since a periodic frequency  
#   is not present.
# In the case above, the tidyverse packages exhibit a strong weekly pattern. 
  
``` 

```{r tidy_time-series_transmute_1-var} 
  
# The period-apply functions from xts can be used to apply aggregations using  
#   common time series intervals such as weekly, monthly, quarterly, and   
#   yearly.  
#  
# The tq_transmute() function from tidyquant enables efficient and  
#   “tidy” application of the functions. We were able to use the period apply  
#   functions to visualize trends and volatility and to expose relationships  
#   between statistical measures.
  
# get data for various tidyverse packages - count of downloads====  
  
pkgs <- c(  
    "tidyr", "lubridate", "dplyr",  
    "broom", "tidyquant", "ggplot2", "purrr",  
    "stringr", "knitr"  
    )  
  
tidyverse_downloads <- cran_downloads(  
    packages = pkgs,  
    from     = "2017-01-01",  
    to       = "2017-06-30") %>%  
    tibble::as_tibble() %>%  
    group_by(package)  
  
# Visualize the package downloads====  
tidyverse_downloads %>%
    ggplot(aes(x = date, y = count, color = package)) +
    geom_point() +
    labs(title = "tidyverse packages: Daily downloads", x = "") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# To perform weekly aggregation, we will use tq_transmute(), which applies  
#   the non-tidy functions in a “tidy” way. 
#   # "apply" functions from xts --   
#   tq_transmute_fun_options()$xts %>%
#      stringr::str_subset("^apply")
# The function we want to use is apply.weekly(), which takes the argument  
#   FUN (the function to be applied weekly) and 
#   ... (additional args that get passed to the FUN function). 
# Set FUN = mean to apply mean() on a weekly interval,and pass the argument 
#   na.rm = TRUE to remove NA values during the calculation.  

mean_tidyverse_downloads_w <- tidyverse_downloads %>%
    tq_transmute(
        select     = count,
        mutate_fun = apply.weekly, 
        FUN        = mean,
        na.rm      = TRUE,
        col_rename = "mean_count"
    )

mean_tidyverse_downloads_w %>%
    ggplot(aes(x = date, y = mean_count, color = package)) +
    geom_point() +
    geom_smooth(method = "loess") + 
    labs(title = "tidyverse packages: Average daily downloads by week", x = "", 
         y = "Mean Daily Downloads by Week") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# Custom function to return mean, sd, quantiles====  
custom_stat_fun <- function(x, na.rm = TRUE, ...) {
    # x     = numeric vector
    # na.rm = boolean, whether or not to remove NA's
    # ...   = additional args passed to quantile
    c(mean    = mean(x, na.rm = na.rm),
      stdev   = sd(x, na.rm = na.rm),
      quantile(x, na.rm = na.rm, ...)) 
}

# Testing custom_stat_fun
options(digits = 4)
set.seed(3366)
nums  <- c(10 + 1.5*rnorm(10), NA)
probs <- c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)
custom_stat_fun(nums, na.rm = TRUE, probs = probs)

# Apply the custom function by week -- tidy & visualize====   
stats_tidyverse_downloads_w <- tidyverse_downloads %>%  
    tq_transmute(  
        select = count,  
        mutate_fun = apply.weekly,  
        FUN = custom_stat_fun,  
        na.rm = TRUE,  
        probs = probs  
    )  

stats_tidyverse_downloads_w %>%   
    ggplot(aes(x = date, y = `50%`, color = package)) +  
    # Ribbon  
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`),  
                color = palette_light()[[1]],  
                fill = palette_light()[[1]],  
                alpha = 0.5) +  
    # Points  
    geom_point() +  
    geom_smooth(method = "loess", se = FALSE) +  
    # Aesthetics  
    labs(title = "tidyverse packages: Median daily downloads by week",  
         x = "",  
         subtitle = "Range of 1st and 3rd quartile to show volatility",  
         y = "Median Daily Downloads By Week") +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    expand_limits(y = 0) +  
    scale_color_tq(theme = "dark") +  
    theme_tq() +  
    theme(legend.position="none")  

stats_tidyverse_downloads_w %>%  
    ggplot(aes(x = stdev, y = mean, color = package)) +  
    geom_point() +  
    geom_smooth(method = "lm") +  
    labs(  
      title = "tidyverse packages: Mean vs SD of daily downloads by week") +  
    facet_wrap(~ package, ncol = 3, scale = "free") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")   
```

```{r tidy_time-series_roll-window}
# Rolling Window Calculations====   
# The rollapply functions from zoo and TTR can be used to apply rolling  
#   window calculations. The tq_mutate() function from tidyquant enables   
#   efficient and “tidy” application of the functions. We were able to use  
#   the rollapply functions to visualize averages and standard deviations on  
#   a rolling basis, which gave us a better perspective of the dynamic trends.  
#   Using custom functions, we are unlimited to the statistics we can apply to  
#   rolling windows.  
#  
#   What are rolling window calculations, and why do we care? In time series  
#   analysis, nothing is static. A correlation may exist for a subset of time  
#   or an average may vary from one day to the next. Rolling calculations  
#   simply apply functions to a fixed width subset of the data (aka a window),  
#   indexing one observation each calculation.  
  
#   There are a few common reasons you may want to use a rolling calculation  
#   in time series analysis:  
#  
#    Measuring the central tendency over time (mean, median)  
#    Measuring the volatility over time (sd, var)  
#    Detecting changes in trend (fast vs slow moving averages)  
#    Measuring a relationship between two time series over time (cor, cov)  
  
# Sample Moving Average Calculation  
# Combining a rolling mean with a rolling standard deviation can help detect  
# regions of abnormal volatility and consolidation. This is the concept behind   
# Bollinger Bands in the financial industry. The bands can be useful in  
# detecting breakouts in trend.    
  
# Time Series Functions for rolling window====  
# "roll" functions from zoo  
tq_mutate_fun_options()$zoo %>%  
    stringr::str_subset("^roll")  
  
##  [1] "rollapply"          "rollapplyr"         "rollmax"            
##  [4] "rollmax.default"    "rollmaxr"           "rollmean"          
##  [7] "rollmean.default"   "rollmeanr"          "rollmedian"        
## [10] "rollmedian.default" "rollmedianr"        "rollsum"           
## [13] "rollsum.default"    "rollsumr"  
  
# "run" functions from TTR  
tq_mutate_fun_options()$TTR %>%  
    stringr::str_subset("^run")  
  
##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"  
  
# Tidy Implementation of Time Series Functions====    
# Condensed function options====    
tq_mutate_fun_options() %>%  
    str()  
  
## List of 5  
##  $ zoo: chr [1:14] "rollapply" "rollapplyr" "rollmax" "rollmax.default" ...  
##  $ xts: chr [1:27] "apply.daily" "apply.monthly" "apply.quarterly"   
##      "apply.weekly" ...  
##  $ quantmod: chr [1:25] "allReturns" "annualReturn" "ClCl" "dailyReturn" ...  
##  $ TTR: chr [1:61] "adjRatios" "ADX" "ALMA" "aroon" ...  
##  $ PerformanceAnalytics: chr [1:7] "Return.annualized" "  
##      Return.annualized.excess" "Return.clean" "Return.cumulative" ...  
  
# Tidy Application of Rolling Functions====  
# Rolling Mean: Inspecting Fast and Slow Moving Averages  
#  Investigate if significant changes in trend are taking place such that  
#   future downloads are likely to continue to increase, decrease or stay the  
#   same. One way to do this is to use moving averages.  
# Rather than try to sift through the noise, we can use a combination of a  
#   fast and slow moving average to detect momentum.  
  
# We’ll create a fast moving average with width = 28 days (just enough to  
#   detrend the data) and a slow moving average with width = 84 days  
#   (slow window = 3X fast window). To do this we apply two calls to  
#   tq_mutate(), the first for the 28 day (fast) and the second for the  
#   84 day (slow) moving average. There are three groups of arguments we   
#   need to supply:  
# tq_mutate args -- These select the column to apply the mutation to “count”  
#   & the mutation function (mutate_fun) to apply (rollapply from zoo).  
# rollapply args: These set the width, align = "right"  
#   (aligns with end of data frame), and  
#   the FUN we wish to apply (mean in this case).  
# FUN args: These are arguments that get passed to the function.  
#   In this case we want to set na.rm = TRUE so NA values are skipped.  
# Also add an optional tq_mutate arg, col_rename, at the end to rename the  
#   column.   
  
# Rolling mean example====    
tidyverse_downloads_rollmean <- tidyverse_downloads %>%  
    tq_mutate(  
        # tq_mutate args  
        select     = count,  
        mutate_fun = rollapply,  
        # rollapply args  
        width      = 28,  
        align      = "right",  
        FUN        = mean,  
        # mean args  
        na.rm      = TRUE,  
        # tq_mutate args  
        col_rename = "mean_28"  
    ) %>%  
    tq_mutate(  
        # tq_mutate args  
        select     = count,  
        mutate_fun = rollapply,  
        # rollapply args  
        width      = 84,  
        align      = "right",  
        FUN        = mean,  
        # mean args  
        na.rm      = TRUE,  
        # tq_mutate args  
        col_rename = "mean_84"  
    )  
  
# ggplot results====  
tidyverse_downloads_rollmean %>%  
    ggplot(aes(x = date, y = count, color = package)) +  
    # Data  
    geom_point(alpha = 0.1) +  
    geom_line(aes(y = mean_28), color = palette_light()[[1]], size = 1) +  
    geom_line(aes(y = mean_84), color = palette_light()[[2]], size = 1) +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Daily Downloads", x = "",  
         subtitle = "28 and 84 Day Moving Average") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")  
  
# Drop the “count” data from the plots and inspect just the moving averages to  
#   identify points where the fast trend is above (has momentum)  
#   or below (is slowing) the slow trend, & inspect for cross-over,  
#   which indicates shifts in trend.  
  
tidyverse_downloads_rollmean %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    # geom_point(alpha = 0.5) +  # Drop "count" from plots  
    geom_line(aes(y = mean_28),  
              color = palette_light()[[1]],  
              linetype = 1,  
              size = 1) +  
    geom_line(aes(y = mean_84),  
              color = palette_light()[[2]],   
              linetype = 1,  
              size = 1) +  
    facet_wrap(~ package,  
               ncol = 3,  
               scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Daily downloads", x = "", y = "",  
         subtitle = "Zoomed In: 28 and 84 Day Moving Average") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")  
  
# The plot shows 'purrr' and 'lubridate' have strong upward momentum,   
#   'dplyr', 'knitr' and 'tidyr' seem to be cycling in a range, &   
#   'ggplot2' and 'stringr' have short term downward trends -- keep in mind  
#   these packages are getting the most downloads of the bunch.  
  
# Rolling Custom Functions: Useful for multiple statistics  
#   Create a custom function, custom_stat_fun_2(), that returns statistics:   
#    mean  
#    standard deviation  
#    95% confidence interval (mean +/- 2SD)  
  
# Custom function to return mean, sd, 95% conf interval====    
custom_stat_fun_2 <- function(x, na.rm = TRUE) {  
  # x     = numeric vector  
  # na.rm = boolean, whether or not to remove NA's  
  m  <- mean(x, na.rm = na.rm)  
  s  <- sd(x, na.rm = na.rm)  
  hi <- m + 2*s  
  lo <- m - 2*s  
  ret <- c(mean = m, stdev = s, hi.95 = hi, lo.95 = lo)   
  return(ret)  
}  
  
# Apply the custom_stat_fun_2() to groups====  
#   using tq_mutate() and the rolling function rollapply()  
#   The output returned is a “tidy” data frame   
#   with each statistic in its own column.  
# The process is almost identical to the process of applying mean() with  
#   the main exception that we need to set by.column = FALSE to prevent a  
#   “length of dimnames [2]” error.  
  
# Roll apply using custom stat function====  
tidyverse_downloads_rollstats <- tidyverse_downloads %>%  
    tq_mutate(  
        select     = count, 
        mutate_fun = rollapply,  
        # rollapply args  
        width      = 28,  
        align      = "right",  
        by.column  = FALSE,  
        FUN        = custom_stat_fun_2,  
        # FUN args  
        na.rm      = TRUE  
    )  
  
# We now have the data needed to visualize the rolling average (trend) and  
#   the 95% confidence bands (volatility) -- this is the concept of the  
#   Bollinger Bands to identify periods of consolidation and periods of high  
#   variability.  
# Many high variability periods are when the package downloads are rapidly  
#   increasing -- 'lubridate', 'purrr' and 'tidyquant' had spikes in  
#   downloads causing the 95% Confidence Interval (CI) bands to widen.  
  
# plot results of roll-apply====      
tidyverse_downloads_rollstats %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    geom_point(aes(y = count), color = "grey40", alpha = 0.5) +  
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), alpha = 0.4) +  
    geom_point(aes(y = mean), size = 1, alpha = 0.5) +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Volatility and Trend", x = "",  
         subtitle = "28-Day Moving Average with 95% CI Bands (+/-2 SD)") +  
    scale_color_tq(theme = "light") +  
    theme_tq() +  
    theme(legend.position="none")  
  
# The Rolling Correlation====  
#   tidyquant::tq_mutate_xy() enables “tidy” application of TTR::runCor()  
#   and other functions with x and y arguments. The corrr package is useful  
#   for computing the correlations and visualizing relationships, and it fits  
#   nicely into the “tidy” framework.  
#   The cowplot package helps with arranging multiple ggplots.  
  
# Investigate correlations to the “broader market”====  
# get the total downloads using cran_downloads() leaving the package argument  
#  "NULL", which is the default.  
  
# Get data for total CRAN downloads and visualize====  
all_downloads <- cran_downloads(from = "2017-01-01",  
                                to = "2017-06-30") %>%  
    tibble::as_tibble()  
  
# Visualize the downloads  
all_downloads %>%  
    ggplot(aes(x = date, y = count)) +  
    # Data  
    geom_point(alpha = 0.5, color = palette_light()[[1]], size = 2) +  
    # Aesthetics  
    labs(title = "Total CRAN Packages: Daily downloads", x = "",  
         subtitle = "2017-01-01 through 2017-06-30",  
         caption = "Downloads data courtesy of cranlogs package") +  
    scale_y_continuous(labels = scales::comma) +  
    theme_tq() +  
    theme(legend.position="none")  
  
# Rolling Correlations====  
# Correlations in time series are very useful because if a relationship   
#   exists, you can actually model/predict/forecast using the correlation.  
# However -- a correlation is NOT static because it changes over time.  
#   Even the best models become useless during periods when correlation is low.  
  
# One of the most important calculations in time series analysis is the  
#   rolling correlation. Rolling correlations are simply applying a  
#   correlation between two time series (say sales of product x and product y)  
#   as a rolling window calculation.  
  
# Rolling Correlation Example====   
# One benefit of a rolling correlation is that we can visualize the change in   
#   correlation over time. Consider if there’s a relatively high correlation   
#   between Sales of Product X and Y until a big shift in December.   
#   The question becomes, “What happened in December?”   
  
# In addition to visualizations, the rolling correlation can signal:    
#   1. events that have occurred causing two correlated time series to   
#     deviate from each other.   
#   2. when modeling, timespans of low correlation can help in determining   
#     whether or not to trust a forecast model.  
#   3. Detect shifts in trend as time series become more or less correlated  
#     over time.  
  
# Time Series Functions====   
# "run" functions from TTR   
tq_mutate_fun_options()$TTR %>%  
    stringr::str_subset("^run")  
  
##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"   
  
# Tidy Implementation of Time Series Functions====  
# Use the tq_mutate_xy() funct. to apply time series functions in a “tidy” way.   
#   Similar to tq_mutate(), the tq_mutate_xy() function is used for tasks   
#   that result in column-wise dimension changes (not row-wise such as  
#   periodicity changes, use tq_transmute for those!).  
# tq_mutate_xy() adds columns to the existing data frame rather than   
#   returning a new data frame like tq_transmute()).  
  
# Most running statistic functions only take one data argument, x.   
#   In these cases you can use tq_mutate(), which has an argument, select.  
#   See how runSD only takes x.  
# If first arg is x (and no y) --> use tq_mutate()     
# args(runSD)  
## function (x, n = 10, sample = TRUE, cumulative = FALSE)  
## NULL   
  
# Functions like runCor and runCov are setup to take in two data arguments,  
#   x and y. In these cases, use tq_mutate_xy(), which takes two arguments,  
#   x and y (as opposed to select from tq_mutate()). This makes it well suited   
#   for functions that have the first two arguments being x and y.  
#   See how runCor has two arguments x and y.  
# If first two arguments are x and y --> use tq_mutate_xy()  
# args(runCor)  
## function (x, y, n = 10, use = "all.obs", sample = TRUE, cumulative = FALSE)  
## NULL  
  
```

```{r tidy_time-series_static_corr}
# Static Correlations====  
# Before we jump into rolling correlations, let’s examine the static    
#   correlations of our package downloads. This gives us an idea of how in  
#   sync the various packages are with each other over the entire timespan.  
  
# Use the correlate() and shave() functions from the corrr package to output   
#   a tidy correlation table. We’ll hone in on the last column “all_cran”,   
#   which measures the correlation between individual packages and the   
#   broader market (i.e. total CRAN downloads).  

# Correlation table -- tidy====  
tidyverse_static_correlations <- tidyverse_downloads %>%  
    # Data wrangling  
    spread(key = package, value = count) %>%  
    left_join(all_downloads, by = "date") %>%  
    rename(all_cran = count) %>%  
    select(-date) %>%  
    # Correlation and formating  
    correlate()  

# Pretty printing
tidyverse_static_correlations %>%
    shave(upper = F)

# The corrr package has a nice visualization called a network_plot() --   
#   to identify strength of correlation. Similar to a “kmeans” analysis,  
#   we are looking for association by distance (or in this case by correlation).  
# The network plot shows us how well the data correlate with each other -- 
#   akin to how associated they are with each other.  
  
# Network plot of correlations====  
# Below, 'tidyquant' has a very low correlation to “all_cran” and the rest  
#   of the “tidyverse” packages -- this would lead us to believe that  
#   tidyquant is trending abnormally with respect to the rest, and thus is  
#   possibly not as associated as we think.  
# Is this really the case?  

gg_all <- tidyverse_static_correlations %>%  
    network_plot(colours = c(palette_light()[[2]],  
                             "white",   
                             palette_light()[[4]]),  
                 legend = TRUE) +  
    labs(  
        title = "Correlations of tidyverse downloads to total CRAN downloads",  
        subtitle = "January through June, tidyquant is a clear outlier"  
        ) +  
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +  
    theme_tq() +  
    theme(legend.position = "bottom")  
gg_all  
```  

```{r tidy_time_series_rolling_corr}
  
# get data for various tidyverse packages - count of downloads====  
pkgs <- c(
    "tidyr", "lubridate", "dplyr", 
    "broom", "tidyquant", "ggplot2", "purrr", 
    "stringr", "knitr"
    )
  
tidyverse_downloads <- cran_downloads(
    packages = pkgs,  
    from     = "2017-01-01",  
    to       = "2017-06-30") %>%  
    tibble::as_tibble() %>%  
    group_by(package)  
  
# Get data for total CRAN downloads and visualize====  
all_downloads <- cran_downloads(from = "2017-01-01",  
                                to = "2017-06-30") %>%  
    tibble::as_tibble()  
  
#Rolling Correlations====  
# Incorporate time using a rolling correlation. The script below uses the 
#   runCor function from the TTR package. We apply it using tq_mutate_xy(),  
#   which is useful for applying functions such has runCor that have both an  
#   x and y input.

# Get rolling correlations  
tidyverse_rolling_corr <- tidyverse_downloads %>%  
    # Data wrangling  
    left_join(all_downloads, by = "date") %>%  
    select(date, package, count.x, count.y) #%>%  
    # Mutation  
    tq_mutate_xy(  
        x          = count.x,  
        y          = count.y,  
        mutate_fun = runCor,  
        # runCor args  
        n          = 30,  
        use        = "pairwise.complete.obs",  
        # tq_mutate args  
        col_rename = "rolling_corr"  
    )  
  
# Join static correlations with rolling correlations====    
tidyverse_static_correlations <- tidyverse_static_correlations %>%  
    select(rowname, all_cran) %>%  
    rename(package = rowname)  
  
tidyverse_rolling_corr <- tidyverse_rolling_corr %>%  
    left_join(tidyverse_static_correlations, by = "package") %>%  
    rename(static_corr = all_cran)  
  
# Plot combined static and rolling correlations    
tidyverse_rolling_corr %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    geom_line(aes(y = static_corr), color = "red") +  
    geom_point(aes(y = rolling_corr), alpha = 0.5) +  
    facet_wrap(~ package, ncol = 3, scales = "free_y") +  
    # Aesthetics  
    scale_color_tq() +  
    labs(  
        title = "tidyverse: 30-Day Rolling Download Correlations, Package vs Total CRAN",  
        subtitle = "Relationships are dynamic vs static correlation (red line)",  
        x = "", y = "Correlation"  
    ) +  
    theme_tq() +  
    theme(legend.position="none")  

# The rolling correlation shows the dynamic nature of the relationship.  
#   If we just went by the static correlation over the full timespan (red line),  
#   we’d be misled about the dynamic nature of these time series. Further, we  
#   can see that most packages are highly correlated with the broader market  
#   (total CRAN downloads) with the exception of various periods where the  
#   correlations dropped. The drops could indicate events or changes in user  
#   behavior that resulted in shocks to the download patterns.  
# Focusing on the main outlier tidyquant, we can see that once April hit   
#   tidyquant is trending closer to a 0.60 correlation meaning that the 0.31  
#   relationship (red line) is likely too low going forward.  

# Last, we can redraw the network plot from April through June to investigate  
#   the shift in relationship. We can use the cowplot package to plot two  
#   ggplots (or corrr network plots) side-by-side.

# Redrawing Network Plot from April through June====    
gg_subset <- tidyverse_downloads %>%  
  # Filter by date >= April 1, 2017  
  filter(date >= ymd("2017-04-01")) %>%  
  # Data wrangling  
  spread(key = package, value = count) %>%  
  left_join(all_downloads, by = "date") %>%  
  rename(all_cran = count) %>%  
  select(-date) %>%  
  # Correlation and formating  
  correlate() %>%  
  # Network Plot  
  network_plot(colours = c(palette_light()[[2]], 
                           "white",  
                           palette_light()[[4]]), 
               legend = TRUE) +  
  labs(  
    title = "April through June (Last 3 Months)",  
    subtitle = "tidyquant correlation is increasing"  
  ) +  
  expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +  
  theme_tq() +  
  theme(legend.position = "bottom")  

# Modify the January through June network plot (previous plot)  
gg_all <- gg_all +  
  labs(  
    title = "January through June (Last 6 months)",  
    subtitle = "tidyquant is an outlier"  
  )  
  
# Format cowplot  
cow_net_plots <- plot_grid(gg_all, gg_subset, ncol = 2)  
title <- ggdraw() +  
  draw_label(label = 'tidyquant is getting "tidy"-er',  
             fontface = 'bold', size = 18)  
cow_out <- plot_grid(title, cow_net_plots,  
                     ncol=1,  
                     rel_heights=c(0.1, 1))  
cow_out  
```  

```{r tidy_time-series-orig} 
# overview of tidy time series----  
# https://www.business-science.io/timeseries-analysis/2017/07/02/tidy-timeseries-analysis.html 
  
# The period apply functions from xts can be used to apply aggregations using  
#   common time series intervals such as weekly, monthly, quarterly, and  
#   yearly. The tq_transmute() function from tidyquant enables efficient and  
#   “tidy” application of the functions. We were able to use the period apply  
#   functions to visualize trends and volatility and to expose relationships  
#   between statistical measures.
  
# get data for various tidyverse packages - count of downloads====  
  
pkgs <- c(
    "tidyr", "lubridate", "dplyr", 
    "broom", "tidyquant", "ggplot2", "purrr", 
    "stringr", "knitr"
    )

tidyverse_downloads <- cran_downloads(
    packages = pkgs, 
    from     = "2017-01-01", 
    to       = "2017-06-30") %>%
    tibble::as_tibble() %>%
    group_by(package)
  
# Visualize the package downloads====  
tidyverse_downloads %>%
    ggplot(aes(x = date, y = count, color = package)) +
    geom_point() +
    labs(title = "tidyverse packages: Daily downloads", x = "") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# Applying functions by period====     
# "apply" functions from xts    
tq_transmute_fun_options()$xts %>%
    stringr::str_subset("^apply")

# To perform weekly aggregation, we will use tq_transmute(), which applies  
#   the non-tidy functions in a “tidy” way. 
# The function we want to use is apply.weekly(), which takes the argument  
#   FUN (the function to be applied weekly) and 
#   ... (additional args that get passed to the FUN function). 
# Set FUN = mean to apply mean() on a weekly interval,and pass the argument 
#   na.rm = TRUE to remove NA values during the calculation.  

mean_tidyverse_downloads_w <- tidyverse_downloads %>%
    tq_transmute(
        select     = count,
        mutate_fun = apply.weekly, 
        FUN        = mean,
        na.rm      = TRUE,
        col_rename = "mean_count"
    )

mean_tidyverse_downloads_w %>%
    ggplot(aes(x = date, y = mean_count, color = package)) +
    geom_point() +
    geom_smooth(method = "loess") + 
    labs(title = "tidyverse packages: Average daily downloads by week", x = "", 
         y = "Mean Daily Downloads by Week") +
    facet_wrap(~ package, ncol = 3, scale = "free_y") +
    expand_limits(y = 0) + 
    scale_color_tq() +
    theme_tq() +
    theme(legend.position="none")

# Custom function to return mean, sd, quantiles====  
custom_stat_fun <- function(x, na.rm = TRUE, ...) {
    # x     = numeric vector
    # na.rm = boolean, whether or not to remove NA's
    # ...   = additional args passed to quantile
    c(mean    = mean(x, na.rm = na.rm),
      stdev   = sd(x, na.rm = na.rm),
      quantile(x, na.rm = na.rm, ...)) 
}

# Testing custom_stat_fun
options(digits = 4)
set.seed(3366)
nums  <- c(10 + 1.5*rnorm(10), NA)
probs <- c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)
custom_stat_fun(nums, na.rm = TRUE, probs = probs)

# Apply the custom function by week -- tidy & visualize====   
stats_tidyverse_downloads_w <- tidyverse_downloads %>%  
    tq_transmute(  
        select = count,  
        mutate_fun = apply.weekly,  
        FUN = custom_stat_fun,  
        na.rm = TRUE,  
        probs = probs  
    )  

stats_tidyverse_downloads_w %>%   
    ggplot(aes(x = date, y = `50%`, color = package)) +  
    # Ribbon  
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`),  
                color = palette_light()[[1]],  
                fill = palette_light()[[1]],  
                alpha = 0.5) +  
    # Points  
    geom_point() +  
    geom_smooth(method = "loess", se = FALSE) +  
    # Aesthetics  
    labs(title = "tidyverse packages: Median daily downloads by week",  
         x = "",  
         subtitle = "Range of 1st and 3rd quartile to show volatility",  
         y = "Median Daily Downloads By Week") +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    expand_limits(y = 0) +  
    scale_color_tq(theme = "dark") +  
    theme_tq() +  
    theme(legend.position="none")  

stats_tidyverse_downloads_w %>%  
    ggplot(aes(x = stdev, y = mean, color = package)) +  
    geom_point() +  
    geom_smooth(method = "lm") +  
    labs(  
      title = "tidyverse packages: Mean vs SD of daily downloads by week") +  
    facet_wrap(~ package, ncol = 3, scale = "free") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")   

# Rolling Window Calculations====  
# The rollapply functions from zoo and TTR can be used to apply rolling  
#   window calculations. The tq_mutate() function from tidyquant enables  
#   efficient and “tidy” application of the functions. We were able to use  
#   the rollapply functions to visualize averages and standard deviations on  
#   a rolling basis, which gave us a better perspective of the dynamic trends.  
#   Using custom functions, we are unlimited to the statistics we can apply to  
#   rolling windows.  
#  
#   What are rolling window calculations, and why do we care? In time series  
#   analysis, nothing is static. A correlation may exist for a subset of time  
#   or an average may vary from one day to the next. Rolling calculations  
#   simply apply functions to a fixed width subset of the data (aka a window),  
#   indexing one observation each calculation.  
  
#   There are a few common reasons you may want to use a rolling calculation  
#   in time series analysis:  
#  
#    Measuring the central tendency over time (mean, median)  
#    Measuring the volatility over time (sd, var)  
#    Detecting changes in trend (fast vs slow moving averages)  
#    Measuring a relationship between two time series over time (cor, cov)  

# Sample Moving Average Calculation  
# Combining a rolling mean with a rolling standard deviation can help detect  
# regions of abnormal volatility and consolidation. This is the concept behind   
# Bollinger Bands in the financial industry. The bands can be useful in  
# detecting breakouts in trend.    

# Time Series Functions for rolling window====  
# "roll" functions from zoo  
tq_mutate_fun_options()$zoo %>%  
    stringr::str_subset("^roll")  

##  [1] "rollapply"          "rollapplyr"         "rollmax"            
##  [4] "rollmax.default"    "rollmaxr"           "rollmean"          
##  [7] "rollmean.default"   "rollmeanr"          "rollmedian"        
## [10] "rollmedian.default" "rollmedianr"        "rollsum"           
## [13] "rollsum.default"    "rollsumr"  

# "run" functions from TTR  
tq_mutate_fun_options()$TTR %>%  
    stringr::str_subset("^run")  
  
##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"  
  
# Tidy Implementation of Time Series Functions====    
# Condensed function options====    
tq_mutate_fun_options() %>%  
    str()  

## List of 5  
##  $ zoo: chr [1:14] "rollapply" "rollapplyr" "rollmax" "rollmax.default" ...  
##  $ xts: chr [1:27] "apply.daily" "apply.monthly" "apply.quarterly"   
##      "apply.weekly" ...  
##  $ quantmod: chr [1:25] "allReturns" "annualReturn" "ClCl" "dailyReturn" ...  
##  $ TTR: chr [1:61] "adjRatios" "ADX" "ALMA" "aroon" ...  
##  $ PerformanceAnalytics: chr [1:7] "Return.annualized" "  
##      Return.annualized.excess" "Return.clean" "Return.cumulative" ...  

# Tidy Application of Rolling Functions====  
# Rolling Mean: Inspecting Fast and Slow Moving Averages  
#  Investigate if significant changes in trend are taking place such that  
#   future downloads are likely to continue to increase, decrease or stay the  
#   same. One way to do this is to use moving averages.  
# Rather than try to sift through the noise, we can use a combination of a  
#   fast and slow moving average to detect momentum.  

# We’ll create a fast moving average with width = 28 days (just enough to  
#   detrend the data) and a slow moving average with width = 84 days  
#   (slow window = 3X fast window). To do this we apply two calls to  
#   tq_mutate(), the first for the 28 day (fast) and the second for the  
#   84 day (slow) moving average. There are three groups of arguments we   
#   need to supply:  
# tq_mutate args -- These select the column to apply the mutation to “count”  
#   & the mutation function (mutate_fun) to apply (rollapply from zoo).  
# rollapply args: These set the width, align = "right"  
#   (aligns with end of data frame), and  
#   the FUN we wish to apply (mean in this case).  
# FUN args: These are arguments that get passed to the function.  
#   In this case we want to set na.rm = TRUE so NA values are skipped.  
# Also add an optional tq_mutate arg, col_rename, at the end to rename the  
#   column.   

# Rolling mean example====    
tidyverse_downloads_rollmean <- tidyverse_downloads %>%  
    tq_mutate(  
        # tq_mutate args  
        select     = count,  
        mutate_fun = rollapply,  
        # rollapply args  
        width      = 28,  
        align      = "right",  
        FUN        = mean,  
        # mean args  
        na.rm      = TRUE,  
        # tq_mutate args  
        col_rename = "mean_28"  
    ) %>%  
    tq_mutate(  
        # tq_mutate args  
        select     = count,  
        mutate_fun = rollapply,  
        # rollapply args  
        width      = 84,  
        align      = "right",  
        FUN        = mean,  
        # mean args  
        na.rm      = TRUE,  
        # tq_mutate args  
        col_rename = "mean_84"  
    )  

# ggplot results====  
tidyverse_downloads_rollmean %>%  
    ggplot(aes(x = date, y = count, color = package)) +  
    # Data  
    geom_point(alpha = 0.1) +  
    geom_line(aes(y = mean_28), color = palette_light()[[1]], size = 1) +  
    geom_line(aes(y = mean_84), color = palette_light()[[2]], size = 1) +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Daily Downloads", x = "",  
         subtitle = "28 and 84 Day Moving Average") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")  
  
# Drop the “count” data from the plots and inspect just the moving averages to  
#   identify points where the fast trend is above (has momentum)  
#   or below (is slowing) the slow trend, & inspect for cross-over,  
#   which indicates shifts in trend.  
  
tidyverse_downloads_rollmean %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    # geom_point(alpha = 0.5) +  # Drop "count" from plots  
    geom_line(aes(y = mean_28),  
              color = palette_light()[[1]],  
              linetype = 1,  
              size = 1) +  
    geom_line(aes(y = mean_84),  
              color = palette_light()[[2]],   
              linetype = 1,  
              size = 1) +  
    facet_wrap(~ package,  
               ncol = 3,  
               scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Daily downloads", x = "", y = "",  
         subtitle = "Zoomed In: 28 and 84 Day Moving Average") +  
    scale_color_tq() +  
    theme_tq() +  
    theme(legend.position="none")  

# The plot shows 'purrr' and 'lubridate' have strong upward momentum,   
#   'dplyr', 'knitr' and 'tidyr' seem to be cycling in a range, &   
#   'ggplot2' and 'stringr' have short term downward trends -- keep in mind  
#   these packages are getting the most downloads of the bunch.  
  
# Rolling Custom Functions: Useful for multiple statistics  
#   Create a custom function, custom_stat_fun_2(), that returns statistics:   
#    mean  
#    standard deviation  
#    95% confidence interval (mean +/- 2SD)  

# Custom function to return mean, sd, 95% conf interval====    
custom_stat_fun_2 <- function(x, na.rm = TRUE) {  
  # x     = numeric vector  
  # na.rm = boolean, whether or not to remove NA's  
  m  <- mean(x, na.rm = na.rm)  
  s  <- sd(x, na.rm = na.rm)  
  hi <- m + 2*s  
  lo <- m - 2*s  
  ret <- c(mean = m, stdev = s, hi.95 = hi, lo.95 = lo)   
  return(ret)  
}  
  
# Apply the custom_stat_fun_2() to groups====  
#   using tq_mutate() and the rolling function rollapply()  
#   The output returned is a “tidy” data frame 
#   with each statistic in its own column.
# The process is almost identical to the process of applying mean() with  
#   the main exception that we need to set by.column = FALSE to prevent a  
#   “length of dimnames [2]” error. 
  
# Roll apply using custom stat function====  
tidyverse_downloads_rollstats <- tidyverse_downloads %>%
    tq_mutate(
        select     = count,
        mutate_fun = rollapply, 
        # rollapply args
        width      = 28,
        align      = "right",
        by.column  = FALSE,
        FUN        = custom_stat_fun_2,
        # FUN args
        na.rm      = TRUE
    )
  
# We now have the data needed to visualize the rolling average (trend) and  
#   the 95% confidence bands (volatility) -- this is the concept of the  
#   Bollinger Bands to identify periods of consolidation and periods of high  
#   variability. 
# Many high variability periods are when the package downloads are rapidly  
#   increasing -- 'lubridate', 'purrr' and 'tidyquant' had spikes in  
#   downloads causing the 95% Confidence Interval (CI) bands to widen.
  
# plot results of roll-apply====      
tidyverse_downloads_rollstats %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    geom_point(aes(y = count), color = "grey40", alpha = 0.5) +  
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), alpha = 0.4) +  
    geom_point(aes(y = mean), size = 1, alpha = 0.5) +  
    facet_wrap(~ package, ncol = 3, scale = "free_y") +  
    # Aesthetics  
    labs(title = "tidyverse packages: Volatility and Trend", x = "",  
         subtitle = "28-Day Moving Average with 95% CI Bands (+/-2 SD)") +  
    scale_color_tq(theme = "light") +  
    theme_tq() +  
    theme(legend.position="none")  
  
# The Rolling Correlation====  
#   tidyquant::tq_mutate_xy() enables “tidy” application of TTR::runCor()  
#   and other functions with x and y arguments. The corrr package is useful  
#   for computing the correlations and visualizing relationships, and it fits  
#   nicely into the “tidy” framework.  
#   The cowplot package helps with arranging multiple ggplots.  
  
# Investigate correlations to the “broader market”====  
# get the total downloads using cran_downloads() leaving the package argument  
#  "NULL", which is the default.  
  
# Get data for total CRAN downloads and visualize====  
all_downloads <- cran_downloads(from = "2017-01-01",  
                                to = "2017-06-30") %>%  
    tibble::as_tibble()  

# Visualize the downloads
all_downloads %>%
    ggplot(aes(x = date, y = count)) +
    # Data
    geom_point(alpha = 0.5, color = palette_light()[[1]], size = 2) +
    # Aesthetics
    labs(title = "Total CRAN Packages: Daily downloads", x = "",
         subtitle = "2017-01-01 through 2017-06-30",
         caption = "Downloads data courtesy of cranlogs package") +
    scale_y_continuous(labels = scales::comma) +
    theme_tq() +
    theme(legend.position="none")

# Rolling Correlations====  
# Correlations in time series are very useful because if a relationship   
#   exists, you can actually model/predict/forecast using the correlation.  
# However -- a correlation is NOT static because it changes over time.  
#   Even the best models become useless during periods when correlation is low.  

# One of the most important calculations in time series analysis is the  
#   rolling correlation. Rolling correlations are simply applying a  
#   correlation between two time series (say sales of product x and product y)  
#   as a rolling window calculation.  
  
# Rolling Correlation Example====   
# One benefit of a rolling correlation is that we can visualize the change in   
#   correlation over time. Consider if there’s a relatively high correlation   
#   between Sales of Product X and Y until a big shift in December.   
#   The question becomes, “What happened in December?”   
  
# In addition to visualizations, the rolling correlation can signal:    
#   1. events that have occurred causing two correlated time series to   
#     deviate from each other.   
#   2. when modeling, timespans of low correlation can help in determining   
#     whether or not to trust a forecast model.  
#   3. Detect shifts in trend as time series become more or less correlated  
#     over time.  

# Time Series Functions====   
# "run" functions from TTR   
tq_mutate_fun_options()$TTR %>%  
    stringr::str_subset("^run")  
  
##  [1] "runCor"         "runCov"         "runMAD"        
##  [4] "runMax"         "runMean"        "runMedian"     
##  [7] "runMin"         "runPercentRank" "runSD"         
## [10] "runSum"         "runVar"   

# Tidy Implementation of Time Series Functions====  
# Use the tq_mutate_xy() funct. to apply time series functions in a “tidy” way.   
#   Similar to tq_mutate(), the tq_mutate_xy() function is used for tasks   
#   that result in column-wise dimension changes (not row-wise such as  
#   periodicity changes, use tq_transmute for those!).  
# tq_mutate_xy() adds columns to the existing data frame rather than   
#   returning a new data frame like tq_transmute()).  
  
# Most running statistic functions only take one data argument, x.  
#   In these cases you can use tq_mutate(), which has an argument, select.  
#   See how runSD only takes x.  
# If first arg is x (and no y) --> use tq_mutate()     
args(runSD)  
  
## function (x, n = 10, sample = TRUE, cumulative = FALSE)  
## NULL   
  
# Functions like runCor and runCov are setup to take in two data arguments,  
#   x and y. In these cases, use tq_mutate_xy(), which takes two arguments,  
#   x and y (as opposed to select from tq_mutate()). This makes it well suited   
#   for functions that have the first two arguments being x and y.  
#   See how runCor has two arguments x and y.  
# If first two arguments are x and y --> use tq_mutate_xy()  
args(runCor)  

## function (x, y, n = 10, use = "all.obs", sample = TRUE, cumulative = FALSE)  
## NULL  

# Static Correlations====  
# Before we jump into rolling correlations, let’s examine the static    
#   correlations of our package downloads. This gives us an idea of how in  
#   sync the various packages are with each other over the entire timespan.  
  
# Use the correlate() and shave() functions from the corrr package to output   
#   a tidy correlation table. We’ll hone in on the last column “all_cran”,   
#   which measures the correlation between individual packages and the   
#   broader market (i.e. total CRAN downloads).  

# Correlation table -- tidy====  
tidyverse_static_correlations <- tidyverse_downloads %>%  
    # Data wrangling  
    spread(key = package, value = count) %>%  
    left_join(all_downloads, by = "date") %>%  
    rename(all_cran = count) %>%  
    select(-date) %>%  
    # Correlation and formating  
    correlate()  

# Pretty printing
tidyverse_static_correlations %>%
    shave(upper = F)

# The corrr package has a nice visualization called a network_plot() --   
#   to identify strength of correlation. Similar to a “kmeans” analysis,  
#   we are looking for association by distance (or in this case by correlation).  
# The network plot shows us how well the data correlate with each other -- 
#   akin to how associated they are with each other.  
  
# Network plot of correlations====  
# Below, 'tidyquant' has a very low correlation to “all_cran” and the rest  
#   of the “tidyverse” packages -- this would lead us to believe that  
#   tidyquant is trending abnormally with respect to the rest, and thus is  
#   possibly not as associated as we think.  
# Is this really the case?  

gg_all <- tidyverse_static_correlations %>%  
    network_plot(colours = c(palette_light()[[2]],  
                             "white",   
                             palette_light()[[4]]),  
                 legend = TRUE) +  
    labs(  
        title = "Correlations of tidyverse downloads to total CRAN downloads",  
        subtitle = "January through June, tidyquant is a clear outlier"  
        ) +  
    expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +  
    theme_tq() +  
    theme(legend.position = "bottom")  
gg_all  
  
#Rolling Correlations====  
# Incorporate time using a rolling correlation. The script below uses the 
#   runCor function from the TTR package. We apply it using tq_mutate_xy(),  
#   which is useful for applying functions such has runCor that have both an  
#   x and y input.

# Get rolling correlations  
tidyverse_rolling_corr <- tidyverse_downloads %>%  
    # Data wrangling  
    left_join(all_downloads, by = "date") %>%  
    select(date, package, count.x, count.y) %>%  
    # Mutation  
    tq_mutate_xy(  
        x          = count.x,  
        y          = count.y,  
        mutate_fun = runCor,  
        # runCor args  
        n          = 30,  
        use        = "pairwise.complete.obs",  
        # tq_mutate args  
        col_rename = "rolling_corr"  
    )  
  
# Join static correlations with rolling correlations====    
tidyverse_static_correlations <- tidyverse_static_correlations %>%  
    select(rowname, all_cran) %>%  
    rename(package = rowname)  
  
tidyverse_rolling_corr <- tidyverse_rolling_corr %>%  
    left_join(tidyverse_static_correlations, by = "package") %>%  
    rename(static_corr = all_cran)  
  
# Plot combined static and rolling correlations    
tidyverse_rolling_corr %>%  
    ggplot(aes(x = date, color = package)) +  
    # Data  
    geom_line(aes(y = static_corr), color = "red") +  
    geom_point(aes(y = rolling_corr), alpha = 0.5) +  
    facet_wrap(~ package, ncol = 3, scales = "free_y") +  
    # Aesthetics  
    scale_color_tq() +  
    labs(  
        title = "tidyverse: 30-Day Rolling Download Correlations, Package vs Total CRAN",  
        subtitle = "Relationships are dynamic vs static correlation (red line)",  
        x = "", y = "Correlation"  
    ) +  
    theme_tq() +  
    theme(legend.position="none")  

# The rolling correlation shows the dynamic nature of the relationship.  
#   If we just went by the static correlation over the full timespan (red line),  
#   we’d be misled about the dynamic nature of these time series. Further, we  
#   can see that most packages are highly correlated with the broader market  
#   (total CRAN downloads) with the exception of various periods where the  
#   correlations dropped. The drops could indicate events or changes in user  
#   behavior that resulted in shocks to the download patterns.  
# Focusing on the main outlier tidyquant, we can see that once April hit   
#   tidyquant is trending closer to a 0.60 correlation meaning that the 0.31  
#   relationship (red line) is likely too low going forward.  

# Last, we can redraw the network plot from April through June to investigate  
#   the shift in relationship. We can use the cowplot package to plot two  
#   ggplots (or corrr network plots) side-by-side.

# Redrawing Network Plot from April through June====    
gg_subset <- tidyverse_downloads %>%  
  # Filter by date >= April 1, 2017  
  filter(date >= ymd("2017-04-01")) %>%  
  # Data wrangling  
  spread(key = package, value = count) %>%  
  left_join(all_downloads, by = "date") %>%  
  rename(all_cran = count) %>%  
  select(-date) %>%  
  # Correlation and formating  
  correlate() %>%  
  # Network Plot  
  network_plot(colours = c(palette_light()[[2]], 
                           "white",  
                           palette_light()[[4]]), 
               legend = TRUE) +  
  labs(  
    title = "April through June (Last 3 Months)",  
    subtitle = "tidyquant correlation is increasing"  
  ) +  
  expand_limits(x = c(-0.75, 0.25), y = c(-0.4, 0.4)) +  
  theme_tq() +  
  theme(legend.position = "bottom")  

# Modify the January through June network plot (previous plot)  
gg_all <- gg_all +  
  labs(  
    title = "January through June (Last 6 months)",  
    subtitle = "tidyquant is an outlier"  
  )  
  
# Format cowplot  
cow_net_plots <- plot_grid(gg_all, gg_subset, ncol = 2)  
title <- ggdraw() +  
  draw_label(label = 'tidyquant is getting "tidy"-er',  
             fontface = 'bold', size = 18)  
cow_out <- plot_grid(title, cow_net_plots,  
                     ncol=1,  
                     rel_heights=c(0.1, 1))  
cow_out  
  
# Lags (Lag Operator) overview====  
# Calculate lags and analyze autocorrelation.  
# The lag operator (also known as backshift operator) is a function that  
#   shifts (offsets) a time series such that the “lagged” values are aligned   
#   with the actual time series. The lags can be shifted any number of units,  
#     which simply controls the length of the backshift.  
  
# Lags are useful in time series analysis because of a phenomenon called   
#   autocorrelation, which is a tendency for the values within a time series   
#   to be correlated with previous copies of itself. One benefit to   
#   autocorrelation is that we can identify patterns within the time series,    
#   which helps in determining seasonality, the tendency for patterns to   
#   repeat at periodic frequencies. 
  
# Lags and autocorrelation are central to forecasting models that incorporate  
#   autoregression, regressing a time series using previous values of itself.   
# Autoregression is the basis for one of the most widely used forecasting  
#   techniques, the autoregressive integrated moving average model or ARIMA  
# for short. The forecast package by Rob Hyndman, implements ARIMA and a  
# number of other forecast modeling techniques. 
# Note -- Autoregression and ARIMA not discussed below.  

# Lag and autocorrelation analysis is a good way to detect seasonality. 
#   The autocorrelation of the lagged values can be used to detect “abnormal”  
#   seasonal patterns. 
# The tq_mutate() function was used to apply lag.xts() to the daily download  
#   counts to efficiently get lags 1 through 28. Once the lags were retrieved,  
#   we use other dplyr functions such as gather() to pivot the data and  
#   summarize() to calculate the autocorrelations. Finally, we saw the power  
#   of visual analysis of the autocorrelations -- created an ACF plot that  
#   showed a visual trend. Then we used a boxplot to detect which lags had  
#   consistent outliers. Ultimately a weekly pattern was confirmed.   
  
# tidyquant Integrated functions====  
tq_mutate_fun_options() %>%  
    glimpse()  
  
## List of 5  
##  $ zoo: chr [1:14] "rollapply" "rollapplyr" "rollmax" "rollmax.default" ...  
##  $ xts: chr [1:27] "apply.daily" "apply.monthly" "apply.quarterly"  
##    "apply.weekly" ...  
##  $ quantmod: chr [1:25] "allReturns" "annualReturn" "ClCl" "dailyReturn" ...  
##  $ TTR: chr [1:62] "adjRatios" "ADX" "ALMA" "aroon" ...  
##  $ PerformanceAnalytics: chr [1:7] "Return.annualized"  
##    "Return.annualized.excess" "Return.clean" "Return.cumulative" ...

# lag.xts() -- tidy====  
# The lag.xts() function from the xts package, has a great function for getting  
#   multiple lags.  
# The lag.xts() function generates a sequence of lags (t-1, t-2, t-3, …, t-k)  
#   using the argument k. However, it only works on xts or other matrix,  
#   vector-based objects). In other words, it fails on our “tidy” tibble.  
#   And, we get an “unsupported type” error.   
  
# Consider a time series of ten values beginning in 2017.  
set.seed(1)  
my_time_series_tbl <- tibble(  
    date   = seq.Date(ymd("2017-01-01"),  
                      length.out = 10,  
                      by = "day"),  
    value  = 1:10 + rnorm(10)  
)  

# Bummer, man! -- I ran into this...   
my_time_series_tbl %>%           
        lag.xts(k = 1:5)  

## <simpleError in FUN(X[[i]], ...): unsupported type>

# The timetk package is a toolkit for working with time series. It has  
#   functions that simplify and make consistent the process of coercion --  
#   converting to and from different time series classes. In addition,  
#   it has functions to aid the process of time series machine learning and  
#   data mining. 

# Convert to an xts object -- use tk_xts() from the timetk package  
#   to coerce from a time-based tibble -- tibble with a date or time component 
#   and xts object.

# Success! Got our lags 1 through 5. One problem: no original values
my_time_series_tbl %>%
    tk_xts(silent = TRUE) %>%
    lag.xts(k = 1:5)

# We still need our original values so we can analyze the counts against  
#   the lags. 
# If we want to get the original values too, we can do something like this.

# Convert to xts  
my_time_series_xts <- my_time_series_tbl %>%  
    tk_xts(silent = TRUE)  
  
# Get original values and lags in xts  
my_lagged_time_series_xts <-   
    merge.xts(my_time_series_xts, lag.xts(my_time_series_xts, k = 1:5))  

# Convert back to tbl  
my_lagged_time_series_xts %>%  
    tk_tbl()  
  
# That’s a lot of work for a simple operation.  
# Fortunately we have tq_mutate() to the rescue!  
  
#  tq_mutate()====  
# The tq_mutate() function from tidyquant enables “tidy” application of the  
#   xts-based functions. The tq_mutate() function works similarly to mutate()  
#   from dplyr in the sense that it adds columns to the data frame.
 
# The tidyquant package enables a “tidy” implementation of the xts-based  
#   functions from packages such as xts, zoo, quantmod, TTR and  
# PerformanceAnalytics. 

# Quick example -- use the select = value to send the “value” column to the  
#   mutation function. In this case our mutate_fun = lag.xts. We supply k = 5  
#   as an additional argument.  
# That’s much easier -- we get the value column returned in addition to the  
#   lags, which is the benefit of using tq_mutate(). If you use tq_transmute()  
#   instead, the result would be the lags only, which is what lag.xts() returns.  
  
# This is nice, we didn't need to coerce to xts and it merged for us  
my_time_series_tbl %>%  
    tq_mutate(  
        select     = value,  
        mutate_fun = lag.xts,  
        k          = 1:5  
    )  
  
# Analyzing tidyverse Downloads: Lag and Autocorrelation Analysis----  
# Scaling the Lag and Autocorrelation Calculation  
# Get lags 1 through 28 (4 weeks of lags):  
#   Take the tidyverse_downloads data frame, which is grouped by package,  
#     and apply tq_mutate() using the lag.xts function.  
#   We can provide column names for the new columns by prefixing “lag_” to   
#   the lag numbers, k, which the sequence from 1 to 28.   
# The output is all of the lags for each package.  

# Use tq_mutate() to get lags 1:28 using lag.xts()====  
k <- 1:28  
col_names <- paste0("lag_", k)  
  
tidyverse_lags <- tidyverse_downloads %>%  
    tq_mutate(  
        select     = count,  
        mutate_fun = lag.xts,  
        k          = 1:28,  
        col_rename = col_names  
    )  

# Next steps with lag.xts====   
# The goal is to get count and each lag side-by-side so we can do a correlation. 
# Correlating each of the lags to the “count” column involves steps  
#   strung together in a dplyr pipe (%>%):  
# 1. Use gather() to pivot each lagged column into a “tidy” long-format df,   
#     and exclude columns: “package”, “date” and “count” columns from the pivot.   
# 2. Convert the new “lag” column from a character string (e.g. “lag_1”)  
#     to numeric (e.g. 1) using mutate() to make ordering the lags easier.  
# 3. group the long data frame by package and lag to calculate subsets of  
#     package and lag.  
# 4. apply the correlation to each group of lags. The summarize() function   
#     can be used to implement cor(), which takes x = count and y = lag_value.   
#   Make sure to pass use = "pairwise.complete.obs", which is almost always   
#   desired.  
# 5. The 95% upper and lower cutoff can be approximated by: cutoff=±2 / N^0.5  
#     Where:  
#       N = number of observations.  

# Calculate the autocorrelations and 95% cutoffs  
tidyverse_count_autocorrelations <- tidyverse_lags %>%  
    gather(key = "lag",  
           value = "lag_value",  
           -c(package, date, count)) %>%  
    mutate(lag = str_sub(lag, start = 5) %>% as.numeric) %>%  
    group_by(package, lag) %>%  
    summarize(  
        cor = cor(x = count,  
                  y = lag_value,  
                  use = "pairwise.complete.obs"),  
        cutoff_upper = 2/(n())^0.5,  
        cutoff_lower = -2/(n())^0.5  
        )  

#Visualizing Autocorrelation: ACF Plot====
# Now correlations are calculated by package and lag number in a “tidy” format,  
#   we can visualize the autocorrelations with ggplot to check for patterns.  
# The plot shown below is known as an ACF plot, which is simply the  
#   autocorrelations at various lags. Initial examination of the ACF plots  
# indicate a weekly frequency.

# Visualize the autocorrelations
tidyverse_count_autocorrelations %>%
    ggplot(aes(x = lag, y = cor, color = package, group = package)) +
    # Add horizontal line a y=0
    geom_hline(yintercept = 0) +
    # Plot autocorrelations
    geom_point(size = 2) +
    geom_segment(aes(xend = lag, yend = 0), size = 1) +
    # Add cutoffs
    geom_line(aes(y = cutoff_upper), color = "blue", linetype = 2) +
    geom_line(aes(y = cutoff_lower), color = "blue", linetype = 2) +
    # Add facets
    facet_wrap(~ package, ncol = 3) +
    # Aesthetics
    expand_limits(y = c(-1, 1)) +
    scale_color_tq() +
    theme_tq() +
    labs(
        title = paste0("Tidyverse ACF Plot: Lags ", rlang::expr_text(k)),
        subtitle = "Appears to be a weekly pattern",
        x = "Lags"
    ) +
    theme(
        legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1)
    )

# We see that there appears to be a weekly pattern, but we want to be sure.   
  
# Get the absolute autocorrelations====  
# Verify the weekly pattern assessment by reviewing the absolute value of the  
#   correlations independent of package. We take the absolute autocorrelation   
#   because we use the magnitude as a proxy for how much explanatory value the   
#   lag provides.   
# Use dplyr functions to manipulate the data for visualization:  
# 1. drop the package group constraint using ungroup(),    
# 2. calculate the absolute correlation using mutate(),  
# 3. convert the lag to a factor, which helps with reordering the plot.  
# 4. Select() only the “lag” and “cor_abs” columns,    
# 5.  group by “lag” to lump all of the lags together -- to determine the  
#       trend independent of package.  
  
tidyverse_absolute_autocorrelations <- tidyverse_count_autocorrelations %>%  
    ungroup() %>%  
    mutate(  
        lag = as_factor(as.character(lag)),  
        cor_abs = abs(cor)  
        ) %>%  
    select(lag, cor_abs) %>%  
    group_by(lag)   
  
# Visualize the absolute correlations====  
# 1. Use a box plot that lumps each of the lags together. 
# 2. Add a line to indicate the presence of outliers at values above 1.5IQR  
#     If the values are consistently above the 1.5IQR limit, the lag can be  
#     considered an outlier. Note that we use the fct_reorder() function from  
#     forcats to organize the boxplot in order of decending magnitude.  

# Visualize boxplot of absolute autocorrelations   
break_point <- 1.5 * IQR(tidyverse_absolute_autocorrelations$cor_abs) %>%  
  signif(3)  

tidyverse_absolute_autocorrelations %>%    
    ggplot(aes(x = fct_reorder(lag, cor_abs, .desc = TRUE),  
               y = cor_abs)) +  
    # Add boxplot   
    geom_boxplot(color = palette_light()[[1]]) +  
    # Add horizontal line at outlier break point  
    geom_hline(yintercept = break_point, color = "red") +  
    annotate("text", label = paste0("Outlier Break Point = ", break_point),  
             x = 24.5, y = break_point + .03, color = "red") +  
    # Aesthetics  
    expand_limits(y = c(0, 1)) +  
    theme_tq() +  
    labs(  
        title = paste0("Absolute Autocorrelations: Lags ", 
                       rlang::expr_text(k)),  
        subtitle = "Weekly pattern is consistently above outlier break point",  
        x = "Lags"  
    ) +  
    theme(  
        legend.position = "none",  
        axis.text.x = element_text(angle = 45, hjust = 1)  
    )  
  
# outcome -- lags in multiples of seven have the highest autocorrelation  
#   and are consistently above the outlier break point indicating the presence  
#   of a strong weekly pattern. The autocorrelation with the seven-day lag is  
#   the highest, with a median of approximately 0.75. Lags 14, 21, and 28 are  
#   also outliers with median autocorrelations in excess of our outlier break  
#   point of 0.471.

# Note -- the median of Lag 1 is essentially at the break point indicating  
#   that half of the packages have a presence of “abnormal” autocorrelation. 
#   However, this is not part of a seasonal pattern since a periodic frequency  
#   is not present.
# In the case above, the tidyverse packages exhibit a strong weekly pattern. 
  
``` 

