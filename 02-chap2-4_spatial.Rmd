---
title: "02-chapt2-4_spatial"
author: "CJ Tinant"
date: "7/15/2019"
output: html_document
---

<!--
Purpose: 
develop a regression function for gaged stations by fitting PC1 of mean daily 
streamflow to environmental variables.  Then estimate PC1 for ungaged stations  
from the regression equation.  

Data: 
  the data to train the model consists of 42 gaged catchments, 
  the data for the fitted model consists of 45 ungaged catchments  

Approach: 
1) EDA 
-- check histograms 
-- check univariate response 
2) Split train/test data based on hydrologic export to create a balanced split. 
   caret::createDataPartition description - For numeric y, the sample is split 
   into groups sections based on percentiles and sampling is done within these 
   subgroups. The number of percentiles is set via the groups argument with 
   5-groups as default.  See also: 
   https://topepo.github.io/caret/model-training-and-tuning.html 
   For the 80/20 training set = 34 stations and test set = 8 stations
3a) Train a ridge regression model 
    Decrease model complexity while keeping all variables in the model by 
    penalizing predictors if they are too far from zero, thus enforcing them to 
    be small in a continuous way. 
    
    The λ parameter is the regularization penalty. Setting λ to 0 is the same 
    as using the OLS estimate. As λ becomes larger, the variance decreases, 
    and the bias increases. 
    
    Cross-validation is used to select the value of λ that minimizes the 
    cross-validated sum of squared residuals (or some other measure) to 
    maximize predictive performance. 







3) Train model - Fit a generalized linear model via penalized maximum 
   likelihood using the glmnet package (Friedman, Hastie, and Tibshirani 2008).  
   The algorithm: 1) computes the regularization path denoted with λ that 
   controls the overall strength of the penalty, and 2) applies an elastic-net 
   penalty called the mixing percentage and denoted with α (Zou and Hastie 
   2005) to bridge the gap between lasso (α = 1) and ridge (α = 0) regression. 

   Model parameters (λ and α) are tuned to minimizes our loss function by 
   nested cross-validation. The approach is as follows:
     1. Set the parameter you want to tune to some value.
     2. Split your data into K ‘folds’ (sections).
     3. Train your model using K-1 folds using the parameter value.
     4. Test your model on the remaining fold.
     5. Repeat steps 3 and 4 so that every fold is the test data once.
     6. Repeat steps 1 to 5 for every possible value of the parameter.
     7. Report the parameter that produced the best result.

Since the test set is used both to select the values of the parameter and 
to evaluate the model, we risk optimistically biasing our model evaluations. 
For this reason, if a test set is used to select model parameters, then we 
need a different test set to get an unbiased evaluation of that selected model.

One way to overcome this problem is to have nested cross-validations. 
First, an inner cross validation is used to tune the parameters and select 
the best model. Second, an outer cross validation is used to evaluate the 
model selected by the inner cross validation.

Steps are as follows (check out the following link)
https://chrisalbon.com/machine_learning/model_evaluation/nested_cross_validation/: 
  1. standardize data
  2. create inner cross validation - for parameter tuning 
  3. create outer cross validation - for model evaluation 

An R approach in glmnetUtils::cva.glmnet function simultaneously performs 
cross-validation for both the alpha and lambda parameters in an elastic net 
model. The procedure creates a vector 'foldid' allocating the observations 
into folds, and then calls cv.glmnet in a loop over different values of alpha, 
but the same values of 'foldid' each time *.

cva.glmnet(x, y, alpha = seq(0, 1, len = 11)^3,
  nfolds = 10, foldid = sample(rep(seq_len(nfolds), length = nrow(x))),
  ..., outerParallel = NULL, checkInnerParallel = TRUE)

   x - a matrix of predictor variables 
   y - a response vector or matrix (for a multinomial response) 
   alpha - 	a vector of alpha values for which to do cross-validation. 
            The default is a sequence of 11 values more closely spaced 
            around alpha = 0. 
nfolds	
The number of cross-validation folds to use. Defaults to 10.
foldid	
Vector of fold IDs for cross-validation. See glmnet::cv.glmnet.
outerParallel	
Method of parallelising the outer loop over alpha. See 'Details' below. If NULL, the loop is run sequentially.
checkInnerParallel	
If the outer loop is run in parallel, check that the inner loop over lambda will not be in contention for cores.
formula	
A model formula; interaction terms are allowed and will be expanded per the usual rules for linear models.
data	
A data frame or matrix containing the variables in the formula.
weights	
An optional vector of case weights to be used in the fitting process. If missing, defaults to an unweighted fit.
offset	
An optional vector of offsets, an a priori known component to be included in the linear predictor.
subset	
An optional vector specifying the subset of observations to be used to fit the model.
na.action	
A function which indicates what should happen when the data contains missing values. For the predict method, na.action = na.pass will predict missing values with NA; na.omit or na.exclude will drop them.
drop.unused.levels	
Should factors have unused levels dropped? Defaults to FALSE.
xlev	
A named list of character vectors giving the full set of levels to be assumed for each factor.
sparse	
Should the model matrix be in sparse format? This can save memory when dealing with many factor variables, each with many levels.
use.model.frame	
Should the base model.frame function be used when constructing the model matrix? This is the standard method that most R modelling functions use, but has some disadvantages. The default is to avoid model.frame and construct the model matrix term-by-term; see discussion.
object	
For the predict and coef methods, an object returned by cva.glmnet.
newx	
For the predict method, a matrix of predictor variables.
which	
An alternative way of specifying alpha; the index number of the desired value within the alpha vector. If both which and alpha are supplied, the former takes precedence.
newdata	
For the predict and coef methods, a data frame containing the observations for which to calculate predictions.
cv.type	
For minlossplot, which cross-validated loss value to plot for each value of alpha. This can be either "min" which is the minimum loss, or "1se" which is the highest loss within 1 standard error of the minimum. The default is "1se".
   
There are two ways in which the matrix of predictors can be generated. 
The default, with use.model.frame = FALSE, is to process the additive terms 
in the formula independently. With wide datasets, this is much faster and more memory-efficient than the standard R approach which uses the model.frame and model.matrix functions. However, the resulting model object is not exactly the 
same as if the standard approach had been used; in particular, it lacks a bona 
fide terms object. If you require interoperability with other packages that 
assume the standard model object structure, set use.model.frame = TRUE. 
See discussion for more information on this topic.

The predict method computes predictions for a specific alpha value given a cva.glmnet object. It looks up the supplied alpha (possibly supplied indirectly via the which argument) in the object's stored alpha vector, and calls glmnet:::predict.cv.glmnet on the corresponding cv.glmnet fit. All the arguments to that function are (or should be) supported.

The coef method is similar, returning the coefficients for the selected alpha value via glmnet:::coef.cv.glmnet.

The plot method for cva.glmnet objects plots the average cross-validated loss by lambda, for each value of alpha. Each line represents one cv.glmnet fit, corresponding to one value of alpha. Note that the specific lambda values can vary substantially by alpha.

The minlossplot function gives the best (lowest) cross-validated loss for each value of alpha.

Value

For cva.glmnet.default, an object of class cva.glmnet. This is a list containing the following:

alpha The vector of alpha values

nfolds The number of folds

modlist A list of cv.glmnet objects, containing the cross-validation results for each value of alpha

The function cva.glmnet.formula adds a few more components to the above, to facilitate working with formulas.

For the predict method, a vector or matrix of predicted values.

For the coef method, a vector of regularised regression coefficients.

See Also

glmnet::cv.glmnet

glmnet::predict.cv.glmnet, glmnet::coef.cv.glmnet

cva.glmnet, glmnet::cv.glmnet, plot

Examples

cva <- cva.glmnet(mpg ~ ., data=mtcars)
predict(cva, mtcars, alpha=1)

## Not run: 

# Leukemia example dataset from Trevor Hastie's website
download.file("http://web.stanford.edu/~hastie/glmnet/glmnetData/Leukemia.RData",
              "Leukemia.RData")
load("Leukemia.Rdata")
leuk <- do.call(data.frame, Leukemia)
leuk.cva <- cva.glmnet(y ~ ., leuk, family="binomial")
leuk.pred <- predict(leuk.cva, leuk, which=6)

## End(Not run)

* Note that the loop over alpha can be parallelised via 'parLapply' in the 
parallel package. To use this, optionally set outerParallel to a valid cluster 
object created by makeCluster or via 'rxExec' as supplied by Microsoft R 
Server's RevoScaleR package. To use this, set outerParallel to a valid compute 
context created by RxComputeContext, or a character string specifying such a 
context.  If the outer loop is run in parallel, cva.glmnet can check if the 
inner loop (over lambda) is also set to run in parallel, and disable this if 
it would lead to contention for cores. This is done if it is likely that the 
parallelisation is local on a multicore machine, ie if outerParallel is a 
SOCKcluster object running on "localhost", or if the RevoScaleR compute 
context is local parallel.

~~~~
LASSO has been a popular algorithm for the variable selection and extremely effective with high-dimension data. However, it often tends to “over-regularize” a model that might be overly compact and therefore under-predictive.

The Elastic Net addresses the aforementioned “over-regularization” by balancing between LASSO and ridge penalties. In particular, a hyper-parameter, namely Alpha, would be used to regularize the model such that the model would become a LASSO in case of Alpha = 1 and a ridge in case of Alpha = 0. In practice, Alpha can be tuned easily by the cross-validation. Below is a demonstration of Elastic Net with R glmnet package and its comparison with LASSO and ridge models. 

First of all, we estimates a LASSO model with Alpha = 1. The function cv.glmnet() is used to search for a regularization parameter, namely Lambda, that controls the penalty strength. As shown below, the model only identifies 2 attributes out of total 12.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
	
# LASSO WITH ALPHA = 1
cv1 <- cv.glmnet(mdlX, mdlY, family = "binomial", nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = 1)
md1 <- glmnet(mdlX, mdlY, family = "binomial", lambda = cv1$lambda.1se, alpha = 1)
coef(md1)
#(Intercept) -1.963030e+00
#AGE          .           
#ACADMOS      .           
#ADEPCNT      .           
#MAJORDRG     .           
#MINORDRG     .           
#OWNRENT      .           
#INCOME      -5.845981e-05
#SELFEMPL     .           
#INCPER       .           
#EXP_INC      .           
#SPENDING     .           
#LOGSPEND    -4.015902e-02
roc(newY, as.numeric(predict(md1, newX, type = "response")))
#Area under the curve: 0.636

We next estimates a ridge model as below by setting Alpha = 0. Similarly, Lambda is searched by the cross-validation. Since the ridge penalty would only regularize the magnitude of each coefficient, we end up with a “full” model with all model attributes. The model performance is slightly better with 10 more variables, which is a debatable outcome.

At last, we use the Elastic Net by tuning the value of Alpha through a line search with the parallelism. In this particular case, Alpha = 0.3 is chosen through the cross-validation. As shown below, 6 variables are used in the model that even performs better than the ridge model with all 12 attributes. 
~~~

# About the train() function in the caret package.  The goal is to 
# automatically split the data, fit the models and assess model performance. 
# One example is from Cross-Validation for Predictive Analytics Using R
# http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/

#
# The train() function requires 1) the model formula, 2) the indication of the 
# model to fit, 3) the grid of tuning parameter values to use, specified 
# through the tuneGrid argument, 4) the method to choose optimal tuning 
# parameter values (in our case, 5-fold cross-validation) through the trControl 
# argument.  The preProcess argument specifies pre-processing operations on the 
# predictors (in our case, centering and scaling the predictor values). 


Next Steps: 
-- check into stylr-packag

Variable naming convention:   
# ~~~~~~~~~~~~~~~~~~~~~~~~~
gpg_layers          names of layers in gpkg file for the watershed summaries 
   _gage_summary 
   _prr_wq_sites
   _wbd_summary

wsd_summary     zonal statistics of watershed environmental parameters 
*   _id          unique id
*  _sta_id      four- or seven-digit station ID   
   _type        distinguishes gaged from ungaged 
   _watshed     describes the HUC06 watershed 
   _HUC12       hydrologic unit code 12
   _sta_name    station name derived from outlet HUC12 catchment name
   _gage_num    USGS site number 
   _gage_nm     station name from USGS gage
   _dec_lat     latitude in decimal degrees
   _dec_lon     longitude in decimal degrees
   _cat_area    catchment area in sq km
*  _cat_area_l  natural logarithm of catchment area 
   _cat_length  catchment length
   _cat_width   catchment width
*  _lw_ratio    catchment length divided by catchment width
   _str_len     stream length
*  _drain_dens  stream length divided by catchment area
*  _prcp_mean   areal mean precipitation depth 1992-2012
*  _t07_mean    average July temperature 1992-2012
*  _vpd_ann     areal mean of max vapor deficit 1992-2012
*  _vpd_07      areal mean of max July vapor deficit 1992-2012
*  _cat_out     catchment outlet elevation
*  _cat_rel     difference in max and min elevation
*  _slop_med    median percent slope
*  _TWI_mean    mean terrain wetness index
*  _perc_cov    percent forest cover from NLCD 2016
*  _fc_mean     mean field capacity
*  _ksat_mean   mean horizontal saturated hydraulic conductivity
*  _kvert_mean  mean vertical saturated hydrologic conductivity 

* indicates variables used in EDA using 'env_vars'

trainIndex      index variable for data splitting 
test            environmental parameters for gaged watersheds - test set 
train           environmental parameters for gaged watersheds - training set 
  _area_linv    recropocal of the natural logarithm of catchment area 
  _cout_sqr     square-root of catchment outlet elevation
  _crel_sqr     square-root of catchment relief
  _dden_sqr     square-root of drainage density
  _fc_mean      mean field capacity
  _kvert_ln     natural log of mean vertical saturated hydrologic conductivity 
  _lwrat_sqr    square-root of catchment outlet elevation
  _pcov_ln      natural log of percent forest cover from NLCD 2016
  _prcp_sq      square of areal mean precipitation depth 1992-2012
  _slop_ln      natural log of median percent slope 
  _t07_sq       square of average July temperature 1992-2012
  _TWI_sq       square of mean terrain wetness index
  _vpdan_sq     square of areal mean of max vapor deficit 1992-2012





vars_ung        environmental parameters for ungaged watersheds
gage_summary    input data used in geospatial analysis 
*   _sta        
*   _log_q1_dep 
*   _log_q7_dep 
*   _log_q30_de 
   _.class 
   _.uncertain 
   _site_no 
   _min_year 
   _max_year 
   _num_year 
   _agency_cd 
   _station_nm 
   _dec_lat_va 
   _dec_long_v 
   _state_cd 
   _county_cd 
   _drain_area 
   _contrib_dr 
   _huc_cd 
   _alt_va 
   _geom    

        




-->

```{r setup, include=FALSE}
# set up global options ------------------------------------------------------
knitr::opts_chunk$set(echo = TRUE)
options(tibble.print_max = 70) # sets tibble output for printing

# load libraries -------------------------------------------------------------
library("here")         # identifies where to save work
library("janitor")      # tools for examining and cleaning dirty data
library("sf")
library("funModeling")  # functions related to exploratory data analysis 
library("Hmisc")        # functions useful for data analysis
library("DataExplorer") # functions related to exploratory data analysis 
library("broom")        # sweep up results into tidy frames 

library("glmnet")        # fit a GLM with lasso or elasticnet regularization 
library("caret")         # classification and regression training 
library("psych")         # for function tr() to compute trace of a matrix
#library("doMC")



#library("rsample")      # functions to create and summarize cross-validation  
#library("scales")       # scales for automatically determining graphical breaks 
#library("mlbench")      # artificial and real-world machine learning datasets 
#library("kernlab")        # kernel-based machine learning methods 
#library(sessioninfo)

library("tidyverse")     # data munging tools 


#library("glmnetUtils")   # formula interface for 'glmnet' elasticnet regression 

#library("styler")

#library("RCurl")        # General Network (HTTP/FTP/...) Client Interface for R 
#library("prettyR")      # Pretty descriptive stats
# library("lubridate")     # easier dates

# set seed for reproducibility & theme for printing  -------------------------
seed <- 1974     # seed for reproducibility - my birth year 
theme_set(theme_bw()) 

# check number of cores for parallel processing
# numCores <- detectCores()


# Create functions ------------------------------------------------------------
# basic EDA function from 
# https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/
basic_eda <- function(data)      
{                                
  glimpse(data)
  df_status(data)
  freq(data) 
  profiling_num(data)
  plot_num(data)
  describe(data)
}
```


```{r import_data} 

# check layer names for the project geopackage 
gpg_layers <- st_layers("sp_data/eco-drought.gpkg")$name[1:5] %>% 
  tibble::enframe(.) %>% 
  select(value) 

# read in geopackage data of zonal summaries for watersheds 
gage_summary <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "gage_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry()

wsd_summary <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "wbd_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry() 

# convert environmental data to tibbles 
gage_summary <- as_tibble(gage_summary) %>% 
  modify_if(., is.factor, as.character) 

wsd_summary <- as_tibble(wsd_summary) %>% 
  modify_if(., is.factor, as.character) %>% 
  select(id, everything()) 

```

```{r munge_explore_enviro_data}
env_vars <-  wsd_summary %>% 
  filter(type == "gaged") %>% 
  select(sta_id, cat_area_l, lw_ratio, drain_dens, prcp_mean, t07_mean, 
         vpd_ann, vpd_07, cat_out, cat_rel, slop_med, TWI_mean, perc_cov, 
         fc_mean, ksat_mean, kvert_mean) %>% 
  rename(cat_area_ln = cat_area_l) 

basic_eda(env_vars) 

```

```{r EDA_01-box-cox}

# use Box Cox to estimate transforms 
lambda <- env_vars %>% 
  select(-sta_id) 

lambda <- enframe(
                  sapply(lambda, forecast::BoxCox.lambda)
                  ) %>% 
  rename(sta = name) %>% 
  rename(lambda_val = value) %>% 
  arrange(lambda_val) 

``` 

```{r EDA_02-transforms} 

# transform variables in the following way: 
## variable    lambda_val     transform
## ------------------------------------------
## cat_area_ln	-1.00			    reciprocal
## cat_out	     0.36			    square root 
## cat_rel       0.46			    square root 			
## drain_dens	   0.66			    square root 			
## fc_mean	     1.36 
## kvert_mean	  -0.13			    natural log 
## ksat_mean  	-0.11			    natural log 
## lw_ratio	    -0.18			    natural log 
## perc_cov	     0.17			    natural log 
## prcp_mean	   2.00			    square
## slop_med	    -0.13			    natural log 
## t07_mean	     2.00			    square
## TWI_mean	     2.00				  square
## vpd_ann	     2.00				  square		
## vpd_07	       2.00					square	

env_vars <- env_vars %>% 
  mutate(area_linv  = 1/cat_area_ln) %>% 
  mutate(cout_sqr   = sqrt(cat_out)) %>% 
  mutate(crel_sqr   = sqrt(cat_rel)) %>% 
  mutate(dden_sqr   = sqrt(drain_dens)) %>% 
  mutate(ksat_ln    = log(ksat_mean)) %>% 		
  mutate(kvert_ln   = log(kvert_mean)) %>% 
  mutate(lwrat_sqr  = log(lw_ratio)) %>% 
  mutate(pcov_ln    = log(1 + perc_cov)) %>% 	
  mutate(prcp_sq    = prcp_mean^2) %>% 
  mutate(slop_ln    = log(slop_med)) %>%   
  mutate(t07_sq     = t07_mean^2) %>%     
  mutate(TWI_sq     = TWI_mean^2) %>%   
  mutate(vpd07_sq   = vpd_07^2) %>% 	
  mutate(vpdan_sq   = vpd_ann^2) %>%   		
  select(area_linv, cat_area_ln, cout_sqr, cat_out, crel_sqr, cat_rel, 
         dden_sqr, drain_dens, fc_mean, ksat_ln, ksat_mean, kvert_ln, 
         kvert_mean, lwrat_sqr, lw_ratio, pcov_ln, perc_cov, prcp_sq, 
         prcp_mean, slop_ln, slop_med, t07_sq, t07_mean, TWI_sq, 
         vpd07_sq, vpd_07, vpd_ann, vpdan_sq) 

# plot quantiles 
plot_qq(env_vars) 

```

```{r EDA_03-correlation} 

# select environmental variables 
env_vars <- env_vars %>% 
  select(area_linv, cout_sqr, crel_sqr, dden_sqr, fc_mean, ksat_ln, 
         kvert_ln, lwrat_sqr, pcov_ln, prcp_sq, slop_ln, t07_sq, 
         TWI_sq, vpd07_sq, vpdan_sq) 

# correlation plots 
plot_correlation(env_vars) 

# highly correlated (> 0.65) data 
## variable    transform_var    correlated vars             drop?
## ---------------------------------------------------------------------------
## drain_dens	  dden_sqr			  fc_mean, prcp_sq, 
##                              ksat_ln, kvert_ln			      maybe
## fc_mean	                    ksat_ln, k_vert_ln, 
##                              dden_sqr           
## ksat_mean  	ksat_ln 		    dden_sqrm fc_mean,
##                              kvert_ln                    yes
## kvert_mean	  kvert_ln        dden_sqr, fc_mean, 
##                              ksat_ln 
## lw_ratio	    lwrat_sqr		     
## prcp_mean	 	prcp_sq         vpd_ann, vpd_07	
## perc_cov	    pcov_ln		      slop_ln, t07_ln            
##                              TWI_sq, vpd07_sq
##                              vpdan_sq                    maybe
## slop_med	    slop_ln         pcov_ln, TWI_sq 
## t07_mean	    t07_sq          pcov_ln, vpdann_sq, 
##                              vpd07_sq 
## TWI_mean	    TWI_sq 				  pcov_ln, slop_ln
## vpd_ann	    vpdan_sq			  pcov_ln, prcp_sq, 
##                              t07_mean, vpd07_sq 
## vpd_07	      vpd07_sq			  pcov_ln, prcp_sq, 
##                              t07_mean, vpdan_sq          yes

``` 

```{r EDA_04-response_var} 

# transformed environmental vars to approximate gaussian 
gaged <- wsd_summary %>% 
  filter(type == "gaged") %>%  
  mutate(area_linv  = 1/cat_area_l) %>%  
  mutate(cout_sqr   = sqrt(cat_out)) %>% 
  mutate(crel_sqr   = sqrt(cat_rel)) %>% 
  mutate(dden_sqr   = sqrt(drain_dens)) %>% 
  mutate(kvert_ln   = log(kvert_mean)) %>% 
  mutate(lwrat_sqr  = log(lw_ratio)) %>% 
  mutate(pcov_ln    = log(1 + perc_cov)) %>% 	
  mutate(prcp_sq    = prcp_mean^2) %>% 
  mutate(slop_ln    = log(slop_med)) %>%   
  mutate(t07_sq     = t07_mean^2) %>%     
  mutate(TWI_sq     = TWI_mean^2) %>%   
  mutate(vpdan_sq   = vpd_ann^2) %>%   		
  select(sta_id, area_linv, cout_sqr, crel_sqr, dden_sqr, fc_mean, 
         kvert_ln, lwrat_sqr, pcov_ln, prcp_sq, slop_ln, t07_sq, 
         TWI_sq, vpdan_sq) 

# response variable needs to be an ordered PC for training/test splits 
resp_vars <- gage_summary %>% 
  select(sta, log_q1_dep, log_q7_dep, log_q30_de) 

pca <- prcomp(resp_vars[,c(2:4)], 
              center = TRUE, scale. = TRUE) 

tidy(pca, "pcs") 

resp_vars <- augment(pca, data = resp_vars) %>% 
  select(sta_id = sta, hydro_exp = .fittedPC1) 
  
# prepare for regression 
gaged <- full_join(gaged, resp_vars, 
                   by = "sta_id") %>% 
  arrange(hydro_exp) %>% 
  select(sta_id, hydro_exp, everything()) 

# DELETE THIS
# a factor variable is required for splitting into training and test data 
#gaged <- gaged %>% 
#  mutate(hydro_grp = case_when(
#    row_num < 15 ~ "low",
#    row_num < 29 ~ "med",
#    TRUE ~ "high")
#    ) %>% 
#  mutate(hydro_grp = as.factor(hydro_grp))

# clean up global environment 
rm(env_vars, gage_summary, gpg_layers, lambda, pca, resp_vars)  
```

```{r split_train/test_data}


# create a training index based on hydrologic export coefficient 
outcomes <- gaged$hydro_exp 
groups <- min(5, length(outcomes))
trainIndex <- createDataPartition(outcomes, 
                                  p = .8, list = FALSE, times = 1)

# create a training and a test set 
train <- gaged[trainIndex,] 
test  <- gaged[-trainIndex,] 

```


```{r ridge-regression-01}
# Load libraries, get data & set seed for reproducibility ---------------------



data("mtcars")
# Center and standardize response vars in the modelling function
y <- mtcars %>% 
  select(mpg) %>% 
  scale(center = TRUE, 
        scale = FALSE) %>% 
  as.matrix()

X <- mtcars %>% 
  select(-mpg) %>% 
  scale(center = TRUE, 
        scale = FALSE) %>% 
  as.matrix() 

# my data
y <- train %>% 
  select(hydro_exp) %>% 
  scale(center = TRUE, 
        scale = TRUE) %>% 
  as.matrix() 

X <- train %>% 
  select(-c(hydro_exp, sta_id)) %>% 
    scale(center = TRUE, 
        scale = FALSE) %>% 
  as.matrix() 


# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100) 

# Setting alpha = 0 implements ridge regression
ridge_cv <- cv.glmnet(X, y,    
                      alpha = 0, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)

# Plot cross-validation results
plot(ridge_cv) 



```

```{r ridge-regression-02}

# Best cross-validated lambda
lambda_cv <- ridge_cv$lambda.min 

# Fit final model, 
model_cv <- glmnet(X, y, 
                   alpha = 0, lambda = lambda_cv, 
                   standardize = TRUE)

# get sum of squared residuals and multiple R-squared
y_hat_cv <- predict(model_cv, X)
ssr_cv <- t(y - y_hat_cv) %*% (y - y_hat_cv)
rsq_ridge_cv <- cor(y, y_hat_cv)^2

# Use information criteria to select lambda -----------------------------------
X_scaled <- scale(X)
aic <- c()
bic <- c()
for (lambda in seq(lambdas_to_try)) {
  # Run model
  model <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try[lambda], 
                  standardize = TRUE)
  # Extract coefficients and residuals (remove first row for the intercept)
  betas <- as.vector((as.matrix(coef(model))[-1, ]))
  resid <- y - (X_scaled %*% betas)
  # Compute hat-matrix and degrees of freedom
  ld <- lambdas_to_try[lambda] * diag(ncol(X_scaled))
  H <- X_scaled %*% solve(t(X_scaled) %*% X_scaled + ld) %*% t(X_scaled)
  df <- tr(H)
  # Compute information criteria
  aic[lambda] <- nrow(X_scaled) * log(t(resid) %*% resid) + 2 * df
  bic[lambda] <- nrow(X_scaled) * log(t(resid) %*% resid) + 2 * df * log(nrow(X_scaled))
}


ic_model <- tibble( 
  lambda = lambdas_to_try, 
  aic = aic, 
  bic = bic
  ) %>% 
  gather(key = "param", value = "value", -lambda)


ggplot(ic_model, aes(lambda, value)) + 
  scale_x_log10() + 
  geom_line() + 
  facet_grid(param ~ .) + 
  labs(y = "Information Criterion") 
```

```{r}
# Optimal lambdas according to both criteria
lambda_aic <- lambdas_to_try[which.min(aic)]
lambda_bic <- lambdas_to_try[which.min(bic)]

# Fit final models, get their sum of squared residuals and multiple R-squared
model_aic <- glmnet(X, y, alpha = 0, lambda = lambda_aic, standardize = TRUE)
y_hat_aic <- predict(model_aic, X)
ssr_aic <- t(y - y_hat_aic) %*% (y - y_hat_aic)
rsq_ridge_aic <- cor(y, y_hat_aic)^2

model_bic <- glmnet(X, y, alpha = 0, lambda = lambda_bic, standardize = TRUE)
y_hat_bic <- predict(model_bic, X)
ssr_bic <- t(y - y_hat_bic) %*% (y - y_hat_bic)
rsq_ridge_bic <- cor(y, y_hat_bic)^2


# See how increasing lambda shrinks the coefficients --------------------------
# Each line shows coefficients for one variables, for different lambdas.
# The higher the lambda, the more the coefficients are shrinked towards zero.
res <- glmnet(X, y, alpha = 0, lambda = lambdas_to_try, standardize = FALSE)


tidied <- tidy(res) %>% filter(term != "(Intercept)")
ggplot(tidied, aes(lambda, estimate, group = term)) +
        geom_line() + scale_x_log10()


ggplot(res_tidy, aes(lambda, estimate)) + 
  scale_x_log10() + 
  geom_line() + 
  facet_grid(term ~ .) + 
  labs(y = "Information Criterion") 
plot(res, xvar = "lambda") 
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .7)
```


```{r}
# Calculate the weights from univariate regressions
weights <- sapply(seq(ncol(X)), function(predictor) {
  uni_model <- lm(y ~ X[, predictor])
  coeff_variance <- summary(uni_model)$coefficients[2, 2]^2
})


# Heteroskedastic Ridge Regression loss function - to be minimized
hridge_loss <- function(betas) {
  sum((y - X %*% betas)^2) + lambda * sum(weights * betas^2)
}


# Heteroskedastic Ridge Regression function
hridge <- function(y, X, lambda, weights) {
  # Use regular ridge regression coefficient as initial values for optimization
  model_init <- glmnet(X, y, alpha = 0, lambda = lambda, standardize = FALSE)
  betas_init <- as.vector(model_init$beta)
  # Solve optimization problem to get coefficients
  coef <- optim(betas_init, hridge_loss)$par
  # Compute fitted values and multiple R-squared
  fitted <- X %*% coef
  rsq <- cor(y, fitted)^2
  names(coef) <- colnames(X)
  output <- list("coef" = coef,
                 "fitted" = fitted,
                 "rsq" = rsq)
  return(output)
}


# Fit model to the data for lambda = 0.001
hridge_model <- hridge(y, X, lambda = 0.001, weights = weights)
rsq_hridge_0001 <- hridge_model$rsq

# Cross-validation or AIC/BIC can be employed to select some better lambda!
# You can find some useful functions for this at https://github.com/MichalOleszak/momisc/blob/master/R/hridge.R


```



```{r}
# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)
# Setting alpha = 1 implements lasso regression
lasso_cv <- cv.glmnet(X, y, alpha = 1, lambda = lambdas_to_try,
                      standardize = TRUE, nfolds = 10)
# Plot cross-validation results
plot(lasso_cv)
```

```{r}
# Best cross-validated lambda
lambda_cv <- lasso_cv$lambda.min
# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(X, y, alpha = 1, lambda = lambda_cv, standardize = TRUE)
y_hat_cv <- predict(model_cv, X)
ssr_cv <- t(y - y_hat_cv) %*% (y - y_hat_cv)
rsq_lasso_cv <- cor(y, y_hat_cv)^2

# See how increasing lambda shrinks the coefficients --------------------------
# Each line shows coefficients for one variables, for different lambdas.
# The higher the lambda, the more the coefficients are shrinked towards zero.
res <- glmnet(X, y, alpha = 1, lambda = lambdas_to_try, standardize = FALSE)



tidied <- tidy(res) %>% filter(term != "(Intercept)")
ggplot(tidied, aes(lambda, estimate, group = term)) +
        geom_line() + scale_x_log10()

```

```{r}
rsq <- cbind("R-squared" = c(rsq_ridge_cv, rsq_ridge_aic, rsq_ridge_bic, rsq_hridge_0001, rsq_lasso_cv))
rownames(rsq) <- c("ridge cross-validated", "ridge AIC", "ridge BIC", "hridge 0.001", "lasso cross_validated")
print(rsq)

##                       R-squared
## ridge cross-validated 0.8536968
## ridge AIC             0.8496310
## ridge BIC             0.8412011
## hridge 0.001          0.7278277
## lasso cross_validated 0.8426777 

rsq <- cbind("R-squared" = c(rsq_ridge_cv, rsq_ridge_aic, rsq_ridge_bic, rsq_hridge_0001, rsq_lasso_cv))
rownames(rsq) <- c("ridge cross-validated", "ridge AIC", "ridge BIC", "hridge 0.001", "lasso cross_validated")
print(rsq)

##   R-squared
## ridge cross-validated      0.89
## ridge AIC                  0.78
## ridge BIC                  0.77
## hridge 0.001               0.26
## lasso cross_validated      0.88
```

```{r}
library(caret)

# Set training control
train_control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_net_model <- train(hydro_exp ~ .,
                           data = cbind(y, X),
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 25,
                           trControl = train_control)

# Check multiple R-squared
y_hat_enet <- predict(elastic_net_model, X)
rsq_enet <- cor(y, y_hat_enet)^2 

# Best tuning parameter
elastic_net_model$bestTune 

coef(elastic_net_model$finalModel, elastic_net_model$bestTune$lambda) 

14 x 1 sparse Matrix of class "dgCMatrix"
                   1
(Intercept) -4.0e-17
area_linv    7.6e-02
cout_sqr    -8.9e-02
crel_sqr     .      
dden_sqr    -1.8e-01
fc_mean     -6.0e-01
kvert_ln     1.3e-01
lwrat_sqr   -8.2e-02
pcov_ln      .      
prcp_sq      1.4e-01
slop_ln      .      
t07_sq       .      
TWI_sq       .      
vpdan_sq     .      
```


```{r}

# First step: 
# estimate a LASSO model, Alpha = 1, by identifying Lambda 

# The function cv.glmnet() is used to search for a regularization parameter, namely Lambda, that controls the penalty strength. As shown below, the model only identifies 2 attributes out of total 12.  

# Example from https://www.r-bloggers.com/variable-selection-with-elastic-net/
# Data from http://pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm 
df1 <- read.csv("example_data/credit_count.csv") 
df2 <- df1[df1$CARDHLDR == 1, ]
set.seed(2017)
n <- nrow(df2)
sample <- sample(seq(n), size = n * 0.5, replace = FALSE) # 50/50 split
train <- df2[sample, -1] 
test <- df2[-sample, -1]

mdlY <- as.factor(as.matrix(train["DEFAULT"]))
mdlX <- as.matrix(train[setdiff(colnames(df1), c("CARDHLDR", "DEFAULT"))])
newY <- as.factor(as.matrix(test["DEFAULT"]))
newX <- as.matrix(test[setdiff(colnames(df1), c("CARDHLDR", "DEFAULT"))])

First of all, we estimates a LASSO model with Alpha = 1. The function cv.glmnet() is used to search for a regularization parameter, namely Lambda, that controls the penalty strength. As shown below, the model only identifies 2 attributes out of total 12.

# Set the model parameters & register cores 
x <- mdlX
y <- mdlY 
family <- "binomial" 
nfold <- 10
type.measure <- "deviance" 
parallel <- TRUE 
alpha <- 1 

# LASSO WITH ALPHA = 1

cv1 <- cv.glmnet(x, y, family = family, 
                 nfold = nfold, 
                 type.measure = type.measure, 
                 parallel = parallel, 
                 alpha = alpha) 


md1 <- glmnet(x, y, family = "binomial", lambda = cv1$lambda.1se, alpha = 1)
coef(md1)
#(Intercept) -1.963030e+00
#AGE          .           
#ACADMOS      .           
#ADEPCNT      .           
#MAJORDRG     .           
#MINORDRG     .           
#OWNRENT      .           
#INCOME      -5.845981e-05
#SELFEMPL     .           
#INCPER       .           
#EXP_INC      .           
#SPENDING     .           
#LOGSPEND    -4.015902e-02
roc(newY, as.numeric(predict(md1, newX, type = "response")))
#Area under the curve: 0.636

We next estimates a ridge model as below by setting Alpha = 0. Similarly, Lambda is searched by the cross-validation. Since the ridge penalty would only regularize the magnitude of each coefficient, we end up with a “full” model with all model attributes. The model performance is slightly better with 10 more variables, which is a debatable outcome.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
	
# RIDGE WITH ALPHA = 0
cv2 <- cv.glmnet(mdlX, mdlY, family = "binomial", nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = 0)
md2 <- glmnet(mdlX, mdlY, family = "binomial", lambda = cv2$lambda.1se, alpha = 0)
coef(md2)
#(Intercept) -2.221016e+00
#AGE         -4.184422e-04
#ACADMOS     -3.085096e-05
#ADEPCNT      1.485114e-04
#MAJORDRG     6.684849e-03
#MINORDRG     1.006660e-03
#OWNRENT     -9.082750e-03
#INCOME      -6.960253e-06
#SELFEMPL     3.610381e-03
#INCPER      -3.881890e-07
#EXP_INC     -1.416971e-02
#SPENDING    -1.638184e-05
#LOGSPEND    -6.213884e-03
roc(newY, as.numeric(predict(md2, newX, type = "response")))
#Area under the curve: 0.6435

At last, we use the Elastic Net by tuning the value of Alpha through a line search with the parallelism. In this particular case, Alpha = 0.3 is chosen through the cross-validation. As shown below, 6 variables are used in the model that even performs better than the ridge model with all 12 attributes.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
	
# ELASTIC NET WITH 0 < ALPHA < 1
a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <- cv.glmnet(mdlX, mdlY, family = "binomial", nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = i)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
md3 <- glmnet(mdlX, mdlY, family = "binomial", lambda = cv3$lambda.1se, alpha = cv3$alpha)
coef(md3)
#(Intercept) -1.434700e+00
#AGE         -8.426525e-04
#ACADMOS      .           
#ADEPCNT      .           
#MAJORDRG     6.276924e-02
#MINORDRG     .           
#OWNRENT     -2.780958e-02
#INCOME      -1.305118e-04
#SELFEMPL     .           
#INCPER      -2.085349e-06
#EXP_INC      .           
#SPENDING     .           
#LOGSPEND    -9.992808e-02
roc(newY, as.numeric(predict(md3, newX, type = "response")))
#Area under the curve: 0.6449

Share
Tweet

To leave a comment for the author, please follow the link and comment on their blog: S+/R – Yet Another Blog in Statistical Computing.
R-bloggers.com offers daily e-mail updates about R news and tutorials on topics such as: Data science, Big Data, R jobs, visualization (ggplot2, Boxplots, maps, animation), programming (RStudio, Sweave, LaTeX, SQL, Eclipse, git, hadoop, Web Scraping) statistics (regression, PCA, time series, trading) and more...

If you got this far, why not subscribe for updates from the site? Choose your flavor: e-mail, twitter, RSS, or facebook...
[Privacy Badger has replaced this Facebook Like button.]
[Privacy Badger has replaced this Twitter button.]
[Privacy Badger has replaced this LinkedIn button.]

Comments are closed.
Search R-bloggers
Most visited articles of the week

    How to write the first for loop in R
    R Studio Shortcuts and Tips – part 2
    Learning R: The Ultimate Introduction (incl. Machine Learning!)
    Modern Data Science with R: A review
    5 Ways to Subset a Data Frame in R
    Part 2: Simple EDA in R with inspectdf
    R – Sorting a data frame by the contents of a column
    Using apply, sapply, lapply in R
    Installing R packages

Sponsors
Mango solutions

```




```{r}
 Set the model parameters & register cores 
x <- mdlX
y <- mdlY 
family <- "binomial" 
nfold <- 10
type.measure <- "deviance" 
parallel <- TRUE 
alpha <- 1 
```


The glmnetUtils package provides tools to streamline elastic net model fitting.  
1) convert a data frame into a predictor matrix and a response vector. 
2) Crossvalidation for α and λ via 'cva.glmnet' 


Under the hood, glmnetUtils creates a model matrix and response vector, and passes them to the glmnet package to do the actual model fitting. A simple print method is also provided, to show the main model details at a glance. I’ll describe shortly what the “sparse model matrix” and “use model.frame” options do.
Predicting from a model works as you’d expect: just pass a data frame containing the new observations to the predict method. You can also specify any arguments that predict.glmnet accepts.
# least squares regression: get predictions for lambda=1
predict(mtcarsMod, newdata=mtcars, s=1)

# multinomial logistic regression: get predicted class
predict(irisMod, newdata=iris, type="class")

# Poisson regression: need to specify offset
predict(InsMod, newdata=MASS::Insurance, offset=log(Holders))
If you want, you can still use the original model matrix-plus-response syntax:
mtcarsX <- as.matrix(mtcars[c("cyl", "disp", "hp")])
mtcarsY <- mtcars$mpg
mtcarsMod2 <- glmnet(mtcarsX, mtcarsY)

summary(as.numeric(predict(mtcarsMod, mtcars) - 
                   predict(mtcarsMod2, mtcarsX)))
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       0       0       0       0       0       0
This shows that the resulting models are identical, in terms of the predictions they make and the regularisation parameters used.
Generating the model matrix

There are two ways in which glmnetUtils can generate a model matrix out of a formula and data frame. The first is to use the standard R machinery comprising model.frame and model.matrix; and the second is to build the matrix one variable at a time.
Using model.frame

This is the simpler option, and the one that is most compatible with other R modelling functions. The model.frame function takes a formula and data frame and returns a model frame: a data frame with special information attached that lets R make sense of the terms in the formula. For example, if a formula includes an interaction term, the model frame will specify which columns in the data relate to the interaction, and how they should be treated. Similarly, if the formula includes expressions like exp(x) or I(x^2) on the RHS, model.frame will evaluate these expressions and include them in the output.
The major disadvantage of using model.frame is that it generates a terms object, which encodes how variables and interactions are organised. One of the attributes of this object is a matrix with one row per variable, and one column per main effect and interaction. At minimum, this is (approximately) a p×p
p
×
p
 square matrix where p
p
 is the number of main effects in the model. For wide datasets with p>10000
p
>
10000
, this matrix can approach or exceed a gigabyte in size. Even if there is enough memory to store such an object, generating the model matrix can be very slow.
Another issue with the standard R approach is the treatment of factors. Normally, model.matrix will turn an N
N
-level factor into an indicator matrix with N−1
N
−
1
 columns, with one column being dropped. This is necessary for unregularised models as fit with lm and glm, since the full set of N
N
columns is linearly dependent. With the usual treatment contrasts, the interpretation is that the dropped column represents a baseline level, while the coefficients for the other columns represent the difference in the response relative to the baseline.
This may not be appropriate for a regularised model as fit with glmnet. The regularisation procedure shrinks the coefficients towards zero, which forces the estimated differences from the baseline to be smaller. But this only makes sense if the baseline level was chosen beforehand, or is otherwise meaningful as a default; otherwise it is effectively making the levels more similar to an arbitrarily chosen level.
Manually building the model matrix

To deal with the problems above, glmnetUtils by default will avoid using model.frame, instead building up the model matrix term-by-term. This avoids the memory cost of creating a terms object, and can be noticeably faster than the standard approach. It will also include one column in the model matrix for all levels in a factor; that is, no baseline level is assumed. In this situation, the coefficients represent differences from the overall mean response, and shrinking them to zero is meaningful (usually).
This works in an additive fashion, ie the formula ~ a + b:c + d*e is treated as consisting of three terms, a, b:c and d*e each of which is processed independently of the others. A dot in the formula includes all main effect terms, ie ~ . + a:b + f(x) expands to ~ a + b + x + a:b + f(x) (assuming a, b and x are the only columns in the data). Note that a formula like ~ (a + b) + (c + d) will be treated as two terms, a + b and c + d.
Speed comparisons

To examine the speed impact of using model.frame, let’s do some simple comparisons of run times. We’ll generate sample datasets with 100, 1,000 and 10,000 predictors, and then run glmnet with both options for generating the model matrix.
# generate sample (uncorrelated) data of a given size
makeSampleData <- function(N, P)
{
    X <- matrix(rnorm(N*P), nrow=N)
    data.frame(y=rnorm(N), X)
}

# test for three sizes: 100/1000/10000 predictors
df1 <- makeSampleData(N=1000, P=100)
df2 <- makeSampleData(N=1000, P=1000)
df3 <- makeSampleData(N=1000, P=10000)

library(microbenchmark)
res <- microbenchmark(
    glmnet(y ~ ., df1, use.model.frame=TRUE), 
    glmnet(y ~ ., df1, use.model.frame=FALSE), 
    glmnet(y ~ ., df2, use.model.frame=TRUE), 
    glmnet(y ~ ., df2, use.model.frame=FALSE), 
    glmnet(y ~ ., df3, use.model.frame=TRUE), 
    glmnet(y ~ ., df3, use.model.frame=FALSE),
    times=10 
)
print(res, unit="s", digits=2)
## Unit: seconds
##                                         expr    min     lq   mean median     uq    max neval
##   glmnet(y ~ ., df1, use.model.frame = TRUE)  0.024  0.024  0.027  0.025  0.029  0.032    10
##  glmnet(y ~ ., df1, use.model.frame = FALSE)  0.023  0.026  0.029  0.026  0.028  0.051    10
##   glmnet(y ~ ., df2, use.model.frame = TRUE)  3.703  3.916  4.258  4.272  4.428  5.153    10
##  glmnet(y ~ ., df2, use.model.frame = FALSE)  3.756  3.874  4.291  4.352  4.561  5.073    10
##   glmnet(y ~ ., df3, use.model.frame = TRUE) 11.973 12.353 13.262 13.350 13.864 14.622    10
##  glmnet(y ~ ., df3, use.model.frame = FALSE)  4.295  4.639  4.992  4.822  5.060  6.111    10
From this, we can see that for datasets with up to 1,000 predictors, both methods are about as fast as each other. However, for 10,000 predictors (not uncommon these days), the model.frame method takes three times as long as building the model matrix term by term.
What happens if we take it up to 100,000 predictors? As seen below, the standard approach of using model.frame fails when R runs out of memory: the data frame itself is about 800MB in size, but trying to allocate the terms object requires more then 67GB. However, building the model matrix by term still works, and (on this machine) finishes in about two minutes.
df4 <- makeSampleData(N=1000, P=100000)

glmnet(y ~ ., df4, use.model.frame=TRUE)
## Warning in terms.formula(formula, data = data): Reached total allocation of
## 32666Mb: see help(memory.size)

## Error: cannot allocate vector of size 37.3 Gb
glmnet(y ~ ., df4, use.model.frame=FALSE)
## Call:
## glmnet.formula(formula = y ~ ., data = df4, use.model.frame = FALSE)
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha: 1
##     Lambda summary:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.002393 0.006506 0.017680 0.032470 0.048080 0.130700
Sparse model matrix

As an option, glmnetUtils can also generate a sparse model matrix, using the sparse.model.matrix function provided in the Matrix package. This works exactly the same as a regular model matrix, but takes up significantly less memory if many of its entries are zero. A scenario where this is the case would be where many of the predictors are factors, each with a large number of levels. This can be combined with both of the previously mentioned options for generating model matrices.
Crossvalidation for α
α

One piece missing from the standard glmnet package is a way of choosing α
α
, the elastic net mixing parameter, similar to how cv.glmnet chooses λ
λ
, the shrinkage parameter. To fix this, glmnetUtils provides the cva.glmnet function, which uses crossvalidation to examine the impact on the model of changing α
α
 and λ
λ
. The interface is the same as for the other functions:
# Leukemia dataset from Trevor Hastie's website:
# http://web.stanford.edu/~hastie/glmnet/glmnetData/Leukemia.RData
leuk <- do.call(data.frame, Leukemia)

leukMod <- cva.glmnet(y ~ ., data=leuk, family="binomial")
leukMod
## Call:
## cva.glmnet.formula(formula = y ~ ., data = leuk, family = "binomial")
## 
## Model fitting options:
##     Sparse model matrix: FALSE
##     Use model.frame: FALSE
##     Alpha values: 0 0.001 0.008 0.027 0.064 0.125 0.216 0.343 0.512 0.729 1
##     Number of crossvalidation folds for lambda: 10
cva.glmnet uses the algorithm described in the help for cv.glmnet, which is to fix the distribution of observations across folds and then call cv.glmnet in a loop with different values of α
α
. Optionally, you can parallelise this outer loop, by setting the outerParallel argument to a non-NULL value. Currently, glmnetUtils supports the following methods of parallelisation:
Via parLapply in the parallel package. To use this, set outerParallel to a valid cluster object created by makeCluster.
Via rxExec as supplied by Microsoft R Server’s RevoScaleR package. To use this, set outerParallel to a valid compute context created by RxComputeContext, or a character string specifying such a context.
If the outer loop is run in parallel, cva.glmnet can check if the inner loop (over λ
λ
) is also set to run in parallel, and disable this if it would lead to contention for cores.
Because crossvalidation is often a statistically noisy procedure, it doesn’t try to automatically choose α
α
 and λ
λ
 for you. Instead you can plot the output, to see how the results depend on the values of these parameters. Using this information, you can choose appropriate values for your data.
plot(leukMod)

In this case, we see that values of α
α
 close to 1
1
 tend to lead to better accuracy. The curves don’t have a well-defined minimum, but they do flatten out for lower values of λ
λ
. As the cv.glmnet documentation recommends though, it’s a good idea to run cva.glmnet multiple times to reduce the impact of noise.
A cva.glmnet object contains a list of individual cv.glmnet objects, corresponding to the different α
α
 values tried. This lets you plot the crossvalidation results easily for a given α
α
:
plot(leukMod$modlist[[10]])  # alpha = 0.729

Conclusion

The glmnetUtils package is a way to improve quality of life for users of glmnet. As with many R packages, it’s always under development; you can get the latest version from my GitHub repo. If you find a bug, or if you want to suggest improvements to the package, please feel free to contact me at hongooi@microsoft.com.


```{r glmnetUtils_example}
# Fit a model to glmnet by providing a formula and data frame, and other
# glmnet arguments 

# least squares regression
(mtcarsMod <- glmnet(mpg ~ cyl + disp + hp, data=mtcars)) 

# multinomial logistic regression with specified elastic net alpha parameter
(irisMod <- glmnet(Species ~ ., data=iris, family="multinomial", alpha=0.5))

# Leukemia dataset from Trevor Hastie's website:
# http://web.stanford.edu/~hastie/glmnet/glmnetData/Leukemia.RData
leuk <- do.call(data.frame, Leukemia) 

leukMod <- cva.glmnet(y ~ ., data=leuk, family="binomial")
leukMod
```


```{r}
# Randomly split the data into k subsets to balance the class distributions 
# within the splits. These functions return vectors of indexes that can then be used to subset the original sample into training and test sets.


predictors <- gaged %>% 
  select(area_linv:vpdan_sq) %>%
  names()
  
train_predictors <- train %>% 
  select(predictors)


 
train_predictors <- predictors[train_set, ]
train_classes <- classes[train_set]
test_predictors <- predictors[-train_set, ]
test_classes <- classes[-train_set]
 
set.seed(seed)
cv_splits <- createFolds(classes, k = 10, returnTrain = TRUE)
str(cv_splits)


rm(classes, trainIndex)  

# Model_method Value	  Type	   Libraries	     Tuning_Parameters
#  Elasticnet	 enet	 Regression	 elasticnet	     fraction, lambda 
```





```{r} 


set.seed(seed)
 
cs_data_train <- cs_data[train_set, ]
cs_data_test <- cs_data[-train_set, ]
 
glmnet_grid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                           lambda = seq(.01, .2, length = 20))



glmnet_ctrl <- trainControl(method = "cv", number = 10)
glmnet_fit <- train(Status ~ ., data = cs_data_train,
                    method = "glmnet",
                    preProcess = c("center", "scale"),
                    tuneGrid = glmnet_grid,
                    trControl = glmnet_ctrl)
glmnet_fit
```

```{r stuff}

# total data is 2

# simulate regression data from mlbench::mlbench.friedman1
sim_data <- function(n) {
  tmp <- mlbench.friedman1(n, sd=1)
  tmp <- cbind(tmp$x, tmp$y)
  tmp <- as.data.frame(tmp)
  names(tmp)[ncol(tmp)] <- "y"
  tmp
}


# generate a training set of 100 data points and a larger characterization set 
set.seed(9815) 
train_dat <- sim_data(100)
large_dat <- sim_data(10^5)

# create the tibble with the resampling specifications:
results <- nested_cv(train_dat, 
                     outside = vfold_cv(repeats = 5), 
                     inside = bootstraps(25))
results

## # 10-fold cross-validation repeated 5 times 
## # Nested : vfold_cv(repeats = 5) / bootstraps(25) 
## # A tibble: 50 x 4
##          splits      id    id2   inner_resamples
##          <list>   <chr>  <chr>            <list>
##  1 <S3: rsplit> Repeat1 Fold01 <tibble [25 x 2]>
##  2 <S3: rsplit> Repeat1 Fold02 <tibble [25 x 2]>
##  3 <S3: rsplit> Repeat1 Fold03 <tibble [25 x 2]>
##  4 <S3: rsplit> Repeat1 Fold04 <tibble [25 x 2]>
##  5 <S3: rsplit> Repeat1 Fold05 <tibble [25 x 2]>
##  6 <S3: rsplit> Repeat1 Fold06 <tibble [25 x 2]>
##  7 <S3: rsplit> Repeat1 Fold07 <tibble [25 x 2]>
##  8 <S3: rsplit> Repeat1 Fold08 <tibble [25 x 2]>
##  9 <S3: rsplit> Repeat1 Fold09 <tibble [25 x 2]>
## 10 <S3: rsplit> Repeat1 Fold10 <tibble [25 x 2]>
## # ... with 40 more rows

The splitting information for each resample is contained in the split objects. Focusing on the second fold of the first repeat:

results$splits[[2]]

## <90/10/100>

<90/10/100> indicates the number of data in the analysis set, assessment set, and the original data.

Each element of inner_resamples has its own tibble with the bootstrapping splits.

results$inner_resamples[[5]]

## # Bootstrap sampling with 25 resamples 
## # A tibble: 25 x 2
##          splits          id
##          <list>       <chr>
##  1 <S3: rsplit> Bootstrap01
##  2 <S3: rsplit> Bootstrap02
##  3 <S3: rsplit> Bootstrap03
##  4 <S3: rsplit> Bootstrap04
##  5 <S3: rsplit> Bootstrap05
##  6 <S3: rsplit> Bootstrap06
##  7 <S3: rsplit> Bootstrap07
##  8 <S3: rsplit> Bootstrap08
##  9 <S3: rsplit> Bootstrap09
## 10 <S3: rsplit> Bootstrap10
## # ... with 15 more rows

These are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:

results$inner_resamples[[5]]$splits[[1]]

## <90/37/90>

To start, we need to define how the model will be created and measured. For our example, a radial basis support vector machine model will be created using the function kernlab::ksvm. This model is generally thought of as having two tuning parameters: the SVM cost value and the kernel parameter sigma. For illustration, only the cost value will be tuned and the function kernlab::sigest will be used to estimate sigma during each model fit. This is automatically done by ksvm.

After the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. One important note: for this model, it is critical to center and scale the predictors before computing dot products. We don't do this operation here because mlbench.friedman1 simulates all of the predictors to be standard uniform random variables.

Our function to fit a single model and compute the RMSE is:

# `object` will be an `rsplit` object from our `results` tibble
# `cost` is the tuning parameter
svm_rmse <- function(object, cost = 1) {
  y_col <- ncol(object$data)
  mod <- ksvm(y ~ ., data = analysis(object),  C = cost)
  holdout_pred <- predict(mod, assessment(object)[-y_col])
  rmse <- sqrt(mean((assessment(object)$y - holdout_pred)^2, na.rm = TRUE))
  rmse
}

# In some case, we want to parameterize the function over the tuning parameter:
rmse_wrapper <- function(cost, object) svm_rmse(object, cost)

For the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, a wrapper can be created:

# `object` will be an `rsplit` object for the bootstrap samples
tune_over_cost <- function(object) {
  results <- tibble(cost = 2^seq(-2, 8, by = 1))
  results$RMSE <- map_dbl(results$cost, 
                          rmse_wrapper,
                          object = object)
  results
}

Since this will be called across the set of outer cross-validation splits, another wrapper is required:

# `object` is an `rsplit` object in `results$inner_resamples` 
summarize_tune_results <- function(object) {
  # Return row-bound tibble that has the 25 bootstrap results
  map_df(object$splits, tune_over_cost) %>%
    # For each value of the tuning parameter, compute the 
    # average RMSE which is the inner bootstrap estimate. 
    group_by(cost) %>%
    summarize(mean_RMSE = mean(RMSE, na.rm = TRUE),
              n = length(RMSE))
}

Now that those functions are defined, we can execute all the inner resampling loops:

tuning_results <- map(results$inner_resamples, summarize_tune_results) 

tuning_results is a list of data frames for each of the 50 outer resamples.

Let's make a plot of the averaged results to see what the relationship is between the RMSE and the tuning parameters for each of the inner bootstrapping operations:

pooled_inner <- tuning_results %>% bind_rows

best_cost <- function(dat) dat[which.min(dat$mean_RMSE),]

p <- ggplot(pooled_inner, aes(x = cost, y = mean_RMSE)) + 
  scale_x_continuous(trans='log2') +
  xlab("SVM Cost") + ylab("Inner RMSE")

for(i in 1:length(tuning_results))
  p <- p  + 
  geom_line(data = tuning_results[[i]], alpha = .2) + 
  geom_point(data = best_cost(tuning_results[[i]]), pch = 16)

p <- p + geom_smooth(data = pooled_inner, se = FALSE)
p

## `geom_smooth()` using method = 'loess'

rmse-plot-1.png

Each grey line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a loess smooth of all the results pooled together.

To determine the best parameter estimate for each of the outer resampling iterations:

cost_vals <- tuning_results %>% map_df(best_cost) %>% select(cost) 
results <- bind_cols(results, cost_vals)

ggplot(results, aes(x = factor(cost))) + geom_bar() + xlab("SVM Cost")

choose-1.png

Most of the resamples produced an optimal cost values of 2.0 but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.

Now that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:

results$RMSE <- map2_dbl(results$splits, results$cost, svm_rmse)
summary(results$RMSE)

##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.086   2.183   2.562   2.689   3.191   4.222

The RMSE estimate using nested resampling is 2.69.

What is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.

not_nested <- map(results$splits, tune_over_cost) %>%
  bind_rows

outer_summary <- not_nested %>% 
  group_by(cost) %>% 
  summarize(outer_RMSE = mean(RMSE),
            n = length(RMSE))
outer_summary

## # A tibble: 11 x 3
##      cost outer_RMSE     n
##     <dbl>      <dbl> <int>
##  1   0.25   3.565595    50
##  2   0.50   3.119439    50
##  3   1.00   2.775602    50
##  4   2.00   2.609950    50
##  5   4.00   2.639033    50
##  6   8.00   2.755651    50
##  7  16.00   2.831902    50
##  8  32.00   2.840183    50
##  9  64.00   2.833896    50
## 10 128.00   2.831717    50
## 11 256.00   2.836863    50

ggplot(outer_summary, aes(x = cost, y = outer_RMSE)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(trans='log2') +
  xlab("SVM Cost") + ylab("Inner RMSE")

not-nested-1.png

The non-nested procedure estimates the RMSE to be 2.61. Both estimates are fairly close and would end up choosing a cost parameter value of 2.0.

The approximately true RMSE for an SVM model with a cost value of 2.0 and be estimated with the large sample that was simulated at the beginning.

finalModel <- ksvm(y ~ ., data = train_dat, C = 2)
large_pred <- predict(finalModel, large_dat[, -ncol(large_dat)])
sqrt(mean((large_dat$y - large_pred)^2, na.rm = TRUE))

## [1] 2.696096

The nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar. There is some optimzation bias here but it is very small (for these data and this model).

The R markdown document used to create this post can be found here.

The session information is:

session_info()

## ─ Session info ──────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 3.3.3 (2017-03-06)
##  os       macOS Sierra 10.12.6        
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/New_York            
##  date     2017-09-03                  
## 
## ─ Packages ──────────────────────────────────────────────────────────────
##  package     * version  date       source         
##  assertthat    0.2.0    2017-04-11 CRAN (R 3.3.2) 
##  bindr         0.1      2016-11-13 CRAN (R 3.3.2) 
##  bindrcpp    * 0.2      2017-06-17 cran (@0.2)    
##  broom       * 0.4.2    2017-02-13 CRAN (R 3.3.2) 
##  clisymbols    1.2.0    2017-05-21 CRAN (R 3.3.2) 
##  colorspace    1.3-2    2016-12-14 CRAN (R 3.3.2) 
##  dplyr       * 0.7.2    2017-07-20 cran (@0.7.2)  
##  evaluate      0.10.1   2017-06-24 CRAN (R 3.3.2) 
##  foreign       0.8-67   2016-09-13 CRAN (R 3.3.3) 
##  ggplot2     * 2.2.1    2016-12-30 CRAN (R 3.3.2) 
##  glue          1.1.1    2017-06-21 CRAN (R 3.3.2) 
##  gtable        0.2.0    2016-02-26 CRAN (R 3.3.0) 
##  highr         0.6      2016-05-09 CRAN (R 3.3.0) 
##  kernlab     * 0.9-25   2016-10-03 CRAN (R 3.3.0) 
##  knitr       * 1.17     2017-08-10 CRAN (R 3.3.2) 
##  labeling      0.3      2014-08-23 CRAN (R 3.3.0) 
##  lattice       0.20-35  2017-03-25 CRAN (R 3.3.3) 
##  lazyeval      0.2.0    2016-06-12 CRAN (R 3.3.0) 
##  magrittr      1.5      2014-11-22 CRAN (R 3.3.0) 
##  mlbench     * 2.1-1    2012-07-10 CRAN (R 3.3.0) 
##  mnormt        1.5-5    2016-10-15 CRAN (R 3.3.0) 
##  munsell       0.4.3    2016-02-13 CRAN (R 3.3.0) 
##  nlme          3.1-131  2017-02-06 CRAN (R 3.3.3) 
##  pkgconfig     2.0.1    2017-03-21 cran (@2.0.1)  
##  plyr          1.8.4    2016-06-08 CRAN (R 3.3.0) 
##  psych         1.7.3.21 2017-03-22 CRAN (R 3.3.2) 
##  purrr       * 0.2.3    2017-08-02 cran (@0.2.3)  
##  R6            2.2.2    2017-06-17 cran (@2.2.2)  
##  Rcpp          0.12.12  2017-07-15 cran (@0.12.12)
##  reshape2      1.4.2    2016-10-22 CRAN (R 3.3.3) 
##  rlang         0.1.2    2017-08-09 cran (@0.1.2)  
##  rsample     * 0.0.1    2017-07-08 CRAN (R 3.3.3) 
##  scales      * 0.5.0    2017-08-24 CRAN (R 3.3.2) 
##  sessioninfo * 1.0.0    2017-06-21 CRAN (R 3.3.2) 
##  stringi       1.1.5    2017-04-07 CRAN (R 3.3.2) 
##  stringr       1.2.0    2017-02-18 CRAN (R 3.3.2) 
##  tibble        1.3.4    2017-08-22 cran (@1.3.4)  
##  tidyr         0.7.0    2017-08-16 cran (@0.7.0)  
##  withr         2.0.0    2017-07-28 CRAN (R 3.3.2)


```

```{r munge_gaged} 

# stratify sample ensure sampling for test data is screate threesplits in the data based on the hyd
gaged <- gaged %>% 
  











data(oil) 
createDataPartition(oilType, 2)

x <- rgamma(50, 3, .5) 
inA <- createDataPartition(x, list = FALSE)

plot(density(x[inA]))
rug(x[inA])

points(density(x[-inA]), type = "l", col = 4)
rug(x[-inA], col = 4)

createResample(oilType, 2)

createFolds(oilType, 10)
createFolds(oilType, 5, FALSE)

createFolds(rnorm(21))

createTimeSlices(1:9, 5, 1, fixedWindow = FALSE)
createTimeSlices(1:9, 5, 1, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = FALSE)

createTimeSlices(1:15, 5, 3)
createTimeSlices(1:15, 5, 3, skip = 2)
createTimeSlices(1:15, 5, 3, skip = 3)

set.seed(131)
groups <- sort(sample(letters[1:4], size = 20, replace = TRUE))
table(groups)
folds <- groupKFold(groups)
lapply(folds, function(x, y) table(y[x]), y = groups)


```



```{r datacamp-shrinkage-example-01}
# this is an example problem on shrinkage methods from:
# http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/ 

# data for this example is from:
# https://github.com/gastonstat/CreditScoring

url <- "https://raw.githubusercontent.com/gastonstat/CreditScoring/master/CleanCreditScoring.csv"


cs_data <- getURL(url)      
cs_data <- read.csv(textConnection(cs_data))  # not sure how this implements...
describe(cs_data) 
```

```{r datacamp-shrinkage-example-02}


classes <- cs_data[, "Status"] 
predictors <- cs_data[, -match(
  c("Status", "Seniority", "Time", "Age", "Expenses", 
    "Income", "Assets", "Debt", "Amount", "Price", "Finrat", "Savings"), 
 colnames(cs_data))]

# The caret package provides functions for splitting the data as well as functions that avutomatically do all the job for us, namely functions that create the resampled data sets, fit the models, and evaluate performance.

#Among the functions for data splitting I just mention createDataPartition() and createFolds(). The former allows to create one or more test/training random partitions of the data, while the latter randomly splits the data into k subsets. In both functions the random sampling is done within the levels of y (when y is categorical) to balance the class distributions within the splits. These functions return vectors of indexes that can then be used to subset the original sample into training and test sets.

# create a random 20/80 test/train partition of the data 
train_set <- createDataPartition(classes, 
                                 p = 0.8, 
                                 list = FALSE) 

str(train_set) 
```




```{r}
Preparing the data

We will be using the following packages:

library(tidyverse)
library(caret)
library(glmnet)

We’ll also be using R’s built in Boston housing market data set as it has many predictor variables

data(“Boston”, package = “MASS”)

#set a seed so you can reproduce the results
set.seed(1212)

#split the data into training and test data
sample_size <- floor(0.75 * nrow(Boston))

training_index <- sample(seq_len(nrow(Boston)), size = sample_size)

train <- Boston[training_index, ]

test <- Boston[-training_index, ]

We also should create two objects to store predictor (x) and response variables (y, median value)

# Predictor
x <- model.matrix(medv~., train)[,-1]

# Response
y <- train$medv

Performing Ridge regression

As we mentioned in the previous sections, lambda values have a large effect on coefficients so now we will compute and chose a suitable one.

Here we perform a cross validation and take a peek at the lambda value corresponding to the lowest prediction error before fitting the data to the model and viewing the coefficients.

cv.r <- cv.glmnet(x, y, alpha = 0)

cv.r$lambda.min

model.ridge <- glmnet(x, y, alpha = 0, lambda = cv.r$lambda.min)

coef(model.ridge)

We can see here that certain coefficients have been pushed towards zero and minimized while RM (number of rooms) has a significantly higher weight than the rest
Ridge regression coefficients

We now look at how our model performs by using our test data on it.

x.test.ridge <- model.matrix(medv ~., test)[,-1]

predictions.ridge <- model.ridge
%>% predict(x.test.ridge)
%>% as.vector()

data.frame(
  RMSE.r = RMSE(predictions.ridge, test$medv),
  Rsquare.r = R2(predictions.ridge, test$medv))

RMSE = 4.8721 and R² = 0.7205
Performing Lasso regression

The steps will be identical to what we have done for ridge regression. The value of alpha is the only change here (remember 𝞪 = 1 denotes lasso)

cv.l <- cv.glmnet(x, y, alpha = 1)

cv.l$lambda.min

model.lasso <- glmnet(x, y, alpha = 1, lambda = cv.l$lambda.min)

coef(model.lasso)

x.test.lasso <- model.matrix(medv ~., test)[,-1]
predictions.lasso <- model.lasso %>%
predict(x.test.lasso) %>% 
as.vector()

data.frame(
RMSE.l = RMSE(predictions.lasso, test$medv),
Rsquare.l = R2(predictions.lasso, test$medv))

RMSE = 4.8494 and R² = 0.7223
Performing Elastic Net regression

Performing Elastic Net requires us to tune parameters to identify the best alpha and lambda values and for this we need to use the caret package. We will tune the model by iterating over a number of alpha and lambda pairs and we can see which pair has the lowest associated error.

model.net <- train(
    medv ~., data = train, method = "glmnet",
    trControl = trainControl("cv", number = 10),
    tuneLength = 10)

model.net$bestTune
  
coef(model.net$finalModel, model.net$bestTune$lambda)

x.test.net <- model.matrix(medv ~., test)[,-1]

predictions.net <- model.net %>% predict(x.test.net)

data.frame(
RMSE.net = RMSE(predictions.net, test$medv),
Rsquare.net = R2(predictions.net, test$medv))

RMSE = 4.8523 and R² = 0.7219
```


```{r datacamp-shrinkage-example-01}
# this is an example problem on shrinkage methods from:
# https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net 
#  
# ridge regression assumes the predictors are standardized and 
# the response is centered! 
# this is a wrong approach because the response variables are not standardized.

# Center y, X will only be centered in the modelling function
mtcar_y <- mtcars %>% 
  select(mpg) %>% 
  scale(center = TRUE, scale = FALSE) %>% # note scale is wrong in this example 
  as.matrix() 

mtcar_x <- mtcars %>% 
  select(-mpg) %>% 
  as.matrix()

# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

# Does k-fold cross-validation for glmnet, produces a plot, and 
# returns a value for lambda --------------------------------------------------
# Setting alpha = 0 implements ridge regression 
ridge_cv <- cv.glmnet(mtcar_x, mtcar_y, alpha = 0,      
                      lambda = lambdas_to_try,
                      standardize = TRUE, 
                      nfolds = 10)

# Plot cross-validation results
plot(ridge_cv)
```

```{r  datacamp-shrinkage-example-01}

# Best cross-validated lambda--selects the value of λ that minimizes the 
# cross-validated sum of squared residuals
lambda_cv <- ridge_cv$lambda.min  

# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(mtcar_x, mtcar_y, alpha = 0, 
                   lambda = lambda_cv, 
                   standardize = TRUE) 

# predict is a generic function for predictions from the results of 
# various model fitting functions. 
# The function invokes particular methods which depend on the class of the 
# first argument.
y_hat_cv <- predict(model_cv, mtcar_x)  
ssr_cv <- t(mtcar_y - y_hat_cv) %*% (mtcar_y - y_hat_cv) 

rsq_ridge_cv <- cor(mtcar_y, y_hat_cv)^2 
```



```{r styler, message=FALSE} 
#tidyverse_style(scope = "tokens", strict = TRUE, indent_by = 2, 
 # start_comments_with_one_space = FALSE, 
 # reindention = tidyverse_reindention(), 
#  math_token_spacing = tidyverse_math_token_spacing())  

# style_file() 
```

```{r import_data} 
# read in geopackage data 
st_layers("sp_data/eco-drought.gpkg")  

# check layer names in .gpkg 
layer <- st_layers("sp_data/eco-drought.gpkg")$name[1:5] %>% 
tibble::enframe(.) 

# read in spatial data 
wsd_spatial <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "wbd_summary", 
                       as_tibble = TRUE) 
# note: GDAL Message 1: GPKG: bad application_id 0x47504B47  
# most likely has to do with the GDAL version 

# convert spatial data to a tibble 
wsd_vars <- as_tibble(wsd_spatial) %>% 
  modify_if(., is.factor, as.character) %>% 
  select(id, everything()) 
```

```{r}
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
               trControl = train.control)
# Summarize the results
print(model) 

data <- swiss
```

```{r styler, message=FALSE} 
#tidyverse_style(scope = "tokens", strict = TRUE, indent_by = 2, 
 # start_comments_with_one_space = FALSE, 
 # reindention = tidyverse_reindention(), 
#  math_token_spacing = tidyverse_math_token_spacing())  

# style_file() 
```