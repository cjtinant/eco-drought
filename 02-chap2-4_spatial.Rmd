---
title: "02-chapt2-4_spatial"
author: "CJ Tinant"
date: "7/15/2019"
output: html_document
---

<!--
Purpose: 
develop a regression function for gaged stations by fitting PC1 of mean daily 
streamflow to environmental variables.  Then estimate PC1 for ungaged stations  
from the regression equation.  

Data: 
  the data to train the model consists of 42 gaged catchments, 
  the data for the fitted model consists of 45 ungaged catchments  

Approach: 
1) EDA 
-- check histograms 
-- check univariate response 
2) Stratify train/test data along PC1 axis & split train/test data
     training set = 33 stations, 11 low, 11 med, 11 high gw contribution 
3) Train model - 
Fit a generalized linear model via penalized maximum likelihood. The 
regularization path is computed for the lasso or elasticnet penalty at 
a grid of values for the regularization parameter lambda. 
4) Test model 
     testing using 9 stations, 3 low, 3 med, 3 high gw contribution 

# About the train() function in the caret package.  The goal is to 
# automatically split the data, fit the models and assess model performance. 
# One example is from Cross-Validation for Predictive Analytics Using R
# http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/

# We fit a generalized linear model via penalized maximum likelihood using 
# the glmnet package (Friedman, Hastie, and Tibshirani 2008).  
# The algorithm 1) computes the regularization path for the elastic-net penalty 
# over a grid of values for the regularization parameter Œª that controls 
# the overall strength of the penalty, and 2) applies an elastic-net penalty 
# (Zou and Hastie 2005), called the mixing percentage and denoted with Œ±, to 
# bridge the gap between lasso (Œ±=1) and ridge (Œ±=0) regression approaches. 
# The mixing percentage parameter takes value in [0,1]. 

# The train() function requires 1) the model formula, 2) the indication of the 
# model to fit, 3) the grid of tuning parameter values to use, specified 
# through the tuneGrid argument, 4) the method to choose optimal tuning 
# parameter values (in our case, 5-fold cross-validation) through the trControl 
# argument.  The preProcess argument specifies pre-processing operations on the 
# predictors (in our case, centering and scaling the predictor values). 


Next Steps: 
-- check into stylr-packag

Variable naming convention:   
# ~~~~~~~~~~~~~~~~~~~~~~~~~
gpg_layers          names of layers in gpkg file for the watershed summaries 
   _gage_summary 
   _prr_wq_sites
   _wbd_summary

wsd_summary     zonal statistics of watershed environmental parameters 
*   _id          unique id
*  _sta_id      four- or seven-digit station ID   
   _type        distinguishes gaged from ungaged 
   _watshed     describes the HUC06 watershed 
   _HUC12       hydrologic unit code 12
   _sta_name    station name derived from outlet HUC12 catchment name
   _gage_num    USGS site number 
   _gage_nm     station name from USGS gage
   _dec_lat     latitude in decimal degrees
   _dec_lon     longitude in decimal degrees
   _cat_area    catchment area in sq km
*  _cat_area_l  natural logarithm of catchment area 
   _cat_length  catchment length
   _cat_width   catchment width
*  _lw_ratio    catchment length divided by catchment width
   _str_len     stream length
*  _drain_dens  stream length divided by catchment area
*  _prcp_mean   areal mean precipitation depth 1992-2012
*  _t07_mean    average July temperature 1992-2012
*  _vpd_ann     areal mean of max vapor deficit 1992-2012
*  _vpd_07      areal mean of max July vapor deficit 1992-2012
*  _cat_out     catchment outlet elevation
*  _cat_rel     difference in max and min elevation
*  _slop_med    median percent slope
*  _TWI_mean    mean terrain wetness index
*  _perc_cov    percent forest cover from NLCD 2016
*  _fc_mean     mean field capacity
*  _ksat_mean   mean horizontal saturated hydraulic conductivity
*  _kvert_mean  mean vertical saturated hydrologic conductivity 

* indicates variables used in EDA using 'env_vars'

trainIndex      index variable for data splitting 
test            environmental parameters for gaged watersheds - test set 
train           environmental parameters for gaged watersheds - training set 
  _area_linv    recropocal of the natural logarithm of catchment area 
  _cout_sqr     square-root of catchment outlet elevation
  _crel_sqr     square-root of catchment relief
  _dden_sqr     square-root of drainage density
  _fc_mean      mean field capacity
  _kvert_ln     natural log of mean vertical saturated hydrologic conductivity 
  _lwrat_sqr    square-root of catchment outlet elevation
  _pcov_ln      natural log of percent forest cover from NLCD 2016
  _prcp_sq      square of areal mean precipitation depth 1992-2012
  _slop_ln      natural log of median percent slope 
  _t07_sq       square of average July temperature 1992-2012
  _TWI_sq       square of mean terrain wetness index
  _vpdan_sq     square of areal mean of max vapor deficit 1992-2012





vars_ung        environmental parameters for ungaged watersheds
gage_summary    input data used in geospatial analysis 
*   _sta        
*   _log_q1_dep 
*   _log_q7_dep 
*   _log_q30_de 
   _.class 
   _.uncertain 
   _site_no 
   _min_year 
   _max_year 
   _num_year 
   _agency_cd 
   _station_nm 
   _dec_lat_va 
   _dec_long_v 
   _state_cd 
   _county_cd 
   _drain_area 
   _contrib_dr 
   _huc_cd 
   _alt_va 
   _geom    

        




-->

```{r setup, include=FALSE}
# set up global options ------------------------------------------------------
knitr::opts_chunk$set(echo = TRUE)
options(tibble.print_max = 70) # sets tibble output for printing

# load libraries -------------------------------------------------------------
library("here")         # identifies where to save work
library("janitor")      # tools for examining and cleaning dirty data
library("sf")
library("funModeling")  # functions related to exploratory data analysis 
library("Hmisc")        # functions useful for data analysis
library("DataExplorer") # functions related to exploratory data analysis 
library("broom")        # sweep up results into tidy frames 
library("rsample")      # functions to create and summarize cross-validation  
library("scales")       # scales for automatically determining graphical breaks 
library("mlbench")      # artificial and real-world machine learning datasets 
library("kernlab")        # kernel-based machine learning methods 
#library(sessioninfo)

library("tidyverse")     # data munging tools 
library("caret")         # classification and regression training 
library("glmnet")        # fit a GLM with lasso or elasticnet regularization 
library("glmnetUtils")   # formula interface for 'glmnet' elasticnet regression 




#library("styler")
#library("psych")           # for function tr() to compute trace of a matrix
#library("RCurl")        # General Network (HTTP/FTP/...) Client Interface for R 
#library("prettyR")      # Pretty descriptive stats
# library("lubridate")     # easier dates

# set seed for reproducibility & theme for printing  -------------------------
seed <- 1974     # seed for reproducibility - my birth year 
theme_set(theme_bw()) 

# Create functions ------------------------------------------------------------
# basic EDA function from 
# https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/
basic_eda <- function(data)      
{                                
  glimpse(data)
  df_status(data)
  freq(data) 
  profiling_num(data)
  plot_num(data)
  describe(data)
}
```


```{r import_data} 

# check layer names for the project geopackage 
gpg_layers <- st_layers("sp_data/eco-drought.gpkg")$name[1:5] %>% 
  tibble::enframe(.) %>% 
  select(value) 

# read in geopackage data of zonal summaries for watersheds 
gage_summary <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "gage_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry()

wsd_summary <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "wbd_summary", 
                       as_tibble = TRUE) %>% 
  st_drop_geometry() 

# convert environmental data to tibbles 
gage_summary <- as_tibble(gage_summary) %>% 
  modify_if(., is.factor, as.character) 

wsd_summary <- as_tibble(wsd_summary) %>% 
  modify_if(., is.factor, as.character) %>% 
  select(id, everything()) 

```

```{r munge_explore_enviro_data}
env_vars <-  wsd_summary %>% 
  filter(type == "gaged") %>% 
  select(sta_id, cat_area_l, lw_ratio, drain_dens, prcp_mean, t07_mean, 
         vpd_ann, vpd_07, cat_out, cat_rel, slop_med, TWI_mean, perc_cov, 
         fc_mean, ksat_mean, kvert_mean) %>% 
  rename(cat_area_ln = cat_area_l) 

basic_eda(env_vars) 

```

```{r EDA_01-box-cox}

# use Box Cox to estimate transforms 
lambda <- env_vars %>% 
  select(-sta_id)

lambda <- enframe(
                  sapply(lambda, forecast::BoxCox.lambda)
                  ) %>% 
  rename(sta = name) %>% 
  rename(lambda_val = value) %>% 
  arrange(lambda_val) 

``` 

```{r EDA_02-transforms} 

# transform variables in the following way: 
## variable    lambda_val     transform
## ------------------------------------------
## cat_area_ln	-1.00			    reciprocal
## cat_out	     0.36			    square root 
## cat_rel       0.46			    square root 			
## drain_dens	   0.66			    square root 			
## fc_mean	     1.36 
## kvert_mean	  -0.13			    natural log 
## ksat_mean  	-0.11			    natural log 
## lw_ratio	    -0.18			    natural log 
## perc_cov	     0.17			    natural log 
## prcp_mean	   2.00			    square
## slop_med	    -0.13			    natural log 
## t07_mean	     2.00			    square
## TWI_mean	     2.00				  square
## vpd_ann	     2.00				  square		
## vpd_07	       2.00					square	

env_vars <- env_vars %>% 
  mutate(area_linv  = 1/cat_area_ln) %>% 
  mutate(cout_sqr   = sqrt(cat_out)) %>% 
  mutate(crel_sqr   = sqrt(cat_rel)) %>% 
  mutate(dden_sqr   = sqrt(drain_dens)) %>% 
  mutate(ksat_ln    = log(ksat_mean)) %>% 		
  mutate(kvert_ln   = log(kvert_mean)) %>% 
  mutate(lwrat_sqr  = log(lw_ratio)) %>% 
  mutate(pcov_ln    = log(1 + perc_cov)) %>% 	
  mutate(prcp_sq    = prcp_mean^2) %>% 
  mutate(slop_ln    = log(slop_med)) %>%   
  mutate(t07_sq     = t07_mean^2) %>%     
  mutate(TWI_sq     = TWI_mean^2) %>%   
  mutate(vpd07_sq   = vpd_07^2) %>% 	
  mutate(vpdan_sq   = vpd_ann^2) %>%   		
  select(area_linv, cat_area_ln, cout_sqr, cat_out, crel_sqr, cat_rel, 
         dden_sqr, drain_dens, fc_mean, ksat_ln, ksat_mean, kvert_ln, 
         kvert_mean, lwrat_sqr, lw_ratio, pcov_ln, perc_cov, prcp_sq, 
         prcp_mean, slop_ln, slop_med, t07_sq, t07_mean, TWI_sq, 
         vpd07_sq, vpd_07, vpd_ann, vpdan_sq) 

# plot quantiles 
plot_qq(env_vars) 

```

```{r EDA_03-correlation} 

# select environmental variables 
env_vars <- env_vars %>% 
  select(area_linv, cout_sqr, crel_sqr, dden_sqr, fc_mean, ksat_ln, 
         kvert_ln, lwrat_sqr, pcov_ln, prcp_sq, slop_ln, t07_sq, 
         TWI_sq, vpd07_sq, vpdan_sq) 

# correlation plots 
plot_correlation(env_vars) 

# highly correlated (> 0.65) data 
## variable    transform_var    correlated vars             drop?
## ---------------------------------------------------------------------------
## drain_dens	  dden_sqr			  fc_mean, prcp_sq, 
##                              ksat_ln, kvert_ln			      maybe
## fc_mean	                    ksat_ln, k_vert_ln, 
##                              dden_sqr           
## ksat_mean  	ksat_ln 		    dden_sqrm fc_mean,
##                              kvert_ln                    yes
## kvert_mean	  kvert_ln        dden_sqr, fc_mean, 
##                              ksat_ln 
## lw_ratio	    lwrat_sqr		     
## prcp_mean	 	prcp_sq         vpd_ann, vpd_07	
## perc_cov	    pcov_ln		      slop_ln, t07_ln            
##                              TWI_sq, vpd07_sq
##                              vpdan_sq                    maybe
## slop_med	    slop_ln         pcov_ln, TWI_sq 
## t07_mean	    t07_sq          pcov_ln, vpdann_sq, 
##                              vpd07_sq 
## TWI_mean	    TWI_sq 				  pcov_ln, slop_ln
## vpd_ann	    vpdan_sq			  pcov_ln, prcp_sq, 
##                              t07_mean, vpd07_sq 
## vpd_07	      vpd07_sq			  pcov_ln, prcp_sq, 
##                              t07_mean, vpdan_sq          yes

``` 

```{r EDA_04-response_var} 

# transformed environmental vars to approximate gaussian 
gaged <- wsd_summary %>% 
  filter(type == "gaged") %>% 
  mutate(area_linv  = 1/cat_area_l) %>% 
  mutate(cout_sqr   = sqrt(cat_out)) %>% 
  mutate(crel_sqr   = sqrt(cat_rel)) %>% 
  mutate(dden_sqr   = sqrt(drain_dens)) %>% 
  mutate(kvert_ln   = log(kvert_mean)) %>% 
  mutate(lwrat_sqr  = log(lw_ratio)) %>% 
  mutate(pcov_ln    = log(1 + perc_cov)) %>% 	
  mutate(prcp_sq    = prcp_mean^2) %>% 
  mutate(slop_ln    = log(slop_med)) %>%   
  mutate(t07_sq     = t07_mean^2) %>%     
  mutate(TWI_sq     = TWI_mean^2) %>%   
  mutate(vpdan_sq   = vpd_ann^2) %>%   		
  select(sta_id, area_linv, cout_sqr, crel_sqr, dden_sqr, fc_mean, 
         kvert_ln, lwrat_sqr, pcov_ln, prcp_sq, slop_ln, t07_sq, 
         TWI_sq, vpdan_sq) 

# response variable needs to be an ordered PC for training/test splits 
resp_vars <- gage_summary %>% 
  select(sta, log_q1_dep, log_q7_dep, log_q30_de) 

pca <- prcomp(resp_vars[,c(2:4)], 
              center = TRUE, scale. = TRUE) 

tidy(pca, "pcs") 

resp_vars <- augment(pca, data = resp_vars) %>% 
  select(sta_id = sta, hydro_exp = .fittedPC1) 
  
# prepare for regression 
gaged <- full_join(gaged, resp_vars, 
                   by = "sta_id") %>% 
  arrange(hydro_exp) %>% 
  mutate(row_num = dplyr::row_number()) %>% 
  select(row_num, sta_id, hydro_exp, everything()) 

# a factor variable is required for splitting into training and test data
gaged <- gaged %>% 
  mutate(hydro_grp = case_when(
    row_num < 15 ~ "low",
    row_num < 29 ~ "med",
    TRUE ~ "high")
    ) %>% 
  mutate(hydro_grp = as.factor(hydro_grp))

# clean up global environment 
rm(env_vars, gage_summary, gpg_layers, lambda, pca, resp_vars)  
```

```{r split_train/test_data}

# create a balanced split within subgroups, e.g. classes, by random 
# sampling to preserve the overall class distribution of the data. 
# https://topepo.github.io/caret/model-training-and-tuning.html 

classes <- gaged$hydro_grp
trainIndex <- createDataPartition(classes, 
                                  p = .75, list = FALSE, times = 1)

train <- gaged[trainIndex,] 
test  <- gaged[-trainIndex,] 

```

```{r}
# Randomly split the data into k subsets to balance the class distributions 
# within the splits. These functions return vectors of indexes that can then be used to subset the original sample into training and test sets.


predictors <- gaged %>% 
  select(area_linv:vpdan_sq) %>%
  names()
  
train_predictors <- train %>% 
  select(predictors)


 
train_predictors <- predictors[train_set, ]
train_classes <- classes[train_set]
test_predictors <- predictors[-train_set, ]
test_classes <- classes[-train_set]
 
set.seed(seed)
cv_splits <- createFolds(classes, k = 10, returnTrain = TRUE)
str(cv_splits)


rm(classes, trainIndex)  

# Model_method Value	  Type	   Libraries	     Tuning_Parameters
#  Elasticnet	 enet	 Regression	 elasticnet	     fraction, lambda 
```





```{r} 


set.seed(seed)
 
cs_data_train <- cs_data[train_set, ]
cs_data_test <- cs_data[-train_set, ]
 
glmnet_grid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                           lambda = seq(.01, .2, length = 20))



glmnet_ctrl <- trainControl(method = "cv", number = 10)
glmnet_fit <- train(Status ~ ., data = cs_data_train,
                    method = "glmnet",
                    preProcess = c("center", "scale"),
                    tuneGrid = glmnet_grid,
                    trControl = glmnet_ctrl)
glmnet_fit
```

```{r stuff}

# total data is 2

# simulate regression data from mlbench::mlbench.friedman1
sim_data <- function(n) {
  tmp <- mlbench.friedman1(n, sd=1)
  tmp <- cbind(tmp$x, tmp$y)
  tmp <- as.data.frame(tmp)
  names(tmp)[ncol(tmp)] <- "y"
  tmp
}


# generate a training set of 100 data points and a larger characterization set 
set.seed(9815) 
train_dat <- sim_data(100)
large_dat <- sim_data(10^5)

# create the tibble with the resampling specifications:
results <- nested_cv(train_dat, 
                     outside = vfold_cv(repeats = 5), 
                     inside = bootstraps(25))
results

## # 10-fold cross-validation repeated 5 times 
## # Nested : vfold_cv(repeats = 5) / bootstraps(25) 
## # A tibble: 50 x 4
##          splits      id    id2   inner_resamples
##          <list>   <chr>  <chr>            <list>
##  1 <S3: rsplit> Repeat1 Fold01 <tibble [25 x 2]>
##  2 <S3: rsplit> Repeat1 Fold02 <tibble [25 x 2]>
##  3 <S3: rsplit> Repeat1 Fold03 <tibble [25 x 2]>
##  4 <S3: rsplit> Repeat1 Fold04 <tibble [25 x 2]>
##  5 <S3: rsplit> Repeat1 Fold05 <tibble [25 x 2]>
##  6 <S3: rsplit> Repeat1 Fold06 <tibble [25 x 2]>
##  7 <S3: rsplit> Repeat1 Fold07 <tibble [25 x 2]>
##  8 <S3: rsplit> Repeat1 Fold08 <tibble [25 x 2]>
##  9 <S3: rsplit> Repeat1 Fold09 <tibble [25 x 2]>
## 10 <S3: rsplit> Repeat1 Fold10 <tibble [25 x 2]>
## # ... with 40 more rows

The splitting information for each resample is contained in the split objects. Focusing on the second fold of the first repeat:

results$splits[[2]]

## <90/10/100>

<90/10/100> indicates the number of data in the analysis set, assessment set, and the original data.

Each element of inner_resamples has its own tibble with the bootstrapping splits.

results$inner_resamples[[5]]

## # Bootstrap sampling with 25 resamples 
## # A tibble: 25 x 2
##          splits          id
##          <list>       <chr>
##  1 <S3: rsplit> Bootstrap01
##  2 <S3: rsplit> Bootstrap02
##  3 <S3: rsplit> Bootstrap03
##  4 <S3: rsplit> Bootstrap04
##  5 <S3: rsplit> Bootstrap05
##  6 <S3: rsplit> Bootstrap06
##  7 <S3: rsplit> Bootstrap07
##  8 <S3: rsplit> Bootstrap08
##  9 <S3: rsplit> Bootstrap09
## 10 <S3: rsplit> Bootstrap10
## # ... with 15 more rows

These are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:

results$inner_resamples[[5]]$splits[[1]]

## <90/37/90>

To start, we need to define how the model will be created and measured. For our example, a radial basis support vector machine model will be created using the function kernlab::ksvm. This model is generally thought of as having two tuning parameters: the SVM cost value and the kernel parameter sigma. For illustration, only the cost value will be tuned and the function kernlab::sigest will be used to estimate sigma during each model fit. This is automatically done by ksvm.

After the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. One important note: for this model, it is critical to center and scale the predictors before computing dot products. We don't do this operation here because mlbench.friedman1 simulates all of the predictors to be standard uniform random variables.

Our function to fit a single model and compute the RMSE is:

# `object` will be an `rsplit` object from our `results` tibble
# `cost` is the tuning parameter
svm_rmse <- function(object, cost = 1) {
  y_col <- ncol(object$data)
  mod <- ksvm(y ~ ., data = analysis(object),  C = cost)
  holdout_pred <- predict(mod, assessment(object)[-y_col])
  rmse <- sqrt(mean((assessment(object)$y - holdout_pred)^2, na.rm = TRUE))
  rmse
}

# In some case, we want to parameterize the function over the tuning parameter:
rmse_wrapper <- function(cost, object) svm_rmse(object, cost)

For the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, a wrapper can be created:

# `object` will be an `rsplit` object for the bootstrap samples
tune_over_cost <- function(object) {
  results <- tibble(cost = 2^seq(-2, 8, by = 1))
  results$RMSE <- map_dbl(results$cost, 
                          rmse_wrapper,
                          object = object)
  results
}

Since this will be called across the set of outer cross-validation splits, another wrapper is required:

# `object` is an `rsplit` object in `results$inner_resamples` 
summarize_tune_results <- function(object) {
  # Return row-bound tibble that has the 25 bootstrap results
  map_df(object$splits, tune_over_cost) %>%
    # For each value of the tuning parameter, compute the 
    # average RMSE which is the inner bootstrap estimate. 
    group_by(cost) %>%
    summarize(mean_RMSE = mean(RMSE, na.rm = TRUE),
              n = length(RMSE))
}

Now that those functions are defined, we can execute all the inner resampling loops:

tuning_results <- map(results$inner_resamples, summarize_tune_results) 

tuning_results is a list of data frames for each of the 50 outer resamples.

Let's make a plot of the averaged results to see what the relationship is between the RMSE and the tuning parameters for each of the inner bootstrapping operations:

pooled_inner <- tuning_results %>% bind_rows

best_cost <- function(dat) dat[which.min(dat$mean_RMSE),]

p <- ggplot(pooled_inner, aes(x = cost, y = mean_RMSE)) + 
  scale_x_continuous(trans='log2') +
  xlab("SVM Cost") + ylab("Inner RMSE")

for(i in 1:length(tuning_results))
  p <- p  + 
  geom_line(data = tuning_results[[i]], alpha = .2) + 
  geom_point(data = best_cost(tuning_results[[i]]), pch = 16)

p <- p + geom_smooth(data = pooled_inner, se = FALSE)
p

## `geom_smooth()` using method = 'loess'

rmse-plot-1.png

Each grey line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a loess smooth of all the results pooled together.

To determine the best parameter estimate for each of the outer resampling iterations:

cost_vals <- tuning_results %>% map_df(best_cost) %>% select(cost) 
results <- bind_cols(results, cost_vals)

ggplot(results, aes(x = factor(cost))) + geom_bar() + xlab("SVM Cost")

choose-1.png

Most of the resamples produced an optimal cost values of 2.0 but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.

Now that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:

results$RMSE <- map2_dbl(results$splits, results$cost, svm_rmse)
summary(results$RMSE)

##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.086   2.183   2.562   2.689   3.191   4.222

The RMSE estimate using nested resampling is 2.69.

What is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.

not_nested <- map(results$splits, tune_over_cost) %>%
  bind_rows

outer_summary <- not_nested %>% 
  group_by(cost) %>% 
  summarize(outer_RMSE = mean(RMSE),
            n = length(RMSE))
outer_summary

## # A tibble: 11 x 3
##      cost outer_RMSE     n
##     <dbl>      <dbl> <int>
##  1   0.25   3.565595    50
##  2   0.50   3.119439    50
##  3   1.00   2.775602    50
##  4   2.00   2.609950    50
##  5   4.00   2.639033    50
##  6   8.00   2.755651    50
##  7  16.00   2.831902    50
##  8  32.00   2.840183    50
##  9  64.00   2.833896    50
## 10 128.00   2.831717    50
## 11 256.00   2.836863    50

ggplot(outer_summary, aes(x = cost, y = outer_RMSE)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(trans='log2') +
  xlab("SVM Cost") + ylab("Inner RMSE")

not-nested-1.png

The non-nested procedure estimates the RMSE to be 2.61. Both estimates are fairly close and would end up choosing a cost parameter value of 2.0.

The approximately true RMSE for an SVM model with a cost value of 2.0 and be estimated with the large sample that was simulated at the beginning.

finalModel <- ksvm(y ~ ., data = train_dat, C = 2)
large_pred <- predict(finalModel, large_dat[, -ncol(large_dat)])
sqrt(mean((large_dat$y - large_pred)^2, na.rm = TRUE))

## [1] 2.696096

The nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar. There is some optimzation bias here but it is very small (for these data and this model).

The R markdown document used to create this post can be found here.

The session information is:

session_info()

## ‚îÄ Session info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
##  setting  value                       
##  version  R version 3.3.3 (2017-03-06)
##  os       macOS Sierra 10.12.6        
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/New_York            
##  date     2017-09-03                  
## 
## ‚îÄ Packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
##  package     * version  date       source         
##  assertthat    0.2.0    2017-04-11 CRAN (R 3.3.2) 
##  bindr         0.1      2016-11-13 CRAN (R 3.3.2) 
##  bindrcpp    * 0.2      2017-06-17 cran (@0.2)    
##  broom       * 0.4.2    2017-02-13 CRAN (R 3.3.2) 
##  clisymbols    1.2.0    2017-05-21 CRAN (R 3.3.2) 
##  colorspace    1.3-2    2016-12-14 CRAN (R 3.3.2) 
##  dplyr       * 0.7.2    2017-07-20 cran (@0.7.2)  
##  evaluate      0.10.1   2017-06-24 CRAN (R 3.3.2) 
##  foreign       0.8-67   2016-09-13 CRAN (R 3.3.3) 
##  ggplot2     * 2.2.1    2016-12-30 CRAN (R 3.3.2) 
##  glue          1.1.1    2017-06-21 CRAN (R 3.3.2) 
##  gtable        0.2.0    2016-02-26 CRAN (R 3.3.0) 
##  highr         0.6      2016-05-09 CRAN (R 3.3.0) 
##  kernlab     * 0.9-25   2016-10-03 CRAN (R 3.3.0) 
##  knitr       * 1.17     2017-08-10 CRAN (R 3.3.2) 
##  labeling      0.3      2014-08-23 CRAN (R 3.3.0) 
##  lattice       0.20-35  2017-03-25 CRAN (R 3.3.3) 
##  lazyeval      0.2.0    2016-06-12 CRAN (R 3.3.0) 
##  magrittr      1.5      2014-11-22 CRAN (R 3.3.0) 
##  mlbench     * 2.1-1    2012-07-10 CRAN (R 3.3.0) 
##  mnormt        1.5-5    2016-10-15 CRAN (R 3.3.0) 
##  munsell       0.4.3    2016-02-13 CRAN (R 3.3.0) 
##  nlme          3.1-131  2017-02-06 CRAN (R 3.3.3) 
##  pkgconfig     2.0.1    2017-03-21 cran (@2.0.1)  
##  plyr          1.8.4    2016-06-08 CRAN (R 3.3.0) 
##  psych         1.7.3.21 2017-03-22 CRAN (R 3.3.2) 
##  purrr       * 0.2.3    2017-08-02 cran (@0.2.3)  
##  R6            2.2.2    2017-06-17 cran (@2.2.2)  
##  Rcpp          0.12.12  2017-07-15 cran (@0.12.12)
##  reshape2      1.4.2    2016-10-22 CRAN (R 3.3.3) 
##  rlang         0.1.2    2017-08-09 cran (@0.1.2)  
##  rsample     * 0.0.1    2017-07-08 CRAN (R 3.3.3) 
##  scales      * 0.5.0    2017-08-24 CRAN (R 3.3.2) 
##  sessioninfo * 1.0.0    2017-06-21 CRAN (R 3.3.2) 
##  stringi       1.1.5    2017-04-07 CRAN (R 3.3.2) 
##  stringr       1.2.0    2017-02-18 CRAN (R 3.3.2) 
##  tibble        1.3.4    2017-08-22 cran (@1.3.4)  
##  tidyr         0.7.0    2017-08-16 cran (@0.7.0)  
##  withr         2.0.0    2017-07-28 CRAN (R 3.3.2)


```

```{r munge_gaged} 

# stratify sample ensure sampling for test data is screate threesplits in the data based on the hyd
gaged <- gaged %>% 
  











data(oil) 
createDataPartition(oilType, 2)

x <- rgamma(50, 3, .5) 
inA <- createDataPartition(x, list = FALSE)

plot(density(x[inA]))
rug(x[inA])

points(density(x[-inA]), type = "l", col = 4)
rug(x[-inA], col = 4)

createResample(oilType, 2)

createFolds(oilType, 10)
createFolds(oilType, 5, FALSE)

createFolds(rnorm(21))

createTimeSlices(1:9, 5, 1, fixedWindow = FALSE)
createTimeSlices(1:9, 5, 1, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = FALSE)

createTimeSlices(1:15, 5, 3)
createTimeSlices(1:15, 5, 3, skip = 2)
createTimeSlices(1:15, 5, 3, skip = 3)

set.seed(131)
groups <- sort(sample(letters[1:4], size = 20, replace = TRUE))
table(groups)
folds <- groupKFold(groups)
lapply(folds, function(x, y) table(y[x]), y = groups)


```



```{r datacamp-shrinkage-example-01}
# this is an example problem on shrinkage methods from:
# http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/ 

# data for this example is from:
# https://github.com/gastonstat/CreditScoring

url <- "https://raw.githubusercontent.com/gastonstat/CreditScoring/master/CleanCreditScoring.csv"


cs_data <- getURL(url)      
cs_data <- read.csv(textConnection(cs_data))  # not sure how this implements...
describe(cs_data) 
```

```{r datacamp-shrinkage-example-02}


classes <- cs_data[, "Status"] 
predictors <- cs_data[, -match(
  c("Status", "Seniority", "Time", "Age", "Expenses", 
    "Income", "Assets", "Debt", "Amount", "Price", "Finrat", "Savings"), 
 colnames(cs_data))]

# The caret package provides functions for splitting the data as well as functions that avutomatically do all the job for us, namely functions that create the resampled data sets, fit the models, and evaluate performance.

#Among the functions for data splitting I just mention createDataPartition() and createFolds(). The former allows to create one or more test/training random partitions of the data, while the latter randomly splits the data into k subsets. In both functions the random sampling is done within the levels of y (when y is categorical) to balance the class distributions within the splits. These functions return vectors of indexes that can then be used to subset the original sample into training and test sets.

# create a random 20/80 test/train partition of the data 
train_set <- createDataPartition(classes, 
                                 p = 0.8, 
                                 list = FALSE) 

str(train_set) 
```




```{r}
Preparing the data

We will be using the following packages:

library(tidyverse)
library(caret)
library(glmnet)

We‚Äôll also be using R‚Äôs built in Boston housing market data set as it has many predictor variables

data(‚ÄúBoston‚Äù, package = ‚ÄúMASS‚Äù)

#set a seed so you can reproduce the results
set.seed(1212)

#split the data into training and test data
sample_size <- floor(0.75 * nrow(Boston))

training_index <- sample(seq_len(nrow(Boston)), size = sample_size)

train <- Boston[training_index, ]

test <- Boston[-training_index, ]

We also should create two objects to store predictor (x) and response variables (y, median value)

# Predictor
x <- model.matrix(medv~., train)[,-1]

# Response
y <- train$medv

Performing Ridge regression

As we mentioned in the previous sections, lambda values have a large effect on coefficients so now we will compute and chose a suitable one.

Here we perform a cross validation and take a peek at the lambda value corresponding to the lowest prediction error before fitting the data to the model and viewing the coefficients.

cv.r <- cv.glmnet(x, y, alpha = 0)

cv.r$lambda.min

model.ridge <- glmnet(x, y, alpha = 0, lambda = cv.r$lambda.min)

coef(model.ridge)

We can see here that certain coefficients have been pushed towards zero and minimized while RM (number of rooms) has a significantly higher weight than the rest
Ridge regression coefficients

We now look at how our model performs by using our test data on it.

x.test.ridge <- model.matrix(medv ~., test)[,-1]

predictions.ridge <- model.ridge
%>% predict(x.test.ridge)
%>% as.vector()

data.frame(
  RMSE.r = RMSE(predictions.ridge, test$medv),
  Rsquare.r = R2(predictions.ridge, test$medv))

RMSE = 4.8721 and R¬≤ = 0.7205
Performing Lasso regression

The steps will be identical to what we have done for ridge regression. The value of alpha is the only change here (remember ùû™ = 1 denotes lasso)

cv.l <- cv.glmnet(x, y, alpha = 1)

cv.l$lambda.min

model.lasso <- glmnet(x, y, alpha = 1, lambda = cv.l$lambda.min)

coef(model.lasso)

x.test.lasso <- model.matrix(medv ~., test)[,-1]
predictions.lasso <- model.lasso %>%
predict(x.test.lasso) %>% 
as.vector()

data.frame(
RMSE.l = RMSE(predictions.lasso, test$medv),
Rsquare.l = R2(predictions.lasso, test$medv))

RMSE = 4.8494 and R¬≤ = 0.7223
Performing Elastic Net regression

Performing Elastic Net requires us to tune parameters to identify the best alpha and lambda values and for this we need to use the caret package. We will tune the model by iterating over a number of alpha and lambda pairs and we can see which pair has the lowest associated error.

model.net <- train(
    medv ~., data = train, method = "glmnet",
    trControl = trainControl("cv", number = 10),
    tuneLength = 10)

model.net$bestTune
  
coef(model.net$finalModel, model.net$bestTune$lambda)

x.test.net <- model.matrix(medv ~., test)[,-1]

predictions.net <- model.net %>% predict(x.test.net)

data.frame(
RMSE.net = RMSE(predictions.net, test$medv),
Rsquare.net = R2(predictions.net, test$medv))

RMSE = 4.8523 and R¬≤ = 0.7219
```


```{r datacamp-shrinkage-example-01}
# this is an example problem on shrinkage methods from:
# https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net 
#  
# ridge regression assumes the predictors are standardized and 
# the response is centered! 
# this is a wrong approach because the response variables are not standardized.

# Center y, X will only be centered in the modelling function
mtcar_y <- mtcars %>% 
  select(mpg) %>% 
  scale(center = TRUE, scale = FALSE) %>% # note scale is wrong in this example 
  as.matrix() 

mtcar_x <- mtcars %>% 
  select(-mpg) %>% 
  as.matrix()

# Perform 10-fold cross-validation to select lambda ---------------------------
lambdas_to_try <- 10^seq(-3, 5, length.out = 100)

# Does k-fold cross-validation for glmnet, produces a plot, and 
# returns a value for lambda --------------------------------------------------
# Setting alpha = 0 implements ridge regression 
ridge_cv <- cv.glmnet(mtcar_x, mtcar_y, alpha = 0,      
                      lambda = lambdas_to_try,
                      standardize = TRUE, 
                      nfolds = 10)

# Plot cross-validation results
plot(ridge_cv)
```

```{r  datacamp-shrinkage-example-01}

# Best cross-validated lambda--selects the value of Œª that minimizes the 
# cross-validated sum of squared residuals
lambda_cv <- ridge_cv$lambda.min  

# Fit final model, get its sum of squared residuals and multiple R-squared
model_cv <- glmnet(mtcar_x, mtcar_y, alpha = 0, 
                   lambda = lambda_cv, 
                   standardize = TRUE) 

# predict is a generic function for predictions from the results of 
# various model fitting functions. 
# The function invokes particular methods which depend on the class of the 
# first argument.
y_hat_cv <- predict(model_cv, mtcar_x)  
ssr_cv <- t(mtcar_y - y_hat_cv) %*% (mtcar_y - y_hat_cv) 

rsq_ridge_cv <- cor(mtcar_y, y_hat_cv)^2 
```



```{r styler, message=FALSE} 
#tidyverse_style(scope = "tokens", strict = TRUE, indent_by = 2, 
 # start_comments_with_one_space = FALSE, 
 # reindention = tidyverse_reindention(), 
#  math_token_spacing = tidyverse_math_token_spacing())  

# style_file() 
```

```{r import_data} 
# read in geopackage data 
st_layers("sp_data/eco-drought.gpkg")  

# check layer names in .gpkg 
layer <- st_layers("sp_data/eco-drought.gpkg")$name[1:5] %>% 
tibble::enframe(.) 

# read in spatial data 
wsd_spatial <- st_read("sp_data/eco-drought.gpkg", 
                       layer = "wbd_summary", 
                       as_tibble = TRUE) 
# note: GDAL Message 1: GPKG: bad application_id 0x47504B47  
# most likely has to do with the GDAL version 

# convert spatial data to a tibble 
wsd_vars <- as_tibble(wsd_spatial) %>% 
  modify_if(., is.factor, as.character) %>% 
  select(id, everything()) 
```

```{r}
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
               trControl = train.control)
# Summarize the results
print(model) 

data <- swiss
```

```{r styler, message=FALSE} 
#tidyverse_style(scope = "tokens", strict = TRUE, indent_by = 2, 
 # start_comments_with_one_space = FALSE, 
 # reindention = tidyverse_reindention(), 
#  math_token_spacing = tidyverse_math_token_spacing())  

# style_file() 
```