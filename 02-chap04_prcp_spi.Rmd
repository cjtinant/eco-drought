---
output: html_document
editor_options: 
  chunk_output_type: console
---


<!--
This R markdown file refactors prior code for spi analysis

1.0)  download and clean daily precip -- identified & filled NA
2.0)  convert daily precip to monthly precip  
2.1)  Updated unit vals - originally in tenths of mm; now in mm 
3.0)  Created plots 

3)  prepare data for drought index 


1. Recreated analysis from the lmomco text ch 12 (author?) in Tidyverse
2. Imported cleaned precipitation data (see 04_prcp-data_munging)   
3. Applied sqrt & log10 transform to explore effects on skew 
4. Explored the data with box plots, violin plot.
5. Applied Weibull plotting position and graphed the data on sqrt plot
6. Calculated L-moments and L-moment ratios 
7. Calculated SPI for 'cot', 'oel', 'rap', 'int', and 'ora' datasets using Pearson III. 


Data:
Predominant datasets used are:  
1) NOAA data
2) USGS daily streamflow and station metadata,  
####3) Summary data from a QGIS analysis of ungaged watersheds of interest.  

## Variable naming convention:   
sta          NOAA weather station locations  
  _meta        metadata  
  _mon         monthly precipitation depths  
    _rap        Rapid City Regional Airport station (NW)
    _ora        Oral station (SW)
    _cot        Cottonwood station (NC)  
    _gor        Gordon staion (SC)  
    _oni        Onida station (NE)
    _mis        Mission station (SE)

gage         USGS streamgage station

lmom_sta     L-moments for stations
lmom_reg     Weighted mean of L-moments for stations

min          minimum non-zero value
L1           first L-moment, similiar to mean
L_CV         first L-moment ratio, similiar to coefficient of var
L_skew       second L-moment ratio, similiar to skewness 
L_kurtosis   third L-moment ratio, similiar to kurtosis
n            number of months in a given record
int#         intermediate variable used to bind rows; # = 1, 2, ...
intc#        intermediate variable used to bind cols; # = 1, 2, ...
intr#        intermediate variable used to bind rows; # = 1, 2, ...

# Used in the L-moment diagram
aep4         4-Parameter Asymmetric Exponential Power Distribution 
gev          Generalized Extreme Value Distribution
glo          Generalized Logistic Distribution
gpa          Generalized Pareto Distribution
gno          Generalized Logistic Distribution 
gov          Govindarajulu Distribution
pe3          Pearson Type III Distribution

# SPI Outputs
spi          Standardized Precipitation Index vals for a station 
 _gath       Dataframe is long rather than wide

'site' is a variable for OST monitoring stations 



## Results: Fits a PE3 distribution
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?


Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0  Formulate your question  
2.0   Read in your data  
3.0  Check the dataset 
3.1  Check the number of rows and columns.
3.2  Check the types of data
3.3  Look at the top and the bottom of your data 
3.4  Check your “n”s & NAs 
3.5  Validate with at least one external data source  
4.0  Try the easy solution first to answer question
5.0  Challenge your solution 
6.0  Follow up questions 

## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?

## Narrower question:
What is the underlying distribution of precipitation data?  

# Next STEPS  
3. Describe the precipitation seasonality
-->  


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)  
options(tibble.print_max = 70) # sets tibble output for printing       
```

```{r library, message=FALSE} 

# Get API key (aka, token) for downloading precip data at http://www.ncdc.noaa.gov/cdo-web/token
# token for NOAA API tied to jtinant@olc.edu -- see 'rnoaa' for details 
options(noaakey = "VpcuARumMpCfFyclKHPfvskEYnaiLJHD") 

# Sets up the library of packages  
library("here")              # identifies where to save work
library("rnoaa")             # R wrapper for NOAA data inc. NCDC 
library("rio")               # more robust I/O - to import and clean data 
library("lubridate")         # easier dates  
library("lmomco")            # lmoments to find distribution 
library("SPEI")              # Calculates SPI-index
library('deldir')            # for Vorononi tesselation - Theissen polygons 
library("broom")             # tidies linear models 
library("tidyverse") 

# Other packages       
#  library("corrplot")          # correlation plots
#  library("janitor")           # tools for examining and cleaning dirty data 
#library("EGRET")            # Exploration and Graphics for RivEr Trends  
#library("dataRetrieval")    # USGS data import  
#library("measurements")     # eases measurement system manipulation 
#library("flextable")        # construct complex table with 'kable'  
#library("officer")          # facilitates '.docx' access for table export   
#library("forecast")         # for BoxCox.lambda 
#library("ggfortify")        # data vis tools for statistical analysis 
#library("stringi")          # character string processing facilities 
#library("sf")               # simple features--spatial geometries for R  
#library("anomalize")        # detect anomalies using the tidyverse   
#library("caret")            # classification and regression training 
#library("doMC")             # parallelization for caret 
#library("glmnet")           # fit a GLM with lasso or elasticnet regularization 
#library("dabestr")          # data analysis using bootstrap estimation 
#library("Metrics")          # evaluation metrics for machine learning  
#library("DataExplorer") #--- used for plot_correlation() --- 
#library("assertthat") 
 
#library('jsonlite') # tools for working with lists 
#library("magrittr") # provides aliases for easier reading
#library("maps") # outlines of continents, countries, states & counties  
#library("mapdata") # higher-resolution outlines
#library('ggmap')
# library("RColorBrewer") - there is a better one?
# library("workflowr") # creates a research website
# library("colorspace")  
# library("bookdown") #  
# library(unpivotr) # fix nasty Excel files  
# library("friendlyeval")  
# library("mathpix")                # support for 'Mathpix' image to 'LaTeX'   
# library("cowplot")  

# a useful description of commits:
# http://r-pkgs.had.co.nz/git.html


#lmomco <- citation("lmomco")
#toBibtex(lmomco)

# Session Info
#a_session <- devtools::session_info()

```  

```{r import_prcp_metadata, eval=FALSE}

# Get possible stations -----------------------------------------------------  
# It's also possible to check station id with the mapping tool at:
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation 

# The geographical extent given as SElat, SElon, NWlat, NWlon
sta_meta_orig <- ncdc_stations(extent = c(42.5, -104.5, 45, -99.5), 
                               limit = 1000) %>%      # n = 777 
# get possible stations into a dataframe 
  pluck("data") %>% 
# turn min & max date into lubridates 
  mutate(mindate = ymd(mindate)) %>% 
  mutate(maxdate = ymd(maxdate)) %>%                 
# remove 'young' stations 
  filter(mindate < "1989-01-01") %>%                    # n = 369 
# remove 'dead' stations 
  filter(maxdate > "2018-01-01") %>%                    # n = 59  
# remove poor data coverage stations   
  filter(datacoverage == 1.0) %>%                       # n = 52  
# keep only the GHCND stations  
  filter(str_detect(id, "^GHCND"))                      # n = 49     

# create groups of stations by region 
sta_meta_orig <- sta_meta_orig %>% 
  mutate(north_group = case_when( 
    latitude > 43.5 ~ "N", 
    TRUE ~ "S") 
    ) %>% 
  mutate(east_group = case_when( 
    longitude > -100.67 ~ "E", 
    longitude > -102.33 ~ "C",  
    TRUE ~ "W") 
    )  %>% 
  mutate(group = str_c(north_group, east_group, sep = "")) %>%  
  select(-c(north_group, east_group)) %>%  
# "GHCND:" doesn't appear in calls to get Global Historical 
# Climatology Network (GHCN) Daily Data   
    separate(  
             col = id, 
             sep = ":",   
             into = c("type", "sta_id")  
             ) 

sta_meta_orig <- sta_meta_orig %>% 
  select(type, sta_id, name, group, longitude, latitude, everything())


export(sta_meta_orig, "data/sta_meta_orig.csv")  

```

```{r plot-theissen-initial, message=FALSE}

# Define the study area using data from the 'maps' package ------------------
# import polygon data - US counties 
counties <- map_data("county")  

# Filter out wyoming & sd counties  
counties_wysd <- subset(counties, region %in%  
   c("south dakota")) %>%  
  filter(   
    subregion == "meade" |  
    subregion == "lawrence" |  
    subregion == "pennington" |  
    subregion == "custer" |  
    subregion == "fall river" |   
    subregion == "shannon" |  
    subregion == "jackson" |  
    subregion == "bennett" |  
    subregion == "haakon" |   
    subregion == "todd" |  
    subregion == "mellette" |   
    subregion == "jones" |   
    subregion == "stanley" |   
    subregion == "lyman" |  
    subregion == "tripp" |  
    subregion == "hughes"   
    )  

# filter nebraska  counties -- some names are duplicated above
counties_ne <- subset(counties, region %in% 
   c("nebraska")) %>% 
  filter(subregion == "sioux" |
         subregion == "dawes" | 
         subregion == "sheridan" | 
         subregion == "box butte" |
         subregion == "cherry" | 
         subregion == "brown" | 
         subregion == "keya paha" | 
         subregion == "rock" 
         )  

counties <- bind_rows(counties_wysd, counties_ne) 

rm(counties_wysd, counties_ne) 

# create Theissen polygons --------------------------------------------------  
voronoi <- deldir(sta_meta_orig$lon, sta_meta_orig$lat) 

# get streamflow gages 
gage_meta_fin <- read_csv("data/gage_meta_fin.csv") %>% 
  filter(!is.na(sta))

# plot stations -------------------------------------------------------------  
# grabbed this code off of a website by googling 'Voroni diag. R'
sta_meta_orig %>% 
ggplot(aes(x = longitude, y = latitude)) +   
# plot weather stations   
  geom_point(  
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333")  +  
# add Theissen polygons  
  geom_segment(  
    aes(x = x1, y = y1, xend = x2, yend = y2),  
    size = 2,  
    data = voronoi$dirsgs,  
    linetype = 1, 
    color = "#FFB958") +  
# add county polygons   
  geom_polygon(data = counties,  
               aes(x = long, y = lat, group = group),  
              color = "black",  
              #linetype = "dashed",  
              fill = "NA") +  
# add stream flow gages  
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(70, 130, 180, 255,  
               maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333" 
    )  +  
  theme_bw() +   
  ggtitle("Original Theissen Polygon Map")  

# export map 
ggplot2::ggsave(filename = "figure/theissen_init.png",   
                width = 6, height = 6, units = "in")  
```

```{r plot-theissen-final_stations}

# select final stations -- based on central location + nearest neighbors 
sta_meta_plus <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW      
         str_detect(name, "^RAPID CITY 4") |     # NW-alt      
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^INTERIOR")     |     # NC-alt 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^KENNEBEC")     |     # NE-alt  
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^OELRICHS")     |     # SW-alt   
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^VALENTINE")    |     # SC-alt     
         str_detect(name, "^MISSION")      |     # SE  
         str_detect(name, "^WOOD")               # SE-alt   
         )     

# select final stations 
sta_meta <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW         
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^MISSION")            # SE
         )     

# update Theissen line segments  
voronoi <- deldir(sta_meta$lon, sta_meta$lat) 

# plot stations 
sta_meta %>%  
ggplot(aes(x = longitude, y = latitude)) + 
# add stations  
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333")  + 
# add Theissen polygons 
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2), 
    size = 2, 
    data = voronoi$dirsgs, 
    linetype = 1, 
    color = "#FFB958")  + 
# add county polygons   
  geom_polygon(data = counties, 
               aes(x = long, y = lat, group = group),  
              color = "black", 
              #linetype = "dashed", 
              fill = "NA") + 
# add gages 
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(130, 130, 180, 255, 
               maxColorValue = 255),
    pch = 21,
    size = 2,
    color = "#333333" 
    )  + 
#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "red", linetype = "dashed", fill = "NA") + 
  theme_bw() +
#  geom_text(data = sta_meta_orig, aes(label = name), size = 2.5) +
  ggtitle("Final Theissen Polygon Map")  

#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "black", linetype = "dashed", fill = "NA") + 

# export map 
ggplot2::ggsave(filename = "figure/theissen_fin.png",   
                width = 6, height = 6, units = "in")  

rm(voronoi, counties)

```  

```{r download_daily-precip}

# date function calls start one-year early for long-term drought calcs  
dateMin = "1989-01-01"      
dateMax = "2017-12-31"  

# download daily precip data from NOAA GHCN database ------------------------   
sta_dv_plus <- sta_meta_plus %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax)  
          )     

rm(dateMin, dateMax)  

# fix date & add year and month
sta_dv_plus <- sta_dv_plus %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything()) 

export(sta_dv_plus, file = "data/sta_dv_plus.csv")  
export(sta_meta_plus, file = "data/sta_meta_plus.csv")  

``` 

```{r fill_missing_precip_data}  

# import data from file 
sta_dv_plus   <- import(file = "data/sta_dv_plus.csv") %>% 
  mutate(date = ymd(date))  

sta_meta_plus <- import(file = "data/sta_meta_plus.csv")
sta_meta      <- import(file = "data/sta_meta.csv")

# join bits of daily data-plus to metadata   
sta_dv_plus <- right_join(sta_meta_plus, sta_dv_plus,  
                   by = c("sta_id" = "id"))   %>%  
  select(name, sta_id, date, year, month, mflag_prcp,  
         prcp, qflag_prcp, sflag_prcp)  %>% 
  select(date, everything())   

# check for missing years  
sta_check <- sta_dv_plus %>%  
  group_by(name, sta_id, year) %>%  
  summarise(num_day = n())  %>% 
  filter(num_day < 365) %>% 
  ungroup() 

# separate north-west data --------------------------------------------------  
sta_nw <-  sta_dv_plus %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>% 
  filter(!is.na(prcp)) 
  
sta_nw_nm <- sta_nw %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nw_len <- sta_nw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nw_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nw_nm$name) %>% 
  mutate(sta_id = sta_nw_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nw_fill <- anti_join(sta_nw_alt, sta_nw, by = "date") 

sta_nw <- bind_rows(sta_nw, sta_nw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nw_alt, sta_nw_nm, sta_nw_fill)  

# separate and fill north-central  ------------------------------------------  
sta_nc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^COTTONWOOD")) %>% 
  filter(!is.na(prcp)) 

sta_nc_nm <- sta_nc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nc_len <- sta_nc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nc_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nc_nm$name) %>% 
  mutate(sta_id = sta_nc_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nc_fill <- anti_join(sta_nc_alt, sta_nc, by = "date") 

sta_nc <- bind_rows(sta_nc, sta_nc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nc_alt, sta_nc_nm, sta_nc_fill)  

# separate and fill north-east ----------------------------------------------    
sta_ne <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ONIDA")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_ne_nm <- sta_ne %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_ne_len <- sta_ne %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_ne_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^KENNEBEC")) %>% 
  mutate(name = sta_ne_nm$name) %>% 
  mutate(sta_id = sta_ne_nm$sta_id)           

sta_ne_fill <- anti_join(sta_ne_alt, sta_ne, by = "date") 

sta_ne <- bind_rows(sta_ne, sta_ne_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_ne_alt, sta_ne_nm, sta_ne_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sw <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ORAL")) %>% 
  filter(!is.na(prcp))      

# prepare to change name 
sta_sw_nm <- sta_sw %>% 
  slice(1) %>% 
  select(name, sta_id) 

# get length 
sta_sw_len <- sta_sw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sw_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^OELRICHS")) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

sta_sw <-  sta_sw %>% 
  filter(!is.na(prcp))      

# download daily precip data for missing values--HotSprings     
dateMin = "2005-12-01"      
dateMax = "2011-05-31"  
sta_sw_alt <- meteo_tidy_ghcnd(stationid = "USC00394007",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

# fill missing values 
sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sw_alt, sta_sw_nm, sta_sw_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^GORDON")) %>% 
  filter(!is.na(prcp))     

# prepare to change name 
sta_sc_nm <- sta_sc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_sc_len <- sta_sc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sc_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^VALENTINE")) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)        

sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>%  
  arrange(name) %>%  
  distinct(date, .keep_all = TRUE)  

sta_check <- sta_sc %>%  
  filter(is.na(prcp))  

sta_sc <-  sta_sc %>%  
  filter(!is.na(prcp))   

# download daily precip data for missing values--FT Robinson 
# note: Hay Springs did not have all data 
dateMin = "2006-08-01"      
dateMax = "2012-03-31"  
sta_sc_alt <- meteo_tidy_ghcnd(stationid = "USC00253015",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)       

# fill missing values 
sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sc %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sc_alt, sta_sc_nm, sta_sc_fill)  

# separate south-west data --------------------------------------------------  
sta_se <-  sta_dv_plus %>% 
   filter(str_detect(name, "^MISSION")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_se_nm <- sta_se %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_se_len <- sta_se %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) 

# get alternate site data & prepare fill 
sta_se_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^WOOD")) %>% 
  mutate(name = sta_se_nm$name) %>% 
  mutate(sta_id = sta_se_nm$sta_id)        

sta_se_fill <- anti_join(sta_se_alt, sta_se, by = "date") 

sta_se <- bind_rows(sta_se, sta_se_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_se_alt, sta_se_nm, sta_se_fill)  

# rejoin daily data and lengths ---------------------------------------------  
sta_dv <- bind_rows(sta_nw, 
                    sta_nc, 
                    sta_ne, 
                    sta_sw, 
                    sta_sc, 
                    sta_se) 

sta_dv_len <- bind_rows(sta_nw_len, 
                        sta_nc_len, 
                        sta_ne_len, 
                        sta_sw_len, 
                        sta_sc_len, 
                        sta_se_len) %>% 
  mutate(max_count = max(count)) %>% 
  mutate(perc_complete =  
    round(count/max_count,  
    digits = 3) 
    ) %>% 
  select(-c(count, max_count))  

# check data & clean up -----------------------------------------------------   
sta_check <- sta_dv %>% 
  filter(is.na(prcp)) 


# join dv-lenth to metadata  
sta_meta <- right_join(sta_meta, sta_dv_len,  
                   by = c("sta_id", "name"))  

rm(sta_dv_len, sta_meta_orig, sta_meta_plus, sta_check, sta_dv_plus, 
   sta_nw, sta_nc, sta_ne, sta_sw, sta_sc, sta_se, 
  sta_nw_len, sta_nc_len, sta_ne_len, sta_sw_len, sta_sc_len, sta_se_len)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")  

``` 

```{r check_prcp_data_quality}  

# Table of Measurement Flag/Attributes -- mflag  
# Blank = no measurement information applicable           
# A     = precip depth is a multi-day total, accumulated since last meas         
# B     = precipitation total formed from two twelve-hour totals
# D     = precipitation total formed from four six-hour totals           
# H     = represents TMAX or TMIN or average of hourly values (TAVG)           
# K     = converted from knots            
# L     = temperature appears lagged with respect to reported hr of observation        # O     = converted from oktas  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
# W    = converted from 16-point WBAN code (for wind direction)  

sta_check <- sta_dv %>%  
  group_by(name, mflag_prcp) %>%  
  summarise(mflag = n())  

# flags are T & A flags  
sta_dv <- sta_dv %>%  
  select(-mflag_prcp)  

# Table of Quality Flag/Attributes  
# Blank = did not fail any quality assurance check  
# D     = failed duplicate check           
# G     = failed gap check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# M     = failed mega-consistency check            
# N     = failed naught check            
# O     = failed climatological outlier check            
# R     = failed lagged range check           
# S     = failed spatial consistency check            
# T     = failed temporal consistency check            
# W     = temperature too warm for snow            
# X     = failed bounds check           
# Z     = flagged as a result of an official Datzilla investigation  

sta_check <- sta_dv %>%  
  group_by(name, qflag_prcp) %>%  
  summarise(qflag = n())  

# flags are L flags  
sta_dv <- sta_dv %>%  
  select(-qflag_prcp)  

# Table Source Flag/Attributes   
# Blank = No source (i.e., data value missing)  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 6  = CDMP Cooperative Summary of the Day (NCDC DSI-3206)  
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder
# A  = U.S. Automated Surface Observing System (ASOS) real-time data 
# a  = Australian data from the Australian Bureau of Meteorology           
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# b  = Belarus update           
# C  = Environment Canada            
# E  = European Climate Assessment and Dataset (Klein Tank et al., 2002)            
# F  = U.S. Fort data             
# G  = Off Global Climate Observing System (GCOS) or other gov-supplied data   
# H  = High Plains Regional Climate Center real-time data            
# I  = International collection (non U.S. data received thru pers. contacts   
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# M  = Monthly METAR Extract (additional ASOS data)           
# N  = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)           
# Q  = Data from African countries w/ later permission granted 
# R  = NCDC Reference Network Database  
# r  = All-Russian Research Inst. Hydromet Information-World Data Center    
# S  = Global Summary of the Day (NCDC DSI-9618)  
#        NOTE: "S" values are derived from hourly synoptic reports   
#         exchanged on the Global Telecommunications System (GTS).  
#         Daily values derived in this fashion may differ significantly from 
#        "true" daily data, particularly for precip (i.e., use with caution).  
# s  = China Met Admin/Nat Met Info/Climate Data Center (http://cdc.cma.gov.cn)   
# T  = SNOwpack TELemtry (SNOTEL) data from Western Regional Climate Center   
# U  = Remote Automatic Weather Station (RAWS) data from West Reg Climate Centr  
# u  = Ukraine update           
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
# z  = Uzbekistan update  

sta_check <- sta_dv %>%  
  group_by(name, sflag_prcp) %>%  
  summarise(sflag = n())  

# flags are 0, 7, K, W, X, Z  
sta_dv <- sta_dv %>%  
  select(-sflag_prcp)  

rm(sta_check)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")
```

```{r prcp_daily2monthly}

sta_dv   <- import(file = "data/sta_dv.csv")  %>% 
  mutate(date = ymd(date))
sta_meta <- import(file = "data/sta_meta.csv")  


# add short names to metadata & update the stored file ----------------------  
sta_meta <- sta_meta %>%  
  mutate(sta = case_when(  
    str_detect(name, "^RAPID CITY R") ~ "RAP",  
    str_detect(name, "^COTTONWOOD")   ~ "COT",  
    str_detect(name, "^ONIDA")        ~ "ONI",  
    str_detect(name, "^ORAL")         ~ "ORA",  
    str_detect(name, "^GORDON")       ~ "GOR",      
    str_detect(name, "^MISSION")      ~ "MIS",  
    TRUE ~ "ERROR"  
         )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation,  
         perc_complete, group, everything())  

export(sta_meta, file = "data/sta_meta.csv")  

# add short names and create monthly data -----------------------------------  
scratch <- sta_meta %>%  
  select(sta, sta_id)  

sta_mon <- full_join(scratch, sta_dv,   
                     by = "sta_id") %>%  
  select(-c(name, sta_id)) %>%  
  group_by(year, month, sta) %>%   
  summarize(prcp_tenths = sum(prcp)) %>%  
  mutate(prcp_mm = prcp_tenths/10) %>%  
  select(-prcp_tenths) %>%  
  ungroup()  

rm(scratch)  

# spread results & create date with day at midpoint of month ----------------  
sta_mon_long <- sta_mon %>%  
  spread(sta, prcp_mm) %>%  
  mutate(day = 15) %>%  
  mutate(date = make_date(year = year, month = month, day = day)) %>%   
  select(date, year, month, everything()) %>%  
  select(-day) %>%  
  ungroup() %>% 
  filter(!is.na(date))

sta_mon <- sta_mon_long %>%  
  gather(key = sta, val = depth_mm,  
         -c(date, year, month))  

rm(sta_dv_check)  
rm(sta_mon_long, sta_dv)  

sta_check <- sta_mon %>% 
  filter(is.na(sta))     

export(sta_mon, file = "data/stations_monthly.csv")  
```

```{r summaries}

sta_summary_mon <- sta_mon %>% 
  group_by(sta, month) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(month) %>%
  arrange(sta)

sta_summary_yr <- sta_mon %>% 
  group_by(sta, year) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(year) %>%
  arrange(sta)

export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
export(sta_summary_yr, file = "data/sta_summary_yr.csv")  
```

```{r ggplot_monthly} 
sta_mon <- import(file = "data/stations_monthly.csv") %>% 
  mutate(date = ymd(date))

sta_mon %>% 
  group_by(sta) %>% 
  summarize(count = n())

sta_mon$sta <- factor(sta_mon$sta,  
                      levels = c("RAP", "ORA", "GOR", "COT", "ONI", "MIS"))  

# plot monthly precips  
sta_mon %>%   
ggplot(aes(month, depth_mm, group = month)) +  
  facet_grid(rows = vars(sta)) +   
  geom_violin() +  
 geom_boxplot(alpha = 0.5) +  
  theme_bw() +  
  labs(title = "Monthly precipitation depths",  
       subtitle = "1990-2017") +   
       xlab("") +  
       ylab("mm")  


# plot annual precips
sta_mon %>% 
ggplot(aes(year, depth_mm, group = year)) + 
  facet_grid(rows =vars(sta)) +
  geom_boxplot() +
  theme_bw() + 
  labs(title = "Annual precipitation depths",
       subtitle = "1990-2017") + 
       xlab("") +
       ylab("mm")

#ggplot2::ggsave(path = "figure/", filename = "precip_mon.png", 
#                width = 6, height = 6, units = "in")
```

```{r correlation, eval=FALSE}  

# Compute a correlation matrix
data(mtcars)

corr <- round(cor(mtcars), 1)
corr


# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

# rewrite code from below
corr_ann   <- as.tibble(import("data/stations_yearly.csv")) %>% 
  gather(key = "station", value = "prcp",  -year, na.rm = TRUE) %>% 
  filter(year > 1972) %>% 
  spread(station, prcp) %>% 
  select(-year) %>% 
  cor()

# save the corrplot - see below -
# By default, RStudio enables inline output (notebook mode) on all R 
# Markdown documents, so you can interact with any R Markdown document # as though it were a notebook. If you have a document with which you 
# prefer to use the traditional console method of interaction, you can # disable notebook mode by clicking the gear in the editor toolbar and # choosing Chunk Output in Console.
corrplot.mixed(corr_ann, order = "hclust", 
                 addrect = 2, upper = "ellipse", 
                 lower = "number")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# this is the old code - saving to make sure it works in the future
# sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
# sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
# sta_yr <- sta_yr %>%
#   arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
# sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
#                    -year, factor_key = TRUE)

# filter NAs
# sta_gath_72 <- sta_gath %>%
#   filter(year > 1972) 

# spread remaining matrix & arrange from west to east
# sta_72 <- sta_gath_72 %>%
#   spread(station, prcp) %>%
#   select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
# sta_M <- cor(sta_72)
# corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper =  "ellipse", 
# lower = "number", title = "Precipitation station correlation")  
```



```{r eda, include=FALSE, eval=FALSE}  

# find the plotting position: using Weibull; a = 0  
sta_mon <- sta_mon %>% 
  group_by(sta) %>% 
  arrange(depth_mm) %>%  
  mutate(position = pp(depth_mm)) %>% 
  ungroup() 

# plot the depths as a function of plotting position
ggplot(sta_mon, aes(position, depth_mm)) + 
  facet_grid(rows = vars(sta)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1980-2017") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

sta_big <- sta_mon %>%
  filter(depth_mm > 100)

summary(sta_big)
ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 


rm(sta_big)  


# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```



START HERE 
```{r lmoments, include=FALSE, eval=FALSE}

# Calculate L-moment ratios 
lmom_sta <- sta_mon %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$depth_mm)) %>%    
  transpose() %>%  
  as_tibble() %>%  
  select(lambdas, ratios) %>%  
    mutate(lambdas = map(lambdas,   
                         ~as_tibble(t(.x)  
                                    )  
                         )    
           ) %>%  
    mutate(lambdas = map(lambdas,  
                         ~set_names(.x, 
                                    c("L1", "L2", "L3", "L4", "L5")  
                                    )  
                         )    
           ) %>%  
    mutate(ratios = map(ratios, 
                        ~as_tibble(t(.x)  
                                   )  
                        )  
           ) %>%
    mutate(ratios = map(ratios, 
                         ~set_names(.x,
                                    c("T1", "T2", "T3", "T4", "T5")  
                                    )  
                        )  
           ) %>% 
  unnest(lambdas) %>%
  unnest(ratios)  %>% 
  select(-T1) %>%
  rename(L_CV = T2) %>% 
  rename(L_skew = T3) %>%
  rename(L_kurtosis = T4) %>%
  select(L1, L_CV, L_skew, L_kurtosis) %>% 
  mutate(L1 = round(L1, 
                    digits = 1)  
         )  

# Get the station data based on L1 -- prepare for a join 
# pulled out the L1 data when it was a list 
# STA    L1 
# RAP  35.26 
# COT  36.29 
# ORA  37.24 
# GOR  42.42 
# ONI  43.94 
# MIS  45.74 

sta_l1 <- lmom_sta %>%  
  select(L1) %>%  
  mutate(L1 = round(L1,  
                    digits = 3)  
         ) %>% 
  arrange(L1) %>% 
  mutate(sta = c("RAP", "COT", "ORA", "GOR", "ONI", "MIS"))

lmom_sta <- full_join(sta_l1, lmom_sta, 
                  by = "L1") %>% 
  select(sta, everything()) %>% 
  gather(key = type, value = val, -sta) %>% 
  mutate(val = round(val, 
                     digits = 2) 
         ) %>% 
  spread(type, val)  
# Note that we might consider Weiss 1964 bias value of 1.018 for L1

# get the lenths of the datasets
sta_count <- sta_mon %>%
  count(sta)  

# join the number of years to the station
lmom_sta <- left_join(lmom_sta, sta_count, by = "sta")

# calculate weighted means for regional L-moments -- 
# not really useful in this case 
L1  <- weighted.mean(lmom_sta$L1, lmom_sta$n)
L_CV  <- weighted.mean(lmom_sta$L_CV, lmom_sta$n)
L_skew  <- weighted.mean(lmom_sta$L_skew, lmom_sta$n)
L_kurtosis  <- weighted.mean(lmom_sta$L_kurtosis, lmom_sta$n)
n       <- sum(lmom_sta$n)

# combine the output into a single weighted mean
lmom_reg <- cbind(L1, L_CV, L_skew, L_kurtosis, n) %>% 
  as_tibble() %>% 
  mutate(sta = "WtMean") %>% 
  select(sta, everything()) 

rm(sta_l1, sta_count, L1, L_CV, L_skew, L_kurtosis, n)
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2  
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list 
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble() 

gev <- lmrdia[[5]] %>% as.tibble()  

glo <- lmrdia[[6]] %>% as.tibble()  

gpa <- lmrdia[[7]] %>% as.tibble()  

gno <- lmrdia[[9]] %>% as.tibble()  

gov <- lmrdia[[10]] %>% as.tibble()   

pe3 <- lmrdia[[12]] %>% as.tibble()   

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
               rename(GEV = V2.x) %>% 
               rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything())  
  
rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}
# plots the theo distributions, the sample vals, and regional mean 
ggplot() + 
  geom_line(data = lmom_theo, aes(L_skew, L_kurtosis,   
                                  group = distribution,   
                                  linetype = distribution)) +  
  geom_point(data = lmom_sta, 
             aes(L_skew, L_kurtosis)) +  
  geom_point(data = lmom_reg, 
             size = 4, 
             show.legend = FALSE, 
             aes(L_skew, L_kurtosis)) +  
  xlim(-0.25, 0.5) + 
  ylim(-0.25, 0.5) + 
  theme_bw() + 
  labs(x = "L-skew", 
       y = "L-kurtosis") + 
  ggtitle("L-moment diagram for monthly precipitation depths",  
          subtitle = "1990-2018")  

ggplot2::ggsave(filename = "lmom_plot.png",  
                width = 6, height = 6, units = "in")  

rm(lmom_reg, lmom_sta, lmom_theo)  

``` 



# OLD CODE 
```{r import-data} 

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# General Purpose: prepare data for drought index   
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) 

# add a small value to zeros to solve a downstream issue 
# maybe have this fixed now using ts rather than date class
#sta_mon <- sta_mon %>% 
#  gather(key = "station", value = "depth", -date, -year, -month) %>%
#  mutate(depth = replace(depth, depth == 0.0, 0.15)) %>%
#  spread(station, depth)

# make the wide data long, remove NA vals, sqrt transform
sta_mon <- sta_mon %>%
  gather(key = "station", value = "depth", -date, -year, -month) %>%
  drop_na(depth)  # %>% 
#  mutate(sqrt_depth = sqrt(depth)) 

# Check on log transformation
#sta_notzero <- sta_mon %>%
#  filter(depth != 0)

#min <- min(sta_notzero$depth)

#sta_zero <- sta_mon %>%
#  filter(depth == 0) %>%
#  mutate(depth = depth + min/2)

#sta_log <- bind_rows(sta_zero, sta_notzero)
#sta_log <- sta_log %>%
#  mutate(log_depth = log10(depth))

#rm(min) 
```

```{r precip-boxplot, include=FALSE, eval=FALSE} 

sta_sum <- as.tibble(summary(sta_mon))
  
# plot the precip data as a boxplot
ggplot(sta_grp, aes(as.factor(station), depth)) +
  geom_violin() +
  geom_boxplot() +
#  scale_y_sqrt() +
#  scale_y_log10() +
  scale_y_sqrt() +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") +
  xlab("") + 
  ylab("Monthly depth in mm") +
  NULL

#ggplot2::ggsave(filename = "rf_boxplot.png", 
#                width = 6, height = 6, units = "in")
```

```{r eda_fiddling, include=FALSE, eval=FALSE}
sta_big <- sta_grp %>%
  filter(depth > 100)
summary(sta_big)
ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```

```{r station_plotting_postion, include=FALSE, eval=FALSE} 
# find the plotting position: using Weibull; a = 0  
sta_grp <- sta_grp %>% 
  group_by(station) %>% 
  arrange(depth) %>% 
  mutate(position = pp(depth)) %>% 
  ungroup() 

# find the plotting position for log-data: using Weibull; a = 0
#sta_log <- sta_log %>% 
#  group_by(station) %>% 
# arrange(log_depth) %>% 
#  mutate(position = pp(log_depth)) %>% 
#  ungroup() 

# plot the depths as a function of plotting position
ggplot(sta_grp, aes(position, depth)) + 
  facet_grid(.~station) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

#ggplot2::ggsave(filename = "rf_freq_plot.png", 
#                width = 6, height = 6, units = "in")

```

```{r lmoments, include=FALSE, eval=FALSE}
# This is a long bit of code that takes things out of a list 

lmom_sta <- sta_grp %>% 
  split(.$station) %>%
  map(~ lmoms(.$depth)) %>%
  transpose() %>%
  as_tibble() %>%
  select(lambdas, ratios) %>%
    mutate(lambdas = map(lambdas, ~as_tibble(t(.x))))  %>%
    mutate(lambdas = map(lambdas, 
                         ~set_names(.x, c("L1", "L2", "L3", 
                                          "L4", "L5")))) %>%
      mutate(ratios = map(ratios, ~as_tibble(t(.x))))  %>%
    mutate(ratios = map(ratios, 
                         ~set_names(.x, c("T1", "T2", "T3", 
                                          "T4", "T5")))) %>% 
  unnest(lambdas) %>%
  unnest(ratios) %>%
  mutate(station = c("oel", "cot", "rap", 
         "int", "ora")) %>%
  select(-T1) %>%
  rename(L_CV = T2) %>% 
  rename(L_skew = T3) %>%
  rename(L_kurtosis = T4) %>%
  select(station, L1, L_CV, L_skew, L_kurtosis)

# Note that we might consider Weiss 1964 bias value of 1.018 for L1

# get the lenths of the datasets
sta_count <- sta_grp %>%
  count(station)  

# join the number of years to the station
lmom_sta <- left_join(lmom_sta, sta_count, by = "station")

# calculate weighted means for regional L-moments 
L1  <- weighted.mean(lmom_sta$L1, lmom_sta$n)
L_CV  <- weighted.mean(lmom_sta$L_CV, lmom_sta$n)
L_skew  <- weighted.mean(lmom_sta$L_skew, lmom_sta$n)
L_kurtosis  <- weighted.mean(lmom_sta$L_kurtosis, lmom_sta$n)
n       <- sum(lmom_sta$n)

# combine the output into a single weighted mean
int1     <- cbind(L1, L_CV)
int2     <- cbind(int1, L_skew)
int3     <- cbind(int2, L_kurtosis)
lmom_reg <- cbind(int3, n)

rm(L1, L_CV, L_skew, L_kurtosis, n, int1, int2, int3, sta_count)

# finalize the regional L-moment
lmom_reg <- as.tibble(lmom_reg) %>%
  mutate(station = "WtMean") %>% 
  select(station, everything())
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2  
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list 
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble()

gev <- lmrdia %>% 
  extract2(5) %>% as.tibble()

glo <- lmrdia %>%
  extract2(6) %>% as.tibble()

gpa <- lmrdia %>%
  extract2(7) %>% as.tibble()

gno <- lmrdia %>%
  extract2(9) %>% as.tibble()

gov <- lmrdia %>%
  extract2(10) %>% as.tibble()

pe3 <- lmrdia %>%
  extract2(12) %>% as.tibble()

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
               rename(GEV = V2.x) %>% 
               rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything())  
  
rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}
# plots the theo distributions, the sample vals, and regional mean 
ggplot() + 
  geom_line(data = lmom_theo, aes(L_skew, L_kurtosis, 
                                  group = distribution, 
                                  linetype = distribution)) +
  geom_point(data = lmom_sta, aes(L_skew, L_kurtosis)) +
  geom_point(data = lmom_reg, aes(L_skew, L_kurtosis, 
                                  size = 2, show.legend = NA)) +
  theme_bw() + 
 # xlim(0.25, 0.5) +
 # ylim(0, 0.25) +
  xlim(0, 0.5) +
  ylim(0, 0.5) +
  ggtitle("L-moment diagram for monthly precipitation depth", 
          subtitle = "Weather stations near Pine Ridge Reservation, 1909-2018")

#ggplot2::ggsave(filename = "lmom_plot.png", 
#                width = 6, height = 6, units = "in")
``` 





