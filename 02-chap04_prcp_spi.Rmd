---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!--
This R markdown file refactors prior code for spi analysis

1) download and clean daily precip
2)  convert daily precip to monthly precip  
3)  prepare data for drought index 

Data:
Predominant datasets used are:  
1) NOAA data
2) USGS daily streamflow and station metadata,  
####3) Summary data from a QGIS analysis of ungaged watersheds of interest.  



1.2. Data munging to check, join and remove NAs 
1.3. Updated unit vals - originally in tenths of mm; now in mm 
1.4. Changed daily data into monthly and annual data 
1.5. Cross-validated monthly data 
1.6. Created plots 
2.1  Data read in from USGS website by dataRetrieval (cfs)
2.2  Data saved as array (.JSON), and flat format (.csv) 

# Next STEPS
1. Update variable names
2. Check on next steps from Chapter 2 list 
3. Describe the precipitation seasonality

## Variable naming convention:   
sta          precipitation station  
_meta        metadata  
_raw         the "mostly" raw dataset  
_geXX        data greater than or equal to year XX  
_geXXmYY     data greater than or equal to year XX and month YY   
_ltXXmYY     data less than year XX and month YY   
_clean       intermediate df - clean part of NA split ;-}  
_dirty       intermediate df - NA part of NA split ;-}  
_trans       final df - after cleaning  

gage         USGS streamgage station


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 'station' is a variable for NOAA weather station locations  
# 'gage' is a variable for USGS stream gages 
# 'site' is a variable for OST monitoring stations 


## Results: 
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?

Variable naming convention:   
# ~~~~~~~~~~~~~~~~~~~~~~~~~
gage_poss           possible USGS gaging stations in the study area 
gage_meta_poss      metadata for possible gaging stations
-- _int1            scratch df for pulling gages with integer fields 
-- _int2            scratch df for pulling gages with integer fields 
-- _char            scratch df for pulling gages with character fields 

--> 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)  
options(tibble.print_max = 70) # sets tibble output for printing       
```

```{r library, message=FALSE} 

# Get API key (aka, token) for downloading precip data at http://www.ncdc.noaa.gov/cdo-web/token
# token for NOAA API tied to jtinant@olc.edu -- see 'rnoaa' for details 
options(noaakey = "VpcuARumMpCfFyclKHPfvskEYnaiLJHD") 

# Sets up the library of packages  
library("here")              # identifies where to save work
library("rnoaa")             #  R wrapper for NOAA data inc. NCDC 
library("rio")               # more robust I/O - to import and clean data 
library("lubridate")         # easier dates
library("tidyverse") 
library("janitor")           # tools for examining and cleaning dirty data 
library('deldir')            # for Vorononi tesselation - Theissen polygons 
library("broom")             # tidies linear models 
library("corrplot")          # correlation plots

# Other packages     
#library("EGRET")            # Exploration and Graphics for RivEr Trends 
#library("dataRetrieval")    # USGS data import  
#library("measurements")     # eases measurement system manipulation 
#library("flextable")        # construct complex table with 'kable'  
#library("officer")          # facilitates '.docx' access for table export  
#library("forecast")         # for BoxCox.lambda 
#library("ggfortify")        # data vis tools for statistical analysis 
#library("stringi")          # character string processing facilities 
#library("sf")               # simple features--spatial geometries for R 
#library("anomalize")        # detect anomalies using the tidyverse  
#library("caret")            # classification and regression training 
#library("doMC")             # parallelization for caret 
#library("glmnet")           # fit a GLM with lasso or elasticnet regularization 
#library("dabestr")          # data analysis using bootstrap estimation 
#library("Metrics")          # evaluation metrics for machine learning  
#library("DataExplorer") #--- used for plot_correlation() --- 
#library("assertthat") 
 
#library('jsonlite') # tools for working with lists 
#library("magrittr") # provides aliases for easier reading
#library("maps") # outlines of continents, countries, states & counties
#library("mapdata") # higher-resolution outlines
#library('ggmap')
# library("RColorBrewer") - there is a better one?
# library("workflowr") # creates a research website
# library("colorspace")
# library("bookdown") # 
# library(unpivotr) # fix nasty Excel files
# library("friendlyeval")
# library("mathpix")                # support for 'Mathpix' image to 'LaTeX'   
# library("cowplot") 

# a useful description of commits:
# http://r-pkgs.had.co.nz/git.html

``` 

```{r import_prcp_metadata, eval=FALSE}

# Get possible stations -----------------------------------------------------  
# It's also possible to check station id with the mapping tool at:
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation 

# The geographical extent given as SElat, SElon, NWlat, NWlon
sta_meta_orig <- ncdc_stations(extent = c(42.5, -104.5, 45, -99.5), 
                               limit = 1000) %>%      # n = 777 
# get possible stations into a dataframe 
  pluck("data") %>% 
# turn min & max date into lubridates 
  mutate(mindate = ymd(mindate)) %>% 
  mutate(maxdate = ymd(maxdate)) %>%                 
# remove 'young' stations 
  filter(mindate < "1989-01-01") %>%                    # n = 369 
# remove 'dead' stations 
  filter(maxdate > "2018-01-01") %>%                    # n = 59  
# remove poor data coverage stations   
  filter(datacoverage == 1.0) %>%                       # n = 52  
# keep only the GHCND stations  
  filter(str_detect(id, "^GHCND"))                      # n = 49     

# create groups of stations by region 
sta_meta_orig <- sta_meta_orig %>% 
  mutate(north_group = case_when( 
    latitude > 43.5 ~ "N", 
    TRUE ~ "S") 
    ) %>% 
  mutate(east_group = case_when( 
    longitude > -100.67 ~ "E", 
    longitude > -102.33 ~ "C",  
    TRUE ~ "W") 
    )  %>% 
  mutate(group = str_c(north_group, east_group, sep = "")) %>%  
  select(-c(north_group, east_group)) %>%  
# "GHCND:" doesn't appear in calls to get Global Historical 
# Climatology Network (GHCN) Daily Data   
    separate(  
             col = id, 
             sep = ":",   
             into = c("type", "sta_id")  
             ) 

sta_meta_orig <- sta_meta_orig %>% 
  select(type, sta_id, name, group, longitude, latitude, everything())


export(sta_meta_orig, "data/sta_meta_orig.csv")  

```

```{r plot-theissen-initial, message=FALSE}

# Define the study area using data from the 'maps' package ------------------
# import polygon data - US counties 
counties <- map_data("county")  

# Filter out wyoming & sd counties  
counties_wysd <- subset(counties, region %in%  
   c("south dakota")) %>%  
  filter(   
    subregion == "meade" |  
    subregion == "lawrence" |  
    subregion == "pennington" |  
    subregion == "custer" |  
    subregion == "fall river" |   
    subregion == "shannon" |  
    subregion == "jackson" |  
    subregion == "bennett" |  
    subregion == "haakon" |   
    subregion == "todd" |  
    subregion == "mellette" |   
    subregion == "jones" |   
    subregion == "stanley" |   
    subregion == "lyman" |  
    subregion == "tripp" |  
    subregion == "hughes"   
    )  

# filter nebraska  counties -- some names are duplicated above
counties_ne <- subset(counties, region %in% 
   c("nebraska")) %>% 
  filter(subregion == "sioux" |
         subregion == "dawes" | 
         subregion == "sheridan" | 
         subregion == "box butte" |
         subregion == "cherry" | 
         subregion == "brown" | 
         subregion == "keya paha" | 
         subregion == "rock" 
         )  

counties <- bind_rows(counties_wysd, counties_ne) 

rm(counties_wysd, counties_ne) 

# create Theissen polygons --------------------------------------------------  
voronoi <- deldir(sta_meta_orig$lon, sta_meta_orig$lat) 

# get streamflow gages 
gage_meta_fin <- read_csv("data/gage_meta_fin.csv") %>% 
  filter(!is.na(sta))

# plot stations -------------------------------------------------------------  
# grabbed this code off of a website by googling 'Voroni diag. R'
sta_meta_orig %>% 
ggplot(aes(x = longitude, y = latitude)) +   
# plot weather stations   
  geom_point(  
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333")  +  
# add Theissen polygons  
  geom_segment(  
    aes(x = x1, y = y1, xend = x2, yend = y2),  
    size = 2,  
    data = voronoi$dirsgs,  
    linetype = 1, 
    color = "#FFB958") +  
# add county polygons   
  geom_polygon(data = counties,  
               aes(x = long, y = lat, group = group),  
              color = "black",  
              #linetype = "dashed",  
              fill = "NA") +  
# add stream flow gages  
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(70, 130, 180, 255,  
               maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333" 
    )  +  
  theme_bw() +   
  ggtitle("Original Theissen Polygon Map")  

# export map 
ggplot2::ggsave(filename = "figure/theissen_init.png",   
                width = 6, height = 6, units = "in")  
```

```{r plot-theissen-final_stations}

# select final stations -- based on central location + nearest neighbors 
sta_meta_plus <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW      
         str_detect(name, "^RAPID CITY 4") |     # NW-alt      
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^INTERIOR")     |     # NC-alt 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^KENNEBEC")     |     # NE-alt  
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^OELRICHS")     |     # SW-alt   
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^VALENTINE")    |     # SC-alt     
         str_detect(name, "^MISSION")      |     # SE  
         str_detect(name, "^WOOD")               # SE-alt   
         )     

# select final stations 
sta_meta <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW         
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^MISSION")            # SE
         )     

# update Theissen line segments  
voronoi <- deldir(sta_meta$lon, sta_meta$lat) 

# plot stations 
sta_meta %>%  
ggplot(aes(x = longitude, y = latitude)) + 
# add stations  
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333")  + 
# add Theissen polygons 
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2), 
    size = 2, 
    data = voronoi$dirsgs, 
    linetype = 1, 
    color = "#FFB958")  + 
# add county polygons   
  geom_polygon(data = counties, 
               aes(x = long, y = lat, group = group),  
              color = "black", 
              #linetype = "dashed", 
              fill = "NA") + 
# add gages 
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(130, 130, 180, 255, 
               maxColorValue = 255),
    pch = 21,
    size = 2,
    color = "#333333" 
    )  + 
#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "red", linetype = "dashed", fill = "NA") + 
  theme_bw() +
#  geom_text(data = sta_meta_orig, aes(label = name), size = 2.5) +
  ggtitle("Final Theissen Polygon Map")  

#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "black", linetype = "dashed", fill = "NA") + 

# export map 
ggplot2::ggsave(filename = "figure/theissen_fin.png",   
                width = 6, height = 6, units = "in")  

rm(voronoi, counties)

```  

```{r download_daily-precip}

# date function calls start one-year early for long-term drought calcs  
dateMin = "1989-01-01"      
dateMax = "2017-12-31"  

# download daily precip data from NOAA GHCN database ------------------------   
sta_dv_plus <- sta_meta_plus %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax)  
          )     

rm(dateMin, dateMax)  

# fix date & add year and month
sta_dv_plus <- sta_dv_plus %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything()) 

export(sta_dv_plus, file = "data/sta_dv_plus.csv")  
export(sta_meta_plus, file = "data/sta_meta_plus.csv")  

``` 

```{r fill_missing_precip_data}  

# import data from file 
sta_dv_plus   <- import(file = "data/sta_dv_plus.csv") %>% 
  mutate(date = ymd(date))  

sta_meta_plus <- import(file = "data/sta_meta_plus.csv")
sta_meta      <- import(file = "data/sta_meta.csv")

# join bits of daily data-plus to metadata   
sta_dv_plus <- right_join(sta_meta_plus, sta_dv_plus,  
                   by = c("sta_id" = "id"))   %>%  
  select(name, sta_id, date, year, month, mflag_prcp,  
         prcp, qflag_prcp, sflag_prcp)  %>% 
  select(date, everything())   

# check for missing years  
sta_check <- sta_dv_plus %>%  
  group_by(name, sta_id, year) %>%  
  summarise(num_day = n())  %>% 
  filter(num_day < 365) %>% 
  ungroup() 

# separate north-west data --------------------------------------------------  
sta_nw <-  sta_dv_plus %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>% 
  filter(!is.na(prcp)) 
  
sta_nw_nm <- sta_nw %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nw_len <- sta_nw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nw_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nw_nm$name) %>% 
  mutate(sta_id = sta_nw_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nw_fill <- anti_join(sta_nw_alt, sta_nw, by = "date") 

sta_nw <- bind_rows(sta_nw, sta_nw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nw_alt, sta_nw_nm, sta_nw_fill)  

# separate and fill north-central  ------------------------------------------  
sta_nc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^COTTONWOOD")) %>% 
  filter(!is.na(prcp)) 

sta_nc_nm <- sta_nc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nc_len <- sta_nc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nc_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nc_nm$name) %>% 
  mutate(sta_id = sta_nc_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nc_fill <- anti_join(sta_nc_alt, sta_nc, by = "date") 

sta_nc <- bind_rows(sta_nc, sta_nc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nc_alt, sta_nc_nm, sta_nc_fill)  

# separate and fill north-east ----------------------------------------------    
sta_ne <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ONIDA")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_ne_nm <- sta_ne %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_ne_len <- sta_ne %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_ne_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^KENNEBEC")) %>% 
  mutate(name = sta_ne_nm$name) %>% 
  mutate(sta_id = sta_ne_nm$sta_id)           

sta_ne_fill <- anti_join(sta_ne_alt, sta_ne, by = "date") 

sta_ne <- bind_rows(sta_ne, sta_ne_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_ne_alt, sta_ne_nm, sta_ne_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sw <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ORAL")) %>% 
  filter(!is.na(prcp))      

# prepare to change name 
sta_sw_nm <- sta_sw %>% 
  slice(1) %>% 
  select(name, sta_id) 

# get length 
sta_sw_len <- sta_sw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sw_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^OELRICHS")) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

sta_sw <-  sta_sw %>% 
  filter(!is.na(prcp))      

# download daily precip data for missing values--HotSprings     
dateMin = "2005-12-01"      
dateMax = "2011-05-31"  
sta_sw_alt <- meteo_tidy_ghcnd(stationid = "USC00394007",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

# fill missing values 
sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sw_alt, sta_sw_nm, sta_sw_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^GORDON")) %>% 
  filter(!is.na(prcp))     

# prepare to change name 
sta_sc_nm <- sta_sc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_sc_len <- sta_sc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sc_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^VALENTINE")) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)        

sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>%  
  arrange(name) %>%  
  distinct(date, .keep_all = TRUE)  

sta_check <- sta_sc %>%  
  filter(is.na(prcp))  

sta_sc <-  sta_sc %>%  
  filter(!is.na(prcp))   

# download daily precip data for missing values--FT Robinson 
# note: Hay Springs did not have all data 
dateMin = "2006-08-01"      
dateMax = "2012-03-31"  
sta_sc_alt <- meteo_tidy_ghcnd(stationid = "USC00253015",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)       

# fill missing values 
sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sc %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sc_alt, sta_sc_nm, sta_sc_fill)  

# separate south-west data --------------------------------------------------  
sta_se <-  sta_dv_plus %>% 
   filter(str_detect(name, "^MISSION")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_se_nm <- sta_se %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_se_len <- sta_se %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) 

# get alternate site data & prepare fill 
sta_se_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^WOOD")) %>% 
  mutate(name = sta_se_nm$name) %>% 
  mutate(sta_id = sta_se_nm$sta_id)        

sta_se_fill <- anti_join(sta_se_alt, sta_se, by = "date") 

sta_se <- bind_rows(sta_se, sta_se_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_se_alt, sta_se_nm, sta_se_fill)  

# rejoin daily data and lengths ---------------------------------------------  
sta_dv <- bind_rows(sta_nw, 
                    sta_nc, 
                    sta_ne, 
                    sta_sw, 
                    sta_sc, 
                    sta_se) 

sta_dv_len <- bind_rows(sta_nw_len, 
                        sta_nc_len, 
                        sta_ne_len, 
                        sta_sw_len, 
                        sta_sc_len, 
                        sta_se_len) %>% 
  mutate(max_count = max(count)) %>% 
  mutate(perc_complete =  
    round(count/max_count,  
    digits = 3) 
    ) %>% 
  select(-c(count, max_count))  

# check data & clean up -----------------------------------------------------   
sta_check <- sta_dv %>% 
  filter(is.na(prcp)) 


# join dv-lenth to metadata  
sta_meta <- right_join(sta_meta, sta_dv_len,  
                   by = c("sta_id", "name"))  

rm(sta_dv_len, sta_meta_orig, sta_meta_plus, sta_check, sta_dv_plus, 
   sta_nw, sta_nc, sta_ne, sta_sw, sta_sc, sta_se, 
  sta_nw_len, sta_nc_len, sta_ne_len, sta_sw_len, sta_sc_len, sta_se_len)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")  

``` 

```{r check_prcp_data_quality}  

# Table of Measurement Flag/Attributes -- mflag  
# Blank = no measurement information applicable           
# A     = precip depth is a multi-day total, accumulated since last meas         
# B     = precipitation total formed from two twelve-hour totals
# D     = precipitation total formed from four six-hour totals           
# H     = represents TMAX or TMIN or average of hourly values (TAVG)           
# K     = converted from knots            
# L     = temperature appears lagged with respect to reported hr of observation        # O     = converted from oktas  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
# W    = converted from 16-point WBAN code (for wind direction)  

sta_check <- sta_dv %>%  
  group_by(name, mflag_prcp) %>%  
  summarise(mflag = n())  

# flags are T & A flags  
sta_dv <- sta_dv %>%  
  select(-mflag_prcp)  

# Table of Quality Flag/Attributes  
# Blank = did not fail any quality assurance check  
# D     = failed duplicate check           
# G     = failed gap check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# M     = failed mega-consistency check            
# N     = failed naught check            
# O     = failed climatological outlier check            
# R     = failed lagged range check           
# S     = failed spatial consistency check            
# T     = failed temporal consistency check            
# W     = temperature too warm for snow            
# X     = failed bounds check           
# Z     = flagged as a result of an official Datzilla investigation  

sta_check <- sta_dv %>%  
  group_by(name, qflag_prcp) %>%  
  summarise(qflag = n())  

# flags are L flags  
sta_dv <- sta_dv %>%  
  select(-qflag_prcp)  

# Table Source Flag/Attributes   
# Blank = No source (i.e., data value missing)  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 6  = CDMP Cooperative Summary of the Day (NCDC DSI-3206)  
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder
# A  = U.S. Automated Surface Observing System (ASOS) real-time data 
# a  = Australian data from the Australian Bureau of Meteorology           
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# b  = Belarus update           
# C  = Environment Canada            
# E  = European Climate Assessment and Dataset (Klein Tank et al., 2002)            
# F  = U.S. Fort data             
# G  = Off Global Climate Observing System (GCOS) or other gov-supplied data   
# H  = High Plains Regional Climate Center real-time data            
# I  = International collection (non U.S. data received thru pers. contacts   
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# M  = Monthly METAR Extract (additional ASOS data)           
# N  = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)           
# Q  = Data from African countries w/ later permission granted 
# R  = NCDC Reference Network Database  
# r  = All-Russian Research Inst. Hydromet Information-World Data Center    
# S  = Global Summary of the Day (NCDC DSI-9618)  
#        NOTE: "S" values are derived from hourly synoptic reports   
#         exchanged on the Global Telecommunications System (GTS).  
#         Daily values derived in this fashion may differ significantly from 
#        "true" daily data, particularly for precip (i.e., use with caution).  
# s  = China Met Admin/Nat Met Info/Climate Data Center (http://cdc.cma.gov.cn)   
# T  = SNOwpack TELemtry (SNOTEL) data from Western Regional Climate Center   
# U  = Remote Automatic Weather Station (RAWS) data from West Reg Climate Centr  
# u  = Ukraine update           
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
# z  = Uzbekistan update  

sta_check <- sta_dv %>%  
  group_by(name, sflag_prcp) %>%  
  summarise(sflag = n())  

# flags are 0, 7, K, W, X, Z  
sta_dv <- sta_dv %>%  
  select(-sflag_prcp)  

rm(sta_check)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")
```

```{r prcp_daily2monthly}

sta_dv   <- import(file = "data/sta_dv.csv")  %>% 
  mutate(date = ymd(date))
sta_meta <- import(file = "data/sta_meta.csv")  


# add short names to metadata & update the stored file ----------------------  
sta_meta <- sta_meta %>%  
  mutate(sta = case_when(  
    str_detect(name, "^RAPID CITY R") ~ "RAP",  
    str_detect(name, "^COTTONWOOD")   ~ "COT",  
    str_detect(name, "^ONIDA")        ~ "ONI",  
    str_detect(name, "^ORAL")         ~ "ORA",  
    str_detect(name, "^GORDON")       ~ "GOR",      
    str_detect(name, "^MISSION")      ~ "MIS",  
    TRUE ~ "ERROR"  
         )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation,  
         perc_complete, group, everything())  

export(sta_meta, file = "data/sta_meta.csv")  

# add short names and create monthly data -----------------------------------  
scratch <- sta_meta %>%  
  select(sta, sta_id)  

sta_mon <- full_join(scratch, sta_dv,   
                     by = "sta_id") %>%  
  select(-c(name, sta_id)) %>%  
  group_by(year, month, sta) %>%   
  summarize(prcp_tenths = sum(prcp)) %>%  
  mutate(prcp_mm = prcp_tenths/10) %>%  
  select(-prcp_tenths) %>%  
  ungroup()  

rm(scratch)  

# spread results & create date with day at midpoint of month ----------------  
sta_mon_long <- sta_mon %>%  
  spread(sta, prcp_mm) %>%  
  mutate(day = 15) %>%  
  mutate(date = make_date(year = year, month = month, day = day)) %>%   
  select(date, year, month, everything()) %>%  
  select(-day) %>%  
  ungroup() %>% 
  filter(!is.na(date))

sta_mon <- sta_mon_long %>%  
  gather(key = sta, val = depth_mm,  
         -c(date, year, month))  

rm(sta_dv_check)  
rm(sta_mon_long, sta_dv)  

sta_check <- sta_mon %>% 
  filter(is.na(sta))     

export(sta_mon, file = "data/stations_monthly.csv")  
```

```{r summaries}

sta_summary_mon <- sta_mon %>% 
  group_by(sta, month) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(month) %>%
  arrange(sta)

sta_summary_yr <- sta_mon %>% 
  group_by(sta, year) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(year) %>%
  arrange(sta)

export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
export(sta_summary_yr, file = "data/sta_summary_yr.csv")  
```

```{r ggplot_monthly} 
sta_mon <- import(file = "data/stations_monthly.csv") %>% 
  mutate(date = ymd(date))

sta_mon %>% 
  group_by(sta) %>% 
  summarize(count = n())

sta_mon$sta <- factor(sta_mon$sta, 
                      levels = c("RAP", "ORA", "GOR", "COT", "ONI", "MIS"))

# plot monthly precips
sta_mon %>% 
ggplot(aes(month, depth_mm, group = month)) + 
  facet_grid(rows =vars(sta)) +
  geom_boxplot() +
  theme_bw() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "1990-2017") + 
       xlab("") +
       ylab("mm")


# plot annual precips
sta_mon %>% 
ggplot(aes(year, depth_mm, group = year)) + 
  facet_grid(rows =vars(sta)) +
  geom_boxplot() +
  theme_bw() + 
  labs(title = "Annual precipitation depths",
       subtitle = "1990-2017") + 
       xlab("") +
       ylab("mm")

#ggplot2::ggsave(path = "figure/", filename = "precip_mon.png", 
#                width = 6, height = 6, units = "in")
```

```{r correlation, eval=FALSE}  

# Compute a correlation matrix
data(mtcars)

corr <- round(cor(mtcars), 1)
corr


# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

# rewrite code from below
corr_ann   <- as.tibble(import("data/stations_yearly.csv")) %>% 
  gather(key = "station", value = "prcp",  -year, na.rm = TRUE) %>% 
  filter(year > 1972) %>% 
  spread(station, prcp) %>% 
  select(-year) %>% 
  cor()

# save the corrplot - see below -
# By default, RStudio enables inline output (notebook mode) on all R 
# Markdown documents, so you can interact with any R Markdown document # as though it were a notebook. If you have a document with which you 
# prefer to use the traditional console method of interaction, you can # disable notebook mode by clicking the gear in the editor toolbar and # choosing Chunk Output in Console.
corrplot.mixed(corr_ann, order = "hclust", 
                 addrect = 2, upper = "ellipse", 
                 lower = "number")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# this is the old code - saving to make sure it works in the future
# sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
# sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
# sta_yr <- sta_yr %>%
#   arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
# sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
#                    -year, factor_key = TRUE)

# filter NAs
# sta_gath_72 <- sta_gath %>%
#   filter(year > 1972) 

# spread remaining matrix & arrange from west to east
# sta_72 <- sta_gath_72 %>%
#   spread(station, prcp) %>%
#   select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
# sta_M <- cor(sta_72)
# corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper =  "ellipse", 
# lower = "number", title = "Precipitation station correlation")  
```

START HERE 

