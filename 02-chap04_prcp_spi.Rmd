---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!--  
These are some useful thoughts:    

lmomco <- citation("lmomco") 
a useful description of commits -- http://r-pkgs.had.co.nz/git.html  

# Some shortcuts in a code chunk styling format----  
# key-bindings====     
# insert operators####    
# pipe operator        %>% 	      Cmd+Shift+M
# assignment operator  <-    	    Option+-  

# Code chunks====  
# collapse                        Alt+L
# expand                          Shift+Alt+L
# collapse all                    Alt+O
# expand all                      Shift+Alt+O

# Navigation====
# insert section                 Cmd+Shift+R 
# jump to                        Shift+Alt+J

Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0   Formulate your question  
2.0   Read in your data  
3.0   Check the dataset 
3.1   Check the number of rows and columns.
3.2   Check the types of data
3.3   Look at the top and the bottom of your data 
3.4   Check your “n”s & NAs 
3.5   Validate with at least one external data source  
4.0   Try the easy solution first to answer question
5.0   Challenge your solution 
6.0  Follow up questions 


-->  

<!-- 
Purpose:  
This R markdown file calculates SCI 
Standardized precipitation index - SPI  
Streamflow runoff index - SRI  

Analysis steps 
1.0)  sets up the library and settings 

2.0)  downloads precipitation daily data and metadata (sta_orig); 
2.1)    creates groups by location;   
2.2)    creates short names;  
2.3)    downloads daily data for original stations (30-years);  
2.4)    removes incomplete years (sta_plus)  
2.5)    defines the final stations  (sta_fin)

3.0)  plots study area map;  
3.1)  prepares map elements: Theissen line segments, stream gages, location  
        data: states, counties, USA map-grob






-- identified & filled NA
2.0)  convert daily precip to monthly precip  
2.1)  Updated unit vals - originally in tenths of mm; now in mm 
3.0)  Created plots 


Data:
Predominant datasets used are:  
1) NOAA data from NOAA GHCN database - 60-years (1959-2019)
2) USGS daily streamflow and station metadata,  
3) 

3)  prepare data for drought index 


1. Recreated analysis from the lmomco text ch 12 (author?) in Tidyverse
2. Imported cleaned precipitation data (see 04_prcp-data_munging)   

5. Applied Weibull plotting position and graphed the data on sqrt plot
6. Calculated L-moments and L-moment ratios 
7. Calculated SPI for 'cot', 'oel', 'rap', 'int', and 'ora' datasets using Pearson III. 


3. Applied sqrt & log10 transform to explore effects on skew 
4. Explored the data with box plots, violin plot.


####3) Summary data from a QGIS analysis of ungaged watersheds of interest.  

## Results: Fits a PE3 distribution
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?  

Variable naming convention----  
a_session    list variable of session information  

precipitation and spi variables====  
sta           NOAA weather station locations  
  _dv         daily values  
  _meta       metadata  
  _orig       all stations (n = 46 sations)  
  _plus       ??? stations (n = 25 stations)  
  _fin        final stations used in the analysis   

mapping variables==== 
voronoi       Theissen polygons  
usa           lower-48 states  
states        lower-48 states & counties 
counties      lower-48 counties  
  _sd         South Dakota  
  _ne         Nebraska  
  _wy         Wyoming  
  _bound      boundary -- a state line  
bbox          bounding box  
usmap         USA map as a grob  

daily value fill variables==== 
sta  
_mon         monthly precipitation depths
_alt         alternate station dv data for filling missing values 
_fil         station to be filled 
_miss_day    checks for missing days 
_short_name  adds a short name for the station to be filled
_rap         Rapid City Regional Airport station (NW)  
_cot         Cottonwood station (NC)  
_oni         Onida station (NE)   
_ora         Oral station (SW)   
_gor         Gordon staion (SC)   
_mis         Mission station (SE)  


sta_input)   
dateMin, 
sta_meta_input,

spi           Standardized Precipitation Index vals from NOAA precip data  

01          one-month SPI etc.
_freq       frequency of periodicity -- used in seasonality calculations  
_trend      trend of periodicity -- used in seasonality calculations  
  
L-moment variables====  
lmom          L-moments  
_sta          L-moments for stations
_reg          Weighted mean of L-moments for stations
  
L1           first L-moment, similiar to mean
L_CV         first L-moment ratio, similiar to coefficient of var
L_skew       second L-moment ratio, similiar to skewness 
L_kurtosis   third L-moment ratio, similiar to kurtosis
n            number of months in a given record
  
Parameters used in the L-moment diagram####  
aep4         4-Parameter Asymmetric Exponential Power Distribution 
gev          Generalized Extreme Value Distribution
glo          Generalized Logistic Distribution
gpa          Generalized Pareto Distribution
gno          Generalized Logistic Distribution 
gov          Govindarajulu Distribution
pe3          Pearson Type III Distribution  

gage          USGS streamgage station  

'site' is a variable for OST monitoring stations 

## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?

## Narrower question:
What is the underlying distribution of precipitation data?  

# Next STEPS  
3. Describe the precipitation seasonality
-->  

```{r setup_&_library, message=FALSE}  
  
knitr::opts_chunk$set(echo = FALSE)     
options(tibble.print_max = 70) # sets tibble output for printing       

# increase memory allocation for dabest() 
# usethis::edit_r_environ("project") 

# Get API key (aka, token) for downloading precip data at   http://www.ncdc.noaa.gov/cdo-web/token  
# token for NOAA API tied to jtinant@olc.edu -- see 'rnoaa' for details  
options(noaakey = "VpcuARumMpCfFyclKHPfvskEYnaiLJHD")  

# Sets up the library of packages   
library("here")              # identifies where to save work  
library("rnoaa")             # R wrapper for NOAA data inc. NCDC  
library("rio")               # more robust I/O - to import and clean data  
library("lubridate")         # easier dates   
library("lmomco")            # lmoments to find distribution   
library('deldir')            # for Vorononi tesselation - Theissen polygons  
library("SCI")               # calculates SPI & RDI   
library("forecast")          # using the BoxCox function   
library("broom")             # tidies linear models   
library("ggbeeswarm")        # plot 1D data as a violin / beeswarm plot  
library("scales")            # graphical scales map data to aesthetics, 
#   & methods for determining breaks and labels 
#   for axes and legends  
library("anomalize")         # detect anomalies using the tidyverse   
library("dabestr")           # data analysis using bootstrap estimation   
library("cowplot")  
#library("flextable")        # construct complex table with 'kable'  
#library("officer")          # facilitates '.docx' access for table export   
library("tidyverse") 

# other packages I have thought about ---------------------------------------  
# library("ggfortify")        # data vis tools for statistical analysis 
# library("ggpubr")           # some easy ggplot wrappers for publication ready                               #   'ggplot2'- based plots

# library("standardize")      # tools for controlling continuous variable 
#  scaling and factor contrasts for linear models 
# library("pdftools")         # utilities for extracting text, fonts, 
#   attachments and metadata from a pdf file.

# Packages to consider----
# Spatial data====
#library("biogeo")            # Functions for error detection & correction 
#   in point-data datasets; includes functions  
#   to parse & convert coords to decimal-degrees
#library("maps")              # Outlines of continents, countries, states & 
#   counties  
#library("mapdata")           # higher-resolution outlines  
#library('ggmap')             # Spatial visualization with ggplot2
#library("sf")                # Simple features--spatial geometries for R  
#library("RColorBrewer")      # Provides color schemes for maps -- see
#   http://colorbrewer2.org

# Colors====
#library("colorspace")        # Manipulate & assess colors & palettes  
#library("munsell")           # Access & manipulate munsell system colours  
#   https://github.com/cwickham/munsell

# working with lists and urls====
#library("jsonlite")          # Convert between JSON data and R objects
#library("curl")              # Drop-in replacement for base url
#library("listviewer")        # htmlwidget for interactive views of R lists  


#library("forecast")         # for BoxCox.lambda  
#library("magrittr") # provides aliases for easier reading  
#library("workflowr") # creates a research website  
#library("bookdown") #  
#library(unpivotr) # fix nasty Excel files  
#library("friendlyeval")  
#library("mathpix")                # support for 'Mathpix' image to 'LaTeX'   
#library("grateful") - not yet ready for R 3.5.0
#lmomco <- citation("lmomco")  
#toBibtex(lmomco)  

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
why_to_write <- function() 
{today <- today(tzone = "")
paper2 <- ymd("2019-11-15") 
until <- paper2 - today
print(paste("You have", until, "days until the second paper is due"
))
}
why_to_write()

```  

```{r download_prcp_data, eval=FALSE} 
 
# Get possible station metadata----   
# It's also possible to check station id with the mapping tool at:    
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation   
  
# The geographical extent given as SElat, SElon, NWlat, NWlon  
sta_meta_orig <- ncdc_stations(extent = c(42.0, -104.5, 45, -99.5),  
                               limit = 1000) %>%      # n = 777  
  # get possible stations into a dataframe  
  pluck("data") %>%  
  # turn min & max date into lubridates  
  mutate(mindate = ymd(mindate)) %>%  
  mutate(maxdate = ymd(maxdate)) %>%                  
  # remove 'young' stations  
  filter(mindate < "1988-01-01") %>%                    # n = 434  
  # remove 'dead' stations  
  filter(maxdate > "2018-01-01") %>%                    # n =  66    
  # keep only the GHCND stations  
  filter(str_detect(id, "^GHCND"))  %>%                 # n =  60      
  filter(!str_detect(name, "^MAGPIE")) %>%              # n =  59  
  filter(elevation < 1200)                              # n =  46    
  
# create groups of stations by region====      
#   Notes: the string "GHCND:" doesn't appear in calls to get Global Historical  
#   Climatology Network (GHCN) Daily Data  
sta_meta_orig <- sta_meta_orig %>%  
  mutate(north_group = case_when(  
    latitude > 43.5 ~ "N",  
    TRUE ~ "S")  
  ) %>%  
  mutate(east_group = case_when(  
    longitude > -100.67 ~ "E",  
    longitude > -102.33 ~ "C",   
    TRUE ~ "W")  
  )  %>%  
  mutate(group = str_c(north_group, east_group, sep = "")) %>%  
  select(-c(north_group, east_group)) %>%  
  separate(  
    col = id,  
    sep = ":",   
    into = c("type", "sta_id")   
  ) %>%  
  arrange(name)  
  
# add short names to metadata====   
sta_meta_orig <- sta_meta_orig %>%  
  mutate(sta = case_when(  
    str_detect(name, "^AINSWORTH")        ~ "AIN",  
    str_detect(name, "^BELLE FOURCHE 22") ~ "BE2",      
    str_detect(name, "^BELLE FOURCHE,")   ~ "BEL",       
    str_detect(name, "^CHADRON")          ~ "CHA",      
    str_detect(name, "^COTTONWOOD")       ~ "COT",  
    str_detect(name, "^DUPREE")           ~ "DUP",  
    str_detect(name, "^EDGEMONT")         ~ "EDG",   
    str_detect(name, "^ELLSWORTH")        ~ "ELL",  
    str_detect(name, "^ELM SPRINGS")      ~ "ELM",  
    str_detect(name, "^ELSMERE")          ~ "ELS",  
    str_detect(name, "^FORT MEADE")       ~ "FME",      
    str_detect(name, "^FORT PIERRE")      ~ "FPI",  
    str_detect(name, "^FORT ROBINSON")    ~ "FRO",      
    str_detect(name, "^GORDON")           ~ "GOR",    
    str_detect(name, "^HARROLD")          ~ "HAR",      
    str_detect(name, "^HAY SPRINGS")      ~ "HAY",   
    str_detect(name, "^HOT SPRINGS")      ~ "HOT",  
    str_detect(name, "^INTERIOR")         ~ "INT",  
    str_detect(name, "^KENNEBEC")         ~ "KEN", 
    str_detect(name, "^KIRLEY")           ~ "KIR",     
    str_detect(name, "^KYLE")             ~ "KYL", 
    str_detect(name, "^MAURINE")          ~ "MAU",     
    str_detect(name, "^MILESVILLE")       ~ "MIL",    
    str_detect(name, "^MISSION")          ~ "MIS",  
    str_detect(name, "^MULLEN")           ~ "MUL",  
    str_detect(name, "^MURDO")            ~ "MUR",      
    str_detect(name, "^NEWELL")           ~ "NEW",    
    str_detect(name, "^OAHE DAM")         ~ "OAH",      
    str_detect(name, "^NEWELL")           ~ "NEW",    
    str_detect(name, "^OELRICHS")         ~ "OEL",  
    str_detect(name, "^ONIDA")            ~ "ONI",  
    str_detect(name, "^ORAL")             ~ "ORA",  
    str_detect(name, "^PHILIP")           ~ "PHI",  
    str_detect(name, "^PLAINVIEW")        ~ "PLA",  
    str_detect(name, "^PIERRE")           ~ "PIE",  
    str_detect(name, "^PURDUM")           ~ "PUR",   
    str_detect(name, "^RAPID CITY 4")     ~ "RA4",        
    str_detect(name, "^RAPID CITY R")     ~ "RAP",   
    str_detect(name, "^SPEARFISH")        ~ "SPE",        
    str_detect(name, "^SPRINGVIEW")       ~ "SPR",      
    str_detect(name, "^RED OWL")          ~ "RED",  
    str_detect(name, "^REDBIRD")          ~ "REB",  
    str_detect(name, "^VALENTINE MILLER") ~ "VAL",  
    str_detect(name, "^VALENTINE NWR")    ~ "VNW",  
    str_detect(name, "^WASTA")            ~ "WAS",  
    str_detect(name, "^WINNER")           ~ "WIN",     
    str_detect(name, "^WOOD")             ~ "WOO",  
    TRUE ~ "ERROR"  
  )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation, group, everything())  
  
# download daily precip data from NOAA GHCN database-all_stations====      
# date function calls start one-year early for long-term drought calcs  
dateMin = "1959-01-01"      
dateMax = "2018-12-31"    
  
sta_dv_orig <- sta_meta_orig %>%   
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags   = TRUE,   
                            var          = "PRCP",   
                            date_min     = dateMin,   
                            date_max     = dateMax)  
  )     
  
# fix date & add year and month & add metadata====   
sta_dv_orig <- sta_dv_orig %>%   
  mutate(date  = ymd(date)) %>%   
  arrange(desc(date)) %>%  
  mutate(year  = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   
   
sta_dv_orig <- left_join(sta_dv_orig, sta_meta_orig,  
                         by = c("id" = "sta_id"))  
  
# check for missing years====           
sta_miss_yr <- sta_dv_orig %>%    
  group_by(name, year)     %>%   
  summarise(num_day = n()) %>%   
  filter(num_day < 345)    %>%   
  ungroup() %>%  
  group_by(name) %>%  
  summarise(num_year = n()) %>%  
  filter(num_year > 3)   
   
# remove stations with 3 years of > 95% completeness====     
sta_dv_plus <- anti_join(sta_dv_orig, sta_miss_yr,   
                         by = "name")    
   
sta_meta_plus <- semi_join(sta_meta_orig, sta_dv_plus,     
                           by = "name")   

# select final stations for each group====   
# this was created from the map below   
sta_meta_fin <- subset(sta_meta_orig,   
                       sta %in%  
                         c("RAP",  
                           "COT",  
                           "ONI",  
                           "OEL",  
                           "GOR",  
                           "MIS"  
                         )    
)   
  
sta_dv_fin <- semi_join(sta_dv_plus, sta_meta_fin,  
                        by = "sta")  
  
sta_dv_alt <- sta_dv_orig %>% 
  filter(sta == "RAP" |  
           sta == "COT" |   
           sta == "ONI" |   
           sta == "OEL" |   
           sta == "GOR" |  
           sta == "MIS" |   
           sta == "DUP" |
           sta == "ELL" |  
           sta == "ELM" |  
           sta == "HOT" |  
           sta == "INT" | 
           sta == "KEN" | 
           sta == "MIL" |    
           sta == "MUL" |  
           sta == "ORA" | 
           sta == "PIE" |
           sta == "RA4" |
           sta == "VAL" | 
           sta == "VNW" | 
           sta == "WOO"  
  ) 
  
# export files and clean up global environment====     
export(sta_meta_orig, "data/sta_meta_orig.csv")    
export(sta_meta_plus, "data/sta_meta_plus.csv")    
export(sta_meta_fin, "data/sta_meta_fin.csv")    
export(sta_dv_alt, "data/sta_dv_alt.csv")   

rm(sta_miss_yr,  
   dateMin,  
   dateMax,  
   sta_dv_plus,  
   sta_dv_orig  
)     
  
```   

```{r plot-theissen-polygons, eval=FALSE}

# prepare map elements----  
# create Theissen line segments====      
voronoi        <- deldir(sta_meta_fin$lon, sta_meta_fin$lat)    

# import stream gages====  (NEED TO FIX THIS!)
gage_meta_fin  <- import("data/gage_meta_fin.csv") %>% 
  mutate(type = case_when(  
    sta == "" ~ "Inactive",  
    TRUE ~ "Active")  
    )  
  
# order stations for plotting====   
sta_meta_plus <- sta_meta_plus %>%  
  mutate(group = fct_relevel(group,   
                             "NW",  
                             "NC",  
                             "NE",  
                             "SW",  
                             "SC",  
                             "SE"   
  )  
  )   
  
sta_meta_orig <- sta_meta_orig %>%  
  mutate(group = fct_relevel(group,   
                             "NW",  
                             "NC",  
                             "NE",  
                             "SW",  
                             "SC",  
                             "SE"  
  )  
  )   
   
sta_meta_fin <- sta_meta_fin %>%  
  mutate(group = fct_relevel(group,   
                             "NW",  
                             "NC",  
                             "NE",  
                             "SW",  
                             "SC",  
                             "SE"   
  )  
  )   
   
# Define the study area using data from the 'maps' package-----   
# import polygon data for maps====   
usa            <- map_data("usa")    
states         <- map_data("state")   
counties       <- map_data("county")   
   
# filter counties for mapping====  
#   need to filter counties for SD & NE separately because of duplicate vals   
#   subset SD counties####   
counties_sd <- subset(counties,  
                      region %in%   
                        c("south dakota"))  
     
counties_sd <- subset(counties_sd,  
                      subregion %in%  
                        c("butte",  
                          "meade",  
                          "lawrence",  
                          "pennington",  
                          "custer",  
                          "fall river",  
                          "shannon",  
                          "jackson",  
                          "bennett",  
                          "haakon",    
                          "todd",     
                          "mellette",  
                          "jones",  
                          "stanley",  
                          "lyman",  
                          "tripp",  
                          "hughes",  
                          "ziebach",  
                          "dewey",  
                          "hughes",  
                          "sully",   
                          "potter"     
                        )  
)    
   
#   subset NE counties####  
counties_ne <- subset(counties,   
                      region %in%  
                        c("nebraska"))  
   
counties_ne <- subset(counties_ne,   
                      subregion %in%  
                        c("sioux",   
                          "dawes",   
                          "sheridan",   
                          "box butte",    
                          "cherry",  
                          "brown",   
                          "keya paha",    
                          "rock"        
                        )   
)   
    
#   join SD & NE counties####   
counties <- bind_rows(counties_sd, counties_ne)  
rm(counties_sd, counties_ne)   
  
# create SD-NE & SD-WY lines====   
ne_bound <- subset(counties,   
                   subregion %in%     
                     c("fall river",   
                       "shannon",  
                       "bennett",   
                       "todd",   
                       "tripp")) %>%  
  filter(lat < 43.1)      
  
wy_bound <- subset(counties,   
                   subregion %in%  
                     c("butte",      
                       "lawrence"  ,  
                       "pennington" ,  
                       "custer",      
                       "fall river",  
                       "sioux"  
                     )) %>%  
  filter(long < -104)  
     
# plot US map as a grob----   
# create a bounding box -- order is SW, NW, NE, SE====    
bbox <- tibble(lat = c(42.0, 45.0, 45.0, 42.0),  
               long = c(-104.5, -104.5, -99.5, -99.5),  
               group = c(1, 1, 1, 1)  
)  
  
# plot US map====     
usmap <- ggplot() +  
  geom_polygon(data = usa,  
               aes(x=long, y = lat, group = group),   
               fill = "gray70",  
               color = "gray70"  
  ) +       
  coord_fixed(1.3) +  
  theme_nothing() +  
  geom_polygon(data = bbox,  
               aes(x=long, y = lat, group = group),  
               fill = "gray30",  
               color = "gray30") +   
  xlab("") +   
  ylab("")   
     
# change the US map into a grob####   
usmap <- as_grob(usmap, device = NULL)  
  
rm(usa)  
  
# plot weather station map====    
# Legend.position can be a numeric vector c(x,y), where x and y are the   
#   coordinates of the legend box between 0 and 1.  
#     c(0,0) corresponds to the “bottom left” and   
#     c(1,1) corresponds to the “top right” position.  
  
ggplot() +   
  theme_classic() +  
  coord_fixed(1.3) +  
  xlab("") +    
  ylab("") +      
  theme(axis.line         = element_line(colour = "grey60")) +  
  theme(axis.ticks        = element_line(colour = "grey60")) +  
  theme(axis.text         = element_text(colour = "grey50")) +  
  theme(legend.position   = c(1.1, 0.4)) +    
  theme(legend.title      = element_text(size = 10)) +   
  theme(legend.text       = element_text(size = 8)) +  
  theme(legend.key.height =  unit(0.5, 'cm')) +  
  guides(fill  = guide_legend(order = 2),  
         color = guide_legend(order = 1)  
  ) +     
  # add counties      
  geom_polygon(data      = counties,   
               aes(x     = long,  
                   y     = lat,     
                   group = group),   
               color     = "gray80",  
               fill      = "NA") +   
  # add NE stateline boundary    
  geom_line(data      = ne_bound,   
            aes(x     = long,    
                y     = lat,     
                group = group),  
            color     = "gray60"  
  ) +   
  #   # add WY stateline boundary    
  geom_line(data = wy_bound,   
            aes(x     = long,  
                y     = lat,    
                group = group),  
            color     = "gray60"    
  ) +    
# add stream gaging stations     
  geom_point(data     = gage_meta_fin,  
             aes(dec_long_va,  
                 dec_lat_va,   
                 fill = factor(gage_meta_fin$type)   
                 ),   
             shape    = 24,  
             size     = 0.8,    
             stroke   = 0.5,  
             color    = "gray50"   
  ) +      
  scale_fill_grey(   
                  start      = 0.4,    
                  end        = 0.8,    
                  na.value   = "red",   
                  aesthetics = "fill",    
                  name       = 'Stream gages',   
                  guide      = 'legend'   
  ) +   
  # add orig stations   
  geom_point(data      = sta_meta_orig,   
             aes(x     = longitude,  
                 y     = latitude,   
                 color = factor(sta_meta_orig$group)   
             ),    
             size      = 1     
  ) +       
  # add plus stations    
  geom_point(data       = sta_meta_plus,  
             aes(x      = longitude,  
                 y      = latitude,   
                 color  = factor(sta_meta_plus$group)   
             ),       
             size       = 2  
  ) +  
  scale_colour_grey(   
                    start      = 0.2,    
                    end        = 0.8,    
                    na.value   = "red",   
                    aesthetics = "colour",  
                    name       = 'Station groups',  
                    guide      = 'legend'   
  ) +   
  # add final stations     
  geom_point(data      = sta_meta_fin,   
             aes(x     = longitude,   
                 y     = latitude,   
                 color = factor(sta_meta_fin$group)   
             ),       
             size      = 3   
  ) +   
  # add Theissen polygons   
  geom_segment(data = voronoi$dirsgs,   
               aes(x           = x1,   
                   y           = y1,   
                   xend        = x2,   
                   yend        = y2),   
               size            = 1,    
               linetype        = 1,    
               color           = "gray35"   
  ) +   
  geom_text(data          = sta_meta_fin,    
            aes(x         = longitude,   
                y         = latitude,  
                label     = sta),    
            vjust         = -0.8,  
            check_overlap = TRUE   
  ) +      
  # add the US map as a grob    
  annotation_custom(grob = usmap,    
                    xmin = -99.3,    
                    xmax = -97.6,   
                    ymin = 44.5,   
                    ymax = 45.5)    
  
# export map & clean up   
ggplot2::ggsave(filename = "figure/theissen_map.png",    
                width    = 7.5,  
                height   = 4.5,  
                units    = "in"  
)     
 
rm(bbox,   
   counties,   
   gage_meta_fin,  
   ne_bound,  
   sta_meta_plus,   
   states,   
   usmap,   
   voronoi,   
   wy_bound, 
   sta_meta_orig  
)    

```

```{r check_prcp_data_quality, eval=FALSE}  

# Table of Measurement Flag/Attributes -- mflag----    
# Blank = no measurement information applicable            
# A     = precip depth is a multi-day total, accumulated since last meas         
# B     = precipitation total formed from two twelve-hour totals  
# D     = precipitation total formed from four six-hour totals           
# H     = represents TMAX or TMIN or average of hourly values (TAVG)           
# K     = converted from knots            
# L     = temperature appears lagged w/ respect to reported hr of observation  
# O     = converted from oktas  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
# W    = converted from 16-point WBAN code (for wind direction)  

# Table of Quality Flag/Attributes----    
# Blank = did not fail any quality assurance check   
# D     = failed duplicate check           
# G     = failed gap check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# M     = failed mega-consistency check            
# N     = failed naught check            
# O     = failed climatological outlier check            
# R     = failed lagged range check           
# S     = failed spatial consistency check            
# T     = failed temporal consistency check            
# W     = temperature too warm for snow            
# X     = failed bounds check           
# Z     = flagged as a result of an official Datzilla investigation  

# Table Source Flag/Attributes----     
# Blank = No source (i.e., data value missing)  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 6  = CDMP Cooperative Summary of the Day (NCDC DSI-3206)   
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder  
# A  = U.S. Automated Surface Observing System (ASOS) real-time data  
# a  = Australian data from the Australian Bureau of Meteorology           
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# b  = Belarus update           
# C  = Environment Canada            
# E  = European Climate Assessment and Dataset (Klein Tank et al., 2002)      
# F  = U.S. Fort data             
# G  = Off Global Climate Observing System (GCOS) or other gov-supplied data   
# H  = High Plains Regional Climate Center real-time data            
# I  = International collection (non U.S. data received thru pers. contacts   
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# M  = Monthly METAR Extract (additional ASOS data)           
# N  = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)           
# Q  = Data from African countries w/ later permission granted  
# R  = NCDC Reference Network Database  
# r  = All-Russian Research Inst. Hydromet Information-World Data Center    
# S  = Global Summary of the Day (NCDC DSI-9618)  
#        NOTE: "S" values are derived from hourly synoptic reports   
#         exchanged on the Global Telecommunications System (GTS).  
#         Daily values derived in this fashion may differ significantly from  
#        "true" daily data, particularly for precip (i.e., use with caution).  
# s  = China Met Admn/Nat Met Info/Climate Data Centr (http://cdc.cma.gov.cn)   
# T  = SNOwpack TELemtry (SNOTEL) data from Western Regional Climate Center   
# U  = Remote Automatic Weather Station (RAWS) data from West Reg Climate Centr  
# u  = Ukraine update           
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
# z  = Uzbekistan update   

sta_check <- sta_dv_orig %>%  
  group_by(sta, mflag_prcp) %>%  
  summarise(mflag = n()) %>% 
  filter(mflag_prcp != " ") %>%  
  group_by(mflag_prcp) %>%    
  summarise(mflag = n()) %>% 
  ungroup()    

# Measurement flags are T & P flags  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   

sta_check <- sta_dv_orig %>%  
  group_by(sta, qflag_prcp) %>%  
  summarise(qflag = n()) %>% 
  filter(qflag_prcp != " ") %>%  
  group_by(qflag_prcp) %>%    
  summarise(qflag = n()) %>% 
  ungroup()    

# quality flags are D, I, K, L, O flags  
# D     = failed duplicate check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# O     = failed climatological outlier check            

sta_check <- sta_dv_orig %>%  
  group_by(sta, sflag_prcp) %>%  
  summarise(sflag = n()) %>% 
  filter(sflag_prcp != " ") %>%  
  group_by(sflag_prcp) %>%    
  summarise(sflag = n()) %>% 
  ungroup()    

# flags are 0, 7, B, H, K, W, X, Z  
sta_filled <- sta_filled %>%  
  select(-sflag_prcp)  

# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder  
# A  = U.S. Automated Surface Observing System (ASOS) real-time data  
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# H  = High Plains Regional Climate Center real-time data            
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           

rm(sta_check)   
  
```  

```{r fill-station-data, eval=FALSE}

# create a df of monthly precipitation values-NW-RAP---- 
station <- "RAP"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_dv_alt   %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^RAPID CITY 4")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ELM")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_rap <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-NC-COT----  
station <- "COT"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^MILES")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^INTERIOR")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# get alternate site data & check if sta_mis_day = 0  
# Phillip & Plainview were big zeros
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^DUPREE")) %>%        
  filter(!is.na(prcp))  

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# make a filled prcp df & clean up      
sta_cot <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-NE-ONI---- 
station <- "ONI"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^KENNE")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^PIERRE")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a summary & monthly prcp df & clean up       
sta_oni <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SW-OEL----   
station <- "OEL"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ORAL")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_oel <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SC-GOR----  
station <- "GOR"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^VALENTINE NWR")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^MULLEN")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^ELLSWORTH")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_gor <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# create a df of monthly precipitation values-SW-MIS----  
station <- "MIS"  
sta_fil <- sta_dv_alt %>% 
  filter(sta == station)  

# fill missing data & check for missing days     
sta_fil <- sta_fil       %>%   
  filter(sta == station) %>%   
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%     
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)     

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)   

# get alternate site data & check if sta_mis_day = 0      
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^WOOD")) %>%        
  filter(!is.na(prcp))    

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    

# get alternate site data & check if sta_mis_day = 0        
sta_alt <- sta_dv_alt %>%   
  filter(str_detect(name, "^VALENTINE MILLER")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)   

sta_fil <- bind_rows(sta_alt, sta_fil)   

sta_miss_day <- sta_fil        %>%    
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%   
  filter(num_day < 365)   

# make a filled prcp df & clean up     
sta_mis <- sta_fil  

rm(sta_alt,  
   sta_fil,  
   sta_miss_day,  
   sta_short_name  
   )     

# join the filled data & clean up----  
sta_dv_fill <- bind_rows(sta_rap,  
                         sta_cot,  
                         sta_oni,  
                         sta_oel,  
                         sta_gor,  
                         sta_mis  
                         )  

rm(sta_rap,  
   sta_cot,  
   sta_oni,  
   sta_oel,  
   sta_gor,  
   sta_mis, 
   station  
)  

# create monthly data from the daily data----    
sta_mon <- sta_dv_fill          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year   = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

# export and clean-up data 
export(sta_dv_fill, "data/sta_dv_fill.csv")    
export(sta_mon, "data/sta_mon.csv")  

rm(sta_dv_alt,  
   sta_dv_fill)

```  

# use bootstrapping to check precip differences 
```{r bootstrap_precip}
  
# Construct bootstrap confidence intervals of precipitation----   
#   to identify differences in means   
# Monthly precipitation data is from 1954 to 2018 = 65 years  
#   Split data into two groups of 30 years & test for differences  
  
# Notes on dabest and memory -- see setup for how to increase memory  
# dabest for 14,027 obs; 40,000 reps takes ~2 hours.    
#   Still had memory allocation issues @ 50,000 reps after increasing memory.   
  
# import final monthly precip data & prepare for bootstrapping====   
sta_mon <- import("data/sta_mon.csv") %>%  
  mutate(date = ymd(date)) %>%  
  mutate(yr = year(date)) %>%     
  
  # create an id variable  
  mutate(Period = case_when(   
    yr < 1989 ~ "<1989",  
    TRUE ~ ">1989")  
  ) %>%  
  # create a grouping variable   
  unite("group",   
        c("sta", "Period"),  
        sep = "",   
        remove = FALSE) %>%   
  arrange(date)   
  
# print a summary of the data====  
sta_mon %>%   
  group_by(group) %>%   
  summarise(count = n()) %>%   
  ungroup() %>%   
  mutate(num_yr = count/(12))  
  
# construct bootstrap confidence interval of prcp====   
prcp_dabest <- sta_mon %>%   
  dabest(x = group,             # grouping variable  
         y = prcp,              # measurement variable   
# list order shows control as first group on the list   
         idx = list(c("RAP<1989",  
                      "OEL<1989",  
                      "COT<1989",  
                      "GOR<1989",  
                      "ONI<1989",  
                      "MIS<1989"),   
                    c("RAP>1989",  
                      "OEL>1989",  
                      "COT>1989",   
                      "GOR>1989",  
                      "ONI>1989",  
                      "MIS>1989")  
         ),  
         paired    = TRUE,  
         reps      = 20000,  
         id.column = group   
  )  
  
# plot the dabest bootstrap====   
plot(prcp_dabest,  
     color.column = Period,  
     tick.fontsize = 5,  
     rawplot.type = "sinaplot",  
     axes.title.fontsize = 9,  
     rawplot.ylabel = "Monthly precipitation (mm)",  
     rawplot.groupwidth = 0.3,  
     palette = "Greys")  
  
# save results====   
ggplot2::ggsave(filename = "figure/prcp_dif.png",   
                width = 6.5, height = 4.5, units = "in")  

rm(prcp_dabest)    
```



<!--
need to finish this 
```{r ci_result_table, eval=FALSE}  

# get mean precipitation 
prcp <- spi_fin %>%  
  group_by(group) %>%  
  summarise(prcp = mean(prcp)) %>% 
  ungroup() 

rap_bef <- prcp %>% 
  filter(group == "RAP<1989") %>% 
  select(-group) %>% 
  as.double()

# pluck results for summary tables & bind====
prcp_results <- prcp_dabest %>% 
  pluck("result") %>% 
  select(-c(func,  
            paired,  
            variable,  
            bootstraps,  
            nboots,  
            pct_ci_low,  
            pct_ci_high  
  )  
  )   

prcp_results <- full_join(prcp_results, prcp, 
                  by = c("test_group" = "group")  
                  ) %>% 
  filter(test_group != "RAP<1989") %>% 
  mutate(control_group = case_when(  
    is.na(control_group) ~ "RAP<1989",  
    TRUE ~ control_group  
    )
  ) %>%  
  mutate(difference = case_when(  
    is.na(difference) ~ prcp - rap_bef,  
    TRUE ~ difference  
    )
  ) %>% 
  mutate(ci = as.integer(ci))  %>% 
  mutate_if(is.numeric, round, digits = 0)  


# convert tibble to a flextable after fixing vars for presentation==== 
prcp_table <- prcp_table %>% 
  flextable() %>% 
  #  colformat_num(col_keys = col_key_num, 
  #                big.mark=",", 
  #                digits = 1, na_str = "N/A") %>% 
  set_header_labels(control_group = "Control Group", 
                    test_group = "Test Groups", 
                    control_size = "Control Size",
                    test_size = "Test Size", 
                    func = "Test Statistic", 
                    variable = "Variable",
                    difference = "Mean Difference", 
                    ci = "CI", 
                    bca_ci_low = "Lower Limit", 
                    bca_ci_high = "Upper Limit") %>% 
  autofit() %>% 
  align(., part = "all", align = "center") %>% 
  theme_booktabs() 


ci_table2 <- read_docx() %>% 
  body_add_flextable(value = ci_table)  

print(ci_table2, target = "output/ci_table.docx") 

rm(ci_results_pc1, ci_results_pc2, ci_table2, ci_table) 

```  
-->


```{r compare_locations}  

# the bootstrapping showed there is some difference between past & present  
sta_pres <- sta_fin %>%  
  filter(yr > 1988)  

export(sta_pres, "data/sta_pres.csv")  

# comparison of locations----   
# split out northwest & southeast to prepare for plot    
nw <- sta_pres %>%  
  filter(sta == "COT" |  
         sta == "RAP" |  
         sta == "OEL") %>%  
  select(sta, date, prcp) %>%  
  mutate(prcp = prcp + 1) %>%  
  spread(sta, prcp) %>%   
  gather(other, prcp, -c(date, COT)) %>%  
  mutate(location = "COT") %>%  
  rename(control = COT)   
  
se <- sta_pres %>%  
  filter(sta == "GOR" |  
         sta == "ONI" |  
         sta == "MIS") %>%  
  select(sta, date, prcp) %>%  
  mutate(prcp = prcp + 1) %>%  
  spread(sta, prcp) %>%   
  gather(other, prcp, -c(date, GOR)) %>%  
  mutate(location = "GOR")  %>%  
  rename(control = GOR)  
  
# join splits & create factor     
prcp_compare <- bind_rows(nw, se) %>%  
  mutate(diff = control - prcp) %>%    
  mutate(other = fct_relevel(other,  
                           "RAP",  
                           "OEL",  
                           "ONI",  
                           "MIS"  
  ))  
  
# plot location differences  
prcp_compare %>%   
ggplot(aes(date, diff)) +  
  theme_bw() +  
  xlab("") +  
  ylab("Monthly precipitation difference (mm)") +  
  facet_grid(cols    = vars(location),  
             rows    = vars(other)) +  
  geom_line(color    = "gray60") +  
  geom_smooth(method = "lm",  
              color  = "gray30",  
              size   = 0.5  
              )  
  
ggplot2::ggsave(filename = "figure/prcp_loc_dif.png",   
                width = 6.5, height = 4.5, units = "in")  

rm(nw,  
   se,  
   prcp_compare  
   )  

```

# L-moments 
```{r lmoments, include=FALSE, eval=FALSE}

# Note that we might consider Weiss 1964 bias value of 1.018 for L1   
  
# Calculate L-moment ratios   
lmom_sta <- sta_pres %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$prcp)) %>%    
  transpose() %>%  
  as_tibble() %>%  
  select(lambdas, ratios) %>%  
  mutate(lambdas = map(lambdas,   
                       ~as_tibble(t(.x))  
  )    
  ) %>%  
  mutate(lambdas = map(lambdas,   
                       ~set_names(.x,  
                                  c("L1", "L2", "L3", "L4", "L5")  
                       )  
  )    
  ) %>%  
  mutate(ratios = map(ratios,  
                      ~as_tibble(t(.x)   
                      )   
  )   
  ) %>%  
  mutate(ratios = map(ratios,  
                      ~set_names(.x,  
                                 c("T1", "T2", "T3", "T4", "T5")   
                      )   
  )   
  ) %>%  
  unnest(lambdas) %>%  
  unnest(ratios)  %>%   
  select(-T1) %>%  
  rename(L_CV = T2) %>%  
  rename(L_skew = T3) %>%  
  rename(L_kurtosis = T4) %>%  
  select(L1, L_CV, L_skew, L_kurtosis) %>%  
  mutate(L1 = round(L1,  
                    digits = 2)  
  )  

# get station names from the list  
lmom_sta_nm <- sta_pres %>%  
  split(.$sta) %>%  
  map(~ lmoms(.$prcp)) %>%  
  transpose()  

lmom_sta_nm <- lmom_sta_nm %>%  
  pluck(1) %>%  
  enframe()  

lmom_sta_nm <- lmom_sta_nm %>%  
  unnest(value)  

lmom_sta_nm <- lmom_sta_nm %>%  
  group_by(name) %>%  
  summarize(L1 = first(value)) %>%  
  ungroup() %>%  
  mutate(L1 = round(L1,  
                    digits = 2))  

# join name to lmom vals & to the metadata  
lmom_sta <- full_join(lmom_sta_nm, lmom_sta,  
                      by = "L1") %>%  
  rename(sta = name) %>%  
  mutate(L_CV = round(L_CV,  
                      digits = 2)  
  ) %>%  
  mutate(L_skew = round(L_skew,  
                        digits = 2)  
  ) %>%  
  mutate(L_kurtosis = round(L_kurtosis,  
                            digits = 2)  
  )  

# join the lmoments to the metadata  
sta_meta <- sta_meta_orig %>% 
  filter(sta == "RAP" |   
           sta == "COT" |   
           sta == "ONI" |   
           sta == "OEL" |   
           sta ==  "GOR" |  
           sta ==  "MIS"  
  )  
  
sta_meta <- full_join(sta_meta, lmom_sta,  
                           by = "sta")  

export(sta_meta, "data/sta_meta.csv")    
rm(sta_meta_orig)  
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2    
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list  
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions  
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble()   

gev <- lmrdia[[5]]  %>% as_tibble()  
glo <- lmrdia[[6]]  %>% as_tibble()  
gpa <- lmrdia[[7]]  %>% as_tibble()  
gno <- lmrdia[[9]]  %>% as_tibble()  
gov <- lmrdia[[10]] %>% as_tibble()   
pe3 <- lmrdia[[12]] %>% as_tibble()   

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
  rename(GEV = V2.x) %>% 
  rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)  

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything())  

rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}

# plot the theoretical distributions, and sample vals----    
# control plotting order    
sta_meta$group <- factor(sta_meta$group,    
                              levels = c(  
                                "NW",   
                                "NC",   
                                "NE",   
                                "SW",   
                                "SC",   
                                "SE"  
                                )  
)  

sta_meta %>% 
ggplot(aes(L_skew, L_kurtosis)
       ) + 
  facet_wrap(vars(group)) + 
  geom_point() +  
  geom_line(data = lmom_theo, 
            aes(L_skew, 
                L_kurtosis, 
                group = distribution, 
                linetype = distribution)) +  
  xlim(-0.25, 0.5) + 
  ylim(-0.25, 0.5) + 
  theme_bw() + 
  #  ggtitle("L-moment diagram for monthly precipitation depths",  
  #          subtitle = "1990-2018")  
  labs(x = "L-skew", 
       y = "L-kurtosis") 

ggplot2::ggsave(filename = "figure/lmom_plot.png",  
                width = 6, height = 4, units = "in")  

rm(lmom_theo,  
   lmom_sta,  
   lmom_sta_nm  
   )    

```  

# calculate SPI  
```{r spi_prepare, eval=FALSE}
  
# fitSCI identifies Standardized Climate Index (SCI) parameters----      
#   some notes about the SCI package:   
#     the SCI package doesn't like snake_case variables,    
#     need to change tibble to a vector as double:   
#       cot <- as.double(sta_cot$depth_mm)   
  
# Initial values for SCI calculations====     
time_scale <- 1     # sets the length of the averaging period   
distrib    <- "pe3" # sets the distribution type   
p_zero     <- TRUE  # sets a function to reduce zero-precip bias  
p_zero_cm  <- TRUE  # uses Weibull plotting position for p_zero    
scale      <- "sd"  # scales input by subtract mean & divide by sd    
warn_me    <- TRUE  # sets explicit warning   
first_mon  <- 1     # Set first month for each station  
sci.limit  <- 3     # Sets a limit of [-3, 3] for limit  
  
# spi function====                                         
spi.fun <- function(sta, time_scale) fitSCI( x = sta,   
                                             start.fun.fix  = TRUE,  
                                             time.scale     = time_scale,   
                                             first.mon      = first_mon,  
                                             distr          = distrib,  
                                             p0             = p_zero,    
                                             p0.center.mass = p_zero_cm,    
                                             scaling        = scale,    
                                             sci.limit      = sci_limit,   
                                             warn	         = warn_me    
)  

# prepare spi function vector inputs (1986-2018)----   
# north-west (NW)  
sta_rap <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "RAP")   
  
rap <- as.double(sta_rap$prcp)   

# north-central (NC)  
sta_cot <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "COT")   
  
cot <- as.double(sta_cot$prcp)   

# north-east (NE)  
sta_oni <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "ONI")    
  
oni <- as.double(sta_oni$prcp)   

# south-west (SW)  
sta_oel <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "OEL")   
  
oel <- as.double(sta_oel$prcp)   

# south-central (SC)  
sta_gor <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "GOR")    
  
gor <- as.double(sta_gor$prcp)   

# south-east (SE)  
sta_mis <- sta_pres %>%  
  arrange(date) %>% 
  filter(sta == "MIS")   
  
mis <- as.double(sta_mis$prcp)   

# calculate SPI-rap----  
# set up the station for spi & make spi list vars====  
sta      <- rap  
spi_1mo  <- spi.fun(sta, 1)                             
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                             
spi_4mo  <- spi.fun(sta, 4)    
spi_6mo  <- spi.fun(sta, 6)    
spi_9mo  <- spi.fun(sta, 9)    
spi_12mo <- spi.fun(sta, 12)   
  
# Apply the transformation identified by fitSCI function====  
spi_1mo  <- transformSCI(sta,    
                         first.mon = first_mon,  
                         obj       = spi_1mo) %>%  
  enframe(name                     = NULL)   
  
spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_2mo) %>%   
  enframe(name                     = NULL)  
  
spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = spi_3mo) %>%  
  enframe(name                     = NULL)  
  
spi_4mo  <- transformSCI(sta,    
                         first.mon = first_mon,   
                         obj       = spi_4mo) %>%  
  enframe(name                     = NULL)    
  
spi_6mo  <- transformSCI(sta,   
                         first.mon = first_mon,  
                         obj       = spi_6mo) %>%  
  enframe(name                     = NULL)  
  
spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,   
                         obj       = spi_9mo) %>%  
  enframe(name                     = NULL)  
  
spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_12mo) %>%   
  enframe(name                     = NULL)   
  
# bind and rename the spi vals====     
spi_rap <- bind_cols(sta_rap,     
                     spi_1mo,   
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,   
                     spi_6mo,  
                     spi_9mo,   
                     spi_12mo  
) %>%      
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%   
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%   
  rename(spi_6mo = value4)  %>%   
  rename(spi_9mo = value5)  %>%  
  rename(spi_12mo = value6) 
  
# clean up global environment====  
rm(spi_1mo,  
   spi_2mo,  
   spi_3mo,  
   spi_4mo,   
   spi_6mo,  
   spi_9mo,  
   spi_12mo  
   )      
rm(sta_rap, rap)   
  
# calculate SPI-cot----  
# set up the station for spi & make spi list vars   
sta      <- cot  
spi_1mo  <- spi.fun(sta, 1)                               
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                            
spi_4mo  <- spi.fun(sta, 4)    
spi_6mo  <- spi.fun(sta, 6)   
spi_9mo  <- spi.fun(sta, 9)   
spi_12mo <- spi.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function    
spi_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_1mo) %>%  
  enframe(name                     = NULL)  
  
spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_2mo) %>%  
  enframe(name                     = NULL)  

spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_3mo) %>%  
  enframe(name                     = NULL)  

spi_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_4mo) %>%  
  enframe(name                     = NULL)  

spi_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_6mo) %>%  
  enframe(name                     = NULL)  

spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_9mo) %>%  
  enframe(name                     = NULL)  

spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_12mo) %>%  
  enframe(name                     = NULL)  

# bind and rename the spi vals     
spi_cot <- bind_cols(sta_cot,     
                     spi_1mo,  
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,  
                     spi_6mo,  
                     spi_9mo,   
                     spi_12mo   
) %>%    
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%  
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%  
  rename(spi_6mo = value4)  %>%  
  rename(spi_9mo = value5)  %>%   
  rename(spi_12mo = value6) 

# clean up global environment  
rm(spi_1mo,  
   spi_2mo,  
   spi_3mo,  
   spi_4mo,  
   spi_6mo,  
   spi_9mo,  
   spi_12mo,  
   spi_24mo  
   )     
rm(sta_cot, cot)  


# calculate SPI-oni----  
# set up the station for spi & make spi list vars     
sta      <- oni    
spi_1mo  <- spi.fun(sta, 1)                              
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                            
spi_4mo  <- spi.fun(sta, 4)   
spi_6mo  <- spi.fun(sta, 6)   
spi_9mo  <- spi.fun(sta, 9)   
spi_12mo <- spi.fun(sta, 12)  

# Apply the transformation identified by fitSCI function      
spi_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_1mo) %>%  
  enframe(name                     = NULL)  

spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_2mo) %>%  
  enframe(name                     = NULL)  

spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_3mo) %>%  
  enframe(name                     = NULL)  

spi_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_4mo) %>%  
  enframe(name                     = NULL)  

spi_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_6mo) %>%  
  enframe(name                     = NULL)  

spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_9mo) %>%  
  enframe(name                     = NULL)  

spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_12mo) %>%  
  enframe(name                     = NULL)  

# bind and rename the spi vals     
spi_oni <- bind_cols(sta_oni,     
                     spi_1mo,  
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,  
                     spi_6mo,  
                     spi_9mo,  
                     spi_12mo  
) %>%  
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%  
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%  
  rename(spi_6mo = value4)  %>%  
  rename(spi_9mo = value5)  %>%  
  rename(spi_12mo = value6)  

# clean up global environment  
rm(spi_1mo,  
   spi_2mo,  
   spi_3mo,  
   spi_4mo,  
   spi_6mo,  
   spi_9mo,  
   spi_12mo  
   )    
rm(sta_oni, oni)  

# calculate SPI-oel----  
# set up the station for spi & make spi list vars  
sta      <- oel  
spi_1mo  <- spi.fun(sta, 1)                              
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                            
spi_4mo  <- spi.fun(sta, 4)   
spi_6mo  <- spi.fun(sta, 6)   
spi_9mo  <- spi.fun(sta, 9)    
spi_12mo <- spi.fun(sta, 12)  

# Apply the transformation identified by fitSCI function       
spi_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_1mo) %>%  
  enframe(name = NULL)  

spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_2mo) %>%  
  enframe(name = NULL)  

spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_3mo) %>%  
  enframe(name = NULL)  

spi_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_4mo) %>%  
  enframe(name = NULL)  

spi_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_6mo) %>%  
  enframe(name = NULL)  

spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_9mo) %>%  
  enframe(name = NULL)  

spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj = spi_12mo) %>%  
  enframe(name = NULL)  

# bind and rename the spi vals   
spi_oel <- bind_cols(sta_oel,     
                     spi_1mo,  
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,  
                     spi_6mo,  
                     spi_9mo,   
                     spi_12mo  
) %>%  
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%  
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%  
  rename(spi_6mo = value4)  %>%  
  rename(spi_9mo = value5)  %>%  
  rename(spi_12mo = value6)      

# clean up global environment        
rm(spi_1mo,  
   spi_2mo,  
   spi_3mo,  
   spi_4mo,  
   spi_6mo,  
   spi_9mo,  
   spi_12mo  
   )    
rm(sta_oel, oel)    

# calculate SPI-gor----  
# set up the station for spi & make spi list vars  
sta      <- gor    
spi_1mo  <- spi.fun(sta, 1)                              
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                            
spi_4mo  <- spi.fun(sta, 4)   
spi_6mo  <- spi.fun(sta, 6)   
spi_9mo  <- spi.fun(sta, 9)   
spi_12mo <- spi.fun(sta, 12)  

# Apply the transformation identified by fitSCI function        
spi_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_1mo) %>%  
  enframe(name                     = NULL)  

spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_2mo) %>%  
  enframe(name                     = NULL)  

spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_3mo) %>%  
  enframe(name                     = NULL)  

spi_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_4mo) %>%  
  enframe(name                     = NULL)  

spi_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_6mo) %>%  
  enframe(name                     = NULL)  

spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_9mo) %>%  
  enframe(name                     = NULL)  

spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the spi vals      
spi_gor <- bind_cols(sta_gor,     
                     spi_1mo,  
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,  
                     spi_6mo,  
                     spi_9mo,   
                     spi_12mo     
) %>%  
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%  
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%  
  rename(spi_6mo = value4)  %>%  
  rename(spi_9mo = value5)  %>%  
  rename(spi_12mo = value6)    

# clean up global environment    
rm(spi_1mo,   
   spi_2mo,  
   spi_3mo,   
   spi_4mo,  
   spi_6mo,  
   spi_9mo,  
   spi_12mo   
   )    
rm(sta_gor, gor)    

# calculate SPI-mis----  
# set up the station for spi & make spi list vars  
sta      <- mis  
spi_1mo  <- spi.fun(sta, 1)                              
spi_2mo  <- spi.fun(sta, 2)                             
spi_3mo  <- spi.fun(sta, 3)                            
spi_4mo  <- spi.fun(sta, 4)    
spi_6mo  <- spi.fun(sta, 6)   
spi_9mo  <- spi.fun(sta, 9)   
spi_12mo <- spi.fun(sta, 12)  
  
# Apply the transformation identified by fitSCI function     
spi_1mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_1mo) %>%  
  enframe(name                     = NULL)  
  
spi_2mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_2mo) %>%  
  enframe(name                     = NULL)  
  
spi_3mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_3mo) %>%  
  enframe(name                     = NULL)  
  
spi_4mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_4mo) %>%  
  enframe(name                     = NULL)  
  
spi_6mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_6mo) %>%  
  enframe(name                     = NULL)  
  
spi_9mo  <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_9mo) %>%  
  enframe(name                     = NULL)  
  
spi_12mo <- transformSCI(sta,  
                         first.mon = first_mon,  
                         obj       = spi_12mo) %>%  
  enframe(name                     = NULL)  
  
# bind and rename the spi vals     
spi_mis <- bind_cols(sta_mis,     
                     spi_1mo,  
                     spi_2mo,   
                     spi_3mo,  
                     spi_4mo,  
                     spi_6mo,  
                     spi_9mo,   
                     spi_12mo   
) %>%  
  rename(spi_1mo = value)   %>%  
  rename(spi_2mo = value1)  %>%  
  rename(spi_3mo = value2)  %>%  
  rename(spi_4mo = value3)  %>%  
  rename(spi_6mo = value4)  %>%  
  rename(spi_9mo = value5)  %>%  
  rename(spi_12mo = value6) 
  
# clean up global environment  
rm(spi_1mo,  
   spi_2mo,  
   spi_3mo,  
   spi_4mo,  
   spi_6mo,  
   spi_9mo,  
   spi_12mo   
   )   
rm(sta_mis, mis)      
  

# join spi data----  
spi_sta <- bind_rows(spi_rap,  
                 spi_cot,  
                 spi_oni,  
                 spi_oel,  
                 spi_gor,  
                 spi_mis,  
                 ) %>% 
  select(-c(Period, group)) %>%  
    # truncate spi vals to [-3, 3]    
  gather(key = type, val = value, -c(sta, date, yr, prcp)) %>%  
  mutate(value = case_when(     
    value < -3.0 ~ -3.0,    
    value >  3.0 ~  3.0,  
    TRUE ~ value)) %>%  
  spread(type, value) %>%    
  # move the longer spi-values to the end of the df    
  select(-spi_12mo, spi_12mo)    
  
# export spi data        
#sta_meta_fin <- semi_join(sta_meta_orig, spi_fin,   
#                          by = "sta")    
#export(sta_meta_fin, "data/sta_meta_fin.csv")  

export(spi, "data/spi_sta.csv")  


# clean up workspace    
rm(dateMax,  
   distrib,  
   first_mon,   
   p_zero,   
   p_zero_cm,   
   scale,   
   sci.limit,   
   sta,   
   time_scale,   
   warn_me,  
   spi.fun,  
   station,  
   spi_cot,  
   spi_gor,  
   spi_mis,  
   spi_oel,  
   spi_oni,  
   spi_rap  
   )         

```  

# Identify seasonality & trend of precipitation data 
```{r make stl decomp function}  
    
# make function to decompose into seasonal, trend, & remainder----  
#    the target is 'groups'   
decomp_fun <- function(df) {  
  arrange(df, .data$date) %>%  
    group_by(.data$group) %>%  
    time_decompose(  
      target         = , TARGET,    
      data           = .,  
      method         = "stl",  
      frequency      = , FREQ,  
      trend          = , TREND,  
      merge          = TRUE,  
      message        = TRUE   
    ) %>%  
    ungroup() %>%  
    anomalize(remainder,  
              method = "gesd",  
              alpha  = 0.003) %>%   
    select(-observed)   
}   
  
# notes: map_dfr can cause issues with indexing; use split and combine  
#   input needs to be a tibble   
  
```

```{r deconvolute_spi}  
  
# deconvolute the grouped prcp into trend, seasonal & random components----  
  
# import working data####     
spi_sta <- import("data/spi_sta.csv") %>%   
  mutate(date = ymd(date))   
  
#sta_meta_fin <- import("data/sta_meta_fin.csv")   
  
# prepare for deconvoluting the grouped precip====  
spi_sta <- spi_sta %>%   
  mutate(prcp = as.numeric(prcp)) %>%   
  arrange(date) %>%   
  mutate(yr = year(date)) %>%  
  as_tibble() %>%  
  # drop precipitation prior to 1989  
  filter(yr > 1988) %>%  
  # make two groups based on the bootstrap results  
  mutate(group = case_when(  
    sta == "RAP" ~ "NW",  
    sta == "COT" ~ "NC",     
    sta == "OEL" ~ "NE",     
    sta == "GOR" ~ "SW",  
    sta == "ONI" ~ "SC",  
    sta == "MIS" ~ "SE"  
  )   
  ) %>%  
  # make a group mean precip value for plotting  
  group_by(group, date) %>%   
  mutate(spi_group = mean(prcp)) %>%  
  ungroup()   
  
# identify frequency of seasonality -- should be 12 months====      
#   the level is group rather than station - so need to combine stations  
freq <- spi_sta  %>%    
  split(.$group) %>%   
  map_dfc(~ anomalize::time_frequency(  
    period       = "auto",  
    data = .)   
  ) %>%  
  gather(key     = group,  
         value   = freq) %>%   
  summarise(freq = mean(freq),   
            max  = max(freq),   
            min  = min(freq),   
            sd   = sd(freq)  
  )   
  
#   identify trend of periodicity -- should be 60 months====  
trend <- spi_sta  %>%  
  split(.$group) %>%  
  map_dfc(~ anomalize::time_trend(  
    period       = "auto", data = .)   
  ) %>%  
  gather(key     = grpup, value = freq) %>%   
  summarise(freq = mean(freq),  
            max  = max(freq),  
            min  = min(freq),  
            sd   = sd(freq)  
  )  
 
# decompose prcp into seasonal, trend, & remainder====    
FREQ   <- "12 months"  
TREND  <- "60 months"   
TARGET <- "spi_1mo"  
  
spi_seas <- spi_sta %>%   
  decomp_fun() #%>%  
  rename(prcp_group = prcp) %>%  
  select(-c(remainder_l1, remainder_l2))  
  
#prcp_group <- right_join(spi_fin, prcp_group,  
#                          by = c("group", "date", "prcp_group")  
#                         ) %>%  
#  rename(remain_group = remainder) %>%  
#  mutate(remain_sta = prcp - season - trend)  
  
# clean up global environment####  
rm(freq,  
   trend,      
   FREQ,  
   TARGET,  
   TREND,  
   decomp_fun  
   )    
  
```  

# Seasonality plotting  
```{r plot_seasonality}

# check for anomonies====     
spi_anom <- spi_seas %>% 
  filter(anomaly == "Yes")  

# order stations for plotting====  
spi_seas <- spi_seas %>% 
  mutate(group = fct_relevel(group,  
                           "NW",  
                           "NC",  
                           "NE",  
                           "SW",  
                           "SC",  
                           "SE"  
  )  
  )   

# vars for plotting====    
sta_size   <- 0.2  
sta_color  <- "black"  
group_size  <- 0.4  
group_color <- "gray"  

# prcp observations plot====  
spi_obs <- 
spi_seas %>%  
  ggplot(aes(date, spi_1mo)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +
  facet_wrap(vars(group)) +   
# plot station vals  
geom_line(size            = sta_size,  
          colour          = sta_color  
          )    

# precipitation seasons plot====  
spi_seas_plot <- 
spi_seas %>%  
  ggplot(aes(date, season)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +
  facet_wrap(vars(group), 
             nrow = 2) +   
# plot group vals  
geom_line(size            = sta_size,  
          colour          = sta_color  
          )    

# precipitation trend plot====  
spi_trend <- 
spi_seas %>%  
  ggplot(aes(date, trend)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +
  facet_wrap(vars(group), 
             nrow = 2) +   
# plot group vals  
geom_line(size            = sta_size,  
          colour          = sta_color  
          )    

# precipitation remainder plot====  
spi_remain <- 
spi_seas %>%  
  ggplot(aes(date, remainder)) +     
  theme_bw() +  
  xlab("") +  
  ylab("SPI 1-month") +   
  scale_y_continuous(limits = c(-3, 3)) +
  facet_wrap(vars(group)) +   
# plot station vals  
geom_line(size            = sta_size,  
          colour          = sta_color  
          )     

# plot the plots above as a grid & save====  
cowplot::plot_grid(  
  spi_obs, spi_seas_plot, spi_trend, spi_remain,  
  ncol = 1,   
  align = "v"  
)  

cowplot::ggsave2("figure/spi_deconv.png",  
                 units = "in",  
                 width = 7,  
                 height = 9)  

rm(spi_obs, 
   spi_seas,  
   spi_seas_plot, 
   spi_trend, 
   spi_remain, 
   spi_anom, 
   group_color, 
   group_size, 
   sta_color, 
   sta_size   
   )  
```  






```{r things}  

spi_all2 %>% 
  ggplot(aes(date, spi_1mo)) + 
  facet_grid(rows = vars(sta)) + 
  geom_line() 




spi_1mo %>% 
  ggplot(aes(ONI, MIS)) + 
  #  facet_grid(rows = vars(sta)) + 
  geom_point() 
# time series arima 
library(tsibble)  
library(feasts) 

spi_1mo <- spi_all2 %>% 
  select(sta, date, spi_1mo) %>% 
  as_tibble()  

spi_1mot <- spi_1mo %>% 
  as_tsibble(key = spi_1mo)



tourism_melb <- tourism %>%
  filter(Region == "Melbourne")
tourism_melb %>%
  group_by(Purpose) %>%
  slice(1)

spi_1mot %>% 
  group_by(sta) %>%
  slice(1)


spi_1mot %>%
  autoplot(spi_1mo)

```

```{r summaries}

sta_summary_mon <- sta_mon %>% 
  group_by(sta, month) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(month) %>%
  arrange(sta)

sta_summary_yr <- sta_mon %>% 
  group_by(sta, year) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(year) %>%
  arrange(sta)

export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
export(sta_summary_yr, file = "data/sta_summary_yr.csv")  
```

```{r plot-annual}
# plot annual precips
sta_mon_plus %>% 
  ggplot(aes(year, depth_mm, group = year)) + 
  facet_grid(rows =vars(group)) +  
  #  geom_quasirandom(size = 0.2,  
  #                position = position_beeswarm()) +  
  geom_boxplot() +  
  theme_bw() +   
  labs(title = "Annual precipitation depths",  
       subtitle = "1990-2017") +  
  xlab("") +  
  ylab("mm")  
```

```{r eda, include=FALSE, eval=FALSE}  

# find the plotting position: using Weibull; a = 0  
sta_mon <- sta_mon %>% 
  group_by(sta) %>% 
  arrange(depth_mm) %>%  
  mutate(position = pp(depth_mm)) %>% 
  ungroup() 

# plot the depths as a function of plotting position
ggplot(sta_mon, aes(position, depth_mm)) + 
  facet_grid(rows = vars(sta)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1980-2017") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

sta_big <- sta_mon %>%
  filter(depth_mm > 100)

summary(sta_big)
ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 


rm(sta_big)  


# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```

```{r precip-EDA, include=FALSE, eval=FALSE}
# Purpose: EDA of precipitation data.
# Outcome: Differences among stations 1971-2018 are small.
#          less than +/-5 mm on average.

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time?  
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.

# drop rows with missing data - this organizes the stations to same 
# time series length
sta_prep <- sta_raw %>%  
  drop_na %>% 
  gather(key = "sta", value = "depth", 
         -date, -year, -month) 

# create abreviated column for plotting & join
abbrev <- data.frame("sta" = c("cot", "int", "oel", "ora", "rap"),
                     "sta_abrev" = c("C", "I", "E", "O", "R")) 

sta_prep <- full_join(sta_prep, abbrev, by = "sta")

# create a year_month column
sta_prep <- sta_prep %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) 

# find mean values by month & join
sta_mean <- sta_prep %>% 
  group_by(month, year) %>% 
  summarize(mean_depth = mean(depth)) %>% 
  ungroup() %>% 
  mutate(mean_depth = round(mean_depth, digits = 2)) %>% 
  mutate(yr_mon = paste(year, month, sep = '_')) %>% 
  select(-year, -month)

sta_prep <- full_join(sta_prep, sta_mean, by = "yr_mon") 
rm(abbrev, sta_mean)

# create a column of deviation from the mean by year & month
sta_prep <- sta_prep %>%
  mutate(deviation = depth - mean_depth) 

# plot the precip data as an overall boxplot 
ggplot(sta_prep, aes(as.factor(sta), depth)) + 
  geom_violin() + 
  geom_boxplot() + 
  scale_y_sqrt() + 
  theme_bw() + 
  ggtitle("Weather stations near Pine Ridge Reservation, SD",  
          subtitle = "1971-2018") + 
  xlab("") +  
  ylab("Monthly depth in mm") + 
  NULL 

ggplot2::ggsave(filename = "prcp_boxplot.png", 
                width = 6, height = 6, units = "in") 

# plot the precip depth as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, depth)) + 
  geom_boxplot() +
  #  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# plot the precip deviations as a monthly boxplot
ggplot(sta_prep, aes(sta_abrev, deviation)) + 
  geom_boxplot() +
  #  geom_histogram(binwidth = 1) +
  facet_grid(cols = vars(month))

# prepare summary of station data & clean up
sta_sum <- sta_prep %>% 
  group_by(sta, month) %>% 
  summarise(mean_depth = mean(depth),
            sd_depth = sd(depth),
            mean_dev = mean(deviation)) 

rm(sta_prep)
```

```{r join_spi_values} 

# clean up some of the unneeded variables 
rm(distrib, p_zero, p_zero_cm, scale, time_scale, warn_me, 
   sta_cot, sta_int, sta_oel, sta_ora, sta_rap, 
   cot, int, oel, ora, rap, 
   first_mon_cot, first_mon_int, first_mon_oel, 
   first_mon_ora, first_mon_rap) 

# Fix month 7 Interior 1-mo-SCI (this is actually Feb!)
# finds dates of record - note that uses Mar rather than Feb
spi_ml_fail <- spi_gath01 %>% 
  mutate(year = year(date)) %>%              # creates ymd
  mutate(mon = month(date)) %>% 
  mutate(day = day(date)) %>% 
  filter(mon == 3) %>%                       # filters dates for 'int'
  filter(sta == "int") %>% 
  mutate(mon = 2) %>% 
  mutate(date = str_c(year, mon, day, sep = "-")) %>% 
  mutate(date = ymd(date)) %>%               # creates updated months
  select(date, sta) %>% 
  group_by(sta) %>% 
  summarise(min_date = min(date))            # summarizes to min month

# calculates the average value for each of the dates
spi_na_fix <- spi_gath01 %>% 
  mutate(mon = month(date)) %>% 
  filter(mon == 2) %>% 
  spread(key = sta, value = spi_index) %>% 
  select(-mon) %>% 
  select(date, everything()) %>% 
  mutate(spi_index = rowMeans(.[,-1], na.rm = TRUE)) %>% 
  mutate(sta = "int") 

# filters by the min date for 'int' & prepares to append to spi_gath01
spi_na_fix <- spi_na_fix %>% 
  filter(date >= spi_ml_fail$min_date) %>% 
  select(date, sta, spi_index)

# append the missing values & clean up
spi_gath01 <- bind_rows(spi_gath01, spi_na_fix) 
rm(spi_ml_fail, spi_na_fix)

#create a list, join, rename, and gather the SPI_indices 
spi_index <- list(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
                  spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
                  spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
                  spi_gath42, spi_gath48, spi_gath54, spi_gath60) %>%
  reduce(left_join, by = c("date", "sta")) 

# renames as spi_length variables 
names(spi_index)[3:18] =
  c("1","2", "3", "4", "5", "6", 
    "9", "12", "18", "24", "30", "36", 
    "42", "48","54", "60") 

# gather the dataframe 
spi_index <- spi_index %>% 
  gather(key = spi_length, val = spi_index, -date, -sta) %>% 
  mutate(spi_length = as.integer(spi_length)) 

# remove the joined dataframes
rm(spi_gath01, spi_gath02, spi_gath03, spi_gath04, 
   spi_gath05, spi_gath06, spi_gath09, spi_gath12, 
   spi_gath18, spi_gath24, spi_gath30, spi_gath36, 
   spi_gath42, spi_gath48, spi_gath54, spi_gath60) 

# check for na values - these are caused by rolling averages
spi_index_na <- spi_index %>% 
  filter(is.na(spi_index) | 
           is.na(sta)) %>% 
  group_by(sta, spi_length) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(count_dif = spi_length - count) %>% 
  distinct(count_dif)                     # if ans = 1 then ok

# remove the na values
spi_index <- spi_index %>% 
  na.omit()

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a dataframe of spi attributes
spi_att01 <- pluck(spi_list_01, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 1)

spi_att02 <- pluck(spi_list_02, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 2) 

spi_att03 <- pluck(spi_list_03, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 3) 

spi_att04 <- pluck(spi_list_04, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 4) 

spi_att05 <- pluck(spi_list_05, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 5) 

spi_att06 <- pluck(spi_list_06, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 6) 

spi_att09 <- pluck(spi_list_09, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 9) 

spi_att12 <- pluck(spi_list_12, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 12) 

spi_att18 <- pluck(spi_list_18, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 18) 

spi_att24 <- pluck(spi_list_24, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 24) 

spi_att30 <- pluck(spi_list_30, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 30) 

spi_att36 <- pluck(spi_list_36, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 36) 

spi_att42 <- pluck(spi_list_42, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 42) 

spi_att48 <- pluck(spi_list_48, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 48) 

spi_att54 <- pluck(spi_list_54, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 54) 

spi_att60 <- pluck(spi_list_60, 1) %>% 
  as_tibble(rownames = "property") %>% 
  gather(key = month, value = val, -property) %>% 
  mutate(month = str_remove(.$month, "M")) %>% 
  mutate(spi_length = 60) 

#create a list, and join SPI attributes 
spi_att <- list(spi_att01, spi_att02, spi_att03, spi_att04, 
                spi_att05, spi_att06, spi_att09, spi_att12, 
                spi_att18, spi_att24, spi_att30, spi_att36, 
                spi_att42, spi_att48, spi_att54, spi_att60) %>%
  reduce(left_join, by = c("property", "month")) %>%
  mutate(month = as.integer(month)) %>% 
  gather(key, spi_len, -month, -property, -starts_with("val")) %>% 
  select(-key) %>% 
  gather(key, val, -month, -property, -spi_len) %>% 
  select(-key) 

# clean up environment 
rm(spi_att01, spi_att02, spi_att03, spi_att04, 
   spi_att05, spi_att06, spi_att09, spi_att12, 
   spi_att18, spi_att24, spi_att30, spi_att36, 
   spi_att42, spi_att48, spi_att54, spi_att60) 

rm(spi_list_01, spi_list_02, spi_list_03, spi_list_04, 
   spi_list_05, spi_list_06, spi_list_09, spi_list_12, 
   spi_list_18, spi_list_24, spi_list_30, spi_list_36, 
   spi_list_42, spi_list_48, spi_list_54, spi_list_60) 
rm(spi_index_na)
```

```{r examine-SCI-values, eval=FALSE}  
# This code chunk examines & fixes the really large negative values 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Visualize initial SPI values----
ggplot(spi_index, aes(as.integer(spi_length), spi_index)) + 
  geom_jitter() +  
  facet_grid(rows = vars(sta)) + 
  theme_bw() +
  ggtitle("Initial SPI Plot", 
          subtitle = "Typical values are between -4 to 4") 

# save intial plot
#ggplot2::ggsave(filename = "figure/initial_spi.png", 
#                width = 6, height = 6, units = "in")

# split outliers from typical values----
spi_index_outlier <- spi_index %>% 
  filter(spi_index < -4) %>% 
  as.tibble() 

spi_index_typical <- spi_index %>% 
  filter(spi_index > -4) %>%
  arrange(spi_index)

# plot outlier years----
sta_outlier <- sta_raw %>% 
  gather(key = sta, value = depth, -date, -year, -month) %>% 
  filter(date == "1977-02-01" | 
           date == "2005-01-01" | 
           date == "2007-01-01" | 
           date == "2012-11-01" | 
           date == "1976-05-01" |
           date == "1981-09-01" |
           date == "2007-02-01"
  ) 

ggplot(sta_outlier, aes(date, depth, color = factor(sta))) + 
  geom_point() + 
  theme_bw() +
  ggtitle("Plot of outlier years") 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# spi_index_typical - calculated above 
# these are the min SPI vals from Cottonwood during depression 
#  date       sta   spi_index spi_length 
#   <date>     <chr>     <dbl>      <dbl> 
# 1 1936-10-01 cot       -3.31          6 
# 2 1937-01-01 cot       -3.27          9 
# 3 1936-09-01 cot       -3.26          4 
# 4 1936-09-01 cot       -3.25          5 
# 5 1936-10-01 cot       -3.25          9  

# spi_index_outlier - calculated above 
# date       sta   spi_index spi_length
#  <date>     <chr>     <dbl>      <dbl>
#1 1977-02-01 ora       -8.03          5   * caused by a zero depth
#2 2005-01-01 ora       -7.51         12
#3 2007-01-01 ora       -8.01          4
#4 2012-09-01 int       -8.03          1 
#5 2012-11-01 ora       -7.71         12 
#6 1976-05-01 ora       -7.93         30
#7 1981-09-01 ora       -7.73         36
#8 2007-02-01 ora       -8.01         36

# Outlier values could be approximated by the mean SPI  
# or the nearest neighbor (Oral <- Oelrichs) - I used mean SPI  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# create replacement values
spi_index_replace <- spi_index_typical %>%          # pulls outliers
  filter(date == "1977-02-01" &  spi_length == 5   | 
           date == "2005-01-01" & spi_length == 12 |  
           date == "2007-01-01" & spi_length == 4  | 
           date == "2012-11-01" & spi_length == 12 | 
           date == "2012-09-01" & spi_length == 1  | 
           date == "1976-05-01" & spi_length == 30 | 
           date == "1981-09-01" & spi_length == 30 | 
           date == "2007-02-01" & spi_length == 30  
  )  %>% 
  group_by(date, spi_length) %>% 
  summarize(spi_index = mean(spi_index)) %>%
  ungroup()                      # calculates mean values 

# update replacement values & join with typical values 
spi_index_replace <- spi_index_outlier %>% 
  select(date, sta) %>% 
  full_join(., spi_index_replace, by = "date")

spi_index2 <- bind_rows(spi_index_typical, spi_index_replace) %>% 
  arrange(sta, spi_length, date)

# check the values after the fix - the outlier values are now fixed 
# spi_index2
# A tibble: 48,011 x 4
#   date       sta   spi_index spi_length
#   <date>     <chr>     <dbl>      <dbl>
# 1 1936-10-01 cot       -3.31          6
# 2 1937-01-01 cot       -3.27          9
# 3 1936-09-01 cot       -3.26          4
# 4 1936-09-01 cot       -3.25          5
# 5 1936-10-01 cot       -3.25          9

# update the index in memory 
spi_index <- spi_index2 %>% 
  arrange(spi_index)

# clean up
rm(spi_index_outlier, spi_index_replace, 
   spi_index_typical, spi_index2, sta_outlier)
```

```{r final-spi-plot}  
spi_index_plot <- spi_index %>% 
  filter(date > "1990-01-01")  %>% 
  filter(spi_length == 1 | 
           spi_length == 6 |
           spi_length == 12 | 
           spi_length == 24 |
           spi_length == 36 |
           spi_length == 48 | 
           spi_length == 60
  ) %>%
  mutate(year = year(date))

ggplot(spi_index_plot, aes(year, spi_index)) + 
  geom_point() + 
  geom_smooth() + 
  geom_hline(yintercept = 0, color = "gray") +
  facet_grid(rows = vars(spi_length), 
             cols = vars(sta)) +
  theme_classic() + 
  ggtitle("SPI Index values for Pine Ridge reservation stations",
          subtitle = "1990-01-01 to 2018-05-30")

ggplot2::ggsave(filename = "figure/spi_vals.png", 
                width = 8, height = 8, units = "in") 

# the model shows spi values are highly correlated with only real 
# difference being the number of years in the calculation.
# However, individual droughts differ in space & time 

#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() + 
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0) + 
#  scale_x_date(limits = as.Date(c('1980-01-01','2018-01-01')), 
#                 date_breaks = "1 year")  
#labels = date_format("%b-%Y")
```

```{r spi-correlation-analysis}
# visually check results
#spi_index_plot <- spi_index %>% 
#  filter(spi_length == 12)

#ggplot(spi_index_plot, aes(date, spi_index)) + 
#  geom_line() +
#  facet_wrap(vars(sta)) + 
#  theme_classic() +
#  geom_hline(yintercept = 0, aes)

#ggplot2::ggsave(path = "figure/", filename = "spi_1mo.png", 
#                width = 6, height = 6, units = "in")  

# create correlation matrix inputs
spi_M <- spi_index %>% 
  select(-date) %>% 
  group_by(sta, spi_length) %>% 
  mutate(grouped_id = row_number()) %>% 
  spread(key = sta, value = spi_index) %>% 
  drop_na() %>% 
  ungroup() %>% 
  select(-grouped_id) %>% 
  select(spi_length, cot, int, oel, ora, rap) # ensure vars order 

# create correlation matrix names from the correlation matrix vars
spi_M_names <- spi_M %>% 
  filter(spi_length == 1) %>% 
  cor() %>% 
  as.tibble() %>% 
  names() %>% 
  as.tibble() %>% 
  slice(-1) %>% 
  mutate(value2 = value) %>% 
  mutate(value3 = value) %>%  
  mutate(value4 = value) %>%
  mutate(value5 = value) %>% 
  gather(key, sta2) %>% 
  select(-key) %>% 
  rownames_to_column() 

# create second station names column 
spi_M_names2 <- spi_M_names %>% 
  arrange(sta2) %>% 
  rename(sta1 = sta2) %>% 
  select(-rowname)

# bind the names columns
spi_M_names <- bind_cols(spi_M_names, spi_M_names2)  
rm(spi_M_names2)

# create a correlation matrix from SPI vals
spi_M <- spi_M %>% 
  split(.$spi_length) %>% 
  purrr::map_dfr(~ cor(.)) %>% 
  drop_na() %>% 
  slice(-1) %>% 
  rownames_to_column() 

# bind names to the correlation matrix 
spi_M <- full_join(spi_M_names, spi_M, by = "rowname")
spi_M <- spi_M %>% 
  select(sta1, sta2, everything()) %>%
  select(-rowname)

# prepare lookup table of lat-lons
sta_loc <- sta_meta %>% 
  arrange(name) %>%
  mutate(sta = c("cot", "int", "oel", "ora", "rap")) %>% 
  select(sta, lat, lon, dur_year)

# join the 'from' lat lons
spi_corr <- full_join(spi_M, sta_loc, by = c("sta1" = "sta")) %>% 
  rename(lat1 = lat) %>% 
  rename(lon1 = lon) 

# join the 'to' lat lons
spi_corr <- full_join(spi_corr, sta_loc, by = c("sta2" = "sta")) %>% 
  rename(lat2 = lat) %>% 
  rename(lon2 = lon) %>% 
  mutate(year_dif = abs(dur_year.x - dur_year.y))


# convert the 'to' and 'from' lat-lons to northings & eastings & distance
lat_to_km <- 111.03 # 1 degree lat to km @ lat 40-degrees 
lon_to_km <- 85.39  # 1 degree lon to km @ lat 40-degrees 

spi_corr <- spi_corr %>% 
  mutate(northing = abs((lat1 - lat2)) * lat_to_km) %>% 
  mutate(easting = abs((lon1 - lon2)) * lat_to_km) %>% 
  mutate(distance = sqrt(northing^2 + easting^2)) %>% 
  select(year_dif, everything()) %>%
  select(-(lat1:easting)) %>% 
  mutate(stations = paste(sta1, sta2, sep = "_")) %>% 
  gather(key = spi_length, value = pears_r, -distance, 
         -stations, -sta1, -sta2, -year_dif) %>% 
  filter(distance > 0) %>% 
  mutate(spi_length = as.double(spi_length))


# clean up the global environment 
rm(spi_M, spi_M_names, sta_loc, lat_to_km, lon_to_km)

# model effect of averaging time & distance on correlation----
# fit a linear model
spi_lm <- lm(pears_r ~ distance + spi_length + year_dif, 
             data = spi_corr)

# augment & gather the original data
spi_corr_aug <- augment(spi_lm, spi_corr) 

spi_corr_gath <- spi_corr_aug %>% 
  select(-(sta1:sta2)) %>%
  select(year_dif:.fitted) %>% 
  gather(key = factor, val, -stations, -pears_r) %>% 
  mutate(val = as.double(val))  

# plot the original data and fitted model for SPI----  
ggplot(spi_corr_gath, aes(val, pears_r)) + 
  geom_point(aes(color = factor(stations))) +
  facet_wrap(ncol = 1, vars(factor), scales = "free") +
  geom_smooth(method = lm) + 
  theme_classic()

spi_lm_fit <- glance(spi_lm) 

spi_lm_tidy <- tidy(spi_lm) %>% 
  mutate(
    low = estimate - std.error,
    high = estimate + std.error
  )

# clean-up Global Environment----
spi_corr <- spi_corr_aug 
rm(spi_corr_aug, spi_corr_gath, spi_lm ,spi_lm_fit)


rm(spi_corr, spi_lm_tidy)
```

```{r SCI-diagnostic-plots, eval=FALSE}
# Diagnostic plots of SPI transforms 

# PE3 scale plot---- 
spi_scale <- spi_att %>% 
  filter(property == "scale")

ggplot(spi_scale, aes(as.factor(month), spi_value)) +  
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_scale.png", 
                width = 6, height = 6, units = "in") 

# PE3 location plot---- 
spi_location <- spi_att %>% 
  filter(property == "location")

ggplot(spi_location, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_location.png", 
                width = 6, height = 6, units = "in") 

# PE3 shape plot---- 
spi_shape <- spi_att %>% 
  filter(property == "shape")

ggplot(spi_shape, aes(as.factor(month), spi_value)) + 
  geom_boxplot() +
  facet_grid(rows = vars(sta),
             cols = vars(spi_length)) + 
  theme_classic() 

ggplot2::ggsave(filename = "figure/spi_shape.png", 
                width = 6, height = 6, units = "in") 

# clean up Global Environment----
rm(spi_location, spi_shape, spi_scale) 
```


# OLD CODE 
```{r import-data} 

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# General Purpose: prepare data for drought index   
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) 

# add a small value to zeros to solve a downstream issue 
# maybe have this fixed now using ts rather than date class
#sta_mon <- sta_mon %>% 
#  gather(key = "station", value = "depth", -date, -year, -month) %>%
#  mutate(depth = replace(depth, depth == 0.0, 0.15)) %>%
#  spread(station, depth)

# make the wide data long, remove NA vals, sqrt transform
sta_mon <- sta_mon %>%
  gather(key = "station", value = "depth", -date, -year, -month) %>%
  drop_na(depth)  # %>% 
#  mutate(sqrt_depth = sqrt(depth)) 

# Check on log transformation
#sta_notzero <- sta_mon %>%
#  filter(depth != 0)

#min <- min(sta_notzero$depth)

#sta_zero <- sta_mon %>%
#  filter(depth == 0) %>%
#  mutate(depth = depth + min/2)

#sta_log <- bind_rows(sta_zero, sta_notzero)
#sta_log <- sta_log %>%
#  mutate(log_depth = log10(depth))

#rm(min) 
```

```{r precip-boxplot, include=FALSE, eval=FALSE} 

sta_sum <- as.tibble(summary(sta_mon))

# plot the precip data as a boxplot
ggplot(sta_grp, aes(as.factor(station), depth)) +
  geom_violin() +
  geom_boxplot() +
  #  scale_y_sqrt() +
  #  scale_y_log10() +
  scale_y_sqrt() +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") +
  xlab("") + 
  ylab("Monthly depth in mm") +
  NULL

#ggplot2::ggsave(filename = "rf_boxplot.png", 
#                width = 6, height = 6, units = "in")
```

```{r eda_fiddling, include=FALSE, eval=FALSE}
sta_big <- sta_grp %>%
  filter(depth > 100)
summary(sta_big)
ggplot(sta_big, aes(month)) +
  geom_histogram(binwidth = 1) 

# the anomolously wet month series is dominated by May & June events.
# what drives precip during this time? 
#   In May & June the area recieves low-level moisture from the Gulf 
#   of Mexico, strong cold fronts, and active upper-level pattern 
#   leading to greater chance for convection.
```

```{r station_plotting_postion, include=FALSE, eval=FALSE} 
# find the plotting position: using Weibull; a = 0  
sta_grp <- sta_grp %>% 
  group_by(station) %>% 
  arrange(depth) %>% 
  mutate(position = pp(depth)) %>% 
  ungroup() 

# find the plotting position for log-data: using Weibull; a = 0
#sta_log <- sta_log %>% 
#  group_by(station) %>% 
# arrange(log_depth) %>% 
#  mutate(position = pp(log_depth)) %>% 
#  ungroup() 

# plot the depths as a function of plotting position
ggplot(sta_grp, aes(position, depth)) + 
  facet_grid(.~station) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_bw() +
  ggtitle("Weather stations near Pine Ridge Reservation, SD", 
          subtitle = "1909-2018") + 
  xlab("Frequency of occurrance") +
  ylab("Monthly depth in mm") +
  scale_y_sqrt() +
  ylab("Depth in mm")

#ggplot2::ggsave(filename = "rf_freq_plot.png", 
#                width = 6, height = 6, units = "in")

```

```{r lmoments, include=FALSE, eval=FALSE}
# This is a long bit of code that takes things out of a list 

lmom_sta <- sta_grp %>% 
  split(.$station) %>%
  map(~ lmoms(.$depth)) %>%
  transpose() %>%
  as_tibble() %>%
  select(lambdas, ratios) %>%
  mutate(lambdas = map(lambdas, ~as_tibble(t(.x))))  %>%
  mutate(lambdas = map(lambdas, 
                       ~set_names(.x, c("L1", "L2", "L3", 
                                        "L4", "L5")))) %>%
  mutate(ratios = map(ratios, ~as_tibble(t(.x))))  %>%
  mutate(ratios = map(ratios, 
                      ~set_names(.x, c("T1", "T2", "T3", 
                                       "T4", "T5")))) %>% 
  unnest(lambdas) %>%
  unnest(ratios) %>%
  mutate(station = c("oel", "cot", "rap", 
                     "int", "ora")) %>%
  select(-T1) %>%
  rename(L_CV = T2) %>% 
  rename(L_skew = T3) %>%
  rename(L_kurtosis = T4) %>%
  select(station, L1, L_CV, L_skew, L_kurtosis)

# Note that we might consider Weiss 1964 bias value of 1.018 for L1

# get the lenths of the datasets
sta_count <- sta_grp %>%
  count(station)  

# join the number of years to the station
lmom_sta <- left_join(lmom_sta, sta_count, by = "station")

# calculate weighted means for regional L-moments 
L1  <- weighted.mean(lmom_sta$L1, lmom_sta$n)
L_CV  <- weighted.mean(lmom_sta$L_CV, lmom_sta$n)
L_skew  <- weighted.mean(lmom_sta$L_skew, lmom_sta$n)
L_kurtosis  <- weighted.mean(lmom_sta$L_kurtosis, lmom_sta$n)
n       <- sum(lmom_sta$n)

# combine the output into a single weighted mean
int1     <- cbind(L1, L_CV)
int2     <- cbind(int1, L_skew)
int3     <- cbind(int2, L_kurtosis)
lmom_reg <- cbind(int3, n)

rm(L1, L_CV, L_skew, L_kurtosis, n, int1, int2, int3, sta_count)

# finalize the regional L-moment
lmom_reg <- as.tibble(lmom_reg) %>%
  mutate(station = "WtMean") %>% 
  select(station, everything())
```

```{r Lmoment_diagram_ratios, include=FALSE, eval=FALSE}
# extract elements from the lmrdia list to plot in ggplot2  
#   the x-value is the L-skewness and y-value is L-kurtosis  

# get vals from the lmrdia list 
# note that as gamma distribution is a 2-parameter dist, it is not shown 
lmrdia <- lmrdia() 

# extract L-skew & L-kurtosis values for several distributions
#aep4 <- lmrdia %>%
#  extract2(2) %>%
#  as.tibble()

gev <- lmrdia %>% 
  extract2(5) %>% as.tibble()

glo <- lmrdia %>%
  extract2(6) %>% as.tibble()

gpa <- lmrdia %>%
  extract2(7) %>% as.tibble()

gno <- lmrdia %>%
  extract2(9) %>% as.tibble()

gov <- lmrdia %>%
  extract2(10) %>% as.tibble()

pe3 <- lmrdia %>%
  extract2(12) %>% as.tibble()

# combine and rename columns as distribution types
#int1      <- full_join(aep4, gev, by = "V1")  
#int1      <- int1 %>% 
#               rename(AEP4 = V2.x) %>% 
#               rename(GEV = V2.y) 

#int2      <- full_join(int1, glo, by = "V1") 
#int2      <- int2 %>% rename(GLO = V2) 

# combine and rename columns as distribution types
int1      <- full_join(gev, glo, by = "V1")  
int1      <- int1 %>% 
  rename(GEV = V2.x) %>% 
  rename(GLO = V2.y) 

int2      <- full_join(int1, gpa, by = "V1") 
int2      <- int2 %>% rename(GPA = V2) 

int3      <- full_join(int2, gno, by = "V1") 
int3      <- int3 %>% rename(GNO = V2) 

int4      <- full_join(int3, gov, by = "V1") 
int4      <- int4 %>% rename(GOV = V2) 

lmom_theo <- full_join(int4, pe3, by = "V1")
lmom_theo <- lmom_theo %>% rename(PE3 = V2) %>% 
  rename(L_skew = V1) %>% 
  arrange(L_skew)

# prepare theoretical distributions for plotting
lmom_theo <- lmom_theo %>%
  gather(key = "distribution", value = "L_kurtosis", -L_skew) %>%
  drop_na(L_kurtosis) %>%
  select(distribution, everything())  

rm(gev, int1, glo, int2, gpa,int3, gno, int4, gov, pe3, lmrdia)
```

```{r plot-lmoment-diagram, include=FALSE, eval=FALSE}
# plots the theo distributions, the sample vals, and regional mean 
ggplot() + 
  geom_line(data = lmom_theo, aes(L_skew, L_kurtosis, 
                                  group = distribution, 
                                  linetype = distribution)) +
  geom_point(data = lmom_sta, aes(L_skew, L_kurtosis)) +
  geom_point(data = lmom_reg, aes(L_skew, L_kurtosis, 
                                  size = 2, show.legend = NA)) +
  theme_bw() + 
  # xlim(0.25, 0.5) +
  # ylim(0, 0.25) +
  xlim(0, 0.5) +
  ylim(0, 0.5) +
  ggtitle("L-moment diagram for monthly precipitation depth", 
          subtitle = "Weather stations near Pine Ridge Reservation, 1909-2018")

#ggplot2::ggsave(filename = "lmom_plot.png", 
#                width = 6, height = 6, units = "in")
``` 

```{r old code}
# download daily precip data from NOAA GHCN database ------------------------   
sta_dv_plus <- sta_meta_plus %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags = TRUE,   
                            var = "PRCP",    
                            date_min = dateMin,   
                            date_max = dateMax)  
  )     
  
rm(dateMin, dateMax)   
  
  
# import data from file 
#sta_dv_plus   <- import(file = "data/sta_dv_plus.csv") %>% 
#  mutate(date = ymd(date))  
  
#sta_meta_plus <- import(file = "data/sta_meta_plus.csv")
#sta_meta      <- import(file = "data/sta_meta.csv")
  
# join bits of daily data-plus to metadata   
#sta_dv_plus <- right_join(sta_meta_plus, sta_dv_plus,  
#                   by = c("sta_id" = "id"))   %>%  
#  select(name, sta_id, date, year, month, mflag_prcp,  
#         prcp, qflag_prcp, sflag_prcp)  %>% 
#  select(date, everything())   
  
  
  
# separate north-west data --------------------------------------------------  
sta_nw <-  sta_dv_plus %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>% 
  filter(!is.na(prcp)) 

sta_nw_nm <- sta_nw %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nw_len <- sta_nw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nw_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nw_nm$name) %>% 
  mutate(sta_id = sta_nw_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nw_fill <- anti_join(sta_nw_alt, sta_nw, by = "date") 

sta_nw <- bind_rows(sta_nw, sta_nw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nw_alt, sta_nw_nm, sta_nw_fill)  

# separate and fill north-central  ------------------------------------------  
sta_nc <-  sta_dv_plus %>% 
  filter(str_detect(name, "^COTTONWOOD")) %>% 
  filter(!is.na(prcp)) 

sta_nc_nm <- sta_nc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nc_len <- sta_nc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nc_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nc_nm$name) %>% 
  mutate(sta_id = sta_nc_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nc_fill <- anti_join(sta_nc_alt, sta_nc, by = "date") 

sta_nc <- bind_rows(sta_nc, sta_nc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nc_alt, sta_nc_nm, sta_nc_fill)  

# separate and fill north-east ----------------------------------------------    
sta_ne <-  sta_dv_plus %>% 
  filter(str_detect(name, "^ONIDA")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_ne_nm <- sta_ne %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_ne_len <- sta_ne %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_ne_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^KENNEBEC")) %>% 
  mutate(name = sta_ne_nm$name) %>% 
  mutate(sta_id = sta_ne_nm$sta_id)           

sta_ne_fill <- anti_join(sta_ne_alt, sta_ne, by = "date") 

sta_ne <- bind_rows(sta_ne, sta_ne_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_ne_alt, sta_ne_nm, sta_ne_fill)  

sta_sw <-  sta_dv_plus %>% 
  filter(str_detect(name, "^ORAL")) %>% 
  filter(!is.na(prcp))      

# prepare to change name 
sta_sw_nm <- sta_sw %>% 
  slice(1) %>% 
  select(name, sta_id) 

# get length 
sta_sw_len <- sta_sw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sw_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^OELRICHS")) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

sta_sw <-  sta_sw %>% 
  filter(!is.na(prcp))      

# download daily precip data for missing values--HotSprings     
dateMin = "2005-12-01"      
dateMax = "2011-05-31"  
sta_sw_alt <- meteo_tidy_ghcnd(stationid = "USC00394007",
                               keep_flags = TRUE,   
                               var = "PRCP",   
                               date_min = dateMin,   
                               date_max = dateMax) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

# fill missing values 
sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sw_alt, sta_sw_nm, sta_sw_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sc <-  sta_dv_plus %>% 
  filter(str_detect(name, "^GORDON")) %>% 
  filter(!is.na(prcp))     

# prepare to change name 
sta_sc_nm <- sta_sc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_sc_len <- sta_sc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sc_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^VALENTINE")) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)        

sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>%  
  arrange(name) %>%  
  distinct(date, .keep_all = TRUE)  

sta_check <- sta_sc %>%  
  filter(is.na(prcp))  

sta_sc <-  sta_sc %>%  
  filter(!is.na(prcp))   

# download daily precip data for missing values--FT Robinson 
# note: Hay Springs did not have all data 
dateMin = "2006-08-01"      
dateMax = "2012-03-31"  
sta_sc_alt <- meteo_tidy_ghcnd(stationid = "USC00253015",
                               keep_flags = TRUE,   
                               var = "PRCP",   
                               date_min = dateMin,   
                               date_max = dateMax) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)       

# fill missing values 
sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sc %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sc_alt, sta_sc_nm, sta_sc_fill)  


# separate south-west data --------------------------------------------------  
sta_se <-  sta_dv_plus %>% 
  filter(str_detect(name, "^MISSION")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_se_nm <- sta_se %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_se_len <- sta_se %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) 

# get alternate site data & prepare fill 
sta_se_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^WOOD")) %>% 
  mutate(name = sta_se_nm$name) %>% 
  mutate(sta_id = sta_se_nm$sta_id)        

sta_se_fill <- anti_join(sta_se_alt, sta_se, by = "date") 

sta_se <- bind_rows(sta_se, sta_se_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_se_alt, sta_se_nm, sta_se_fill)  

# rejoin daily data and lengths ---------------------------------------------  
sta_dv <- bind_rows(sta_nw, 
                    sta_nc, 
                    sta_ne, 
                    sta_sw, 
                    sta_sc, 
                    sta_se) 

sta_dv_len <- bind_rows(sta_nw_len, 
                        sta_nc_len, 
                        sta_ne_len, 
                        sta_sw_len, 
                        sta_sc_len, 
                        sta_se_len) %>% 
  mutate(max_count = max(count)) %>% 
  mutate(perc_complete =  
           round(count/max_count,  
                 digits = 3) 
  ) %>% 
  select(-c(count, max_count))  

# check data & clean up -----------------------------------------------------   
sta_check <- sta_dv %>% 
  filter(is.na(prcp)) 


# join dv-lenth to metadata  
sta_meta <- right_join(sta_meta, sta_dv_len,  
                       by = c("sta_id", "name"))  

rm(sta_dv_len, sta_meta_orig, sta_meta_plus, sta_check, sta_dv_plus, 
   sta_nw, sta_nc, sta_ne, sta_sw, sta_sc, sta_se, 
   sta_nw_len, sta_nc_len, sta_ne_len, sta_sw_len, sta_sc_len, sta_se_len)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")  



# add short names to metadata & update the stored file ----------------------  
sta_meta_plus <- sta_meta_plus %>%  
  mutate(sta = case_when(  
    str_detect(name, "^RAPID CITY R") ~ "RAP",  
    str_detect(name, "^COTTONWOOD")   ~ "COT",  
    str_detect(name, "^ONIDA")        ~ "ONI",  
    str_detect(name, "^ORAL")         ~ "ORA",  
    str_detect(name, "^GORDON")       ~ "GOR",      
    str_detect(name, "^MISSION")      ~ "MIS",  
    TRUE ~ "ERROR"  
  )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation,  
         perc_complete, group, everything())  



# get the lenths of the datasets  
sta_count <- sta_mon %>%  
  count(sta)   

# join the number of years to the station  
lmom_sta <- left_join(lmom_sta, sta_count, by = "sta")  

# calculate weighted means for regional L-moments --  
# not really useful in this case  
L1  <- weighted.mean(lmom_sta$L1, lmom_sta$n)  
L_CV  <- weighted.mean(lmom_sta$L_CV, lmom_sta$n)  
L_skew  <- weighted.mean(lmom_sta$L_skew, lmom_sta$n)  
L_kurtosis  <- weighted.mean(lmom_sta$L_kurtosis, lmom_sta$n)  
n       <- sum(lmom_sta$n)  

# combine the output into a single weighted mean  
lmom_reg <- cbind(L1, L_CV, L_skew, L_kurtosis, n) %>%  
  as_tibble() %>%  
  mutate(sta = "WtMean") %>%  
  select(sta, everything())  

rm(sta_l1, sta_count, L1, L_CV, L_skew, L_kurtosis, n)  

# Get the station data based on L1 -- prepare for a join  
# pulled out the L1 data when it was a list  
# STA    L1  
# RAP  35.26  
# COT  36.29  
# ORA  37.24  
# GOR  42.42  
# ONI  43.94  
# MIS  45.74  

sta_l1 <- lmom_sta %>%  
  select(L1) %>%  
  mutate(L1 = round(L1,  
                    digits = 3)  
  ) %>%  
  arrange(L1) %>%  
  mutate(sta = c("RAP", "COT", "ORA", "GOR", "ONI", "MIS"))  




```

# TO DELETE  
```{r plot-theissen-polygons-delete}

sta_meta_fin   <- import("data/sta_meta_fin.csv")   
sta_meta_orig  <- import("data/sta_meta_orig.csv")  
gage_meta_fin  <- import("data/gage_meta_fin.csv")   

# plot a map of the study area----   
# create Theissen line segments====      
voronoi        <- deldir(sta_meta_fin$lon, sta_meta_fin$lat)    

# import polygon data for maps====   
usa            <- map_data("usa")    
states         <- map_data("state")   
counties       <- map_data("county")   

# filter counties for AOI====   
#   need to filter counties for SD & NE separately because of duplicate vals  
#   subset SD counties####   
counties_sd <- subset(counties, 
                      region %in% 
                        c("south dakota"))  

counties_sd <- subset(counties_sd,   
                      subregion %in%   
                        c("butte",   
                          "meade",   
                          "lawrence", 
                          "pennington",   
                          "custer",   
                          "fall river",   
                          "shannon",   
                          "jackson",   
                          "bennett",  
                          "haakon",   
                          "todd",   
                          "mellette",   
                          "jones", 
                          "stanley",   
                          "lyman",  
                          "tripp",  
                          "hughes",  
                          "ziebach",  
                          "dewey",  
                          "hughes", 
                          "sully",   
                          "potter"     
                        )    
)      

#   subset NE counties####
counties_ne <- subset(counties, region %in%    
                        c("nebraska")) %>%    
  filter(subregion == "sioux" |    
           subregion == "dawes" |    
           subregion == "sheridan" |    
           subregion == "box butte" |   
           subregion == "cherry" |    
           subregion == "brown" |    
           subregion == "keya paha" |  
           subregion == "rock"  
  )  

#   join SD & NE counties####   
counties <- bind_rows(counties_sd, counties_ne)  
rm(counties_sd, counties_ne)  
# create a bounding box -- order is SW, NW, NE, SE====  
bbox <- tibble(lat = c(42.0, 45.0, 45.0, 42.0), 
               long = c(-104.5, -104.5, -99.5, -99.5), 
               group = c(1, 1, 1, 1)
)

# create SD-NE & SD-WY lines for AOI==== 
ne_bound <- subset(counties,   
                   subregion %in% c("fall river",     
                                    "shannon",   
                                    "bennett",    
                                    "todd",    
                                    "tripp")   
) %>%  
  filter(lat < 43.1)    

wy_bound <- subset(counties,     
                   subregion %in% c("butte",   
                                    "lawrence"  , 
                                    "pennington" , 
                                    "custer", 
                                    "fall river", 
                                    "sioux"
                   )  
) %>%  
  filter(long < -104) 

# plot US map====
usmap <- ggplot() + 
  geom_polygon(data = usa, 
               aes(x=long, y = lat, group = group),  
               fill = "gray70", 
               color = "gray70"
  ) + 
  coord_fixed(1.3) + 
  theme_nothing() + 
  geom_polygon(data = bbox, 
               aes(x=long, y = lat, group = group), 
               fill = "gray30", 
               color = "gray30") +
  
  xlab("") +  
  ylab("")  

# change the US map into a grob####  
usmap <- as_grob(usmap, device = NULL)

# plot county map====  
ggplot() +  
  theme_nothing() +  
  coord_fixed(1.3) +  
  # add counties  
  geom_polygon(data      = counties,  
               aes(x     = long, 
                   y     = lat, 
                   group = group),  
               color     = "gray80",  
               fill      = "NA") +  
  # add stateline boundaries  
  geom_line(data      = ne_bound,  
            aes(x     = long, 
                y     = lat, 
                group = group),  
            color     = "gray60"  
  ) +  
  geom_line(data = wy_bound,  
            aes(x     = long, 
                y     = lat, 
                group = group),  
            color     = "gray60"    
  ) +   
  # add small stations  
  geom_point(data  = sta_meta_orig,  
             aes(x = longitude, 
                 y = latitude),   
             size  = 1,  
             color = "gray40") +  
  # add stream gages 
  geom_point(data = gage_meta_fin, 
             aes(dec_long_va, dec_lat_va),  
             shape  = 24, 
             fill   = NA,  
             size   = 0.8,   
             stroke = 0.5, 
             color  = "gray50"  
  ) +     
  # add final stations   
  geom_point(data  = sta_meta_fin, 
             aes(x = longitude,   
                 y = latitude),   
             size  = 3,   
             color = "black"
  ) +     
  # add Theissen polygons  
  geom_segment(data = voronoi$dirsgs,
               aes(x           = x1, 
                   y           = y1, 
                   xend        = x2,
                   yend        = y2), 
               size            = 1, 
               linetype        = 1, 
               color           = "gray20"
  ) +  
  geom_text(data          = sta_meta_fin, 
            aes(x         = longitude, 
                y         = latitude, 
                label     = sta), 
            vjust         = 1.6, 
            check_overlap = TRUE
  ) +  
  # add the US map as a grob  
  annotation_custom(grob = usmap2, 
                    xmin = -99.5, 
                    xmax = -97.8, 
                    ymin = 44.5,  
                    ymax = 45.5)  

# export map  
ggplot2::ggsave(filename = "figure/theissen_fin.png",    
                width = 7.5, height = 4.5, units = "in")  

rm(bbox,   
   counties,   
   counties_sd,   
   ne_bound,  
   sd_bound,  
   sta_meta_orig,  
   states,  
   usa,  
   usmap,  
   voronoi,  
   wy_bound  
)   

``` 

```{r fill_missing_precip_data, eval=FALSE}  

# fill stations to exam
# fill north-west stations --------------------------------------------------  
#   ELM SPRINGS 3 ESE, SD US
#   FORT MEADE, SD US
#   NEWELL, SD US
#   RED OWL, SD US
#   RAPID CITY REGIONAL AIRPORT, SD US

# Note that fully filled is n = 10,957

# ELM SPRINGS ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^ELM SPRINGS")) %>%        
  filter(!is.na(prcp))                                 # n = 10,868

# get alternate site data & prepare fill 
sta_alt <- sta_dv_alt %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nw <- sta_fil 


# FORT MEADE ---- 
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^FORT MEADE")) %>%        
  filter(!is.na(prcp))                                 # n = 10,933  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_alt %>% 
  filter(str_detect(name, "^ELM SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nw <- bind_rows(sta_filled_nw, sta_fil) 


# NEWELL ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^NEWELL")) %>%        
  filter(!is.na(prcp))                                 # n = 10,787  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^ELM SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nw <- bind_rows(sta_filled_nw, sta_fil) 

# RED OWL ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^RED OWL")) %>%        
  filter(!is.na(prcp))                                 # n = 10,776   

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nw <- bind_rows(sta_filled_nw, sta_fil) 

# RAPID CITY REGIONAL----
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>%        # n = 10,957
  filter(!is.na(prcp))                                 # n = 10,951

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^ELM SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

sta_filled_nw <- bind_rows(sta_fil, sta_filled_nw)     

```  

```{r fill_missing_precip_data-nc, eval=FALSE}  
# fill north-central stations ------------------------------------------------  
#   COTTONWOOD 2 E, SD US  
#   DUPREE 15 SSE, SD US  
#   MILESVILLE 5 NE, SD US  
#   MURDO, SD US  
#   PHILIP AIRPORT, SD US   
#   PLAINVIEW 6 SSW, SD US
#   INTERIOR 3 NE, SD US
#  KIRLEY 6 N, SD US

# Note that fully filled is n = 10,957  

# COTTONWOOD ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^COTTONWOOD")) %>%        
  filter(!is.na(prcp))                                 # n = 10,857 

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^INTERIOR")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nc <- sta_fil 

# DUPREE ---- 
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^DUPREE")) %>%        
  filter(!is.na(prcp))                                 # n = 10,725  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^MILES")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nc <- bind_rows(sta_filled_nc, sta_fil) 

# MILESVILLE ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^MILES")) %>%        
  filter(!is.na(prcp))                                 # n = 10,901 

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^COTTONWOOD")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nc <- bind_rows(sta_filled_nc, sta_fil) 

# MURDO ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^MURDO")) %>%        
  filter(!is.na(prcp))                                 # n = 10,488   

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^COTTONWOOD")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_nc <- bind_rows(sta_filled_nc, sta_fil) 

# PHILIP----
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^PHILIP")) %>%        
  filter(!is.na(prcp))                                 # n = 10,452

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^MILESVILLE")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

sta_filled_nc <- bind_rows(sta_fil, sta_filled_nc)   
```  

```{r fill_missing_precip_data-ne, eval=FALSE}  
# fill north-eastern stations ----------------------------------------------  
#   PIERRE REGIONAL AIRPORT, SD US
#   ONIDA 4 NW, SD US
#   KENNEBEC, SD US

# Note that fully filled is n = 10,957

# PIERRE ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^PIERRE")) %>%        
  filter(!is.na(prcp))                                 # n = 10,954 

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^ONIDA")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_ne <- sta_fil 

# ONIDA ---- 
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^ONIDA")) %>%        
  filter(!is.na(prcp))                                 # n = 10,887    

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^KENNEBEC")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_ne <- bind_rows(sta_filled_ne, sta_fil) 

# KENNEBEC ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^KENNEBEC")) %>%        
  filter(!is.na(prcp))                                 # n = 10,895  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^ONIDA")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_ne <- bind_rows(sta_filled_ne, sta_fil) 

```

```{r fill_missing_precip_data-sw, eval=FALSE}  
# separate and fill southwestern stations ----------------------------------- 
#   REDBIRD, WY US  
#   EDGEMONT, SD US  
#   HOT SPRINGS, SD US  
#   ORAL, SD US   
#   OELRICHS, SD US

# Note that fully filled is n = 10,957

# REDBIRD, WY ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^REDBIRD")) %>%        
  filter(!is.na(prcp))                                 # n = 10,803

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^EDGEMONT")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_sw <- sta_fil 

# EDGEMONT ---- 
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^EDGEMONT")) %>%        
  filter(!is.na(prcp))                                 # n = 10,802  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^REDBIRD")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_sw <- bind_rows(sta_filled_sw, sta_fil) 

# HOT SPRINGS ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))                                 # n = 10,852  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^FORT")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_sw <- bind_rows(sta_filled_sw, sta_fil) 

# ORAL ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^ORAL")) %>%        
  filter(!is.na(prcp))                                 # n = 10,639    

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^OELRICHS")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = not check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) #%>% 
#  select(date, name, mflag_prcp:sflag_prcp)  

sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))            

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_sw <- bind_rows(sta_filled_sw, sta_fil) 

# OELRICHS----
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^OELRICHS")) %>%        
  filter(!is.na(prcp))                                 # n = 10,417 

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^ORAL")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = not check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) #%>% 
#  select(date, name, mflag_prcp:sflag_prcp)  

sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

sta_filled_sw <- bind_rows(sta_fil, sta_filled_sw)     

```  

```{r fill_missing_precip_data-sc, eval=FALSE}  
# separate and fill southcentral stations ----------------------------------- 
#   GORDON 6 N, NE US 
# Note that fully filled is n = 10,957

# GORDON, NE ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^GORDON")) %>%        
  filter(!is.na(prcp))                                 # n = 10,842 

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^VALENTINE")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_sc <- sta_fil 

```  

```{r fill_missing_precip_data-se, eval=FALSE}  
# separate and fill southwestern stations ----------------------------------- 
#   MISSION 14 S, SD US
#   VALENTINE MILLER FIELD, NE US
#   WOOD, SD US
#   AINSWORTH, NE US
# Note that fully filled is n = 10,957

# MISSION ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^MISSION")) %>%        
  filter(!is.na(prcp))                                 # n = 10,956

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^WOOD")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_se <- sta_fil 

# WOOD ---- 
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^WOOD")) %>%        
  filter(!is.na(prcp))                                 # n = 10,836  

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^MISSION")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_se <- bind_rows(sta_filled_se, sta_fil) 

# VALENTINE ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^VALENTINE")) %>%        
  filter(!is.na(prcp))                                 # n = 10,592   

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^AINSWORTH")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_se <- bind_rows(sta_filled_se, sta_fil) 

# AINSWORTH ----  
sta_fil <- sta_dv_plus %>% 
  filter(str_detect(name, "^AINSWORTH")) %>%        
  filter(!is.na(prcp))                                 # n = 10,926    

# get alternate site data & prepare fill 
sta_alt <- sta_dv_orig %>% 
  filter(str_detect(name, "^VALENTINE")) %>%        
  filter(!is.na(prcp))                             

sta_name <- sta_fil %>% 
  slice(1) %>% 
  select(name, id) 

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(name = sta_name$name) 

sta_fil <- bind_rows(sta_alt, sta_fil)  %>%            # n = not check 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>% 
  select(date, name, mflag_prcp:sflag_prcp)  

# Cumulatively add dvs 
sta_filled_se <- bind_rows(sta_filled_se, sta_fil) 
```  

```{r join_filled_precip_data, eval=FALSE}

sta_filled <- bind_rows(  
  sta_filled_nw,  
  sta_filled_nc,  
  sta_filled_ne,  
  sta_filled_sw,  
  sta_filled_sc,  
  sta_filled_se  
)  

sta_filled <- full_join(sta_filled, sta_meta_plus, 
                        by = "name")  

rm(sta_filled_nw, sta_filled_nc,sta_filled_ne, sta_filled_sw, 
   sta_filled_sc, sta_filled_se, sta_fil, sta_name)
```

```{r delete-possibly-old-spi-seasonality}
# spi_02mo====  
TARGET <- "spi_2mo"  

spi02  <- spi_fin %>%  
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>%  
  rename(spi = spi_2mo) %>%  
  mutate(duration = 2)  

# spi_03mo====  
TARGET <- "spi_3mo"  

spi03  <- spi_fin %>% 
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>%  
  rename(season_03mo    = season) %>% 
  rename(trend_03mo     = trend) %>%  
  rename(remain_03mo    = remainder) %>%  
  rename(hival_03mo     = remainder_l1) %>%  
  rename(loval_03mo     = remainder_l2) %>%  
  rename(anom_03mo      = anomaly)  

# spi_04mo====  
TARGET <- "spi_4mo"   

spi04  <- spi_fin %>%  
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>%  
  rename(season_04mo    = season) %>%  
  rename(trend_04mo     = trend) %>%  
  rename(remain_04mo    = remainder) %>%  
  rename(hival_04mo     = remainder_l1) %>%  
  rename(loval_04mo     = remainder_l2) %>%  
  rename(anom_04mo      = anomaly)  

# spi_06mo====  
TARGET <- "spi_6mo"  

spi06  <- spi_fin %>%  
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>%  
  rename(season_06mo    = season) %>%  
  rename(trend_06mo     = trend) %>%  
  rename(remain_06mo    = remainder) %>%  
  rename(hival_06mo     = remainder_l1) %>%  
  rename(loval_06mo     = remainder_l2) %>% 
  rename(anom_06mo      = anomaly)  

# spi_09mo====  
TARGET <- "spi_9mo"  

spi09  <- spi_fin %>% 
  select(sta, date, paste0(TARGET)) %>% 
  decomp_fun() %>% 
  rename(season_09mo    = season) %>% 
  rename(trend_09mo     = trend) %>%  
  rename(remain_09mo    = remainder) %>%  
  rename(hival_09mo     = remainder_l1) %>%  
  rename(loval_09mo     = remainder_l2) %>%  
  rename(anom_09mo      = anomaly)  

# spi12mo====  
TARGET <- "spi_12mo"   

spi12  <- spi_fin %>%  
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>%  
  rename(season_12mo    = season) %>%  
  rename(trend_12mo     = trend) %>%  
  rename(remain_12mo    = remainder) %>%  
  rename(hival_12mo     = remainder_l1) %>%  
  rename(loval_12mo     = remainder_l2) %>%  
  rename(anom_12mo      = anomaly)  

# spi_24mo====   
TARGET <- "spi_24mo"  

spi24  <- spi_fin %>%  
  select(sta, date, paste0(TARGET)) %>%  
  decomp_fun() %>% 
  rename(season_24mo    = season) %>%   
  rename(trend_24mo     = trend) %>%   
  rename(remain_24mo    = remainder) %>%   
  rename(hival_24mo     = remainder_l1) %>%   
  rename(loval_24mo     = remainder_l2) %>%   
  rename(anom_24mo      = anomaly)   

# prepare to rejoin the dataframes 
gage_mon0 <- gage_mon1 %>% 
  select(sta:ecoreg) 

gage_mon1 <- gage_mon1 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC1") %>% 
  rename(sta = sta_PC1) %>% 
  rename(Date = Date_PC1)

gage_mon2 <- gage_mon2 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC2") %>% 
  rename(sta = sta_PC2) %>% 
  rename(Date = Date_PC2)

# join the datatables   
gage_mon1 <- full_join(gage_mon0, gage_mon1, 
                       by = c("sta", "Date")
) 

gage_mon <- full_join(gage_mon1, gage_mon2, 
                      by = c("sta", "Date")
) 

rm(gage_mon0, gage_mon1, gage_mon2, decomp_input, mon_trend_sta, decomp_fun)  


# create station and ecoregion summaries 
gage_mon <- gage_mon %>% 
  group_by(sta) %>%  
  mutate(sta_q1 = mean(q1_mon)) %>% 
  mutate(sta_q7 = mean(q7_mon)) %>%   
  mutate(sta_q30 = mean(q30_mon)) %>% 
  mutate(sta_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(sta_PC1 = mean(mon_PC1)) %>% 
  mutate(sta_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>% 
  group_by(ecoreg) %>%  
  mutate(ecoreg_q1 = mean(q1_mon)) %>% 
  mutate(ecoreg_q7 = mean(q7_mon)) %>%   
  mutate(ecoreg_q30 = mean(q30_mon)) %>% 
  mutate(ecoreg_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(ecoreg_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>%  
  group_by(Date, ecoreg) %>% 
  mutate(ecoreg_mean_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_seas_PC1 = mean(season_PC1)) %>% 
  mutate(ecoreg_trend_PC1 = mean(trend_PC1)) %>%   
  mutate(ecoreg_remain_PC1 = mean(remainder_PC1)) %>%   
  ungroup() %>% 
  arrange(ecoreg_PC1) %>% 
  mutate(ecoreg = fct_reorder( 
    as.factor(ecoreg), ecoreg_PC1)  # sets up the ordering by pc-axis 
  ) %>% 
  arrange(sta_PC1) %>%   
  mutate(sta = fct_reorder( 
    as.factor(sta), sta_PC1)  # sets up the ordering by pc-axis 
  ) 
```  

```{r ggplot_monthly, eval=FALSE} 
#sta_mon <- import(file = "data/stations_monthly.csv") %>% 
#  mutate(date = ymd(date))

#sta_mon %>% 
#  group_by(sta) %>% 
#  summarize(count = n())

sta_mon_plus$group <- factor(sta_mon_plus$group,  
                             levels = c("NW", "NC", "NE", "SW", "SC", "SE"))  

# plot monthly precips  
sta_mon_plus %>%   
  ggplot(aes(month, depth_mm, group = month)) +  
  facet_grid(rows = vars(group)) +   
  geom_quasirandom(size = 0.2, 
                   position = position_beeswarm()  
  ) + 
  scale_x_discrete(limits = c("1", "2", "3", "4", "5", "6", 
                              "7", "8", "9", "10", "11", "12"),   
                   labels = c("J", "F", "M", "A", "M", "J", 
                              "J", "A", "S", "O", "N", "D")  
  ) + 
  theme_bw() +  
  #  labs(title = "Monthly precipitation depths",  
  #       subtitle = "Southwestern South Dakota for 1990-2017") +   
  xlab("") +  
  ylab("mm")  


ggplot2::ggsave(path = "figure/", filename = "precip_mon.png", 
                width = 6, height = 6, units = "in")

```

```{r prcp_daily2monthly, eval=FALSE} 

# add short names and create monthly data -----------------------------------  
scratch <- sta_meta_plus %>%  
  select(sta, sta_id)   

sta_mon_plus <- full_join(scratch, sta_filled,   
                          by = "sta_id")  %>%   
  select(-c(name, sta_id))           %>%  
  mutate(year = year(date))          %>%  
  mutate(month = month(date))        %>%  
  group_by(year, month, sta)         %>%   
  summarize(prcp_tenths = sum(prcp)) %>%  
  mutate(prcp_mm = prcp_tenths/10)   %>%  
  select(-prcp_tenths) %>%  
  ungroup()  

rm(scratch)  

# spread results & create date with day at midpoint of month ----------------  
sta_mon_long <- sta_mon_plus %>%  
  spread(sta, prcp_mm) %>%  
  mutate(day = 15) %>%  
  mutate(date = make_date(year = year, month = month, day = day)) %>%   
  select(date, year, month, everything()) %>%  
  select(-day) %>%   
  ungroup() %>%  
  filter(!is.na(date))  

sta_mon_plus <- sta_mon_long %>%  
  gather(key = sta, val = depth_mm,  
         -c(date, year, month))  

rm(sta_mon_long, sta_filled, sta_dv_plus)   

sta_mon_plus <- full_join(sta_mon_plus, sta_meta_plus,  
                          by = "sta")  

export(sta_mon_plus, file = "data/stations_mon_plus.csv")  

```  

```{r save_prior_work, eval=FALSE}  
#export(sta_dv_orig, "data/sta_dv_orig.csv")  
# export(sta_meta_orig, "data/sta_meta_orig.csv")   -- saved above 

export(sta_mon_plus, "data/sta_mon_plus.csv")  


sta_mon_plus <- import("data/sta_mon_plus.csv")
sta_meta_plus <- import("data/sta_meta_plus.csv")



sta_mon <- semi_join(sta_mon_plus, sta_meta, 
                     by = "sta")  

export(sta_mon, "data/sta_mon.csv")  
export(sta_meta, "data/sta_meta.csv")  
```  

```{r plot_precip_seasonality}

# check for anomonies   
spi01_anom <- test %>% 
  filter(anomaly == "Yes")

# prepare for plotting 
sta_size   <- 0.2  
sta_color  <- "black" 
mean_size  <- 0.4  
mean_color <- "gray"  

# SPI-1mo observations plot====
spi01_obs <- spi01 %>% 
  ggplot(aes(date, spi_1mo)) + 
  facet_wrap(vars(sta)) +   
  # plot mean vals 
  geom_line(aes(date, spi01_mean), 
            size            = mean_size, 
            colour          = mean_color 
  ) + 
  # plot station vals 
  geom_line(size            = sta_size, 
            colour          = sta_color
  ) + 
  scale_y_continuous(limits = c(-3, 3)) + 
  theme_bw() + 
  xlab("") + 
  ylab("SPI one-month values")

# SPI-1mo seasons plot==== 
spi01_seas  <- spi01 %>% 
  ggplot(aes(date, season_01mo)) + 
  facet_wrap(~ sta) + 
  # plot mean vals 
  geom_line(aes(date, seas01_mean), 
            size            = mean_size, 
            colour          = mean_color 
  ) + 
  # plot station vals 
  geom_line(size            = sta_size, 
            colour          = sta_color
  ) + 
  scale_y_continuous(limits = c(-3, 3)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Seasonality")  

# SPI-1mo trend plot====  
spi01_trend  <- 
  spi01 %>% 
  ggplot(aes(date, trend_01mo)) + 
  facet_wrap(~ sta) + 
  # plot mean vals 
  geom_line(aes(date, trend01_mean), 
            size            = mean_size, 
            colour          = mean_color 
  ) + 
  # plot station vals 
  geom_line(size            = sta_size, 
            colour          = sta_color
  ) + 
  scale_y_continuous(limits = c(-3, 3)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Trend")

# SPI-1mo remainder plot==== 
spi01_remainder <- spi01 %>%   
  ggplot(aes(date, remain_01mo)) + 
  facet_wrap(~ sta) + 
  # plot mean vals
  geom_line(aes(date, remain01_mean), 
            size            = mean_size, 
            colour          = mean_color 
  ) + 
  # plot station vals 
  geom_line(size            = sta_size, 
            colour          = sta_color
  ) + 
  
  #  geom_jitter(aes(Date, ecoreg_remain_PC1), 
  #              data         = anom_PC1, 
  #              shape        = 4, 
  #              size         = 0.7) + 
  scale_y_continuous(limits = c(-3, 3)) + 
  theme_bw() + 
  xlab("") + 
  ylab("SPI remainder values")

# plot the plots above as a grid & save==== 
cowplot::plot_grid(
  spi01_obs, spi01_seas, spi01_trend, spi01_remainder, 
  ncol = 1, 
  align = "v"
)

cowplot::ggsave2("figure/spi01_deconv.png", 
                 units = "in", 
                 width = 7, 
                 height = 9)


rm(spi01_obs, spi01_seas, spi01_trend, spi01_remainder, spi01_anom) 
```  

```{r to-delete}

# import precipitation metadata----  
#sta_meta_orig <- import("data/sta_meta_orig.csv")  
#dateMax       <- "2018-12-31"    

 
# prepare to download data  
#station <- "RAP"  
#dateMin <- "1949-01-01"    
  
#sta_meta_input <- sta_meta_orig %>%   
#  filter(group == "NW")     

# download daily precip data from NOAA GHCN database 
#sta_input <- sta_meta_input %>%   
#  dplyr::select(sta_id) %>%  
#  split(.$sta_id) %>%  
#  map_dfr(~meteo_tidy_ghcnd(stationid  = .$sta_id,   
#                            keep_flags = TRUE,   
#                            var        = "PRCP",     
#                            date_min   = dateMin,   
#                            date_max   = dateMax)   
#  )     

# fix date & add year and month    
#sta_input<- sta_input %>%  
#  mutate(date  = ymd(date)) %>%  
#  mutate(year  = year(date)) %>%   
#  mutate(month = month(date)) %>%  
#  select(date, year, month, everything())    

#sta_input <- left_join(sta_input, sta_meta_orig,  
#                       by = c("id" = "sta_id"))  


# get alternate site data & check if sta_mis_day = 0      
#sta_alt <- sta_input %>%   
#  filter(str_detect(name, "^RAPID CITY 4")) %>%        
#  filter(!is.na(prcp))      

# make a monthly prcp df     
sta_mon_rap <- sta_fil          %>%  
  arrange(date)                 %>%  
#  mutate(mon    = month(date))  %>%  
#  mutate(yr      = year(date))  %>%  
  group_by(sta, month, year)        %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) #%>%  
#  arrange(date)  

# prepare to download data    
station <- "COT"   
dateMin <- "1946-01-01"      

sta_meta_input <- sta_meta_orig %>%  
  filter(group == "NC"   
  )    

# download daily precip data from NOAA GHCN database    
sta_input <- sta_meta_input %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags = TRUE,   
                            var = "PRCP",   
                            date_min = dateMin,   
                            date_max = dateMax)  
  )       

# fix date & add year and month     
sta_input<- sta_input %>%  
  mutate(date = ymd(date)) %>%  
  mutate(year = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   

sta_input <- left_join(sta_input, sta_meta_orig, 
                       by = c("id" = "sta_id")) 

# prepare fill & check for missing days    
sta_fil <- sta_input   %>%  
  filter(sta == station) %>%  
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)  

# get alternate site data & check if sta_mis_day = 0  
sta_alt <- sta_input %>%   
  filter(str_detect(name, "^MILES")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(sta = sta_short_name$sta) 

sta_fil <- bind_rows(sta_alt, sta_fil) 

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
  
  
# get alternate site data & check if sta_mis_day = 0  
sta_alt <- sta_input %>%   
  filter(str_detect(name, "^INTERIOR")) %>%        
  filter(!is.na(prcp))  

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
# get alternate site data & check if sta_mis_day = 0  
# Phillip & Plainview were big zeros
sta_alt <- sta_input %>%   
  filter(str_detect(name, "^DUPREE")) %>%        
  filter(!is.na(prcp))  

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# make a monthly prcp df  
sta_fil <- sta_fil              %>%  
  arrange(date)                 %>%  
  mutate(mon    = month(date))  %>%  
  mutate(yr      = year(date))  %>%  
  group_by(sta, mon, yr)        %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = yr,  
                              month = mon,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) %>%  
  arrange(date)  

# prepare for input to calculate SCI and clean up   
sta_cot <- sta_fil %>%  
  arrange(date)  

cot <- as.double(sta_cot$prcp)    

rm(dateMin, sta_alt, sta_fil, sta_meta_input, sta_miss_day, sta_short_name, 
   sta_input)   
  
  
# create a df of monthly precipitation values-NE-ONI----  
# prepare to download data     
station <- "ONI"  
dateMin <- "1913-01-01"     

sta_meta_input <- sta_meta_orig %>% 
  filter(group == "NE" 
  ) 

# download daily precip data from NOAA GHCN database   
sta_input <- sta_meta_input %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid  = .$sta_id,   
                            keep_flags = TRUE,   
                            var        = "PRCP",   
                            date_min   = dateMin,   
                            date_max   = dateMax)    
  )      

# fix date & add year and month        
sta_input<- sta_input %>%        
  mutate(date = ymd(date)) %>%   
  mutate(year = year(date)) %>%    
  mutate(month = month(date)) %>%    
  select(date, year, month, everything())    

sta_input <- left_join(sta_input, sta_meta_orig,  
                       by = c("id" = "sta_id")) 

# prepare fill & check for missing days  
sta_fil <- sta_input   %>%  
  filter(sta == station) %>%  
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)  
  
# get alternate site data & check if sta_mis_day = 0  
sta_alt <- sta_input %>%   
  filter(str_detect(name, "^KENNE")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta) 

sta_fil <- bind_rows(sta_alt, sta_fil) 

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)    
  
# get alternate site data & check if sta_mis_day = 0   
sta_alt <- sta_input %>%   
  filter(str_detect(name, "^PIERRE")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta) 

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# make a monthly prcp df   
sta_fil <- sta_fil               %>%  
  arrange(date)                  %>%  
  mutate(mon      = month(date)) %>%  
  mutate(yr       = year(date))  %>%  
  group_by(sta, mon, yr)         %>%  
  summarize(prcp  = sum(prcp))   %>%  
  ungroup()                      %>% 
  mutate(day      = 15)          %>%  
  mutate(date     = make_date(  
    year  = yr,  
    month = mon,  
    day   = day)  
  ) %>%           
  select(sta, date, prcp) %>%  
  arrange(date)  

# prepare for input to calculate SCI and clean up   
sta_oni <- sta_fil %>%  
  arrange(date)  

oni <- as.double(sta_oni$prcp)  

rm(dateMin, sta_alt, sta_fil, sta_meta_input, sta_miss_day, sta_short_name, 
   sta_input)     
   
   
# create a df of monthly precipitation values-SW-OEL----  
# prepare to download data  
station <- "OEL"  
dateMin <- "1948-01-01"      

sta_meta_input <- sta_meta_orig %>%   
  filter(group == "SW" 
  )   
  
# download daily precip data from NOAA GHCN database       
sta_input <- sta_meta_input %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags = TRUE,   
                            var = "PRCP",   
                            date_min = dateMin,    
                            date_max = dateMax)  
  )     
  
# fix date & add year and month   
sta_input<- sta_input %>%  
  mutate(date = ymd(date)) %>%  
  mutate(year = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   
  
sta_input <- left_join(sta_input, sta_meta_orig,   
                       by = c("id" = "sta_id"))   
  
# prepare fill & check for missing days   
sta_fil <- sta_input   %>%  
  filter(sta == station) %>%  
  filter(!is.na(prcp))                                     
  
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)  
  
# get alternate site data & check if sta_mis_day = 0  
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^ORAL")) %>%        
  filter(!is.na(prcp))                             
  
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  
  
sta_fil <- bind_rows(sta_alt, sta_fil)  
  
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# get alternate site data & check if sta_mis_day = 0    
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^HOT SPRINGS")) %>%        
  filter(!is.na(prcp))                             
  
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  
  
sta_fil <- bind_rows(sta_alt, sta_fil)  
  
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
# make monthly prcp df  
sta_fil <- sta_fil               %>%  
  arrange(date)                  %>%  
  mutate(mon      = month(date)) %>%  
  mutate(yr       = year(date))  %>%  
  group_by(sta, mon, yr)         %>%  
  summarize(prcp  = sum(prcp))   %>%  
  ungroup()                      %>% 
  mutate(day      = 15)          %>%  
  mutate(date     = make_date(  
    year  = yr,  
    month = mon,  
    day   = day)  
  ) %>%           
  select(sta, date, prcp) %>%  
  arrange(date)  
  
# prepare for input to calculate SCI and clean up    
sta_oel <- sta_fil %>% 
  arrange(date)  
  
oel <- as.double(sta_oel$prcp) 
  
rm(dateMin, sta_alt, sta_fil, sta_meta_input, sta_miss_day, sta_short_name, 
   sta_input)  

# create a df of monthly precipitation values-SC-GOR----  
# prepare to download data    
station <- "GOR"  
dateMin <- "1937-01-01"       
  
# prepare to download data  
sta_meta_input <- sta_meta_orig %>%   
  filter(group == "SC" ) 

# download daily precip data from NOAA GHCN database     
sta_input <- sta_meta_input %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%   
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags = TRUE,   
                            var = "PRCP",    
                            date_min = dateMin,   
                            date_max = dateMax)  
  )     
  
# fix date & add year and month      
sta_input<- sta_input %>%  
  mutate(date = ymd(date)) %>%   
  mutate(year = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   
   
sta_input <- left_join(sta_input, sta_meta_orig,    
                       by = c("id" = "sta_id"))  
   
# prepare fill & check for missing days    
sta_fil <- sta_input   %>%  
  filter(sta == station) %>%  
  filter(!is.na(prcp))                                     
  
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)  
  
# get alternate site data & check if sta_mis_day = 0   
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^VALENTINE NWR")) %>%        
  filter(!is.na(prcp))                             
   
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  
   
sta_fil <- bind_rows(sta_alt, sta_fil)  
   
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%    
  summarise(num_day = n())     %>%   
  ungroup()                    %>%   
  filter(num_day < 365)   
  
# get alternate site data & check if sta_mis_day = 0  
# Kyle was a big zero  
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^MULLEN")) %>%        
  filter(!is.na(prcp))                             
   
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta) 
  
sta_fil <- bind_rows(sta_alt, sta_fil)  
  
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
# get alternate site data & check if sta_mis_day = 0   
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^ELLSWORTH")) %>%        
  filter(!is.na(prcp))                             
   
sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>%  
  mutate(sta = sta_short_name$sta)  
   
sta_fil <- bind_rows(sta_alt, sta_fil)  
   
sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   
  
# make monthly prcp df    
sta_fil <- sta_fil               %>%  
  arrange(date)                  %>%  
  mutate(mon      = month(date)) %>%  
  mutate(yr       = year(date))  %>%  
  group_by(sta, mon, yr)         %>%  
  summarize(prcp  = sum(prcp))   %>%  
  ungroup()                      %>%  
  mutate(day      = 15)          %>%  
  mutate(date     = make_date(  
    year  = yr,  
    month = mon,  
    day   = day)  
  ) %>%           
  select(sta, date, prcp) %>%  
  arrange(date)  
  
# prepare for input to calculate SCI and clean up     
sta_gor <- sta_fil %>%  
  arrange(date) 
  
gor <- as.double(sta_gor$prcp)  
  
rm(dateMin, sta_alt, sta_fil, sta_meta_input, sta_miss_day, sta_short_name, 
   sta_input)    
  
# create a df of monthly precipitation values-SW-MIS----  
# prepare to download data     
station <- "MIS"  
dateMin <- "1952-01-01"  

sta_meta_input <- sta_meta_orig %>% 
  filter(group == "SE" 
  )   

# download daily precip data from NOAA GHCN database     
sta_input <- sta_meta_input %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                            keep_flags = TRUE,   
                            var = "PRCP",   
                            date_min = dateMin,    
                            date_max = dateMax)  
  )     

# fix date & add year and month     
sta_input<- sta_input %>%  
  mutate(date = ymd(date)) %>%  
  mutate(year = year(date)) %>%  
  mutate(month = month(date)) %>%  
  select(date, year, month, everything())   

sta_input <- left_join(sta_input, sta_meta_orig, 
                       by = c("id" = "sta_id")) 

# prepare fill & check for missing days     
sta_fil <- sta_input   %>%  
  filter(sta == station) %>%  
  filter(!is.na(prcp))                                     

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

sta_short_name <- sta_fil %>%   
  slice(1) %>%  
  select(sta, id)    

# get alternate site data & check if sta_mis_day = 0   
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^WOOD")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(sta = sta_short_name$sta) 

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

sta_mon_rap <- sta_fil          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

sta_mon_cot <- sta_fil          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

sta_mon_oni <- sta_fil          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

sta_mon_oel <- sta_fil          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

sta_mon_gor <- sta_fil          %>%  
  arrange(date)                 %>%  
  group_by(sta, month, year)    %>%  
  summarize(prcp = sum(prcp))   %>%  
  ungroup()                     %>% 
  mutate(day     = 15)          %>%  
  mutate(date    = make_date(year  = year,  
                              month = month,  
                              day   = day)  
  ) %>%           
  select(sta, date, prcp) 

# get alternate site data & check if sta_mis_day = 0  
sta_alt <- sta_input %>%    
  filter(str_detect(name, "^VALENTINE MILLER")) %>%        
  filter(!is.na(prcp))                             

sta_alt <- anti_join(sta_alt, sta_fil, by = "date") %>% 
  mutate(sta = sta_short_name$sta) 

sta_fil <- bind_rows(sta_alt, sta_fil)  

sta_miss_day <- sta_fil        %>%   
  group_by(sta, year)          %>%   
  summarise(num_day = n())     %>%  
  ungroup()                    %>%  
  filter(num_day < 365)   

# make monthly prcp df    
sta_fil <- sta_fil               %>%  
  arrange(date)                  %>%  
  mutate(mon      = month(date)) %>%  
  mutate(yr       = year(date))  %>%  
  group_by(sta, mon, yr)         %>%  
  summarize(prcp  = sum(prcp))   %>%  
  ungroup()                      %>% 
  mutate(day      = 15)          %>%  
  mutate(date     = make_date(  
    year  = yr,  
    month = mon,  
    day   = day)  
  ) %>%           
  select(sta, date, prcp) %>%  
  arrange(date)  

# prepare for input to calculate SCI and clean up    
sta_mis <- sta_fil %>% 
  arrange(date) 

mis <- as.double(sta_mis$prcp) 

rm(dateMin, sta_alt, sta_fil, sta_meta_input, sta_miss_day, sta_short_name, 
   sta_input) 
  
# prepare final spi values----      
sta_fin <- bind_rows(sta_cot,     
                     sta_gor,    
                     sta_mis,   
                     sta_oel,  
                     sta_oni,    
                     sta_rap) %>%    
  # remove shorter record stations   
  mutate(yr = year(date)) %>%   
  filter(yr > 1953) %>%    
  # filter to a 60-yr time period   
  filter(yr > 1958)       
```   