---
output: html_document
editor_options: 
  chunk_output_type: console
---

<!--
This R markdown file refactors prior code for spi analysis

1) download and clean daily precip
2)  convert daily precip to monthly precip  
3)  prepare data for drought index 

Data:
Predominant datasets used are:  
1) NOAA data
2) USGS daily streamflow and station metadata,  
####3) Summary data from a QGIS analysis of ungaged watersheds of interest.  



1.2. Data munging to check, join and remove NAs 
1.3. Updated unit vals - originally in tenths of mm; now in mm 
1.4. Changed daily data into monthly and annual data 
1.5. Cross-validated monthly data 
1.6. Created plots 
2.1  Data read in from USGS website by dataRetrieval (cfs)
2.2  Data saved as array (.JSON), and flat format (.csv) 

# Next STEPS
1. Update variable names
2. Check on next steps from Chapter 2 list 
3. Describe the precipitation seasonality

## Variable naming convention:   
sta          precipitation station  
_meta        metadata  
_raw         the "mostly" raw dataset  
_geXX        data greater than or equal to year XX  
_geXXmYY     data greater than or equal to year XX and month YY   
_ltXXmYY     data less than year XX and month YY   
_clean       intermediate df - clean part of NA split ;-}  
_dirty       intermediate df - NA part of NA split ;-}  
_trans       final df - after cleaning  

gage         USGS streamgage station


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# 'station' is a variable for NOAA weather station locations  
# 'gage' is a variable for USGS stream gages 
# 'site' is a variable for OST monitoring stations 


## Results: 
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?

Variable naming convention:   
# ~~~~~~~~~~~~~~~~~~~~~~~~~
gage_poss           possible USGS gaging stations in the study area 
gage_meta_poss      metadata for possible gaging stations
-- _int1            scratch df for pulling gages with integer fields 
-- _int2            scratch df for pulling gages with integer fields 
-- _char            scratch df for pulling gages with character fields 

--> 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)  
options(tibble.print_max = 70) # sets tibble output for printing       
```

```{r library, message=FALSE} 

# Get API key (aka, token) for downloading precip data at http://www.ncdc.noaa.gov/cdo-web/token
# token for NOAA API tied to jtinant@olc.edu -- see 'rnoaa' for details 
options(noaakey = "VpcuARumMpCfFyclKHPfvskEYnaiLJHD") 

# Sets up the library of packages  
library("here")              # identifies where to save work
library("rnoaa")             #  R wrapper for NOAA data inc. NCDC 
library("rio")               # more robust I/O - to import and clean data 
library("lubridate")         # easier dates
library("tidyverse") 
library("janitor")           # tools for examining and cleaning dirty data 
library('deldir')            # for Vorononi tesselation - Theissen polygons 
library("broom")             # tidies linear models 
library("corrplot")          # correlation plots

# Other packages     
#library("EGRET")            # Exploration and Graphics for RivEr Trends 
#library("dataRetrieval")    # USGS data import  
#library("measurements")     # eases measurement system manipulation 
#library("flextable")        # construct complex table with 'kable'  
#library("officer")          # facilitates '.docx' access for table export  
#library("forecast")         # for BoxCox.lambda 
#library("ggfortify")        # data vis tools for statistical analysis 
#library("stringi")          # character string processing facilities 
#library("sf")               # simple features--spatial geometries for R 
#library("anomalize")        # detect anomalies using the tidyverse  
#library("caret")            # classification and regression training 
#library("doMC")             # parallelization for caret 
#library("glmnet")           # fit a GLM with lasso or elasticnet regularization 
#library("dabestr")          # data analysis using bootstrap estimation 
#library("Metrics")          # evaluation metrics for machine learning  
#library("DataExplorer") #--- used for plot_correlation() --- 
#library("assertthat") 
 
#library('jsonlite') # tools for working with lists 
#library("magrittr") # provides aliases for easier reading
#library("maps") # outlines of continents, countries, states & counties
#library("mapdata") # higher-resolution outlines
#library('ggmap')
# library("RColorBrewer") - there is a better one?
# library("workflowr") # creates a research website
# library("colorspace")
# library("bookdown") # 
# library(unpivotr) # fix nasty Excel files
# library("friendlyeval")
# library("mathpix")                # support for 'Mathpix' image to 'LaTeX'   
# library("cowplot") 

# a useful description of commits:
# http://r-pkgs.had.co.nz/git.html

``` 

```{r import_prcp_metadata, eval=FALSE}

# Get possible stations -----------------------------------------------------  
# It's also possible to check station id with the mapping tool at:
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation 

# The geographical extent given as SElat, SElon, NWlat, NWlon
sta_meta_orig <- ncdc_stations(extent = c(42.5, -104.5, 45, -99.5), 
                               limit = 1000) %>%      # n = 777 
# get possible stations into a dataframe 
  pluck("data") %>% 
# turn min & max date into lubridates 
  mutate(mindate = ymd(mindate)) %>% 
  mutate(maxdate = ymd(maxdate)) %>%                 
# remove 'young' stations 
  filter(mindate < "1989-01-01") %>%                    # n = 369 
# remove 'dead' stations 
  filter(maxdate > "2018-01-01") %>%                    # n = 59  
# remove poor data coverage stations   
  filter(datacoverage == 1.0) %>%                       # n = 52  
# keep only the GHCND stations  
  filter(str_detect(id, "^GHCND"))                      # n = 49     

# create groups of stations by region 
sta_meta_orig <- sta_meta_orig %>% 
  mutate(north_group = case_when( 
    latitude > 43.5 ~ "N", 
    TRUE ~ "S") 
    ) %>% 
  mutate(east_group = case_when( 
    longitude > -100.67 ~ "E", 
    longitude > -102.33 ~ "C",  
    TRUE ~ "W") 
    )  %>% 
  mutate(group = str_c(north_group, east_group, sep = "")) %>%  
  select(-c(north_group, east_group)) %>%  
# "GHCND:" doesn't appear in calls to get Global Historical 
# Climatology Network (GHCN) Daily Data   
    separate(  
             col = id, 
             sep = ":",   
             into = c("type", "sta_id")  
             ) 

sta_meta_orig <- sta_meta_orig %>% 
  select(type, sta_id, name, group, longitude, latitude, everything())


export(sta_meta_orig, "data/sta_meta_orig.csv")  

```

```{r plot-theissen-initial, message=FALSE}

# Define the study area using data from the 'maps' package ------------------
# import polygon data - US counties 
counties <- map_data("county")  

# Filter out wyoming & sd counties  
counties_wysd <- subset(counties, region %in%  
   c("south dakota")) %>%  
  filter(   
    subregion == "meade" |  
    subregion == "lawrence" |  
    subregion == "pennington" |  
    subregion == "custer" |  
    subregion == "fall river" |   
    subregion == "shannon" |  
    subregion == "jackson" |  
    subregion == "bennett" |  
    subregion == "haakon" |   
    subregion == "todd" |  
    subregion == "mellette" |   
    subregion == "jones" |   
    subregion == "stanley" |   
    subregion == "lyman" |  
    subregion == "tripp" |  
    subregion == "hughes"   
    )  

# filter nebraska  counties -- some names are duplicated above
counties_ne <- subset(counties, region %in% 
   c("nebraska")) %>% 
  filter(subregion == "sioux" |
         subregion == "dawes" | 
         subregion == "sheridan" | 
         subregion == "box butte" |
         subregion == "cherry" | 
         subregion == "brown" | 
         subregion == "keya paha" | 
         subregion == "rock" 
         )  

counties <- bind_rows(counties_wysd, counties_ne) 

rm(counties_wysd, counties_ne) 

# create Theissen polygons --------------------------------------------------  
voronoi <- deldir(sta_meta_orig$lon, sta_meta_orig$lat) 

# get streamflow gages 
gage_meta_fin <- read_csv("data/gage_meta_fin.csv") %>% 
  filter(!is.na(sta))

# plot stations -------------------------------------------------------------  
# grabbed this code off of a website by googling 'Voroni diag. R'
sta_meta_orig %>% 
ggplot(aes(x = longitude, y = latitude)) +   
# plot weather stations   
  geom_point(  
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333")  +  
# add Theissen polygons  
  geom_segment(  
    aes(x = x1, y = y1, xend = x2, yend = y2),  
    size = 2,  
    data = voronoi$dirsgs,  
    linetype = 1, 
    color = "#FFB958") +  
# add county polygons   
  geom_polygon(data = counties,  
               aes(x = long, y = lat, group = group),  
              color = "black",  
              #linetype = "dashed",  
              fill = "NA") +  
# add stream flow gages  
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(70, 130, 180, 255,  
               maxColorValue = 255),  
    pch = 21,  
    size = 4,  
    color = "#333333" 
    )  +  
  theme_bw() +   
  ggtitle("Original Theissen Polygon Map")  

# export map 
ggplot2::ggsave(filename = "figure/theissen_init.png",   
                width = 6, height = 6, units = "in")  
```

```{r plot-theissen-final_stations}

# select final stations -- based on central location + nearest neighbors 
sta_meta_plus <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW      
         str_detect(name, "^RAPID CITY 4") |     # NW-alt      
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^INTERIOR")     |     # NC-alt 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^KENNEBEC")     |     # NE-alt  
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^OELRICHS")     |     # SW-alt   
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^VALENTINE")    |     # SC-alt     
         str_detect(name, "^MISSION")      |     # SE  
         str_detect(name, "^WOOD")               # SE-alt   
         )     

# select final stations 
sta_meta <- sta_meta_orig %>% 
  filter(str_detect(name, "^RAPID CITY R") |     # NW         
         str_detect(name, "^COTTONWOOD")   |     # NC 
         str_detect(name, "^ONIDA")        |     # NE
         str_detect(name, "^ORAL")         |     # SW    
         str_detect(name, "^GORDON")       |     # SC         
         str_detect(name, "^MISSION")            # SE
         )     

# update Theissen line segments  
voronoi <- deldir(sta_meta$lon, sta_meta$lat) 

# plot stations 
sta_meta %>%  
ggplot(aes(x = longitude, y = latitude)) + 
# add stations  
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333")  + 
# add Theissen polygons 
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2), 
    size = 2, 
    data = voronoi$dirsgs, 
    linetype = 1, 
    color = "#FFB958")  + 
# add county polygons   
  geom_polygon(data = counties, 
               aes(x = long, y = lat, group = group),  
              color = "black", 
              #linetype = "dashed", 
              fill = "NA") + 
# add gages 
  geom_point(data = gage_meta_fin, aes(dec_long_va, dec_lat_va),  
             shape = 24, 
    fill = rgb(130, 130, 180, 255, 
               maxColorValue = 255),
    pch = 21,
    size = 2,
    color = "#333333" 
    )  + 
#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "red", linetype = "dashed", fill = "NA") + 
  theme_bw() +
#  geom_text(data = sta_meta_orig, aes(label = name), size = 2.5) +
  ggtitle("Final Theissen Polygon Map")  

#  geom_polygon(data = prr, aes(x = long, y = lat, group = group),  
#              color = "black", linetype = "dashed", fill = "NA") + 

# export map 
ggplot2::ggsave(filename = "figure/theissen_fin.png",   
                width = 6, height = 6, units = "in")  

rm(voronoi, counties)

```  

```{r download_daily-precip}

# date function calls start one-year early for long-term drought calcs  
dateMin = "1989-01-01"      
dateMax = "2017-12-31"  

# download daily precip data from NOAA GHCN database ------------------------   
sta_dv_plus <- sta_meta_plus %>%  
  select(sta_id) %>%  
  split(.$sta_id) %>%  
  map_dfr(~meteo_tidy_ghcnd(stationid = .$sta_id,   
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax)  
          )     

rm(dateMin, dateMax)  

# fix date & add year and month
sta_dv_plus <- sta_dv_plus %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything()) 

export(sta_dv_plus, file = "data/sta_dv_plus.csv")  
export(sta_meta_plus, file = "data/sta_meta_plus.csv")  

``` 

```{r fill_missing_precip_data}  

# import data from file 
sta_dv_plus   <- import(file = "data/sta_dv_plus.csv") %>% 
  mutate(date = ymd(date))  

sta_meta_plus <- import(file = "data/sta_meta_plus.csv")
sta_meta      <- import(file = "data/sta_meta.csv")

# join bits of daily data-plus to metadata   
sta_dv_plus <- right_join(sta_meta_plus, sta_dv_plus,  
                   by = c("sta_id" = "id"))   %>%  
  select(name, sta_id, date, year, month, mflag_prcp,  
         prcp, qflag_prcp, sflag_prcp)  %>% 
  select(date, everything())   

# check for missing years  
sta_check <- sta_dv_plus %>%  
  group_by(name, sta_id, year) %>%  
  summarise(num_day = n())  %>% 
  filter(num_day < 365) %>% 
  ungroup() 

# separate north-west data --------------------------------------------------  
sta_nw <-  sta_dv_plus %>% 
  filter(str_detect(name, "^RAPID CITY R")) %>% 
  filter(!is.na(prcp)) 
  
sta_nw_nm <- sta_nw %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nw_len <- sta_nw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nw_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nw_nm$name) %>% 
  mutate(sta_id = sta_nw_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nw_fill <- anti_join(sta_nw_alt, sta_nw, by = "date") 

sta_nw <- bind_rows(sta_nw, sta_nw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nw_alt, sta_nw_nm, sta_nw_fill)  

# separate and fill north-central  ------------------------------------------  
sta_nc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^COTTONWOOD")) %>% 
  filter(!is.na(prcp)) 

sta_nc_nm <- sta_nc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_nc_len <- sta_nc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

sta_nc_alt <-  sta_dv_plus %>% 
  filter(str_detect(name, "^INTERIOR")) %>% 
  mutate(name = sta_nc_nm$name) %>% 
  mutate(sta_id = sta_nc_nm$sta_id) 

# get alternate site data & prepare fill 
sta_nc_fill <- anti_join(sta_nc_alt, sta_nc, by = "date") 

sta_nc <- bind_rows(sta_nc, sta_nc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_nc_alt, sta_nc_nm, sta_nc_fill)  

# separate and fill north-east ----------------------------------------------    
sta_ne <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ONIDA")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_ne_nm <- sta_ne %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_ne_len <- sta_ne %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_ne_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^KENNEBEC")) %>% 
  mutate(name = sta_ne_nm$name) %>% 
  mutate(sta_id = sta_ne_nm$sta_id)           

sta_ne_fill <- anti_join(sta_ne_alt, sta_ne, by = "date") 

sta_ne <- bind_rows(sta_ne, sta_ne_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_ne_alt, sta_ne_nm, sta_ne_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sw <-  sta_dv_plus %>% 
   filter(str_detect(name, "^ORAL")) %>% 
  filter(!is.na(prcp))      

# prepare to change name 
sta_sw_nm <- sta_sw %>% 
  slice(1) %>% 
  select(name, sta_id) 

# get length 
sta_sw_len <- sta_sw %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sw_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^OELRICHS")) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

sta_sw <-  sta_sw %>% 
  filter(!is.na(prcp))      

# download daily precip data for missing values--HotSprings     
dateMin = "2005-12-01"      
dateMax = "2011-05-31"  
sta_sw_alt <- meteo_tidy_ghcnd(stationid = "USC00394007",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sw_nm$name) %>% 
  mutate(sta_id = sta_sw_nm$sta_id) 

# fill missing values 
sta_sw_fill <- anti_join(sta_sw_alt, sta_sw, by = "date") 

sta_sw <- bind_rows(sta_sw, sta_sw_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sw %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sw_alt, sta_sw_nm, sta_sw_fill)  

# separate and fill southwest ------------------------------------------------    
sta_sc <-  sta_dv_plus %>% 
   filter(str_detect(name, "^GORDON")) %>% 
  filter(!is.na(prcp))     

# prepare to change name 
sta_sc_nm <- sta_sc %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_sc_len <- sta_sc %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n())  %>% 
  ungroup()  

# get alternate site data & prepare fill 
sta_sc_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^VALENTINE")) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)        

sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>%  
  arrange(name) %>%  
  distinct(date, .keep_all = TRUE)  

sta_check <- sta_sc %>%  
  filter(is.na(prcp))  

sta_sc <-  sta_sc %>%  
  filter(!is.na(prcp))   

# download daily precip data for missing values--FT Robinson 
# note: Hay Springs did not have all data 
dateMin = "2006-08-01"      
dateMax = "2012-03-31"  
sta_sc_alt <- meteo_tidy_ghcnd(stationid = "USC00253015",
                         keep_flags = TRUE,   
                         var = "PRCP",   
                         date_min = dateMin,   
                         date_max = dateMax) %>% 
  mutate(name = sta_sc_nm$name) %>% 
  mutate(sta_id = sta_sc_nm$sta_id)       

# fill missing values 
sta_sc_fill <- anti_join(sta_sc_alt, sta_sc, by = "date") 

sta_sc <- bind_rows(sta_sc, sta_sc_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE) %>%  
  select(-id)  

sta_check <- sta_sc %>% 
  filter(is.na(prcp)) 

rm(dateMin, dateMax, sta_sc_alt, sta_sc_nm, sta_sc_fill)  

# separate south-west data --------------------------------------------------  
sta_se <-  sta_dv_plus %>% 
   filter(str_detect(name, "^MISSION")) %>% 
  filter(!is.na(prcp)) 

# prepare to change name 
sta_se_nm <- sta_se %>% 
  slice(1) %>% 
  select(name, sta_id) 

sta_se_len <- sta_se %>% 
  group_by(name, sta_id) %>% 
  summarise(count = n()) 

# get alternate site data & prepare fill 
sta_se_alt <-  sta_dv_plus %>% 
   filter(str_detect(name, "^WOOD")) %>% 
  mutate(name = sta_se_nm$name) %>% 
  mutate(sta_id = sta_se_nm$sta_id)        

sta_se_fill <- anti_join(sta_se_alt, sta_se, by = "date") 

sta_se <- bind_rows(sta_se, sta_se_fill)  %>% 
  arrange(name) %>% 
  distinct(date, .keep_all = TRUE)

rm(sta_se_alt, sta_se_nm, sta_se_fill)  

# rejoin daily data and lengths ---------------------------------------------  
sta_dv <- bind_rows(sta_nw, 
                    sta_nc, 
                    sta_ne, 
                    sta_sw, 
                    sta_sc, 
                    sta_se) 

sta_dv_len <- bind_rows(sta_nw_len, 
                        sta_nc_len, 
                        sta_ne_len, 
                        sta_sw_len, 
                        sta_sc_len, 
                        sta_se_len) %>% 
  mutate(max_count = max(count)) %>% 
  mutate(perc_complete =  
    round(count/max_count,  
    digits = 3) 
    ) %>% 
  select(-c(count, max_count))  

# check data & clean up -----------------------------------------------------   
sta_check <- sta_dv %>% 
  filter(is.na(prcp)) 


# join dv-lenth to metadata  
sta_meta <- right_join(sta_meta, sta_dv_len,  
                   by = c("sta_id", "name"))  

rm(sta_dv_len, sta_meta_orig, sta_meta_plus, sta_check, sta_dv_plus, 
   sta_nw, sta_nc, sta_ne, sta_sw, sta_sc, sta_se, 
  sta_nw_len, sta_nc_len, sta_ne_len, sta_sw_len, sta_sc_len, sta_se_len)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")  

``` 

```{r check_prcp_data_quality}  

# Table of Measurement Flag/Attributes -- mflag  
# Blank = no measurement information applicable           
# A     = precip depth is a multi-day total, accumulated since last meas         
# B     = precipitation total formed from two twelve-hour totals
# D     = precipitation total formed from four six-hour totals           
# H     = represents TMAX or TMIN or average of hourly values (TAVG)           
# K     = converted from knots            
# L     = temperature appears lagged with respect to reported hr of observation        # O     = converted from oktas  
# P     = identified as "missing presumed zero" in DSI 3200 and 3206           
# T     = trace of precipitation, snowfall, or snow depth   
# W    = converted from 16-point WBAN code (for wind direction)  

sta_check <- sta_dv %>%  
  group_by(name, mflag_prcp) %>%  
  summarise(mflag = n())  

# flags are T & A flags  
sta_dv <- sta_dv %>%  
  select(-mflag_prcp)  

# Table of Quality Flag/Attributes  
# Blank = did not fail any quality assurance check  
# D     = failed duplicate check           
# G     = failed gap check           
# I     = failed internal consistency check           
# K     = failed streak/frequent-value check           
# L     = failed check on length of multiday period  
# M     = failed mega-consistency check            
# N     = failed naught check            
# O     = failed climatological outlier check            
# R     = failed lagged range check           
# S     = failed spatial consistency check            
# T     = failed temporal consistency check            
# W     = temperature too warm for snow            
# X     = failed bounds check           
# Z     = flagged as a result of an official Datzilla investigation  

sta_check <- sta_dv %>%  
  group_by(name, qflag_prcp) %>%  
  summarise(qflag = n())  

# flags are L flags  
sta_dv <- sta_dv %>%  
  select(-qflag_prcp)  

# Table Source Flag/Attributes   
# Blank = No source (i.e., data value missing)  
# 0  = U.S. Cooperative Summary of the Day (NCDC DSI-3200)   
# 6  = CDMP Cooperative Summary of the Day (NCDC DSI-3206)  
# 7  = U.S. Cooperative Summary of the Day -- Transmitted via WxCoder
# A  = U.S. Automated Surface Observing System (ASOS) real-time data 
# a  = Australian data from the Australian Bureau of Meteorology           
# B  = U.S. ASOS data for October 2000-December 2005 (NCDC  DSI-3211)  
# b  = Belarus update           
# C  = Environment Canada            
# E  = European Climate Assessment and Dataset (Klein Tank et al., 2002)            
# F  = U.S. Fort data             
# G  = Off Global Climate Observing System (GCOS) or other gov-supplied data   
# H  = High Plains Regional Climate Center real-time data            
# I  = International collection (non U.S. data received thru pers. contacts   
# K  = U.S. Coop Summary of the Day data digitized from paper observer forms  
# M  = Monthly METAR Extract (additional ASOS data)           
# N  = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)           
# Q  = Data from African countries w/ later permission granted 
# R  = NCDC Reference Network Database  
# r  = All-Russian Research Inst. Hydromet Information-World Data Center    
# S  = Global Summary of the Day (NCDC DSI-9618)  
#        NOTE: "S" values are derived from hourly synoptic reports   
#         exchanged on the Global Telecommunications System (GTS).  
#         Daily values derived in this fashion may differ significantly from 
#        "true" daily data, particularly for precip (i.e., use with caution).  
# s  = China Met Admin/Nat Met Info/Climate Data Center (http://cdc.cma.gov.cn)   
# T  = SNOwpack TELemtry (SNOTEL) data from Western Regional Climate Center   
# U  = Remote Automatic Weather Station (RAWS) data from West Reg Climate Centr  
# u  = Ukraine update           
# W  = WBAN/ASOS Summary of the Day from NCDC's Integrated Surface Data (ISD)  
# X  = U.S. First-Order Summary of the Day (NCDC DSI-3210)           
# Z  = Datzilla official additions or replacements           
# z  = Uzbekistan update  

sta_check <- sta_dv %>%  
  group_by(name, sflag_prcp) %>%  
  summarise(sflag = n())  

# flags are 0, 7, K, W, X, Z  
sta_dv <- sta_dv %>%  
  select(-sflag_prcp)  

rm(sta_check)  

# export daily data   
export(sta_dv, file = "data/sta_dv.csv")  
export(sta_meta, file = "data/sta_meta.csv")
```


```{r prcp_daily2monthly}

sta_dv   <- import(file = "data/sta_dv.csv")  %>% 
  mutate(date = ymd(date))
sta_meta <- import(file = "data/sta_meta.csv")  


# add short names to metadata & update the stored file ----------------------  
sta_meta <- sta_meta %>%  
  mutate(sta = case_when(  
    str_detect(name, "^RAPID CITY R") ~ "RAP",  
    str_detect(name, "^COTTONWOOD")   ~ "COT",  
    str_detect(name, "^ONIDA")        ~ "ONI",  
    str_detect(name, "^ORAL")         ~ "ORA",  
    str_detect(name, "^GORDON")       ~ "GOR",      
    str_detect(name, "^MISSION")      ~ "MIS",  
    TRUE ~ "ERROR"  
         )   
  ) %>%   
  select(sta, name, longitude, latitude, elevation,  
         perc_complete, group, everything())  

export(sta_meta, file = "data/sta_meta.csv")  

# add short names and create monthly data -----------------------------------  
scratch <- sta_meta %>%  
  select(sta, sta_id)  

sta_mon <- full_join(scratch, sta_dv,   
                     by = "sta_id") %>%  
  select(-c(name, sta_id)) %>%  
  group_by(year, month, sta) %>%   
  summarize(prcp_tenths = sum(prcp)) %>%  
  mutate(prcp_mm = prcp_tenths/10) %>%  
  select(-prcp_tenths) %>%  
  ungroup()  

rm(scratch)  

# spread results & create date with day at midpoint of month ----------------  
sta_mon_long <- sta_mon %>%  
  spread(sta, prcp_mm) %>%  
  mutate(day = 15) %>%  
  mutate(date = make_date(year = year, month = month, day = day)) %>%   
  select(date, year, month, everything()) %>%  
  select(-day) %>%  
  ungroup() %>% 
  filter(!is.na(date))

sta_mon <- sta_mon_long %>%  
  gather(key = sta, val = depth_mm,  
         -c(date, year, month))  

rm(sta_dv_check)  
rm(sta_mon_long, sta_dv)  

sta_check <- sta_mon %>% 
  filter(is.na(sta))     

export(sta_mon, file = "data/stations_monthly.csv")  
```



```{r summaries}

sta_summary_mon <- sta_mon %>% 
  group_by(sta, month) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(month) %>%
  arrange(sta)

sta_summary_yr <- sta_mon %>% 
  group_by(sta, year) %>%
  summarise(mean = mean(depth_mm),      #, na.rm = TRUE
            med = median(depth_mm),
            IQR = IQR(depth_mm), 
            min = min(depth_mm), 
            max = max(depth_mm)) %>%
  arrange(year) %>%
  arrange(sta)

export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
export(sta_summary_yr, file = "data/sta_summary_yr.csv")  
```


START HERE 
```{r ggplot_monthly} 
sta_mon <- import(file = "data/stations_monthly.csv") %>% 
  mutate(date = ymd(date))

sta_mon %>% 
  group_by(sta) %>% 
  summarize(count = n())

sta_mon$sta <- factor(sta_mon$sta, 
                      levels = c("RAP", "ORA", "GOR", "COT", "ONI", "MIS"))

# plot monthly precips
sta_mon %>% 
ggplot(aes(month, depth_mm, group = month)) + 
  facet_grid(rows =vars(sta)) +
  geom_boxplot() +
  theme_bw() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "1990-2017") + 
       xlab("") +
       ylab("mm")


# plot annual precips
sta_mon %>% 
ggplot(aes(year, depth_mm, group = year)) + 
  facet_grid(rows =vars(sta)) +
  geom_boxplot() +
  theme_bw() + 
  labs(title = "Annual precipitation depths",
       subtitle = "1990-2017") + 
       xlab("") +
       ylab("mm")

#ggplot2::ggsave(path = "figure/", filename = "precip_mon.png", 
#                width = 6, height = 6, units = "in")
```

```{r correlation}
# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

# rewrite code from below
corr_ann   <- as.tibble(import("data/stations_yearly.csv")) %>% 
  gather(key = "station", value = "prcp",  -year, na.rm = TRUE) %>% 
  filter(year > 1972) %>% 
  spread(station, prcp) %>% 
  select(-year) %>% 
  cor()

# save the corrplot - see below -
# By default, RStudio enables inline output (notebook mode) on all R 
# Markdown documents, so you can interact with any R Markdown document # as though it were a notebook. If you have a document with which you 
# prefer to use the traditional console method of interaction, you can # disable notebook mode by clicking the gear in the editor toolbar and # choosing Chunk Output in Console.
corrplot.mixed(corr_ann, order = "hclust", 
                 addrect = 2, upper = "ellipse", 
                 lower = "number")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# this is the old code - saving to make sure it works in the future
# sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
# sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
# sta_yr <- sta_yr %>%
#   arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
# sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
#                    -year, factor_key = TRUE)

# filter NAs
# sta_gath_72 <- sta_gath %>%
#   filter(year > 1972) 

# spread remaining matrix & arrange from west to east
# sta_72 <- sta_gath_72 %>%
#   spread(station, prcp) %>%
#   select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
# sta_M <- cor(sta_72)
# corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper =  "ellipse", 
# lower = "number", title = "Precipitation station correlation")  
```


# OLD CODE to delete 
```{r import_prcp, eval=FALSE}

# gather daily values  
#sta_gath <- sta_dv %>%  
#  gather(key = "sta", value = "prcp",  
#         -c(date, year, month, name, sta_id)  
#         )  
# make a table for manual import  
# sta <- as.tibble(select(station_meta, 1:2))  


<!--
## Fixing NA values  
These are the stations that NA values were fixed
NAs are caused in part by different start dates  
OEL - OELRICHS    1893 - 2018  - fixed with Murdo  
COT - COTTONWOOD  1909 - 2018  - fixed with Interior & Murdo  
RAP - RAPID CITY  1948 - 2018  - fixed with Interior    
INT - INTERIOR    1949 - 2018  - fixed with Cottonwood & Murdo    
ORA - ORAL        1971 - 2018  - fixed with Oelrichs & Harrison  
-->

# Results: 
# We can drop Ainsworth, Harrison that are outside of study area &
# Oral would be better than Hot Springs for elev, location & coverage 
#  geom_text(data = sta_meta_orig, aes(label = name), size = 2.5) +

## Old Stations and reason for removal, if removed
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US - removed for short record  
# GHCND:USC00253615 HARRISON, NE US - removed for outside region  
#GHCND:USC00396212 OELRICHS, SD US    
#GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US - too close to Oelrichs  
#GHCND:USC00396304 ORAL, SD US    
#GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US    
##GHCND:USC00394184 INTERIOR 3 NE, SD US    
#GHCND:USC00391972 COTTONWOOD 2 E, SD US   
#GHCND:USC00394983 LONG VALLEY, SD US                  
#GHCND:US1SDJK0006 KADOKA 0.3 N, SD US - removed short period record   
#GHCND:USC00395891 MURDO, SD US - removed for outside region   
#GHCND:USC00395638 MISSION 14 S, SD US    
#GHCND:USC00250050 AINSWORTH, NE US 


# iterate across a list of station ids by purrr::map 
# to get NOAA station meta data using rnoaa::ncdc_stationsn
# the output is a list of 7 x 2 x 9.n
# flatten into a dataframe by purrr::flatten 
# reorder and rename columns by dplyr 


# Also possible to check station id with the mapping tool at:
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation 
# iterate across a list of station ids by purrr::map 
# to get NOAA station meta data using rnoaa::ncdc_stationsn
# the output is a list of 7 x 2 x 9.n
# flatten into a dataframe by purrr::flatten 
# reorder and rename columns by dplyr 
# save the station df by rion
sta_input <- data.frame(name = c('RAPID CITY RGNL AP',  
                                 'HOT SPRINGS',  
                                'OELRICHS',  
                                'INTERIOR 3 NE',  
                                'COTTONWOOD 2 E', 
                                'LONG VALLEY',
                                'HARRISON',
                                'AINSWORTH, NE',
                                'MURDO, SD US',
                                'MISSION 14 S, SD US',
                                'ORAL, SD US',
                                'KADOKA 0.3 N, SD US'),
                        id = c("GHCND:USW00024090", 
                               "GHCND:US1SDFR0001",
                               "GHCND:USC00396212",
                               "GHCND:USC00394184",
                               "GHCND:USC00391972",
                               "GHCND:USC00394983",
                               "GHCND:USC00253615",
                               "GHCND:USC00250050",
                               "GHCND:USC00395891",
                               "GHCND:USC00395638",
                               "GHCND:USC00396304",
                               "GHCND:US1SDJK0006"),
                        stringsAsFactors = FALSE)

station <- map(sta_input$id, ncdc_stations)
station <- flatten_dfr(station)

station <- station %>%
  rename(lat = latitude) %>%
  rename(lon = longitude) %>%
  select(id, name, lat, lon, everything())

# export(station, "data/sta_meta_orig.csv")  

# id                name                              
# <chr>             <chr>                             
# GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US        
# GHCND:USC00396212 OELRICHS, SD US                   
# GHCND:USC00394184 INTERIOR 3 NE, SD US              
# GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# GHCND:USC00394983 LONG VALLEY, SD US                
# GHCND:USC00253615 HARRISON, NE US                   
# GHCND:USC00250050 AINSWORTH, NE US                  
# GHCND:USC00395891 MURDO, SD US                      
# GHCND:USC00395638 MISSION 14 S, SD US               
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US           
# GHCND:USC00396304 ORAL, SD US                       
# GHCND:US1SDJK0006 KADOKA 0.3 N, SD US    
```

```{r import_site-data, eval=FALSE}
# site
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
site <- import("data/Chemistry-1993-2013_17Mar21.csv")
eco  <- import("data/MacroSummaries.csv")

eco <- eco %>%
  clean_names() %>%
  select(1:2) %>%
  rename(id = station) %>%
  distinct(id, .keep_all = TRUE)
  
site <- site %>%
  clean_names() %>% 
  arrange(sample_sites) %>%
  filter(sample_sites != "Bear in the Lodge USGS1") %>% 
  filter(sample_sites != "Bear in the Lodge USGS2") %>%
  filter(sample_sites != "Bear in the Lodge USGS3") %>%
  filter(sample_sites != "Black Pipe II") %>% 
  filter(sample_sites != "Corn Creek I") %>%
  filter(sample_sites != "Little Corn Creek I") %>% 
  filter(sample_sites != "Medicine Root II") %>% 
  filter(sample_sites != "Porcupine Lagoon downstream") %>%
  filter(sample_sites != "Porcupine Lagoon upstream") %>%
  filter(sample_sites != "Pine Ridge Lift Station Downstream") %>%
  distinct(sample_sites, .keep_all = TRUE) %>%
  select(1:4) %>%
  select(-1) %>%
  rename(name = sample_sites) %>%
  rename(lat = latitude) %>%
  rename(lon = longitude) %>%
  mutate(lon = -1 * lon)

site_id <- data.frame(id = c("AMH1", "BEA1", "BEA2", "BEA3", "BEL1", 
                              "BEL2", "BLP1", "BUZ1", "CHR1", "CHR2",
                              "CRA1", "EAN1", "EAN2", "LWR1", "LWR2", 
                              "LWR3", "LWR4", "LON1", "LOD1", "MER1", 
                              "MER2", "MER3", "MER4", "NFL1", "PAS1", 
                              "PAS2", "PAS3", "POR1", "POR2", "POR3", 
                              "POT1", "RED1", "WCC1", "WCC2", "WCC3", 
                              "WHR1", "WHR2", "WHR3", "WHR4", "WHR5", 
                              "WOL1", "WOK1", "WOK2", "WOK3", "WOK4"),
                       name = c("American Horse I", 
                               "Bear Creek I", 
                               "Bear Creek II", 
                               "Bear Creek III", 
                               "Bear in the Lodge I", 
                               "Bear in the Lodge II", 
                               "Black Pipe I", 
                               "Buzzard Creek I", 
                               "Cheyenne River I", 
                               "Cheyenne River II", 
                               "Craven Creek I", 
                               "Eagle Nest I", 
                               "Eagle Nest II", 
                               "Little White River I",  
                               "Little White River II", 
                               "Little White River III", 
                               "Little White River IV", 
                               "Long Creek I", 
                               "Lost Dog Creek I", 
                               "Medicine Root I", 
                               "Medicine Root II", 
                               "Medicine Root III", 
                               "Medicine Root IV", 
                               "No Flesh Creek I", 
                               "Pass Creek I", 
                               "Pass Creek II", 
                               "Pass Creek III", 
                               "Porcupine Creek I", 
                               "Porcupine Creek II", 
                               "Porcupine Creek III", 
                               "Potato Creek", 
                               "Red Water Creek", 
                               "White Clay Creek I", 
                               "White Clay Creek II", 
                               "White Clay Creek III", 
                               "White River I", 
                               "White River II", 
                               "White River III", 
                               "White River IV", 
                               "White River V", 
                               "Wolf Creek I", 
                               "Wounded Knee I", 
                               "Wounded Knee II", 
                               "Wounded Knee III", 
                               "Wounded Knee IV"),
                      stringsAsFactors = FALSE)

site <- full_join(site, site_id, by = "name")  

site <- site %>%  
  drop_na()  

site <- full_join(site, eco, by="id")  

site <- site %>%  
  replace_na(list(ecoregion = "Tablelands")) %>%  
  filter(id != "CHR1") %>%  
  filter(id != "CHR2") %>%  
  mutate(ecoregion = case_when(  
    id == "WHR1" ~ "Tablelands",  
    id == "WHR2" ~ "Badlands",  
    id == "WHR3" ~ "Badlands",  
    id == "WHR4" ~ "Badlands",  
    id == "WHR5" ~ "Badlands",  
    TRUE ~ as.character(ecoregion)))  
  
rm(site_id, eco)  
# export(site, "data/site_meta.csv")  
```  

```{r download-precip, eval=FALSE}
# DL Global Historical Climatology Network (GHCN) Daily Data   
# Note: to flatten a list use: Reduce(rbind, list)  

# load metadata 
sta_meta <- import("data/sta_meta_orig.csv") 

# make a table for manual import
# sta <- as.tibble(select(station_meta, 1:2))

# id                name                              
# <chr>             <chr>                             
# GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US        
# GHCND:USC00396212 OELRICHS, SD US                   
# GHCND:USC00394184 INTERIOR 3 NE, SD US              
# GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# GHCND:USC00394983 LONG VALLEY, SD US                
# GHCND:USC00253615 HARRISON, NE US                   
# GHCND:USC00250050 AINSWORTH, NE US                  
# GHCND:USC00395891 MURDO, SD US                      
# GHCND:USC00395638 MISSION 14 S, SD US               
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US           
# GHCND:USC00396304 ORAL, SD US                       
# GHCND:US1SDJK0006 KADOKA 0.3 N, SD US    

# download data from NOAA
sta_rap <- meteo_tidy_ghcnd('USW00024090', keep_flags = FALSE, 
                         var = "PRCP")
sta_hot <- meteo_tidy_ghcnd('US1SDFR0001', keep_flags = FALSE, 
                         var = "PRCP")
sta_oel <- meteo_tidy_ghcnd('USC00396212', keep_flags = FALSE, 
                         var = "PRCP")
sta_int <- meteo_tidy_ghcnd('USC00394184', keep_flags = FALSE, 
                         var = "PRCP")
sta_cot <- meteo_tidy_ghcnd('USC00391972', keep_flags = FALSE, 
                         var = "PRCP")
sta_lon <- meteo_tidy_ghcnd('USC00394983', keep_flags = FALSE, 
                         var = "PRCP")
sta_har <- meteo_tidy_ghcnd('USC00253615', keep_flags = FALSE, 
                         var = "PRCP")
sta_ain <- meteo_tidy_ghcnd('USC00250050', keep_flags = FALSE, 
                         var = "PRCP")
sta_mur <- meteo_tidy_ghcnd('USC00395891', keep_flags = FALSE, 
                         var = "PRCP")
sta_mis <- meteo_tidy_ghcnd('USC00395638', keep_flags = FALSE, 
                         var = "PRCP")
sta_her <- meteo_tidy_ghcnd('US1SDCS0027', keep_flags = FALSE, 
                         var = "PRCP")
sta_ora <- meteo_tidy_ghcnd('USC00396304', keep_flags = FALSE, 
                         var = "PRCP")
sta_kad <- meteo_tidy_ghcnd('US1SDJK0006', keep_flags = FALSE, 
                         var = "PRCP")

# save the data
# export(sta_rap, file = "data/sta_rap.csv")
# export(sta_hot, file = "data/sta_hot.csv")
# export(sta_oel, file = "data/sta_oel.csv")
# export(sta_int, file = "data/sta_int.csv")
# export(sta_cot, file = "data/sta_cot.csv")
# export(sta_lon, file = "data/sta_lon.csv")
# export(sta_har, file = "data/sta_har.csv")
# export(sta_ain, file = "data/sta_ain.csv")
# export(sta_mur, file = "data/sta_mur.csv")
# export(sta_mis, file = "data/sta_mis.csv")
# export(sta_her, file = "data/sta_her.csv")
# export(sta_ora, file = "data/sta_ora.csv")
# export(sta_kad, file = "data/sta_kad.csv")
```  

```{r merge-precip-data, eval=FALSE}

# load metadata 
sta_meta <- import("data/sta_meta_fin.csv") 
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv")) 

# these were downloaded from NOAA on 2018-06-01 

# stations dropped in the subsequent analysis
# sta_her <- import(file = "data/sta_her.csv") # n = 304 obs so drop
# sta_hot <- import(file = "data/sta_hot.csv") # dropped by Theissen
# sta_lon <- import(file = "data/sta_lon.csv") # dropped by Theissen
# sta_har <- import(file = "data/sta_har.csv") # dropped by Theissen
# sta_ain <- import(file = "data/sta_ain.csv") # dropped by Theissen
# sta_kad <- import(file = "data/sta_kad.csv") # dropped by Theissen

# import prior saved files
sta_oel <- import(file = "data/sta_oel.csv") # 
sta_cot <- import(file = "data/sta_cot.csv") # 
sta_rap <- import(file = "data/sta_rap.csv") # needs fixed & saved 
sta_int <- import(file = "data/sta_int.csv") # 
sta_mis <- import(file = "data/sta_mis.csv") # 
sta_ora <- import(file = "data/sta_ora.csv") # 
sta_mur <- import(file = "data/sta_mur.csv") # need for NA vals 

# join tables and rename prcp cols - data is in mm 
# going from oldest to youngest; 'date' is chr 
merge_sta1 <- full_join(sta_oel, sta_cot, by = "date") 
merge_sta1 <- merge_sta1 %>% 
  rename(oel = prcp.x) %>% 
  rename(cot = prcp.y) %>% 
  select(-id.x, -id.y) 
rm(sta_oel, sta_cot) 

merge_sta2 <- full_join(merge_sta1, sta_rap, by = "date") 
merge_sta2 <- merge_sta2 %>% 
  rename(rap = prcp) %>% 
  select(-id) 
rm(sta_rap) 

merge_sta3 <- full_join(merge_sta2, sta_int, by = "date") 
merge_sta3 <- merge_sta3 %>% 
  rename(int = prcp) %>% 
  select(-id) 
rm(sta_int) 

merge_sta4 <- full_join(merge_sta3, sta_mis, by = "date") 
merge_sta4 <- merge_sta4 %>% 
  rename(mis = prcp) %>% 
  select(-id) 
rm(sta_mis) 

merge_sta5 <- full_join(merge_sta4, sta_ora, by = "date") 
merge_sta5 <- merge_sta5 %>% 
  rename(ora = prcp) %>% 
  select(-id) 
rm(sta_ora) 

# final join & fix date & add year and month 
sta <- full_join(merge_sta5, sta_mur, by = "date") 
sta <- sta %>% 
  select(-starts_with("id")) %>% 
  rename(mur = prcp)  %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

rm(sta_mur, merge_sta1, merge_sta2, merge_sta3, merge_sta4,
   merge_sta5) 

# export(sta, file = "data/stations.csv") 
```  

```{r munge-precip-data-oral, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Oral  

# Variable naming convention:   
# sta          precipitation station  
# _har         Harrision precip station - used to fill NA vals  
# _meta        metadata  
# _raw         the "mostly" raw dataset  
# _geXX        data greater than or equal to year XX  
# _geXXmYY     data greater than or equal to year XX and month YY   
# _ltXXmYY     data less than year XX and month YY   
# _clean       intermediate df - clean part of NA split ;-}  
# _dirty       intermediate df - NA part of NA split ;-}  
# _transZ       final df - after cleaning  

# NAs are caused in part by different start dates
# OELRICHS    1893 - 2018  -fixed with Murdo
# COTTONWOOD  1909 - 2018  -fixed with Interior & Murdo
# RAPID CITY  1948 - 2018  -fixed with Interior  
# INTERIOR    1949 - 2018  -fixed with Cottonwood & Murdo    
# ORAL        1971 - 2018  -fixed with Oelrichs & Harrison
# LONG VALLEY 1927 - 2012  - Removed from further analysis
# MISSION     1951 - 2018  - Removed from further analysis 

# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv"))
sta_raw   <- import("data/stations.csv") # N = 45,784
sta_har   <- as.tibble(import("data/sta_har.csv"))
  sta_har <- sta_har %>%
    mutate(date = ymd(date)) # harrison used to fill NA

# fix date & add year and month
sta_raw <- sta_raw %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())


# remove NA in 2018 data - old code not listed in naming convention
# chopping off everything after 2018-05-31; N= 45,684
sta_not_2018    <- sta_raw %>% filter(year != 2018)
  sta_2018_filt <- sta_raw %>% filter(year == 2018 & month != 6)
  sta_raw       <- bind_rows(sta_not_2018, sta_2018_filt) %>%
    arrange(desc(date)) 
rm(sta_2018_filt, sta_not_2018)

# check NA in original
intro_raw <- as.tibble(introduce(sta_raw)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_raw

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840


# NAs are caused in part by different start dates
# OELRICHS    1893 - 2018    
# COTTONWOOD  1909 - 2018  
# RAPID CITY  1948 - 2018    
# INTERIOR    1949 - 2018       
# MISSION     1951 - 2018       
# ORAL        1971 - 2018 - in.progress


# Clean Oral & part of Oelrichs 1971-05-01 to present
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1971-05-01
  sta_ge72      <- sta_raw %>% filter(year >= 1972) #
  sta_71m05     <- sta_raw %>% filter(year == 1971 & month >= 5)
sta_ge71m05     <- bind_rows(sta_ge72, sta_71m05) # this is active
  rm(sta_ge72, sta_71m05)

  sta_le71      <- sta_raw %>% filter(year < 1971)
  sta_71m01_m05 <- sta_raw %>% filter(year == 1971 & month < 5)
sta_lt71m05     <- bind_rows(sta_le71, sta_71m01_m05) # this is not
  rm(sta_le71, sta_71m01_m05)

#   2. filter NA vals from Oral
#     Before = 368 NA <- filling with Oelrichs
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(ora)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(ora)) %>%
  mutate(ora = oel) # fix oral with oelrichs

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
  filter(is.na(ora)) # check is :-] NA is 1 obs

#   3. filter NA vals from Oelrichs (oel)
#     Before = 368 NA <- filling with Oral
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = ora) # fix oelrichs with oral

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] down to 1 NA
rm(sta_NA)

#   4. fix Oelrichs & Oral NA with Harrision
#     Before: 1 concurrent NA values 

sta_ge71m05 <- left_join(sta_ge71m05, sta_har, by = "date")
  rm(sta_har)

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = prcp) %>%
  mutate(ora = prcp)

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] zero NA vals
rm(sta_NA) 
  
#   4. Put the pieces back together
sta_ge71m05 <- sta_ge71m05 %>%
  select(-prcp, -id) 
sta_trans <- bind_rows(sta_ge71m05, sta_lt71m05)
rm(sta_ge71m05,sta_lt71m05)

# Check work
intro_raw

#A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# export the work to a file
# export(sta_trans, file = "data/stations_trans1.csv")  
```

```{r munge-precip-data-mission, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Mission 
# Variable naming convention - see munge-precip-data-oral code chunk    
# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv"))
sta_trans <- import("data/stations_trans1.csv")
sta_lon   <- import("data/sta_lon.csv")

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

sta_lon <- sta_lon %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  select(date, everything())


# Clean Mission using Long Valley station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1951-08-01
  sta_ge52      <- sta_trans %>% filter(year >= 1952) 
  sta_51m08     <- sta_trans %>% filter(year == 1951 & month >= 8)
sta_ge51m08     <- bind_rows(sta_ge52, sta_51m08) # this is active
  rm(sta_ge52, sta_51m08)

  sta_le51      <- sta_trans %>% filter(year < 1951)
  sta_51m01_m08 <- sta_trans %>% filter(year == 1951 & month < 8)
sta_lt51m08     <- bind_rows(sta_le51, sta_51m01_m08) # this is not
  rm(sta_le51, sta_51m01_m08)

#   2. Attach Long Valley to df
sta_ge51m08 <- left_join(sta_ge51m08, sta_lon, by = "date")
  rm(sta_lon)
  
#   3. filter NA vals from Mission with Long Valley
#     Before = 858 NA <- filling with Long Valley
#     After: 14 NA values 
sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = prcp) # fix Mission with Long Valley

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge51m08 %>%
  filter(is.na(mis)) # check is :-] # down to 6 vals
rm(sta_NA)

#   4. Use the Interior vals to fill missing vals
#     Before = 6 NA <- filling with Interior
#     After: XX  NA values 

sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = int) # fix Mission with Interior

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge51m08 %>%
   filter(is.na(mis)) # check is :-] down to 2 NA vals
 rm(sta_NA)
 
#   5. Use the Oelrich vals to fill missing vals
#     Before = 2 NA <- filling with Interior
#     After: zero NA values 

sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = oel) # fix Mission with Oelrichs

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge51m08 %>%
   filter(is.na(mis)) # check is :-] down to 2 NA vals
 rm(sta_NA)
 
#   6. Put the pieces back together
sta_ge51m08 <- sta_ge51m08 %>%
  select(-prcp, -id) 
sta_trans2 <- bind_rows(sta_ge51m08, sta_lt51m08)
rm(sta_ge51m08,sta_lt51m08)

# Check work
# A tibble: 1 x 4 (RAW)
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans
# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

intro_trans2 <- as.tibble(introduce(sta_trans2)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans2
# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# export the work to a file
# export(sta_trans2, file = "data/stations_trans2.csv")  
```

```{r munge-precip-data-int, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Interior
# Variable naming convention - see munge-precip-data-oral code chunk  


# load metadata & data 
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_trans <- import("data/stations_trans2.csv") 
sta_mur   <- import("data/sta_mur.csv") 

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

sta_mur <- sta_mur %>% 
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Interior using Cottonwood station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1949-11-01
  sta_ge50      <- sta_trans %>% filter(year >= 1950) 
  sta_49m11     <- sta_trans %>% filter(year == 1949 & month >= 11)
sta_ge49m11     <- bind_rows(sta_ge50, sta_49m11) # this is active
  rm(sta_ge50, sta_49m11)

  sta_le50      <- sta_trans %>% filter(year < 1949)
  sta_49m01_m11 <- sta_trans %>% filter(year == 1949 & month < 11)
sta_lt49m11     <- bind_rows(sta_le50, sta_49m01_m11) # this is not
  rm(sta_le50, sta_49m01_m11)

#   2. filter NA vals from Interior with Cottonwood
#     Before = 2,578 NA <- filling with Cottonwood
#     After: 11 NA values 
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(int)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(int)) %>%
  mutate(int = cot) # fix Interior with Cottonwood

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge49m11 %>%
  filter(is.na(int)) # check is :-] # down to 11 vals
  rm(sta_NA)

#   3. Fill NA with Rapid
#     Before: 11 NA <- filling with Murdo
#     After:  zero NA values 
  
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(int)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(int)) %>%
  mutate(int = rap) # fix Interior with Interior

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge49m11 %>%
   filter(is.na(int)) # check is :-] down to zero vals
 rm(sta_NA)

#   4. filter NA vals from Cottonwood with Interior 
#     Before: 850 NA <- filling with Interior
#     After: 11 NA values 
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(cot)) %>%
  mutate(cot = int) # fix Cottonwood with Interior

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge49m11 %>%
  filter(is.na(cot)) # check is :-] # down to zero vals
  rm(sta_NA)
  
#   3. Put the pieces back together
sta_trans3 <- bind_rows(sta_ge49m11, sta_lt49m11)
rm(sta_ge49m11, sta_lt49m11)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

intro_trans3 <- as.tibble(introduce(sta_trans3)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans3

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# export the work to a file
# export(sta_trans3, file = "data/stations_trans3.csv")  
```

```{r munge-precip-data-rap, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data
# Specific purpose - clean Rapid City NA with Interior & Cottonwood
# Rationale - this is the closest to the project area;
#  remember the point estimates are for the project area.

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_trans <- import("data/stations_trans3.csv")

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Rapid City using Interior & Cottonwood station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1948-05-01
  sta_ge49      <- sta_trans %>% filter(year >= 1949) # yr above
  sta_48m05     <- sta_trans %>% filter(year == 1948 & month >= 5)
sta_ge48m05     <- bind_rows(sta_ge49, sta_48m05) # this is active
  rm(sta_ge49, sta_48m05)

  sta_lt48     <- sta_trans %>% filter(year < 1948)
  sta_48m01_m05 <- sta_trans %>% filter(year == 1948 & month < 5)
sta_lt09m06     <- bind_rows(sta_lt48, sta_48m01_m05) # this is not
  rm(sta_lt48, sta_48m01_m05)

#   2. filter NA vals from Rapid City and fill with Interior
#     Before: 6 NA <- filling with Interior
#     After: zero NA values 
sta_clean <- sta_ge48m05 %>%
  filter(!is.na(rap)) # this is not active

sta_dirty <- sta_ge48m05 %>%
  filter(is.na(rap)) %>%
  mutate(rap = int) # fix Rapid City with Interior

sta_ge48m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge48m05 %>%
  filter(is.na(rap)) # check is :-] # down to zero vals
  rm(sta_NA)

#   4. Put the pieces back together
#sta_ge48m05 <- sta_ge48m05 %>%
#  select(-prcp, -id) 
sta_trans4 <- bind_rows(sta_ge48m05, sta_lt09m06)
rm(sta_ge48m05, sta_lt48m05)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

intro_trans4 <- as.tibble(introduce(sta_trans4)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans4

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# export the work to a file
# export(sta_trans4, file = "data/stations_trans4.csv")  
```

```{r munge-precip-data-cot, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Cottonwood with Rapid City
# Rationale - Refactored - RC has similarity to Cottonwood

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta      <- as.tibble(import("data/sta_meta_fin.csv"))
sta_trans     <- import("data/stations_trans4.csv")
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv"))

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Cottonwood precip NA using Murdo station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1909-06-01
  sta_ge09      <- sta_trans %>% filter(year >= 1910) # yr above
  sta_09m06     <- sta_trans %>% filter(year == 1909 & month >= 6)
sta_ge09m06     <- bind_rows( sta_ge09, sta_09m06) # this is active
  rm(sta_ge09, sta_09m06)

  sta_lt09     <- sta_trans %>% filter(year < 1909)
  sta_09m01_m06 <- sta_trans %>% filter(year == 1909 & month < 6)
sta_lt09m06     <- bind_rows(sta_lt09, sta_09m01_m06) # this is not
  rm(sta_lt09, sta_09m01_m06)

#Check on split :-)
count_check <- bind_rows(sta_ge09m06, sta_lt09m06) 
rm(count_check)
  
#   2. filter NA vals from Cottonwood & fill with Murdo
#     Before: 96 NA <- filling with Murdo
#     After: 12 NA values 
sta_clean <- sta_ge09m06 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge09m06 %>%
  filter(is.na(cot)) %>%
  mutate(cot = mur) # fix Cottonwood with Murdo

sta_ge09m06 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge09m06 %>%
  filter(is.na(cot)) # check is :-] # down to 12 vals
  rm(sta_NA)

# ~~~~~~~~~~~~~~~~~~~~~~~~
#   3. filter NA vals from Cottonwood & fill with Oelrichs
#     Before: 12 NA <- filling with Murdo
#     After: zero NA values 
sta_clean <- sta_ge09m06 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge09m06 %>%
  filter(is.na(cot)) %>%
  mutate(cot = oel) # fix Cottonwood with Murdo

sta_ge09m06 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge09m06 %>%
  filter(is.na(cot)) # check is :-] # down to zero vals
  rm(sta_NA)
  
#   4. Put the pieces back together
sta_trans5 <- bind_rows(sta_ge09m06, sta_lt09m06)
rm(sta_ge09m06, sta_lt09m06)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

intro_trans5 <- as.tibble(introduce(sta_trans5)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans5

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

# export the work to a file
# export(sta_trans5, file = "data/stations_trans5.csv")  
```

```{r munge-precip-data-oel, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Oelrichs with Murdo &  
#   define oldest date with continuous data 

# Variable naming convention - see munge-precip-data-oral code chunk   

# load metadata & data 
sta_meta      <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv")) 
sta_trans     <- import("data/stations_trans5.csv") 

# fix date & add year and month 
sta_trans <- sta_trans %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# Clean Oelrichs precip NA using Murdo station data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Using nearest year as split-point  
#   In this case it is the filler, Murdo, rather than the recipient 
#   1. Split the raw data into two parts at 1907-12-01 
  sta_ge08      <- sta_trans %>% filter(year >= 1908) # yr above 
  sta_07m12     <- sta_trans %>% filter(year == 1907 & month >= 12) 
sta_ge07m12     <- bind_rows( sta_ge08, sta_07m12) # this is active 
  rm(sta_ge08, sta_07m12) 

  sta_lt07     <- sta_trans %>% filter(year < 1907) 
  sta_07m1_m12 <- sta_trans %>% filter(year == 1907 & month < 12) 
sta_lt07m12     <- bind_rows(sta_lt07, sta_07m1_m12) # this is not 
  rm(sta_lt07, sta_07m1_m12) 

#Check on split :-) 
count_check <- bind_rows(sta_ge07m12, sta_lt07m12)  
rm(count_check) 
  
#   2. filter NA vals from Oelrichs & fill with Murdo 
#     Before: 1478 NA <- filling with Murdo 
#     After: 186 NA values  
sta_clean <- sta_ge07m12 %>% 
  filter(!is.na(oel)) # this is not active 

sta_dirty <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  mutate(oel = mur) # oelrichs with Murdo 

sta_ge07m12 <- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

  #Check on split :-) 
count_check <- bind_rows(sta_ge07m12, sta_lt07m12)  
rm(count_check) 
  
sta_NA <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  arrange() # check is :-] # down to 186 vals; fill with Cottonwood 
rm(sta_NA) 

#   3. filter NA vals from Oelrichs & fill with Cottonwood 
#     Before: 186 NA <- filling with Cottonwood 
#     After: 22 NA values  
sta_clean <- sta_ge07m12 %>% 
  filter(!is.na(oel)) # this is not active 

sta_dirty <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  mutate(oel = cot) # oelrichs with Cottonwood 

sta_ge07m12 <- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

sta_NA <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  arrange() # check is :-] # down to 22 vals; 
# Oldest is 1909-05-28, so this is the end of the record 
# The end of the complete record is 1909-06-01 
  
#   4. Put the pieces back together & cut to end of complete record 
sta_trans6 <- bind_rows(sta_ge07m12, sta_lt07m12) 
rm(sta_ge07m12, sta_lt07m12, sta_NA) 

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

intro_trans6 <- as.tibble(introduce(sta_trans6)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans6

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               104652             456840
   
# export the work to a file
# export(sta_trans6, file = "data/stations_trans6.csv") 
```

```{r munge-precip-data-lon, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - Append Long Valley with Mission  
# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data 
# Note: the metadata was changed in this step.  A once only change. 
#sta_meta      <- as.tibble(import("data/sta_meta_fin.csv")) 
#sta_meta_orig <- as.tibble(import("data/Archived/station_meta2.csv") 
sta_trans     <- as.tibble(import("data/stations_trans6.csv")) 
sta_lon       <- as.tibble(import("data/sta_lon.csv")) 

# add Long Valley & remove Mission metadata 
#sta_meta_lon <- sta_meta_orig %>% 
#  filter(name == "LONG VALLEY, SD US") 
#sta_meta2    <- bind_rows(sta_meta, sta_meta_lon)  
#sta_meta2    <- sta_meta2 %>%  
#  filter(name != "MISSION 14 S, SD US") 
#export(sta_meta2, file = "data/sta_meta_fin2.csv") 
sta_meta      <- as.tibble(import("data/sta_meta_fin2.csv")) 


# fix date, add year and month, and join Long Valley 
sta_trans <- sta_trans %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

sta_lon <- sta_lon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, everything()) 

sta_trans <- left_join(sta_trans, sta_lon) 
rm(sta_lon) 

sta_trans <- sta_trans %>% 
  select(-id) %>% 
  rename(lon = prcp) 

# Clean Long Valley precip NA using Mission station data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
# Using nearest year as split-point 
#   1. Split the raw data into two parts at 1927-07-01 
  sta_ge28      <- sta_trans %>% filter(year >= 1928) # yr above 
  sta_27m07    <- sta_trans %>% filter(year == 1927 & month >= 7) 
sta_ge27m07    <- bind_rows( sta_ge28, sta_27m07) # this is active 
  rm(sta_ge28, sta_27m07) 

  sta_lt27     <- sta_trans %>% filter(year < 1927) 
  sta_27m1_m7 <- sta_trans %>% filter(year == 1927 & month < 7) 
sta_lt27m07     <- bind_rows(sta_lt27, sta_27m1_m7) # this is not 
  rm(sta_lt27, sta_27m1_m7) 

#Check on split :-) 
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
#   2. filter NA vals from Long Valley & fill with Mission 
#     Before: 2904 NA <- filling with Mission 
#     After: 423 NA values  
sta_clean <- sta_ge27m07%>% 
  filter(!is.na(lon)) # this is not active 

sta_dirty <- sta_ge27m07%>% 
  filter(is.na(lon)) %>% 
  mutate(lon = mis) # Long Valley with Mission 

sta_ge27m07<- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

# Check on split :-) 
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
sta_NA <- sta_ge27m07 %>% 
  filter(is.na(lon)) %>% 
  arrange() # check is :-] # down to 423 vals; fill with Cottonwood 
rm(sta_NA) 

#   3. filter NA vals from Long Valley & fill with Cottonwood 
#     Before: 423 NA <- filling with Mission 
#     After: zero NA values  
sta_clean <- sta_ge27m07%>% 
  filter(!is.na(lon)) # this is not active 

sta_dirty <- sta_ge27m07%>% 
  filter(is.na(lon)) %>% 
  mutate(lon = cot) # Long Valley with Mission 

sta_ge27m07<- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

  #Check on split :-)
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
sta_NA <- sta_ge27m07 %>% 
  filter(is.na(lon)) %>% 
  arrange() # check is :-] # down to zero vals 

#   4. Put the pieces back together & cut to end of complete record 
sta_trans7 <- bind_rows(sta_ge27m07, sta_lt27m07) 
rm(sta_ge27m07, sta_lt27m07, sta_NA) 

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

# A tibble: 1 x 4 - Trans6
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               104652             456840

intro_trans7 <- as.tibble(introduce(sta_trans7)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans7
   
# A tibble: 1 x 4 - Trans7 - note the extra column
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      11               117128             502524

# export the work to a file
# export(sta_trans7, file = "data/stations_trans7.csv") 

# This is to cut back to complete part of Oelrichs
  sta_ge10      <- sta_trans7 %>% filter(year >= 1910) # yr above 
  sta_09m06     <- sta_trans7 %>% filter(year == 1909 & month >= 6) 
sta_fin     <- bind_rows( sta_ge10, sta_09m06) # this is active 
  rm(sta_ge10, sta_09m06) 

intro_sta_fin <- as.tibble(introduce(sta_fin)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_sta_fin

# A tibble: 1 x 4 - Final
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 39812      11                76046             437932

# export(sta_fin, file = "data/stations_final.csv") 
```

```{r eval-precip-data-lon, message=FALSE}
# Deciding whether to keep Long Valley
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# General Purpose: fill NA prior to upscaling to monthly data
# Specific purpose - Check Mission & Long Valley covariance
# Variable naming convention - see munge-precip-data-oral code chunk   

# load metadata & data 
sta_meta      <- as.tibble(import("data/sta_meta_fin2.csv")) 
sta_fin     <- as.tibble(import("data/stations_final.csv")) 

# Split data to the end of Mission - the purpose here is  
#   to look at a double mass plot with Mission & Long Valley

sta_test <- sta_fin %>%
  filter(year > 1951) %>%
  filter(year < 2012) 

# gather values & create groups
sta_gath <- gather(sta_test, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

sta_group <- sta_gath %>%
  group_by(year, month, station)

# sum daily precip over a month
sta_gath_mon <- sta_group %>%
  summarize(prcp_tenths = sum(prcp)) %>%
  mutate(prcp_mm = prcp_tenths/10) %>%
  select(-prcp_tenths)

# spread result - now in months 
sta_mon <- sta_gath_mon %>%
  spread(station, prcp_mm) %>%
  mutate(day = 1) %>%
  mutate(date = make_date(year = year, month = month, day = day)) %>%
  select(date, year, month, everything()) %>%
  select(-day) %>%
  ungroup()
rm(sta_gath, sta_group, sta_gath_mon)

# filter Mission, Long Valley, Interior
sta_gath2 <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, -lon, factor_key = TRUE) %>%
  filter(station == "int" | 
           station == "mis")

# plot the graphs of Interior and Mission
# see which precip is more similiar.

ggplot(sta_gath2, aes(prcp, lon)) +
  geom_point() +
  geom_smooth(method = "lm", aes(color = "red")) +
  facet_grid(.~station) +
   scale_x_sqrt() +
  scale_y_sqrt() +
  geom_smooth() + 
  ggtitle("Long Valley similarity to nearest stations")
```  

```{r check_precip_data2}

sta_dv <- import(file = "data/sta_dv.csv") 
sta_meta_orig <- import(file = "data/sta_meta_orig.csv") 

# fix missing data -----------------------------------------------------------  

# appended station data to fix missing years 
# Region   Station       Add-Station   Year 
#  NW    RC Regional         NA         NA
#  NC    Cottonwood      Interior 3    1996   
#  NE      Onida          Kennebec     1995 
#  SW      Oral           Oelrichs     2005 
#  SC     Gordon         Valentine     2012 
#  SE     Mission           NA          NA  


# calculate percentages  
length <- sta_dv %>%    
  group_by(name) %>%  
  summarize(length = n()) 
  
sta_check_sum <- sta_check %>% 
  group_by(name, mindate, maxdate) %>%  
  summarize(count = n())  


# Clean Oral & part of Oelrichs 1971-05-01 to present
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1971-05-01
  sta_ge72      <- sta_raw %>% filter(year >= 1972) #
  sta_71m05     <- sta_raw %>% filter(year == 1971 & month >= 5)
sta_ge71m05     <- bind_rows(sta_ge72, sta_71m05) # this is active
  rm(sta_ge72, sta_71m05)

  sta_le71      <- sta_raw %>% filter(year < 1971)
  sta_71m01_m05 <- sta_raw %>% filter(year == 1971 & month < 5)
sta_lt71m05     <- bind_rows(sta_le71, sta_71m01_m05) # this is not
  rm(sta_le71, sta_71m01_m05)

#   2. filter NA vals from Oral
#     Before = 368 NA <- filling with Oelrichs
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(ora)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(ora)) %>%
  mutate(ora = oel) # fix oral with oelrichs

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
  filter(is.na(ora)) # check is :-] NA is 1 obs

#   3. filter NA vals from Oelrichs (oel)
#     Before = 368 NA <- filling with Oral
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = ora) # fix oelrichs with oral

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] down to 1 NA
rm(sta_NA)

#   4. fix Oelrichs & Oral NA with Harrision
#     Before: 1 concurrent NA values 

sta_ge71m05 <- left_join(sta_ge71m05, sta_har, by = "date")
  rm(sta_har)

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = prcp) %>%
  mutate(ora = prcp)

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] zero NA vals
rm(sta_NA) 
  
#   4. Put the pieces back together
sta_ge71m05 <- sta_ge71m05 %>%
  select(-prcp, -id) 
sta_trans <- bind_rows(sta_ge71m05, sta_lt71m05)
rm(sta_ge71m05,sta_lt71m05)

# Check work
intro_raw

#A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# export the work to a file
# export(sta_trans, file = "data/stations_trans1.csv")  
```  

```{r modeling-results-1}
# continued from above 
lm_both <- lm(data = sta_mon, sqrt(lon) ~ sqrt(mis) + sqrt(int)) 
lm_both_tidy <- tidy(lm_both)
lm_both_glance <- glance(lm_both)

lm_int <- lm(data = sta_mon, sqrt(lon) ~ sqrt(int))
lm_int_tidy <- tidy(lm_int)
lm_int_glance <- glance(lm_int)

lm_mis <- lm(data = sta_mon, sqrt(lon) ~ sqrt(mis))
lm_mis_tidy <- tidy(lm_mis)
lm_mis_glance <- glance(lm_mis)

# combine models
lm_glance <- bind_rows(lm_both_glance, lm_int_glance)
lm_glance <- bind_rows(lm_glance, lm_mis_glance)

lm_glance <- as.tibble(lm_glance) %>%
  mutate(name = c("both", "int", "mis")) %>%
  select(name, everything())
lm_glance
```

```{r modeling-results-2}
# continued from above 
# result: drop the long valley data and use Interior for southeast 
# remove not-needed stations from metadata 
sta_meta <- sta_meta %>% 
  filter(name != "LONG VALLEY, SD US") 

# export(sta_meta, file = "data/sta_meta_fin3.csv")
```  

```{r voroni-diagram-inter}
# import site location data and filter out:
#   hermosa, ainsworth, harrison, hot springs

sta_meta <- import("data/sta_meta_orig.csv")
sta_meta <- sta_meta %>%
  filter(id != "GHCND:US1SDCS0027") %>% # very short length - HER
  filter(id != "GHCND:USC00250050") %>% # outside region - AIN
  filter(id != "GHCND:USC00253615") %>% # outside region - HAR
  filter(id != "GHCND:US1SDFR0001") %>% # close to oral - HOT
  filter(id != "GHCND:USC00395891") %>% # outside range - MUR
  filter(id != "GHCND:USC00394983") %>% # love - but stopped 2012 - LON
  filter(id != "GHCND:US1SDJK0006") 

# import gage location data
gage_meta <- import("data/gage_meta.csv")
  
# define the study area using data from the 'maps' package
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# import polygon data - counties
counties <- map_data("county") 
counties <- subset(counties, region %in% 
   c("south dakota", "nebraska"))
prr <- subset(counties, subregion %in% 
   c("shannon", "jackson", "bennett"))

# create voroni line segments
voronoi <- deldir(sta_meta$lon, sta_meta$lat)

#Plot the points, voronoi lines, and annotate
p_site <- ggplot(data = sta_meta, aes(x = lon, y = lat)) +
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333") +   
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 2,
    data = voronoi$dirsgs,
    linetype = 1,
    color = "#FFB958") +
  geom_polygon(data = prr, aes(x = long, y = lat, group = group), 
              color = "black", linetype = "dashed", fill = "NA") +
  theme_bw() +
  geom_text(data = sta_meta, aes(label = name), size = 2.5) +
  ggtitle("Intermediate Theissen Polygon Map")
  
p_site +
  geom_point(data = gage_meta, aes(x = dec_long_va, 
                                   y = dec_lat_va),
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    fill = rgb(70, 130, 180, 255, maxColorValue = 255)) 
  

#sta_meta <- as.tibble(sta_meta)
# final list of stations
# id                name                              
#  <chr>             <chr>                             
# 1 GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# 2 GHCND:USC00396212 OELRICHS, SD US                   
# 3 GHCND:USC00394184 INTERIOR 3 NE, SD US              
# 4 GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# 5 GHCND:USC00395638 MISSION 14 S, SD US               
# 6 GHCND:USC00396304 ORAL, SD US   

ggplot2::ggsave(path = "figure/", filename = "theissen_inter.png", 
                width = 6, height = 6, units = "in")
# export(sta_meta, file = "index/data/sta_meta_fin.csv")

### Note: Mission is kept on the map to extend the INT - COT line 
```

```{r voroni-diagram-final, eval=FALSE}
# import site location data and filter out:
#   hermosa, ainsworth, harrison, hot springs
sta_meta <- import("data/sta_meta_fin3.csv")

# import gage location data
gage_meta <- import("data/gage_meta.csv")
  
# define the study area using data from the 'maps' package
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# import polygon data - counties
counties <- map_data("county") 
counties <- subset(counties, region %in% 
   c("south dakota", "nebraska"))
prr <- subset(counties, subregion %in% 
   c("shannon", "jackson", "bennett"))

# create voroni line segments
voronoi <- deldir(sta_meta$lon, sta_meta$lat)

#Plot the points, voronoi lines, and annotate
p_site <- ggplot(data = sta_meta, aes(x = lon, y = lat)) +
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333") +   
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 2,
    data = voronoi$dirsgs,
    linetype = 1,
    color = "#FFB958") +
  geom_polygon(data = prr, aes(x = long, y = lat, group = group), 
              color = "black", linetype = "dashed", fill = "NA") +
  theme_bw() +
  geom_text(data = sta_meta, aes(label = name), size = 2.5) +
  ggtitle("Final Theissen Polygon Map")

# this adds all of the weather stations as points
#p_site +
#  geom_point(data = gage_meta, aes(x = dec_long_va, 
#                                   y = dec_lat_va),
#    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
#    fill = rgb(70, 130, 180, 255, maxColorValue = 255)) 
  

#sta_meta <- as.tibble(sta_meta)
# final list of stations
# id                name                              
#  <chr>             <chr>                             
# 1 GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# 2 GHCND:USC00396212 OELRICHS, SD US                   
# 3 GHCND:USC00394184 INTERIOR 3 NE, SD US              
# 4 GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# 5 GHCND:USC00395638 MISSION 14 S, SD US               
# 6 GHCND:USC00396304 ORAL, SD US   

ggplot2::ggsave(path = "figure/", filename = "theissen_fin.png", 
               width = 6, height = 6, units = "in")
# export(sta_meta, file = "index/data/sta_meta_fin.csv")
```  

```{r daily2monthly-precip}

# fix date & add year and month 
sta_day <- sta_day %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# gather daily values 
sta_gath <- gather(sta_day, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

# create groups 
sta_group <- sta_gath %>% 
  group_by(year, month, station) 

# sum daily precip over a month 
sta_gath_mon <- sta_group %>% 
  summarize(prcp_tenths = sum(prcp)) %>% 
  mutate(prcp_mm = prcp_tenths/10) %>% 
  select(-prcp_tenths) 

# spread result - now in months  
#   ...and take a bow, because this is MAGIC!  Thnx Tidyverse. 
sta_mon <- sta_gath_mon %>% 
  spread(station, prcp_mm) %>% 
  mutate(day = 1) %>% 
  mutate(date = make_date(year = year, month = month, day = day)) %>% 
  select(date, year, month, everything()) %>% 
  select(-day) %>% 
  ungroup()

rm(sta_day, sta_gath, sta_gath_mon, sta_group) 
# export(sta_mon, file = "data/stations_monthly.csv") 
```  

```{r eval-precip-data-ora}
# General Purpose: check a short-term record: Oral 

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))
sta_mon     <- as.tibble(import("data/stations_monthly.csv"))

# Split data to the end of Oral - the purpose here is 
#   to look at a double mass plot with Oelrichs

sta_test <- sta_mon %>%
  filter(year > 1971) 

# check a linear model
lm <- lm(data = sta_mon, sqrt(ora) ~ sqrt(oel))
lm.tidy <- tidy(lm)
lm.glance <- glance(lm)

# plot the graphs of Oral & Oelrichs

ggplot(sta_test, aes(ora, oel)) +
  geom_point() +
  geom_smooth(method = "lm", aes(color = "red")) +
   scale_x_sqrt() +
  scale_y_sqrt() +
  geom_smooth() + 
  theme_bw() +
  ggtitle("Double mass plots")
# it's ok, keep Oral...
```  

```{r monthly2yearly-precip}
# General Purpose: prepare data for drought index   
# Specific purpose: convert monthly precip to yearly prcp  
 
# load metadata & data 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything())  

# gather monthly values  
sta_gath <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE) 

# create groups 
sta_group <- sta_gath %>% 
  group_by(year,  station) 

# sum monthly precip over a year 
sta_gath_yr <- sta_group %>% 
  summarize(prcp = sum(prcp))  

# spread result - now in years 
sta_yr <- sta_gath_yr %>% 
  spread(station, prcp) %>% 
  filter(year != 1909) %>% 
  filter(year != 2018) %>% 
  ungroup()

rm(sta_mon, sta_gath, sta_gath_yr, sta_group)
# export(sta_yr, file = "data/stations_yearly.csv") 
```  

```{r summaries}

# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon   <- as.tibble(import("data/stations_monthly.csv")) 

# fix dates
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything()) 

# gather and summarize monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE)

sta_summary_mon <- as.tibble(sta_gath_mon) %>%
  group_by(station, month) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(month) %>%
  arrange(station)

#sta_summary_mon 
# export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
```  

```{r yearly_summaries}
# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr   <- as.tibble(import("data/stations_yearly.csv")) 

# gather and summarize yearly values 
# Next step - do by water year???
sta_gath_yr <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

sta_summary_yr <- as.tibble(sta_gath_yr) %>%
  group_by(station) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(desc(med))

#sta_summary_yr
# export(sta_summary_yr, file = "data/sta_summary_yr.csv") 
```  

```{r ggplot_yearly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA - yearly 

# NEED TO FIX - screwed up variables 

sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr     <- as.tibble(import("data/stations_yearly.csv"))

# gather monthly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp",  
                   -year, factor_key = TRUE) 

# plot
ggplot(sta_gath, aes(year, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Annual precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

ggplot2::ggsave(path = "figure/", filename = "precip_yr.png", 
                width = 6, height = 6, units = "in") 
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(month, prcp, group = month)) +
  geom_boxplot() +
  facet_wrap(~station) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

ggplot2::ggsave(path = "figure/", filename = "precip_boxpl_mon.png",
                width = 6, height = 6, units = "in")
```  



