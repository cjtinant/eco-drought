
<!--
Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0  Formulate your question  
Spend a few minutes to figure out the question you’re really interested in, and narrow it down to be as specific as possible (without becoming uninteresting).

General question:
Are air pollution levels higher on the east coast than on the west coast?
More specific question:
Are hourly ozone levels on average higher in New York City than they are in Los Angeles?

2.0   Read in your data  
Sometimes the data will need some cleaning and every dataset has its unique quirks. The dataset is a comma-separated value (CSV) file, where each row of the file contains one daily measurement of precipation depth.  The readr package can rewrite column names to remove spaces.
> names(ozone) <- make.names(names(ozone))

3.0  Check the dataset 
3.1  Check the number of rows and columns.
3.2  Check the types of data
3.3  Look at the top and the bottom of your data 
3.4  Check your “n”s & NAs 
3.5  Validate with at least one external data source  
4.0  Try the easy solution first to answer question
5.0  Challenge your solution 
6.0  Follow up questions 

## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?
What is streamflow variation across the Pine Ridge Reservation?

## Narrower questions:

What is streamflow variation across the Pine Ridge Reservation?

## Analysis Steps:
1.  Data read in from USGS website by EGRET (cfs) 
2.  Added names & station numbers & joined data 
3.  Data saved in a flat format (.csv) 


Didn't work these
1.  Data read in from USGS website by dataRetrieval (cfs)
2.  Data saved as array (.JSON), and flat format (.csv) 


# Next STEPS
1. blc_wan <- readNWISDaily("06446700") # this not working!
2. Identify percent coverage

2. Check on next steps from Chapter 2 list 


# Someday Maybe
# USGS 06447050 UNNAMED TRIB BUZZARD CREEK NR LONG VALLEY, SD -instant meas
https://nwis.waterdata.usgs.gov/usa/nwis/qwdata/?huc_cd=10140202&format=station_list&sort_key=site_no&index_pmcode_00065=3&index_pmcode_00060=4&index_pmcode_00062=5&index_pmcode_72020=6&sort_key=site_no&group_key=county_cd&sitefile_output_format=station_list

## Variable naming convention:   
  for USGS functions: EGRET & dataRetrieval:
startDate    Beginning date for downloading USGS gage data
endDate      End date for downloading USGS gage data
parameter_cd USGS parameter codes

xxx_yyy      individual gage names xxx = stream & yyy = location

gage         USGS streamgage station 
  _cont      set of active gages 
  _disc      set of discontinued gages   
  _othr      set of gages not found in initial analysis 

  _full      equals {_cont + _disc + _othr}; 741,215 obs for 61 sta 
  _drp#      observations sequentially dropped from _full
  _fin#      equals {_full} - {_drp#} 
  _int       intermediate dataframe for binding 
  
  _meta      metadata  
  _incomp    temporary variable to find station-years w/ incomp data 
  _mdp#      gages sequentially dropped from _meta
  _met#      equals {_meta} - {_mdp#} 
  _yr        temporary variable to calculate years of stations
  
check_sta    check variable


## Results: 


-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) 
options(tibble.print_max = 20) # sets tibble output for printing 
```

```{r library, message=FALSE} 
# Sets up the library of packages   
library("here") # identifies where to save work  
library("dataRetrieval") # USGS data import  
library("EGRET") # Exploration and Graphics for RivEr Trends 
library("rio") # more robust I/O - to import and clean data  
library("lubridate") # easier dates 
library("tidyverse") 
library("janitor") # tools for examining and cleaning dirty data  
# library("DataExplorer") # quick look at NA vals 
#library('jsonlite') # tools for working with lists  
#library("magrittr") # provides aliases for easier reading 
# library("friendlyeval") 

# a useful description of commits: 
# http://r-pkgs.had.co.nz/git.html 
```

```{r import_daily_flow_active_EGRET, eval=FALSE} 
# Loads USGS gage data individually, bind_cols & exports data
# Note: found gage ids by USGS watermapper.  However, this was an
#   iterative process in deciding sites to choose.  This is reflected
#   in the row numbers (see table below) non-sequential arrangement
# Also note: the joined file of active & discontinued stations >87 mb 
# Saving this file separately may protect data integrity
# 
#   readNWISDaily(siteNumber, parameterCd = "00060", startDate = "",
#      endDate = "", interactive = TRUE, convert = TRUE) 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# List of active sites
# ~~~~~~~~~~~~~~~~~~~~
# A tibble: 55 x 3
#   sta     site_no station_nm                                   
# * <chr>     <int> <chr>                                        
# 1 bad_mid 6441000 BAD R NEAR MIDLAND,SD                        
# 2 bat_bhr 6406500 BATTLE CR BELOW HERMOSA,SD                   
# 3 brsf_co 6440200 SOUTH FORK BAD R NEAR COTTONWOOD,SD          
# 4 blp_bel 6447230 BLACK PIPE CREEK NR BELVIDERE, SD            
# 5 bev_buf 6402500 BEAVER CR NEAR BUFFALO GAP,SD                
# 6 bev_pri 6402430 BEAVER CREEK NEAR PRINGLE, SD   
#26 che_buf 6402600 CHEYENNE R NEAR BUFFALO GAP SD                        
# 7 che_red 6403700 CHEYENNE RIVER AT RED SHIRT, SD              
# 8 che_sce 6408650 CHEYENNE RIVER NEAR SCENIC, SD       
#27 che_was 6423500 CHEYENNE RIVER NEAR WASTA, SD 
# 9 fal_hot 6402000 FALL R AT HOT SPRINGS,SD                     
#10 hat_edg 6400000 HAT CR NEAR EDGEMONT,SD                      
#11 hor_oel 6400875 HORSEHEAD CR AT OELRICHS,SD                  
#12 lcr_bel 6449000 LAKE CR BELOW REFUGE NEAR TUTHILL,SD 
#13 lcr_vet 6449100 LITTLE WHITE R NEAR VETAL,SD                 
#14 lwr_mar 6447500 LITTLE WHITE R NEAR MARTIN,SD                
#15 lwr_ros 6449500 LITTLE WHITE R NEAR ROSEBUD SD               
#16 lwr_whi 6450500 LITTLE WHITE R BELOW WHITE RIVER,SD          
#17 rap_far 6421500 RAPID CR NEAR FARMINGDALE,SD                 
#18 wcc_ogl 6445980 WHITE CLAY CR NEAR OGLALA SD                 
#19 whi_int 6446500 WHITE R NEAR INTERIOR,SD                     
#20 whi_kad 6447000 WHITE R NEAR KADOKA,SD                       
#21 whi_ogl 6446000 WHITE R NEAR OGLALA SD       
#28 whi_roc 6446200 WHITE R NEAR ROCKYFORD SD         
#22 whi_slm 6445700 WHITE RIVER AT SLIM BUTTE, SD                
#23 whi_sta 6445685 WHITE R NR NE-SD STATE LINE                  
#24 whi_whi 6447450 WHITE RIVER NEAR WHITE RIVER, SD             
#25 wkc_wok 6446100 WOUNDED KNEE CREEK AT WOUNDED KNEE, SD       
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1. load individual gages
# add an end date to remove provisional data & gaps in vals
startDate    <- "" # blank gets earliest date
endDate      <- "2017-09-30"
parameter_cd <- "00060" 
# ~~~~~~~~~~~~~~~~~~~~~ 

bad_mid <- readNWISDaily("06441000", parameter_cd, startDate, endDate) 
bat_bhr <- readNWISDaily("06406500", parameter_cd, startDate, endDate) 
bev_buf <- readNWISDaily("06402500", parameter_cd, startDate, endDate) 
bev_pri <- readNWISDaily("06402430", parameter_cd, startDate, endDate) 
blp_bel <- readNWISDaily("06447230", parameter_cd, startDate, endDate) 
brsf_co <- readNWISDaily("06440200", parameter_cd, startDate, endDate) 
che_buf <- readNWISDaily("06402600", parameter_cd, startDate, endDate) 
che_red <- readNWISDaily("06403700", parameter_cd, startDate, endDate)  
che_sce <- readNWISDaily("06408650", parameter_cd, startDate, endDate) 
che_was <- readNWISDaily("06423500", parameter_cd, startDate, endDate) 
fal_hot <- readNWISDaily("06402000", parameter_cd, "1947-06-01", endDate) 
hat_edg <- readNWISDaily("06400000", parameter_cd, startDate, endDate) 
hor_oel <- readNWISDaily("06400875", parameter_cd, startDate, endDate) 
lcr_bel <- readNWISDaily("06449000", parameter_cd, startDate, endDate) 
lcr_vet <- readNWISDaily("06449100", parameter_cd, startDate, endDate) 
lwr_mar <- readNWISDaily("06447500", parameter_cd, startDate, endDate) 
lwr_ros <- readNWISDaily("06449500", parameter_cd, startDate, endDate) 
lwr_whi <- readNWISDaily("06450500", parameter_cd, startDate, endDate)
rap_far <- readNWISDaily("06421500", parameter_cd, startDate, endDate)
wcc_ogl <- readNWISDaily("06445980", parameter_cd, startDate, endDate) 
wkc_wok <- readNWISDaily("06446100", parameter_cd, startDate, endDate) 
whi_int <- readNWISDaily("06446500", parameter_cd, startDate, endDate) 
whi_kad <- readNWISDaily("06447000", parameter_cd, startDate, endDate)
whi_ogl <- readNWISDaily("06446000", parameter_cd, startDate, endDate)  
whi_roc <- readNWISDaily("06446200", parameter_cd, startDate, endDate) 
whi_slm <- readNWISDaily("06445700", parameter_cd, startDate, endDate) 
whi_sta <- readNWISDaily("06445685", parameter_cd, startDate, endDate) 
whi_whi <- readNWISDaily("06447450", parameter_cd, startDate ,endDate)

# 2. add short name and abbreviation to gage data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
bad_mid <- bad_mid %>% 
  mutate(sta = "bad_mid") %>%
  mutate(site_no = "06441000") 

bat_bhr <- bat_bhr %>%
  mutate(sta = "bat_bhr") %>%
  mutate(site_no = "06406500") 

bev_buf <- bev_buf %>%
  mutate(sta = "bev_buf") %>%
  mutate(site_no = "06402500") 

bev_pri <- bev_pri %>%
  mutate(sta = "bev_pri") %>%
  mutate(site_no = "06402430") 

blp_bel <- blp_bel %>% 
  mutate(sta = "blp_bel") %>%
  mutate(site_no = "06447230")

 brsf_co <- brsf_co %>% 
  mutate(sta = "brsf_co") %>%
  mutate(site_no = "06440200")

che_buf <- che_buf %>% 
  mutate(sta = "che_buf") %>%
  mutate(site_no = "06402600")

che_red <- che_red %>% 
  mutate(sta = "che_red") %>%
  mutate(site_no = "06403700")

che_sce <- che_sce %>% 
  mutate(sta = "che_sce") %>%
  mutate(site_no = "06408650") 

che_was <- che_was %>% 
  mutate(sta = "che_was") %>%
  mutate(site_no = "06423500") 

fal_hot <- fal_hot %>% 
  mutate(sta = "fal_hot") %>%
  mutate(site_no = "06402000") 

hat_edg <- hat_edg %>% 
  mutate(sta = "hat_edg") %>%
  mutate(site_no = "06400000") 

hor_oel <- hor_oel %>% 
  mutate(sta = "hor_oel") %>%
  mutate(site_no = "06400875") 

lcr_bel <- lcr_bel %>% 
  mutate(sta = "lcr_bel") %>%
  mutate(site_no = "06449000")

lcr_vet <- lcr_vet %>% 
  mutate(sta = "lcr_vet") %>%
  mutate(site_no = "06449100")

lwr_mar <- lwr_mar  %>% 
  mutate(sta = "lwr_mar") %>%
  mutate(site_no = "06447500")

lwr_ros <- lwr_ros  %>% 
  mutate(sta = "lwr_ros") %>%
  mutate(site_no = "06449500")

lwr_whi <- lwr_whi  %>% 
  mutate(sta = "lwr_whi") %>%
  mutate(site_no = "06450500")
 
rap_far <- rap_far  %>% 
  mutate(sta = "rap_far") %>%
  mutate(site_no = "06421500")

wcc_ogl <- wcc_ogl %>% 
  mutate(sta = "wcc_ogl") %>%
  mutate(site_no = "06445980") 

whi_int <- whi_int %>% 
  mutate(sta = "whi_int") %>%
  mutate(site_no = "06446500") 

whi_kad <- whi_kad %>% 
  mutate(sta = "whi_kad") %>%
  mutate(site_no = "06447000") 

whi_ogl <- whi_ogl %>% 
  mutate(sta = "whi_ogl") %>%
  mutate(site_no = "06446000") 

whi_slm <- whi_slm %>% 
  mutate(sta = "whi_slm") %>%
  mutate(site_no = "06445700") 

whi_roc <- whi_roc %>% 
  mutate(sta = "whi_roc") %>%
  mutate(site_no = "06446200") 

whi_sta <- whi_sta %>% 
  mutate(sta = "whi_sta") %>%
  mutate(site_no = "06445685") 

whi_whi <- whi_whi %>% 
  mutate(sta = "whi_whi") %>%
  mutate(site_no = "06447450") 

wkc_wok <- wkc_wok %>% 
  mutate(sta = "wkc_wok") %>%
  mutate(site_no = "06446100") 

# 3. join together in long format
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_int1  <- bind_rows(bad_mid, bat_bhr) 
gage_int2  <- bind_rows(gage_int1, brsf_co) 
gage_int3  <- bind_rows(gage_int2, blp_bel) 
gage_int4  <- bind_rows(gage_int3, bev_buf) 
gage_int5  <- bind_rows(gage_int4, bev_pri) 
gage_int6  <- bind_rows(gage_int5, che_red) 
gage_int7  <- bind_rows(gage_int6, che_sce) 
gage_int8  <- bind_rows(gage_int7, fal_hot) 
gage_int9  <- bind_rows(gage_int8, hat_edg) 
gage_int10 <- bind_rows(gage_int9, hor_oel) 
gage_int11 <- bind_rows(gage_int10, lcr_bel) 
gage_int12 <- bind_rows(gage_int11, lcr_vet) 
gage_int13 <- bind_rows(gage_int12, lwr_mar) 
gage_int14 <- bind_rows(gage_int13, lwr_ros) 
gage_int15 <- bind_rows(gage_int14, lwr_whi) 
gage_int16 <- bind_rows(gage_int15, rap_far) 
gage_int17 <- bind_rows(gage_int16, wcc_ogl) 
gage_int18 <- bind_rows(gage_int17, whi_int) 
gage_int19 <- bind_rows(gage_int18, whi_kad) 
gage_int20 <- bind_rows(gage_int19, whi_ogl) 
gage_int21 <- bind_rows(gage_int20, whi_slm) 
gage_int22 <- bind_rows(gage_int21, whi_sta) 
gage_int23 <- bind_rows(gage_int22, whi_whi) 
gage_int24 <- bind_rows(gage_int23, wkc_wok) 
gage_int25 <- bind_rows(gage_int24, che_buf) 
gage_int26 <- bind_rows(gage_int25, che_was) 
gage_int27 <- bind_rows(gage_int26, whi_roc) 

# export daily flows to data folder
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# export(gage_int27, "data/gage_cont.csv") 

#>>> git push origin refs/heads/master
#remote: warning: GH001: Large files detected. You may want to try Git #Large File Storage - https://git-lfs.github.com.        
#remote: warning: See http://git.io/iEPt8g for more information.        
#remote: warning: File data/gage_cont.csv is 60.77 MB; this is larger than #GitHub's recommended maximum file size of 50.00 MB        
#To https://github.com/cjtinant/eco-drought.git
#   be32000..a6c9c26  master -> master
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# export top half of active gage data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
export(gage_int14, "data/gage_cont_1st_half.csv")

# bind bottom half of active gage data - start at 36 to avoid dups
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

gage_int36 <- bind_rows(lwr_whi, rap_far) 
gage_int37 <- bind_rows(gage_int36, wcc_ogl) 
gage_int38 <- bind_rows(gage_int37, whi_int) 
gage_int39 <- bind_rows(gage_int38, whi_kad) 
gage_int40 <- bind_rows(gage_int39, whi_ogl) 
gage_int41 <- bind_rows(gage_int40, whi_slm) 
gage_int42 <- bind_rows(gage_int41, whi_sta) 
gage_int43 <- bind_rows(gage_int42, whi_whi) 
gage_int44 <- bind_rows(gage_int43, wkc_wok) 
gage_int45 <- bind_rows(gage_int44, che_buf) 
gage_int46 <- bind_rows(gage_int45, che_was) 
gage_int47 <- bind_rows(gage_int46, whi_roc) 

# export bottom half of active gage data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# export(gage_int47, "data/gage_cont_2nd_half.csv")

```

```{r import_dailyflow_EGRET-bearLodge, eval=FALSE}
# long story, but Bear in the Lodge streamflow is restricted.  So,
# need to find another approach to getting the data. 
# one way is annual reports - but only to 2013
# Need to email the webmaster to get streamflow data.
# https://waterdata.usgs.gov/sd/nwis/inventory/?site_no=06446700&agency_cd= 

#blc_wan <- readNWISDaily("06446700") # this not working! 
#blc_wan <- blc_wan %>%
#  mutate(sta = blc_wan) %>%
#  mutate(site_no = "06446700")

```

```{r import_daily_flow_discontinued_EGRET} 

# List of discontinued sites w/in 30 mi. of PRR boundary
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A tibble: 55 x 3
#   sta     site_no station_nm                                   
# * <chr>     <int> <chr>          
#29 ant_gor 6458000 Antelope Creek near Gordon, Nebr.           
#33 brnf_ph 6440500 NORTH FORK BAD R NEAR PHILIP SD    
#30 bea_eli 6458500 BEAR C NR ELI NEBR                           
#31 bev_abf 6402470 BEAVER CREEK ABOVE BUFFALO GAP, SD           
#32 bor_cha 6445590 BIG BORDEAUX CREEK NEAR CHADRON NEBR         
#34 cas_hot 6400497 CASCADE SPRINGS NEAR HOT SPRINGS SD          
#35 che_hot 6400500 CHEYENNE R NEAR HOT SPRINGS SD               
#36 fre_fai 6403500 FRENCH CR NEAR FAIRBURN SD                   
#37 hor_aoe 6400870 HORSEHEAD CR NEAR OELRICHS SD                
#38 lcr_abv 6448000 LAKE CR ABOVE REFUGE NEAR TUTHILL,SD         
#39 lwr_abv 6449300 LITTLE WHITE R ABV ROSEBUD SD                
#40 min_kil 6460900 MINNECHADUZA CREEK NEAR KILGORE, NEBRASKA   
#41 nio_abb 6454500 NIOBRARA RIVER ABOVE BOX BUTTE RESERVOIR, NE 
#42 nio_bbb 6455500 NIOBRARA RIVER BELOW BOX BUTTE RESERVOIR NEBR
#43 nio_cod 6459000 NIOBRARA R NEAR CODY, NEBR.                  
#44 nio_col 6457000 NIOBRARA RIVER NEAR COLCLESSER, NEBR.        
#45 nio_dun 6455900 NIOBRARA RIVER NEAR DUNLAP, NEBR.            
#46 nio_hay 6456500 NIOBRARA RIVER NR HAY SPRINGS, NEBR.         
#47 rap_cre 6422000 RAPID CR AT CRESTON SD                       
#48 ros_ros 6449400 ROSEBUD CR AT ROSEBUD SD                     
#49 sna_bur 6459500 SNAKE RIVER NEAR BURGE, NEBR.                
#50 sna_dou 6459175 SNAKE R AT DOUGHBOY, NE                      
#51 sna_mer 6459200 SNAKE RIVER ABV MERRITT RESERVOIR NEBR       
#52 spr_stf 6449250 SPRING CR NEAR ST FRANCIS SD                 
#53 whi_cha 6445500 WHITE R NEAR CHADRON NEBR                    
#54 whi_cra 6444000 White River at Crawford, Nebr.               
#55 whi_wht 6445000 WHITE R BELW COTTONWOOD C N WHITNEY, NEBR.  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1. load individual gages
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# add an end date to remove provisional data & gaps in vals
startDate    <- "" #Gets earliest date
endDate      <- "2017-09-30"
parameter_cd <- "00060" 
# ~~~~~~~~~~~~~~~~~~~~~ 

ant_gor <- readNWISDaily("06458000", parameter_cd, startDate ,endDate) 
brnf_ph <- readNWISDaily("06440500", parameter_cd, startDate, endDate) 
bea_eli <- readNWISDaily("06458500", parameter_cd, startDate, endDate) 
bev_abf <- readNWISDaily("06402470", parameter_cd, startDate, endDate) 
bor_cha <- readNWISDaily("06445590", parameter_cd, startDate, endDate) 
cas_hot <- readNWISDaily("06400497", parameter_cd, startDate, endDate) 
che_hot <- readNWISDaily("06400500", parameter_cd, startDate, endDate) 
fre_fai <- readNWISDaily("06403500", parameter_cd, "1946-01-14", endDate) 
hor_aoe <- readNWISDaily("06400870", parameter_cd, startDate, endDate) 
lcr_abv <- readNWISDaily("06448000", parameter_cd, startDate, endDate) 
lwr_abv <- readNWISDaily("06449300", parameter_cd, startDate, endDate) 
min_kil <- readNWISDaily("06460900", parameter_cd, startDate, endDate) 
nio_abb <- readNWISDaily("06454500", parameter_cd, startDate, endDate) 
nio_bbb <- readNWISDaily("06455500", parameter_cd, startDate, endDate) 
nio_col <- readNWISDaily("06457000", parameter_cd, startDate, endDate) 
nio_cod <- readNWISDaily("06459000", parameter_cd, startDate, endDate) 
nio_dun <- readNWISDaily("06455900", parameter_cd, startDate, endDate) 
nio_hay <- readNWISDaily("06456500", parameter_cd, startDate, endDate) 
rap_cre <- readNWISDaily("06422000", parameter_cd, startDate, endDate) 
ros_ros <- readNWISDaily("06449400", parameter_cd, startDate, endDate) 
sna_bur <- readNWISDaily("06459500", parameter_cd, startDate, endDate) 
sna_dou <- readNWISDaily("06459175", parameter_cd, startDate, endDate) 
sna_mer <- readNWISDaily("06459200", parameter_cd, startDate, endDate) 
spr_stf <- readNWISDaily("06449250", parameter_cd, startDate, endDate) 
whi_cha <- readNWISDaily("06445500", parameter_cd, startDate, endDate) 
whi_cra <- readNWISDaily("06444000", parameter_cd, startDate, endDate)
whi_wht <- readNWISDaily("06445000", parameter_cd, startDate, endDate) 

# 2. add short name and abbreviation to gage data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ant_gor <- ant_gor %>% 
  mutate(sta = "ant_gor") %>%
  mutate(site_no = "06458000") 

brnf_ph <- brnf_ph %>% 
  mutate(sta = "brnf_ph") %>%
  mutate(site_no = "06440500") 

bea_eli <- bea_eli %>% 
  mutate(sta = "bea_eli") %>%
  mutate(site_no = "06458500") 

bev_abf <- bev_abf %>% 
  mutate(sta = "bev_abf") %>%
  mutate(site_no = "06402470") 

bor_cha <- bor_cha %>% 
  mutate(sta = "bor_cha") %>%
  mutate(site_no = "06445590") 

cas_hot <- cas_hot %>% 
  mutate(sta = "cas_hot") %>%
  mutate(site_no = "06400497") 

che_hot <- che_hot %>% 
  mutate(sta = "che_hot") %>%
  mutate(site_no = "06400500") 

fre_fai <- fre_fai %>% 
  mutate(sta = "fre_fai") %>%
  mutate(site_no = "06403500") 

hor_aoe <- hor_aoe %>% 
  mutate(sta = "hor_aoe") %>%
  mutate(site_no = "06400870") 

lcr_abv <- lcr_abv %>% 
  mutate(sta = "lcr_abv") %>%
  mutate(site_no = "06448000") 

lwr_abv <- lwr_abv %>% 
  mutate(sta = "lwr_abv") %>%
  mutate(site_no = "06449300") 

min_kil <- min_kil %>% 
  mutate(sta = "min_kil") %>%
  mutate(site_no = "06460900") 

nio_abb <- nio_abb %>% 
  mutate(sta = "nio_abb") %>%
  mutate(site_no = "06454500") 

nio_bbb <- nio_bbb %>% 
  mutate(sta = "nio_bbb") %>%
  mutate(site_no = "06455500") 

nio_col <- nio_col %>% 
  mutate(sta = "nio_col") %>%
  mutate(site_no = "06457000") 

nio_cod <- nio_cod %>% 
  mutate(sta = "nio_cod") %>%
  mutate(site_no = "06459000") 

nio_dun <- nio_dun %>% 
  mutate(sta = "nio_dun") %>%
  mutate(site_no = "06455900")

nio_hay <- nio_hay %>% 
  mutate(sta = "nio_hay") %>%
  mutate(site_no = "06456500") 

rap_cre <- rap_cre %>% 
  mutate(sta = "rap_cre") %>%
  mutate(site_no = "06422000")

ros_ros <- ros_ros %>% 
  mutate(sta = "ros_ros") %>%
  mutate(site_no = "06449400") 

sna_bur <- sna_bur %>% 
  mutate(sta = "sna_bur") %>%
  mutate(site_no = "06459500")

sna_dou <- sna_dou %>% 
  mutate(sta = "sna_dou") %>%
  mutate(site_no = "06459175") 

sna_mer <- sna_mer %>% 
  mutate(sta = "sna_mer") %>%
  mutate(site_no = "06459200")

spr_stf <- spr_stf %>% 
  mutate(sta = "spr_stf") %>%
  mutate(site_no = "06449250") 

whi_cha <- whi_cha %>% 
  mutate(sta = "whi_cha") %>%
  mutate(site_no = "06445500") 

whi_cra <- whi_cra %>% 
  mutate(sta = "whi_cra") %>%
  mutate(site_no = "06444000") 

whi_wht <- whi_wht %>% 
  mutate(sta = "whi_wht") %>%
  mutate(site_no = "06445000") 

# 3. join together in long format
# Next step: probably could be done more efficiently with map_df()
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_int1  <- bind_rows(ant_gor, bea_eli) 
gage_int2  <- bind_rows(gage_int1, bev_abf) 
gage_int3  <- bind_rows(gage_int2, bor_cha) 
gage_int4  <- bind_rows(gage_int3, brnf_ph) 
gage_int5  <- bind_rows(gage_int4, cas_hot) 
gage_int6  <- bind_rows(gage_int5, che_hot) 
gage_int7  <- bind_rows(gage_int6, fre_fai) 
gage_int8  <- bind_rows(gage_int7, hor_aoe) 
gage_int9  <- bind_rows(gage_int8, lcr_abv) 
gage_int10 <- bind_rows(gage_int9, lwr_abv) 
gage_int11 <- bind_rows(gage_int10, min_kil) 
gage_int12 <- bind_rows(gage_int11, nio_abb) 
gage_int13 <- bind_rows(gage_int12, nio_bbb) 
gage_int14 <- bind_rows(gage_int13, nio_cod) 
gage_int15 <- bind_rows(gage_int14, nio_col) 
gage_int16 <- bind_rows(gage_int15, nio_dun) 
gage_int17 <- bind_rows(gage_int16, nio_hay) 
gage_int18 <- bind_rows(gage_int17, rap_cre) 
gage_int19 <- bind_rows(gage_int18, ros_ros) 
gage_int20 <- bind_rows(gage_int19, sna_bur) 
gage_int21 <- bind_rows(gage_int20, sna_dou) 
gage_int22 <- bind_rows(gage_int21, sna_mer) 
gage_int23 <- bind_rows(gage_int22, spr_stf) 
gage_int24 <- bind_rows(gage_int23, whi_cha) 
gage_int25 <- bind_rows(gage_int24, whi_cra) 
gage_int26 <- bind_rows(gage_int25, whi_wht) 

# export daily flows to data folder
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
export(gage_int26, "data/gage_disc.csv") 
```

```{r import_daily_flow_overlooked} 

# List of possible extra sites - list was iteratively constructed
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# A tibble: 6 x 2
#   sta     site_no station_nm                                   
# * <chr>     <int> <chr>          
#1 che_ang 06401500 CHEYENNE R BELOW ANGOSTURA DAM,SD
#2 frn_fai 06403300 FRENCH CR ABOVE FAIRBURN SD      
#3 bat_key 06404000 BATTLE CR NEAR KEYSTONE,SD       
#4 bat_bhr 06406000 BATTLE CR AT HERMOSA,SD          
#5 spr_her 06408500 SPRING CR NEAR HERMOSA,SD        
#6 nio_gor 06457500 NIOBRARA RIVER NEAR GORDON, NEBR.
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# this is what was done to find the stations above: 

# this code chunk checks for "missed" stations.  
# for filtering info see: https://help.waterdata.usgs.gov/site_tp_cd
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1. get huc codes for gages already selected
gage_meta <- import("data/gage_meta.csv") %>% 
  arrange(sta)

hucs <- gage_meta %>% 
  distinct(huc_cd) %>%
  arrange() %>%
  mutate(huc_cd = as.character(huc_cd))

# 2. split the API query into smaller parts
hucs1 <- hucs %>%
  slice(1:6)

hucs2 <- hucs %>%
  slice(7:13) 

# 3. get the set of NWIS data for each of the slices
gages1 <- hucs1 %>% 
     map_df(~ whatNWISdata(huc = .x)) 

gages2 <- hucs2 %>% 
     map_df(~ whatNWISdata(huc = .x)) 

# 4. create a single dataframe and clean up 
gages1 <- gages1 %>% 
  mutate(alt_acy_va = as.numeric(alt_acy_va))

gages_pos <- bind_rows(gages1, gages2)
rm(hucs, hucs1, hucs2, gages1, gages2)

# 5. explore data types
# ~~~~~~~~~~~~~~~~~~~~~~~

#   a. find site_types 
NWIS_type <- gages_pos %>% 
  distinct(site_tp_cd) %>%  
  arrange(site_tp_cd)

#   b. remove atmosphere (AT), land (LA), & subsurface (SB) site types
gage_pos1 <- gages_pos %>% 
  filter(site_tp_cd != "AT" &
           site_tp_cd != "LA-SH" & 
           site_tp_cd != "LA-SNK" &
           site_tp_cd != "SB-CV" &
           site_tp_cd != "SB" 
         ) %>% 
  arrange(site_tp_cd) 

#   c. remove facilities (FA) site type 
gage_pos2 <- gage_pos1 %>% 
  filter(site_tp_cd != "FA-CI" &
           site_tp_cd != "FA-OF" & 
           site_tp_cd != "FA-SEW" & 
           site_tp_cd != "FA-STS" & 
           site_tp_cd != "FA-WDS"
         )

#   d. remove groundwater (GW) & spring site types 
gw <- gage_pos2 %>% 
  filter(site_tp_cd == "GW" |
           site_tp_cd == "GW-CR" | 
           site_tp_cd == "GW-IW" |
           site_tp_cd == "GW-MW" | 
           site_tp_cd == "GW-TH" | 
          site_tp_cd == "SP"
         )

gage_pos3 <- gage_pos2 %>% 
  filter(site_tp_cd != "GW" &
           site_tp_cd != "GW-CR" & 
           site_tp_cd != "GW-IW" &
           site_tp_cd != "GW-MW" & 
           site_tp_cd != "GW-TH" & 
          site_tp_cd != "SP"
         )

#   e. remove lake (LK) & canal (CA) site types 
gage_pos4 <- gage_pos3 %>% 
  filter(site_tp_cd != "LK" & 
           site_tp_cd != "ST-CA" &
           site_tp_cd != "ST-DCH"
           ) 

#   e. check streams == gage_pos4 & clean up
gage_pos5 <- gage_pos4 %>% 
  filter(site_tp_cd != "ST") 

gage_strm <- gages_pos %>% 
  filter(site_tp_cd == "ST") 

rm(NWIS_type, gage_pos, gage_pos1, gage_pos2, gage_pos3, 
   gage_pos4, gage_pos5) 

#   f. remove bio, sediment, wq, report, peak, temp data
bio <- gage_strm %>% 
  filter(medium_grp_cd == "bio")

sed <- gage_strm %>% 
  filter(medium_grp_cd == "sed") 

wq <- gage_strm %>% 
  filter(data_type_cd == "qw") 

ad <- gage_strm %>% 
  filter(data_type_cd == "ad") 

pk <- gage_strm %>% 
  filter(data_type_cd == "pk") 

tmp <- gage_strm %>% 
  filter(parm_cd == "00010") 

gages_dv <- gage_strm %>% 
  filter(medium_grp_cd != "bio" & 
           medium_grp_cd != "sed" & 
           data_type_cd != "qw" & 
           data_type_cd != "uv" & 
           data_type_cd != "ad" &
           data_type_cd != "sv" &
           data_type_cd != "pk" & 
           parm_cd != "00010" & 
           parm_cd != "00065" & 
           parm_cd != "00095" & 
           parm_cd != "00300" & 
           parm_cd != "00400" & 
           parm_cd != "63680" & 
           parm_cd != "80154" & 
           parm_cd != "80155" 
           ) 

# g. clean dataframe to fit the area
gages_pot <- gages_dv %>% 
  mutate(alt_va = as.numeric(alt_va)) %>% 
  filter(count_nu > 365*5) %>%
  filter(dec_lat_va < 43.97) %>% 
  filter(dec_long_va > -103.58825) %>% 
  filter(dec_long_va < -100.55167) %>% 
  filter(alt_va < 4050) %>% 
  mutate(end_date = ymd(end_date)) %>% 
  mutate(end_yr = year(end_date)) %>% 
  filter(end_yr > 1990)

# bind new and old metadata cols together to check
gages_pot_list <- gages_pot %>% 
  select(site_no, station_nm) %>% 
  mutate(site_no = as.integer(site_no))
gages_exist <- gage_meta %>% 
    select(site_no, station_nm) 

gages_new <- full_join(gages_pot_list, gages_exist, by = "site_no") 
gages_new <- gages_new %>% 
  filter(is.na(station_nm.y)) %>% 
  rename(station_nm = station_nm.x) %>% 
  select(-station_nm.y) %>% 
  as.tibble() 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1. load individual gages
# ~~~~~~~~~~~~~~~~~~~~~~~~~
# add an end date to remove provisional data & gaps in vals
startDate    <- "" #Gets earliest date
endDate      <- "2017-09-30"
parameter_cd <- "00060" 
# ~~~~~~~~~~~~~~~~~~~~~ 
bat_bhr <- readNWISDaily("06406000", parameter_cd, startDate ,endDate) 
bat_key <- readNWISDaily("06404000", parameter_cd, startDate ,endDate) 
che_ang <- readNWISDaily("06401500", parameter_cd, startDate ,endDate)
frn_fai <- readNWISDaily("06403300", parameter_cd, startDate ,endDate) 
nio_gor <- readNWISDaily("06457500", parameter_cd, startDate ,endDate) 
spr_her <- readNWISDaily("06408500", parameter_cd, startDate ,endDate)  

# 2. add short name and abbreviation to gage data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

bat_bhr <- bat_bhr %>% 
  mutate(sta = "bat_her") %>%
  mutate(site_no = "6406000") 

bat_key <- bat_key %>% 
  mutate(sta = "bat_key") %>%
  mutate(site_no = "6404000") 

che_ang <- che_ang  %>% 
  mutate(sta = "che_ang") %>%
  mutate(site_no = "6401500")

frn_fai <- frn_fai %>% 
  mutate(sta = "frn_fai") %>%
  mutate(site_no = "6403300") 

nio_gor <- nio_gor %>% 
  mutate(sta = "nio_gor") %>%
  mutate(site_no = "6457500") 

spr_her <- spr_her %>% 
  mutate(sta = "spr_her") %>%
  mutate(site_no = "6408500")

# 3. join together in long format
# Next step: probably could be done more efficiently with map_df()
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_int1  <- bind_rows(bat_bhr, bat_key) 
gage_int2  <- bind_rows(gage_int1, che_ang) 
gage_int3  <- bind_rows(gage_int2, frn_fai) 
gage_int4  <- bind_rows(gage_int3, nio_gor) 
gage_int5  <- bind_rows(gage_int4, spr_her) 

# export daily flows to data folder
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# export(gage_int5, "data/gage_other.csv") 

``` 

```{r import_gage_metadata, eval=FALSE} 
  
# import daily streamflow datasets  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
gage_cont <- import("data/gage_cont.csv") 
gage_disc <- import("data/gage_disc.csv") 
gage_othr <- import("data/gage_other.csv") 
 
# check for duplicate data  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
gage_cont_ck  <- gage_cont %>% 
  distinct(sta, site_no) 

gage_dist_ck  <- gage_disc %>% 
  distinct(sta, site_no) 

gage_othr_ck  <- gage_othr %>% 
  distinct(sta, site_no) 

gage_names <- bind_rows(gage_cont_ck, gage_dist_ck) 
gage_names <- bind_rows(gage_names, gage_othr_ck) 

rm(gage_cont_ck, gage_dist_ck, gage_othr_ck) 

# join data & import metadata 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

gage_full  <- bind_rows(gage_cont, gage_disc)  
gage_full  <- bind_rows(gage_full, gage_othr) 

# gage$site_no is brought in as an integer but needs to be 8-dig char 
gage_full <- gage_full %>% 
  mutate(sta = as.character(sta)) %>%  
  mutate(site_no = zeroPad(site_no, 8)) %>% 
  select(site_no, sta, everything()) 

# reduces the set down to a small number to reduce API calls 
gage_id <- gage_full %>% 
  distinct(site_no, sta) 

# iterate across a list of gage ids by purrr::map_dfr 
# & get gage metadata using dataRetrieval::readNWISsite 
gage_meta_list <- map(gage_id$site_no, readNWISsite) 

# extract the wanted dataframe items from the list 
gage_meta <- gage_meta_list %>% 
  map_df(extract, 
               c("site_no", "station_nm", "site_tp_cd", "dec_lat_va", 
                 "dec_long_va", "dec_coord_datum_cd", "state_cd", 
                 "county_cd", "alt_va", "alt_datum_cd", "huc_cd", 
                 "drain_area_va", "contrib_drain_area_va")) 
rm(gage_meta_list) 

# join the abbreviation to the metadata 
gage_meta <- gage_meta %>%  
  full_join(gage_id, gage_meta, by = "site_no") 

gage_meta <- gage_meta %>% 
  select(sta, everything()) 

# check on integrity of the data by looking at counts 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
ck_sta <- gage_meta %>% 
  distinct(sta) 

ck_num <- gage_meta %>% 
  distinct(site_no) 

ck_sta_num <- gage_meta %>% 
  distinct(sta, site_no) 

ck_nam <- gage_meta %>% 
  distinct(station_nm) 

ck_sta_nam <- gage_meta %>% 
  distinct(sta, station_nm) 

num_nam <- gage_meta %>% 
  distinct(site_no, station_nm) 

ck_sta_nam_num <- gage_meta %>% 
  distinct(sta, site_no, station_nm) 

# export dataframes of the largest population of gages 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# export(gage, "data/gage_full.csv") # too big for GitHub 
# export(gage_meta, "data/gage_meta.csv")  

# create a table to show abbreviation, site no & station name 
gage_table <- gage_meta %>% 
  select(sta, site_no, station_nm) %>% 
  as.tibble() 
```

```{r clean_daily flow}
# this code chunk checks and removes gages w/o useful characteristics
#   for future analysis 

# Steps: 
# 1. Assemble a full dataset (N = 61 stations)
# 2. Join metadata information & year count to observations
# 3. Subset stations with observations 1992-1997 for clustering 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# 1. Join & arrange a set of all possible recorded streamflows
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_meta <- import("data/gage_meta.csv") %>% 
  arrange(sta) 

gage_cont <- import("data/gage_cont.csv") 
gage_disc <- import("data/gage_disc.csv") 
gage_othr <- import("data/gage_other.csv") 

# Assemble a complete set of stations identifed in prior steps
gage_full  <- bind_rows(gage_cont, gage_disc)  
gage_full  <- bind_rows(gage_full, gage_othr) 

gage_full <- gage_full %>% 
  arrange(sta) %>% 
  select(site_no, sta, Date, i, everything())

# check results
check_sta <- gage_full %>% 
  distinct(sta)

rm(gage_cont, gage_disc, gage_othr, check_sta)  

# 2. join together gage & gage metadata information 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

gage_yr <- gage_full %>% 
  group_by(sta) %>% 
  summarize(min_yr = min(waterYear), 
            max_yr = max(waterYear)) %>% 
  mutate(count_yr = 1 + max_yr - min_yr)

gage_meta <- full_join(gage_meta, gage_yr, by = "sta")

gage_meta <- gage_meta %>% 
  arrange(count_yr) %>% 
  select(-c(site_tp_cd, dec_coord_datum_cd, alt_datum_cd, huc_cd)) %>% 
  as.tibble() %>% 
  select(count_yr, everything())

gage_full <- full_join(gage_meta, gage_full, by = c("site_no", "sta"))
rm(gage_yr)

# 3. Identify a subset of gages for cluster analysis 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Filter set of reservation stations; note wkc-wok is 1992-1997
gage_met1 <- gage_meta %>% 
  filter(state_cd == 46) %>% 
  filter(county_cd == 7 | 
           county_cd == 71 |
           county_cd == 95 |
           county_cd == 102
  )

gage_fin1 <- gage_full %>% 
  filter(state_cd == 46) %>% 
  filter(county_cd == 7 | 
           county_cd == 71 |
           county_cd == 95 |
           county_cd == 102
  ) 

# Filter the gages with 1992-1997 observations
gage_met2 <- gage_meta %>% 
  filter(min_yr < 1993 & 
           max_yr > 1995
         ) %>% 
  arrange(count_yr) %>%
  as.tibble() 

gage_mdp2 <- gage_meta %>% 
  filter(min_yr > 1992 |
           max_yr < 1996
         ) %>% 
  arrange(count_yr) %>%
  as.tibble() 

gage_fin2 <- gage_full %>% 
  filter(min_yr < 1993 & 
           max_yr > 1995
         ) %>% 
  as.tibble() 

gage_drp2 <- gage_full %>% 
  filter(min_yr > 1992 |
           max_yr < 1996
         ) %>% 
  as.tibble() 

# check work
chk_sta <- bind_rows(gage_met2, gage_mdp2) # count is ok
chk_sta <- bind_rows(gage_fin2, gage_drp2) # count is ok
#rm(chk_sta, gage_drp1, gage_fin1, gage_mdp1, gage_met1)

# 4. Identify serially complete observations 
#   4.0: 105 station years,
#   4.1:  48 station years,
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_incomp <- gage_fin2 %>% 
  group_by(sta, waterYear) %>% 
  summarize(i_count = n()) %>% 
  filter(i_count <= 364) %>% 
  arrange(sta, waterYear) # 

# 4.1 remove bad_mid & che_ang: too much missing data 
gage_incomp <- gage_incomp %>% 
  filter(sta != "bad_mid" & 
           sta != "che_ang") %>% 
  arrange(sta)

gage_met3 <- gage_met2 %>% 
  filter(sta != "bad_mid" & 
           sta != "che_ang") %>% 
  arrange(sta)

gage_int <- gage_meta %>% 
  filter(sta == "bad_mid" | 
           sta == "che_ang") %>% 
  arrange(sta)

gage_mdp3 <- bind_rows(gage_mdp2, gage_int)

gage_fin3 <- gage_fin2 %>% 
  filter(sta != "bad_mid" & 
           sta != "che_ang") %>% 
  arrange(sta) 

gage_int <- gage_full %>% 
  filter(sta == "bad_mid" | 
           sta == "che_ang") %>% 
  arrange(sta) 

gage_drp3 <- bind_rows(gage_drp2, gage_int) 

# check work - remove unneeded tibbles
chk_sta <- bind_rows(gage_met3, gage_mdp3) # count is ok
chk_sta <- bind_rows(gage_fin3, gage_drp3) # count is ok
# rm(gage_drp2, gage_fin2, gage_int, gage_mdp2, gage_met2)

# Remove stations without 1992-1997
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Remove stations with no annual data 1992-1997 
chk_sta <- gage_fin3 %>% 
  filter(waterYear > 1992) %>% 
  filter(waterYear < 1998) %>% 
  group_by(sta, waterYear) %>%  
  count() %>% 
  arrange(n) %>% 
  ungroup() 

gage_incomp <- anti_join(gage_met3, chk_sta)

gage_fin4 <- gage_fin3 %>% 
  filter(sta != "che_buf") %>% 
  filter(sta != "whi_cra") %>% 
  filter((sta != "whi_int")) 

gage_int <- gage_fin3 %>% 
  filter(sta == "che_buf" | 
           sta == "whi_cra" | 
           sta == "whi_int"
         )

gage_drp4 <- bind_rows(gage_drp3, gage_int)

gage_int <- gage_met3 %>% 
  filter(sta == "che_buf" | 
           sta == "whi_cra" | 
           sta == "whi_int"
         ) 

gage_mdp4 <- bind_rows(gage_mdp3, gage_int)

gage_met4 <- gage_met3 %>% 
  filter(sta != "che_buf") %>% 
  filter(sta != "whi_cra") %>% 
  filter((sta != "whi_int")) 

# Remove stations with incomplete annual data 1992-1997 
gage_incomp <- gage_fin4 %>% 
  filter(waterYear > 1992) %>% 
  filter(waterYear < 1998) %>% 
  group_by(sta, waterYear) %>%  
  count() %>% 
  arrange(sta) %>% 
  ungroup() 

gage_incomp <- gage_incomp %>% 
  group_by(sta) %>% 
  count() %>% 
  filter(nn < 5)

# remove lcr_abv - not complete for 1992-1997  
gage_fin5 <- gage_fin4 %>% 
  filter(sta != "lcr_abv") 

gage_int <- gage_fin4 %>% 
  filter(sta == "lcr_abv")

gage_drp5 <- bind_rows(gage_drp4, gage_int)

gage_int <- gage_met4 %>% 
  filter(sta == "lcr_abv")

gage_mdp5 <- bind_rows(gage_mdp4, gage_int)

gage_met5 <- gage_met4 %>% 
  filter(sta != "lcr_abv") %>% 
  arrange(sta)

export(gage_fin5, "data/gage_92_97.csv") 
export(gage_met5, "data/gage_meta_92_97.csv")
```

```{r remove_incomplete, eval=FALSE}
#  6. Use the EGRET input data to increase serial completeness
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# bat_key discharge data jumps from 1947-07-31 to 1961-10-02 
chk_sta <- gage_fin5 %>% 
  filter(sta == "bat_key" & 
           waterYear < 1962) %>% 
  distinct(waterYear)

gage_int <- gage_fin5 %>% 
  filter(sta == "bat_key" & 
           waterYear < 1962) 

gage_fin6 <- gage_fin5 %>% 
  filter(sta != "bat_key" | 
           waterYear > 1961)

chk_sta <- anti_join(gage_fin5, gage_fin6) 
gage_drp6 <- bind_rows(gage_drp3, gage_int)

# che_was discharge data jumps from 1934-03-06 to 1934-03-18 
chk_sta <- gage_fin6 %>% 
  filter(sta == "che_was" & 
           waterYear < 1934) %>%
  distinct(waterYear)

gage_int <- gage_fin6 %>% 
  filter(sta == "che_was" & 
           waterYear < 1934) 

gage_fin7 <- gage_fin6 %>% 
  filter(sta != "che_was" | 
           waterYear > 1933)

chk_sta <- anti_join(gage_fin6, gage_fin7) 
gage_drp7 <- bind_rows(gage_drp6, gage_int)

# lcr_bel discharge data jumps from 1940-09-29 to 1962-08-01 
chk_sta <- gage_fin6 %>% 
  filter(sta == "lcr_bel" & 
           waterYear < 1963) %>%
  distinct(waterYear)

gage_int <- gage_fin6 %>% 
  filter(sta == "lcr_bel" & 
           waterYear < 1963) 

gage_fin7 <- gage_fin6 %>% 
  filter(sta != "lcr_bel" | 
           waterYear > 1962)

chk_sta <- anti_join(gage_fin6, gage_fin7) 
gage_drp7 <- bind_rows(gage_drp6, gage_int) 

# lwr_abv discharge data jumps from 1999-09-30 to 2000-01-01
chk_sta <- gage_fin7 %>% 
  filter(sta == "lwr_abv" & 
           waterYear > 1999) %>%
  distinct(waterYear)

gage_int <- gage_fin7 %>% 
  filter(sta == "lwr_abv" & 
           waterYear > 1999) 

gage_fin8 <- gage_fin7 %>% 
  filter(sta != "lwr_abv" | 
           waterYear < 2000)

chk_sta <- anti_join(gage_fin7, gage_fin8) 
gage_drp8 <- bind_rows(gage_drp7, gage_int)

# lwr_mar discharge data jumps from 1940-09-29 to 1962-08-01 
chk_sta <- gage_fin8 %>% 
  filter(sta == "lwr_mar" & 
           waterYear < 1963) %>%
  distinct(waterYear)

gage_int <- gage_fin8 %>% 
  filter(sta == "lwr_mar" & 
           waterYear < 1963) 

gage_fin9 <- gage_fin8 %>% 
  filter(sta != "lwr_mar" | 
           waterYear > 1962)

chk_sta <- anti_join(gage_fin8, gage_fin9) 
gage_drp9 <- bind_rows(gage_drp8, gage_int) 

# ros_ros   discharge data jumps from 1997-09-30 to 2003-03-16 
#  End @ 1997-09-30

# rap_far discharge data jumps from 1989-09-29 to 1990-10-01 
chk_sta <- gage_fin9 %>% 
  filter(sta == "rap_far" & 
           waterYear < 1990) %>%
  distinct(waterYear)

gage_int <- gage_fin9 %>% 
  filter(sta == "rap_far" & 
           waterYear < 1990) 

gage_fin10 <- gage_fin9 %>% 
  filter(sta != "rap_far" | 
           waterYear > 1991)

chk_sta    <- anti_join(gage_fin9, gage_fin10) 
gage_drp10 <- bind_rows(gage_drp9, gage_int) 

# wcc_ogl  discharge data jumps from 1981-09-29 to 1987-10-01 
# No change
# whi_slm   discharge data jumps from 1970-09-29 to  
# Start @ 1990-12-04















```

table <- gage_full %>% 
  group_by(sta) %>% 
  summarize(num_days = max(i)) %>% 
  arrange(num_days) %>% 
  mutate(num_months = round(num_days/30)) %>% 
  mutate(num_years = round(num_months/12)) %>% 
  ungroup() %>%
  as.tibble()


```

# 2. result - remove very short record stations
# A tibble: 61 x 4
#  sta     num_days num_months num_years
# <chr>      <dbl>      <dbl>     <dbl>
# 1 ant_gor      254          8         1
# 2 fre_fai      595         20         2
# 3 hor_aoe      623         21         2
# 4 rap_cre     1320         44         4


# # blc_wan <- readNWISDaily("06446700") # this not working!
```

```{r next_steps_munging}
# Also add BEAR in the LODGE


# check ice - starts 2017-10-23 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ice <- gage_int %>%
  filter(code == "P Ice") %>% 
  arrange(date) 

# check provisional values 
# ~~~~~~~~~~~~~~~~~~~~~~~~
provis <- gage_int %>%
  filter(code == "P" | code == "P e" | code == "P <") %>%
  arrange(date) # starts 2016-10-05

# remove data after 2017-10-01
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_int <- gage_int %>%
  filter(year != 2018)

gage_2017 <- gage_int %>%
  filter(year == 2017)

gage_prior <- gage_int %>%
  filter(year < 2017) 

gage_2017 <- gage_2017 %>%
  filter(month != 10) %>% 
  filter(month != 11) %>%
filter(month != 12)

gage_imp <- bind_rows(gage_prior, gage_2017) %>%
  arrange(desc(date)) 
rm(gage_2017, gage_int, gage_list, gage_prior, ice, provis, gage_raw) 

# simplify the naming convention for gages 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_imp <- gage_imp %>% 
  select(site_no, date, discharge, code)

station_nm <- gage_meta %>% 
  select(site_no, station_nm) %>% 
  as.tibble()

abrev <- c("whi_sta", "whi_ogl", "wcc_ogl", "whi_int", "wkc_wok", 
           "blc_wan", "whi_kad", "blp_bel", "lwr_mar", "lcr_bel", 
           "lcr_vet", "brsf_co", "bad_mid") %>%
  as.tibble() %>%
  rename(sta = value)

abrev <- bind_cols(abrev, station_nm) 

gage_imp <- full_join(abrev, gage_imp, by = "site_no")
gage_meta <- full_join(abrev, gage_meta, by = "site_no")
rm(station_nm, abrev)

# Split and spread data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_dis_imp <- gage_imp %>% 
  select(sta, date, discharge)

gage_dis_imp <- gage_dis_imp %>% 
  spread(key = sta, value = discharge) 

# Add short name to metadata
# ~~~~~~~~~~~~~~~~~~~~~~~~~~
abrev <- gage_imp %>%
  distinct(sta, site_no)

gage_meta <- full_join(abrev, gage_meta, by = "site_no") 

# export and import station df by rio
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# export(gage_raw, "data/gage_raw.csv") 
# write_lines(gage_json, "data/gage_list.json")
# export(gage_imp, "data/gage_imperial.csv") 

# export(gage_dis_imp, "data/gage_discharge_imp.csv") 


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# get site numbers from prior metadata
gage_meta <- import("data/gage_meta.csv")
#site_nums <- gage_meta %>%
#  select(sta, site_no) %>% 
#  print()


#Sample <- readNWISSample(siteNumber,parameter_cd,startDate,endDate)
# this code chunk uses dataRetreval to get discharge in cfs 
# removes provisional values and ice and saves the data as a csv.
```

```{r daily2monthly-precip}
# continued from above 
# General Purpose: prepare data for drought index 
# Specific purpose: convert daily precip to monthly precip  

# load metadata & data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_day   <- as.tibble(import("data/stations_final2.csv")) 

# remove Murdo, Mission, Long Valley - see above 
#sta_day   <- sta_day %>% 
#  select(-c(lon, mur, mis)) 
#export(sta_day, file = "data/stations_final2.csv")  

# fix date & add year and month 
sta_day <- sta_day %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# gather daily values 
sta_gath <- gather(sta_day, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

# create groups 
sta_group <- sta_gath %>% 
  group_by(year, month, station) 

# sum daily precip over a month 
sta_gath_mon <- sta_group %>% 
  summarize(prcp_tenths = sum(prcp)) %>% 
  mutate(prcp_mm = prcp_tenths/10) %>% 
  select(-prcp_tenths) 

# spread result - now in months  
#   ...and take a bow, because this is MAGIC!  Thnx Tidyverse. 
sta_mon <- sta_gath_mon %>% 
  spread(station, prcp_mm) %>% 
  mutate(day = 1) %>% 
  mutate(date = make_date(year = year, month = month, day = day)) %>% 
  select(date, year, month, everything()) %>% 
  select(-day) %>% 
  ungroup()

rm(sta_day, sta_gath, sta_gath_mon, sta_group) 
# export(sta_mon, file = "data/stations_monthly.csv") 
```


```{r ggplot_monthly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA 
 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv"))

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values & order them
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

# x$name <- factor(x$name, levels = x$name[order(x$val)])

# plot
ggplot(sta_gath_mon, aes(date, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1909-2018") +
       xlab("") +
       ylab("mm")

#ggplot2::ggsave(filename = "precip_mon.png", 
#                width = 6, height = 6, units = "in")
```

```{r monthly2yearly-precip}
# General Purpose: prepare data for drought index   
# Specific purpose: convert monthly precip to yearly prcp  
 
# load metadata & data 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything())  

# gather monthly values  
sta_gath <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE) 

# create groups 
sta_group <- sta_gath %>% 
  group_by(year,  station) 

# sum monthly precip over a year 
sta_gath_yr <- sta_group %>% 
  summarize(prcp = sum(prcp))  

# spread result - now in years 
sta_yr <- sta_gath_yr %>% 
  spread(station, prcp) %>% 
  filter(year != 1909) %>% 
  filter(year != 2018) %>% 
  ungroup()

rm(sta_mon, sta_gath, sta_gath_yr, sta_group)
# export(sta_yr, file = "data/stations_yearly.csv") 
```

```{r ggplot_yearly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA - yearly 

# NEED TO FIX - screwed up variables 

sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr     <- as.tibble(import("data/stations_yearly.csv"))

# gather monthly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp",  
                   -year, factor_key = TRUE) 

# plot
ggplot(sta_gath, aes(year, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Annual precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

# ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```


# Annual Summaries
```{r yearly_summaries}
# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr   <- as.tibble(import("data/stations_yearly.csv")) 

# gather and summarize yearly values 
# Next step - do by water year???
sta_gath_yr <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

sta_summary_yr <- as.tibble(sta_gath_yr) %>%
  group_by(station) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(desc(med))

sta_summary_yr
# export(sta_summary_yr, file = "data/sta_summary_yr.csv") 
```

# Monthly Summaries
```{r  monthly_summaries}

# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon   <- as.tibble(import("data/stations_monthly.csv")) 

# fix dates
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything()) 

# gather and summarize monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE)

sta_summary_mon <- as.tibble(sta_gath_mon) %>%
  group_by(station, month) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(month) %>%
  arrange(station)

sta_summary_mon 
# export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(month, prcp, group = month)) +
  geom_boxplot() +
  facet_wrap(~station) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

#ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(date, prcp)) +
  geom_line() +
  facet_grid(station~.) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

#ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```

# Correlation Plots with Pearson Coefficients
```{r correlation}
# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
sta_yr <- sta_yr %>%
  arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

# filter NAs
sta_gath_72 <- sta_gath %>%
  filter(year > 1972) 

# spread remaining matrix & arrange from west to east
sta_72 <- sta_gath_72 %>%
  spread(station, prcp) %>%
  select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
sta_M <- cor(sta_72)
corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper = "ellipse", lower = "number", title = "Precipitation station correlation")
```
