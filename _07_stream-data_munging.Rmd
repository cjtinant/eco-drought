
<!--
Exploratory Data Analysis Checklist by Roger Peng 
https://leanpub.com/exdata  

1.0  Formulate your question  
Spend a few minutes to figure out the question you’re really interested in, and narrow it down to be as specific as possible (without becoming uninteresting).

General question:
Are air pollution levels higher on the east coast than on the west coast?
More specific question:
Are hourly ozone levels on average higher in New York City than they are in Los Angeles?

2.0   Read in your data  
Sometimes the data will need some cleaning and every dataset has its unique quirks. The dataset is a comma-separated value (CSV) file, where each row of the file contains one daily measurement of precipation depth.  The readr package can rewrite column names to remove spaces.
> names(ozone) <- make.names(names(ozone))

3.0  Check the dataset 
3.1  Check the number of rows and columns.
3.2  Check the types of data
3.3  Look at the top and the bottom of your data 
3.4  Check your “n”s & NAs 
3.5  Validate with at least one external data source  
4.0  Try the easy solution first to answer question
5.0  Challenge your solution 
6.0  Follow up questions 


## Broad questions:
What is the drought history of the Pine Ridge Reservation?  
Does the drought extent differ across the study area?
What is streamflow variation across the Pine Ridge Reservation?

## Narrower questions:

What is streamflow variation across the Pine Ridge Reservation?

## Analysis Steps:
1.  Data read in from USGS website by dataRetrieval (cfs)
2.  Data saved as array (.JSON), and flat format (.csv) 

# Next STEPS
1. Update variable names
2. Check on next steps from Chapter 2 list 
3. Describe the precipitation seasonality

## Variable naming convention:   
sta          precipitation station  
_meta        metadata  
_raw         the "mostly" raw dataset  
_geXX        data greater than or equal to year XX  
_geXXmYY     data greater than or equal to year XX and month YY   
_ltXXmYY     data less than year XX and month YY   
_clean       intermediate df - clean part of NA split ;-}  
_dirty       intermediate df - NA part of NA split ;-}  
_trans       final df - after cleaning  

gage         USGS streamgage station
_meta        metadata  
_raw         the "mostly" raw dataset  
_list        gage data as a list
_json        gage data as a list in JSON format 
_imp         gage data in imperial units in long format
_dis_imp     gage discharge in imperial units in wide format

## Results: 
The results from the precipitation analysis indicate: 1) an annual trend of increasing aridity across the project area that trends from northwest to southeast that may be a result of the Black Hills rainshadow, 2) the 1900s were the wettest time in regions recorded history.  ??? what about the seasonal trend?


# DOWN the ROAD
# USGS 06447050 UNNAMED TRIB BUZZARD CREEK NR LONG VALLEY, SD -instant meas
https://nwis.waterdata.usgs.gov/usa/nwis/qwdata/?huc_cd=10140202&format=station_list&sort_key=site_no&index_pmcode_00065=3&index_pmcode_00060=4&index_pmcode_00062=5&index_pmcode_72020=6&sort_key=site_no&group_key=county_cd&sitefile_output_format=station_list
-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r library, message=FALSE}
# Sets up the library of packages  
library("here") # identifies where to save work
library("dataRetrieval") # USGS data import
library("EGRET") # Exploration and Graphics for RivEr Trends
library("rio") # more robust I/O - to import and clean data 
library("lubridate") # easier dates
library("tidyverse")
library("janitor") # tools for examining and cleaning dirty data 
#library("DataExplorer") # quick look at NA vals
#library('jsonlite') # tools for working with lists 
#library("magrittr") # provides aliases for easier reading
# library("friendlyeval")

# a useful description of commits:
# http://r-pkgs.had.co.nz/git.html
```

```{r import_streamflow, eval=FALSE}
# this code chunk uses dataRetreval to get discharge in cfs 
# removes provisional values and ice and saves the data as a csv.

# 'gage' is a USGS stream gage
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# get gage ids by USGS watermapper
gage_id <- data.frame(name = c("WHITE R NR NE-SD STATE LINE", 
                              "WHITE R NEAR OGLALA SD", 
                              "WHITE CLAY CR NEAR OGLALA SD", 
                              "WHITE R NEAR INTERIOR SD", 
                              "WOUNDED KNEE CREEK AT WOUNDED KNEE SD",
                              "BEAR IN THE LODGE CR NEAR WANBLEE SD", 
                              "WHITE R NEAR KADOKA SD", 
                              "BLACK PIPE CREEK NR BELVIDERE SD", 
                              "LITTLE WHITE R NEAR MARTIN SD", 
                              "LAKE CR BELOW REFUGE NEAR TUTHILL SD", 
                              "LITTLE WHITE R NEAR VETAL SD", 
                              "SOUTH FORK BAD R NEAR COTTONWOOD SD", 
                              "BAD R NEAR MIDLAND SD"),
                     id = c('06445685', '06446000', '06445980', 
                            '06446500', '06446100', '06446700', 
                            '06447000', '06447230', '06447500', 
                            '06449000',  '06449100', '06440200',
                            '06441000'),
                     stringsAsFactors = FALSE)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Iterate across a list of gage ids by purrr::map_dfr 
# Get gage metadata using dataRetrieval::readNWISsite
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_meta <- map_dfr(gage_id$id, readNWISsite) 
# reorder and rename columns by dplyr
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_meta <- gage_meta %>%
  select(site_no, station_nm, dec_lat_va, dec_long_va, 
         everything()) %>% 
  mutate(site_no = as.character(site_no))
rm(gage_id)

# Turn the site data into a character vector
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# not sure why this didn't work!
#siteNumbers <- gage_meta %>%
#  select(site_no) %>%
#  mutate(site_no = as.character(site_no)) %>%
#  as.vector()

siteNumbers <- c("06445685", "06446000", "06445980", "06446500", 
                 "06446100", "06446700",  "06447000", "06447230",
                 "06447500", "06449000", "06449100", "06440200",
                 "06441000") 

# read in the mean daily flow; statCd = "00003"
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
parameterCd <- '00060' # '00060' should be parameter for discharge 
#gage_raw <- readNWISdv(siteNumbers, parameterCd, startDate = "", 
                   endDate = "", statCd = "00003") 
rm(siteNumbers, parameterCd) 

# convert attribute data to json by jsonlite & readr
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_list <- list(attributes(gage_raw))
gage_json <- serializeJSON(gage_list, digits = 8, pretty = FALSE)

# rename gage vars
# ~~~~~~~~~~~~~~~~~~~
gage_int <- gage_raw %>% 
  rename(discharge = "X_00060_00003") %>%
  rename(code = "X_00060_00003_cd") %>% 
  rename(date = Date) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  arrange(desc(date))

# check ice - starts 2017-10-23 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ice <- gage_int %>%
  filter(code == "P Ice") %>% 
  arrange(date) 

# check provisional values 
# ~~~~~~~~~~~~~~~~~~~~~~~~
provis <- gage_int %>%
  filter(code == "P" | code == "P e" | code == "P <") %>%
  arrange(date) # starts 2016-10-05

# remove data after 2017-10-01
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_int <- gage_int %>%
  filter(year != 2018)

gage_2017 <- gage_int %>%
  filter(year == 2017)

gage_prior <- gage_int %>%
  filter(year < 2017) 

gage_2017 <- gage_2017 %>%
  filter(month != 10) %>% 
  filter(month != 11) %>%
filter(month != 12)

gage_imp <- bind_rows(gage_prior, gage_2017) %>%
  arrange(desc(date)) 
rm(gage_2017, gage_int, gage_list, gage_prior, ice, provis, gage_raw) 

# simplify the naming convention for gages 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_imp <- gage_imp %>% 
  select(site_no, date, discharge, code)

station_nm <- gage_meta %>% 
  select(site_no, station_nm) %>% 
  as.tibble()

abrev <- c("wrr_sta", "wrr_ogl", "wcc_ogl", "wrr_int", "wkc_wok", 
           "blc_wan", "wrr_kad", "blp_bel", "lwr_mar", "lcr_tut", 
           "lcr_vet", "brsf_co", "bad_mid") %>%
  as.tibble() %>%
  rename(sta = value)

abrev <- bind_cols(abrev, station_nm) 

gage_imp <- full_join(abrev, gage_imp, by = "site_no")
gage_meta <- full_join(abrev, gage_meta, by = "site_no")
rm(station_nm, abrev)

# Split and spread data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
gage_dis_imp <- gage_imp %>% 
  select(sta, date, discharge)

gage_dis_imp <- gage_dis_imp %>% 
  spread(key = sta, value = discharge) 

# Add short name to metadata
# ~~~~~~~~~~~~~~~~~~~~~~~~~~
abrev <- gage_imp %>%
  distinct(sta, site_no)

gage_meta <- full_join(abrev, gage_meta, by = "site_no") 

# export and import station df by rio
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# export(gage_raw, "data/gage_raw.csv") 
# write_lines(gage_json, "data/gage_list.json")
# export(gage_imp, "data/gage_imperial.csv") 
export(gage_meta, "data/gage_meta.csv") 
# export(gage_dis_imp, "data/gage_discharge_imp.csv") 
```

```{r}
# Issues: I am having challenges w EGRET to return a batch file.
#   Tried to use 'map' but still have issues.
# Solution: avoid gold plating. Load data individually and bind_cols
# 
#   readNWISDaily(siteNumber, parameterCd = "00060", startDate = "",
#      endDate = "", interactive = TRUE, convert = TRUE)

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Notes - data jumps - should use a different end date to fix
# wcc_ogl - discharge data jumps from 1981-09-29 to 1987-10-01
# whr_int discharge data jumps from 1928-11-30 to 1929-03-01
#         discharge data jumps from 1930-11-30 to 1931-01-20
#         discharge data jumps from 1931-01-20 to 1931-02-01
#         discharge data jumps from 1931-10-16 to 1931-10-18
#         discharge data jumps from 1931-11-30 to 1932-02-01
#         discharge data jumps from 1932-06-30 to 1939-10-02
#         discharge data jumps from 1942-09-29 to 2002-10-01 
# blc_wan <- readNWISDaily("06446700") # this not working!
# lcr_tut discharge data jumps from 1940-09-29 to 1962-08-01 
# bad_mid discharge data jumps from 1979-09-29 to 1984-05-02
#          discharge data jumps from 1984-05-02 to 1984-06-05
#          discharge data jumps from 1984-06-07 to 1984-06-12
#          discharge data jumps from 1984-06-14 to 1984-06-16
#          discharge data jumps from 1984-06-18 to 1984-06-21
#          discharge data jumps from 1984-06-24 to 1991-05-18
#          discharge data jumps from 1991-05-20 to 1991-05-28
#          discharge data jumps from 1991-06-17 to 1991-06-30
#          discharge data jumps from 1991-07-03 to 1993-03-07
#          discharge data jumps from 1993-03-11 to 1993-04-23
#          discharge data jumps from 1993-04-25 to 1993-05-03
#          discharge data jumps from 1993-05-04 to 1993-05-07
#          discharge data jumps from 1993-05-14 to 1993-06-07
#          discharge data jumps from 1993-06-12 to 1993-06-16
#          discharge data jumps from 1993-06-23 to 1993-07-03
#          discharge data jumps from 1993-07-08 to 1993-07-16
#          discharge data jumps from 1993-07-16 to 1993-07-21
#          discharge data jumps from 1993-07-22 to 1993-07-28
#          discharge data jumps from 1993-07-30 to 1993-08-22
#          discharge data jumps from 1993-08-25 to 1994-02-17
#          discharge data jumps from 1994-03-15 to 1994-06-11
#          discharge data jumps from 1994-06-11 to 1994-07-13
#          discharge data jumps from 1994-07-13 to 1995-03-13
#          discharge data jumps from 1995-03-14 to 1995-04-15
#          discharge data jumps from 1995-04-28 to 1995-05-01
#          discharge data jumps from 1995-06-19 to 1995-06-22
#          discharge data jumps from 1995-06-29 to 1996-02-08
#          discharge data jumps from 1996-02-22 to 1996-03-11
#          discharge data jumps from 1996-03-19 to 1996-05-26
#          discharge data jumps from 1996-06-10 to 1996-07-07
#          discharge data jumps from 1996-07-09 to 1996-10-29
#          discharge data jumps from 1996-11-02 to 1997-01-04
#          discharge data jumps from 1997-01-27 to 1997-01-31
#          discharge data jumps from 1997-05-17 to 1997-05-26
#          discharge data jumps from 1997-06-25 to 1997-06-29
#          discharge data jumps from 1997-07-03 to 1997-07-22
#          discharge data jumps from 1997-08-12 to 1997-08-28
#          discharge data jumps from 1997-09-01 to 1998-03-20
#          discharge data jumps from 1998-04-12 to 1998-05-24
#          discharge data jumps from 1998-05-29 to 1998-06-10
#          discharge data jumps from 1998-06-29 to 1998-07-04
#          discharge data jumps from 1998-07-13 to 1998-08-23
#          discharge data jumps from 1998-08-24 to 1998-08-27
#          discharge data jumps from 1998-09-02 to 1998-10-06
#          discharge data jumps from 1998-10-10 to 1998-10-18
#          discharge data jumps from 1998-10-22 to 1998-11-01
#          discharge data jumps from 1998-11-01 to 1998-11-09
#          discharge data jumps from 1998-11-21 to 1999-03-18
#          discharge data jumps from 1999-03-19 to 1999-04-11
#          discharge data jumps from 1999-04-16 to 1999-04-22
#          discharge data jumps from 1999-05-05 to 1999-05-14
#          discharge data jumps from 1999-05-25 to 1999-06-05
#          discharge data jumps from 1999-06-20 to 1999-07-19
#          discharge data jumps from 1999-07-22 to 1999-09-03
#          discharge data jumps from 1999-09-07 to 2000-03-09
#          discharge data jumps from 2000-03-15 to 2000-04-20
#          discharge data jumps from 2000-05-09 to 2001-02-04
#          discharge data jumps from 2001-02-13 to 2001-03-08
#          discharge data jumps from 2001-03-24 to 2001-04-25
#          discharge data jumps from 2001-04-28 to 2003-02-21
#          discharge data jumps from 2003-02-21 to 2003-03-22
#          discharge data jumps from 2003-03-22 to 2004-06-11
#          discharge data jumps from 2004-06-14 to 2005-04-24
#          discharge data jumps from 2005-04-24 to 2005-05-12
#          discharge data jumps from 2005-05-18 to 2005-06-15
#          discharge data jumps from 2005-06-17 to 2006-04-01
#          discharge data jumps from 2006-04-02 to 2015-05-13
#          discharge data jumps from 2015-06-12 to 2015-06-14
#          discharge data jumps from 2015-07-07 to 2015-08-04
#          discharge data jumps from 2015-08-05 to 2015-08-09
#          discharge data jumps from 2015-08-11 to 2016-02-15
#          discharge data jumps from 2016-02-23 to 2016-04-17
#          discharge data jumps from 2016-05-06 to 2016-05-13
#          discharge data jumps from 2016-05-13 to 2016-05-25
#          discharge data jumps from 2016-05-25 to 2016-05-27
#          discharge data jumps from 2016-05-27 to 2016-07-23
#          discharge data jumps from 2016-07-23 to 2016-08-01
#          discharge data jumps from 2016-08-01 to 2017-02-21 
# lwr_mar  discharge data jumps from 1940-09-29 to 1962-08-01 
# rap_far  discharge data jumps from 1989-09-29 to 1990-10-01 
# wcc_ogl  discharge data jumps from 1981-09-29 to 1987-10-01 
# wrr_int  discharge data jumps from 1928-11-30 to 1929-03-01
#           discharge data jumps from 1930-11-30 to 1931-01-20
#           discharge data jumps from 1931-01-20 to 1931-02-01
#           discharge data jumps from 1931-10-16 to 1931-10-18
#           discharge data jumps from 1931-11-30 to 1932-02-01
#           discharge data jumps from 1932-06-30 to 1939-10-02
#           discharge data jumps from 1942-09-29 to 2002-10-01
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# get site numbers from prior metadata
gage_meta <- import("data/gage_meta.csv")
#site_nums <- gage_meta %>%
#  select(sta, site_no) %>% 
#  print()

#wrr_sta <- "06445685" #wrr_ogl <- "06446000" #wcc_ogl <- "06445980" 
#wrr_int <- "06446500" #wkc_wok <- "06446100" #blc_wan <- "06446700" 
#wrr_kad <- "06447000" #blp_bel <- "06447230" #lwr_mar <- "06447500" 
#lcr_tut <- "06449000" #lcr_vet <- "06449100" #brsf_co <- "06440200" 
#bad_mid <- "06441000" 

# add an end date to remove provisional data & gaps in vals
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
startDate    <- "" #Gets earliest date
endDate      <- "2017-09-30"
parameter_cd <- "00060" 
#Sample <- readNWISSample(siteNumber,parameter_cd,startDate,endDate)

# Additional sites to add: 
## Site Number: 06421500 - RAPID CR NEAR FARMINGDALE,SD 
## Site Number: 06408650 - CHEYENNE RIVER NEAR SCENIC, SD 
## Site Number: 06447450 - WHITE RIVER NEAR WHITE RIVER, SD 
## Site Number: 06450500 - LITTLE WHITE R BELOW WHITE RIVER,SD 
## Site Number: 06403700 - CHEYENNE RIVER AT RED SHIRT, SD 
## Site Number: 06406500 - BATTLE CR BELOW HERMOSA,SD 
## Site Number: 06402500 - BEAVER CR NEAR BUFFALO GAP,SD
## Site Number: 06402430 - BEAVER CREEK NEAR PRINGLE, SD 
## Site Number: 06402600 - CHEYENNE R NEAR BUFFALO GAP SD 
# Site Number: 06400875 - HORSEHEAD CR AT OELRICHS,SD
## Site Number: 06449500 - LITTLE WHITE R NEAR ROSEBUD SD
## Site Number: 06400000 - HAT CR NEAR EDGEMONT,SD
## Site Number: 06402000 - FALL R AT HOT SPRINGS,SD 


# load individual gages
# ~~~~~~~~~~~~~~~~~~~~~~~~ 
bad_mid <- readNWISDaily("06441000", parameter_cd, startDate, endDate) 
#blc_wan <- readNWISDaily("06446700") # this not working!
bat_her <- readNWISDaily("06421800", parameter_cd, startDate, endDate) 
bev_buf <- readNWISDaily("06402500", parameter_cd, startDate, endDate) 
bev_pri <- readNWISDaily("06402430", parameter_cd, startDate, endDate) 
blp_bel <- readNWISDaily("06447230", parameter_cd, startDate, endDate) 
brsf_co <- readNWISDaily("06440200", parameter_cd, startDate, endDate) 
che_buf <- readNWISDaily("06402600", parameter_cd, startDate, endDate) 
che_red <- readNWISDaily("06403700", parameter_cd, startDate, endDate) 
che_sce <- readNWISDaily("06408650", parameter_cd, startDate, endDate) 
fal_hot <- readNWISDaily("06402000", parameter_cd, "1947-06-01", endDate) 
hat_edg <- readNWISDaily("06400000", parameter_cd, startDate, endDate) 
hor_oel <- readNWISDaily("06449500", parameter_cd, startDate, endDate) 
lcr_tut <- readNWISDaily("06449000", parameter_cd, startDate, endDate) 
lcr_vet <- readNWISDaily("06449100", parameter_cd, startDate, endDate) 
lwr_mar <- readNWISDaily("06447500", parameter_cd, startDate, endDate) 
lwr_ros <- readNWISDaily("06449500", parameter_cd, startDate, endDate) 
lwr_whr <- readNWISDaily("06450500", parameter_cd, startDate, endDate)
rap_far <- readNWISDaily("06421500", parameter_cd, startDate, endDate)
wcc_ogl <- readNWISDaily("06445980", parameter_cd, startDate, endDate) 
wkc_wok <- readNWISDaily("06446100", parameter_cd, startDate, endDate) 
wrr_int <- readNWISDaily("06446500", parameter_cd, startDate, endDate) 
wrr_kad <- readNWISDaily("06447000", parameter_cd, startDate, endDate)
wrr_ogl <- readNWISDaily("06446000", parameter_cd, startDate, endDate)  
wrr_sta <- readNWISDaily("06445685", parameter_cd, startDate, endDate) 
whr_whr <- readNWISDaily("06447450", parameter_cd,startDate,endDate)



# add short name and abbreviation to gage data 
wrr_sta <- wrr_sta %>% 
  mutate(sta = wrr_sta) %>%
  mutate(site_no = "06445685") 

wrr_ogl <- wrr_ogl %>% 
  mutate(sta = wrr_ogl) %>%
  mutate(site_no = "06446000") 

wcc_ogl <- wcc_ogl %>% 
  mutate(sta = wcc_ogl) %>%
  mutate(site_no = "06445980")

wrr_int <- wrr_int %>% 
  mutate(sta = wrr_int) %>%
  mutate(site_no = "06446500")

wkc_wok <- wkc_wok %>% 
  mutate(sta = wkc_wok) %>%
  mutate(site_no = "06446100")

blc_wan <- blc_wan %>% 
  mutate(sta = blc_wan) %>%
  mutate(site_no = "06446700")

wrr_kad <- wrr_kad %>% 
  mutate(sta = wrr_kad) %>%
  mutate(site_no = "06447000") 

blp_bel <- blp_bel %>% 
  mutate(sta = blp_bel) %>%
  mutate(site_no = "06447230")

lwr_mar  <- lwr_mar  %>% 
  mutate(sta = lwr_mar) %>%
  mutate(site_no = "06447500")

lcr_tut <- lcr_tut %>% 
  mutate(sta = lcr_tut) %>%
  mutate(site_no = "06449000")

lcr_vet <- lcr_vet %>% 
  mutate(sta = lcr_vet) %>%
  mutate(site_no = "06449100")

 brsf_co <- brsf_co %>% 
  mutate(sta = brsf_co) %>%
  mutate(site_no = "06440200")

bad_mid <- bad_mid %>% 
  mutate(sta = bad_mid) %>%
  mutate(site_no = "06441000")

# join together to save
gage_int1  <- bind_rows(wrr_sta, wrr_ogl) 
rm(wrr_sta, wrr_ogl)
gage_int2  <- bind_rows(gage_int1, wcc_ogl) 
rm(gage_int1, wcc_ogl)
gage_int3  <- bind_rows(gage_int2, wrr_int) 
rm(gage_int2, wrr_int)
gage_int4  <- bind_rows(gage_int3, wkc_wok) 
rm(gage_int3, wkc_wok)
gage_int5  <- bind_rows(gage_int4, wrr_kad) 
rm(gage_int4, wrr_kad)
gage_int6  <- bind_rows(gage_int5, blp_bel) 
rm(gage_int5, blp_bel)
gage_int7  <- bind_rows(gage_int6, lwr_mar) 
rm(gage_int6, lwr_mar)
gage_int8  <- bind_rows(gage_int7, lcr_tut) 
rm(gage_int7, lcr_tut)
gage_int9  <- bind_rows(gage_int8, lcr_vet) 
rm(gage_int8, lcr_vet)
gage_int10 <- bind_rows(gage_int9, brsf_co) 
rm(gage_int9, brsf_co)
gage_int11 <- bind_rows(gage_int10, bad_mid) 
rm(gage_int10, bad_mid)
export(gage_int11, "data/gage_cms.csv")

```


```{r daily2monthly-precip}
# continued from above 
# General Purpose: prepare data for drought index 
# Specific purpose: convert daily precip to monthly precip  

# load metadata & data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_day   <- as.tibble(import("data/stations_final2.csv")) 

# remove Murdo, Mission, Long Valley - see above 
#sta_day   <- sta_day %>% 
#  select(-c(lon, mur, mis)) 
#export(sta_day, file = "data/stations_final2.csv")  

# fix date & add year and month 
sta_day <- sta_day %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# gather daily values 
sta_gath <- gather(sta_day, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

# create groups 
sta_group <- sta_gath %>% 
  group_by(year, month, station) 

# sum daily precip over a month 
sta_gath_mon <- sta_group %>% 
  summarize(prcp_tenths = sum(prcp)) %>% 
  mutate(prcp_mm = prcp_tenths/10) %>% 
  select(-prcp_tenths) 

# spread result - now in months  
#   ...and take a bow, because this is MAGIC!  Thnx Tidyverse. 
sta_mon <- sta_gath_mon %>% 
  spread(station, prcp_mm) %>% 
  mutate(day = 1) %>% 
  mutate(date = make_date(year = year, month = month, day = day)) %>% 
  select(date, year, month, everything()) %>% 
  select(-day) %>% 
  ungroup()

rm(sta_day, sta_gath, sta_gath_mon, sta_group) 
# export(sta_mon, file = "data/stations_monthly.csv") 
```


```{r ggplot_monthly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA 
 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv"))

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values & order them
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

# x$name <- factor(x$name, levels = x$name[order(x$val)])

# plot
ggplot(sta_gath_mon, aes(date, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1909-2018") +
       xlab("") +
       ylab("mm")

#ggplot2::ggsave(filename = "precip_mon.png", 
#                width = 6, height = 6, units = "in")
```

```{r monthly2yearly-precip}
# General Purpose: prepare data for drought index   
# Specific purpose: convert monthly precip to yearly prcp  
 
# load metadata & data 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything())  

# gather monthly values  
sta_gath <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE) 

# create groups 
sta_group <- sta_gath %>% 
  group_by(year,  station) 

# sum monthly precip over a year 
sta_gath_yr <- sta_group %>% 
  summarize(prcp = sum(prcp))  

# spread result - now in years 
sta_yr <- sta_gath_yr %>% 
  spread(station, prcp) %>% 
  filter(year != 1909) %>% 
  filter(year != 2018) %>% 
  ungroup()

rm(sta_mon, sta_gath, sta_gath_yr, sta_group)
# export(sta_yr, file = "data/stations_yearly.csv") 
```

```{r ggplot_yearly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA - yearly 

# NEED TO FIX - screwed up variables 

sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr     <- as.tibble(import("data/stations_yearly.csv"))

# gather monthly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp",  
                   -year, factor_key = TRUE) 

# plot
ggplot(sta_gath, aes(year, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Annual precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

# ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```


# Annual Summaries
```{r yearly_summaries}
# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr   <- as.tibble(import("data/stations_yearly.csv")) 

# gather and summarize yearly values 
# Next step - do by water year???
sta_gath_yr <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

sta_summary_yr <- as.tibble(sta_gath_yr) %>%
  group_by(station) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(desc(med))

sta_summary_yr
# export(sta_summary_yr, file = "data/sta_summary_yr.csv") 
```

# Monthly Summaries
```{r  monthly_summaries}

# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon   <- as.tibble(import("data/stations_monthly.csv")) 

# fix dates
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything()) 

# gather and summarize monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE)

sta_summary_mon <- as.tibble(sta_gath_mon) %>%
  group_by(station, month) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(month) %>%
  arrange(station)

sta_summary_mon 
# export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(month, prcp, group = month)) +
  geom_boxplot() +
  facet_wrap(~station) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

#ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(date, prcp)) +
  geom_line() +
  facet_grid(station~.) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

#ggplot2::ggsave(filename = "precip_yr.png", 
#                width = 6, height = 6, units = "in")
```

# Correlation Plots with Pearson Coefficients
```{r correlation}
# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
sta_yr <- sta_yr %>%
  arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

# filter NAs
sta_gath_72 <- sta_gath %>%
  filter(year > 1972) 

# spread remaining matrix & arrange from west to east
sta_72 <- sta_gath_72 %>%
  spread(station, prcp) %>%
  select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
sta_M <- cor(sta_72)
corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper = "ellipse", lower = "number", title = "Precipitation station correlation")
```
