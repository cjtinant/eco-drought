---
title: "02-ch2-5-junk"
author: "CJ Tinant"
date: "10/7/2019"
output: html_document
---

<!--
This R markdown file includes the junk that should be deleted but has not been...

-->

```{r export-table_ecoreg_mae} 
  
# set the numeric columns  
col_key_num <- mae_ecoreg %>% 
  select(-c(model, ecoreg)) %>%  
  names() 
  
# convert tibble to a flextable   
coef_table <- mae_ecoreg %>%    
  flextable() %>%  
  colformat_num(col_keys = col_key_num,  
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%   
  set_header_labels(   
    ecoreg  = "Ecoregion",   
    model  = "Model",   
    MAE = "MAE") %>%    
  autofit() %>%   
  theme_booktabs()  
  
# export a docx of flextables    
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_mae_ecoreg.docx")  
  
rm(coef_table, col_key_num)     
  
```  

```{r delete-this}

# model plot test - ecoreg --- remove later   
#try_plot <- try %>%  
#  mutate(obs_antilog = 10^observed) %>% 
#  mutate(fit_antilog = 10^value)    

# observations vs predictions plot

obs_pred_plot <- fit_model_test %>% 
ggplot(., aes(observed, fitted)) +  
  facet_wrap(vars(model), ncol = 2) +  
# plot points as a density plot to reduce overplotting 
geom_point( 
  shape = ".", 
  color = "gray80")  + 
 # plot points as a density plot to reduce overplotting  
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
#  geom_abline( 
#    intercept = 0,  
#    slope = 1,  
#    size = 0.25) +  
# set limits  
#  scale_x_continuous(limits = c(-1, 4)) + 
#  scale_x_reverse(limits = c(-1, 4)) + 
  scale_y_continuous(limits = c(-1, 4)) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") 

obs_pred_plot 

# residuals plot 
resid_plot <- fit_model_test %>% 
ggplot(., aes(observed, residual)) +  
  facet_wrap(vars(model), ncol = 2) +  
# plot points as a density plot to reduce overplotting 
geom_point( 
  shape = ".", 
  color = "gray80")  + 
 # plot points as a density plot to reduce overplotting  
geom_density2d( 
   color = "gray50")  + 
#  geom_smooth( 
#    method = "lm", 
#    color = "black") + 
# plot a horizontal line 
  geom_abline( 
    intercept = 0,  
    slope = 0,  
    size = 0.25) +  
# set limits  
  scale_x_continuous(limits = c(4, -1)) + 
  scale_y_continuous(limits = c(4, -1)) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") 

resid_plot 

cowplot::plot_grid( 
  freq_plot, obs_pred_plot, resid_plot, 
  ncol = 1, align = "v")   

cowplot::ggsave2("figure/dv_obs_pred_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

# find mean values for any date ---------------------------------------------- 
# -- this is the null model to test if there is a better fit by lasso coefs  
fit_train_ecoreg <- fit_train_ecoreg %>% 
  group_by(ecoreg) %>% 
  mutate(mean_vals = mean(observed)) %>% 
  ungroup() 
  
fit_test_ecoreg <- fit_test_ecoreg %>% 
  group_by(ecoreg) %>% 
  mutate(mean_vals = mean(observed)) %>% 
  ungroup() 

fit_ecoreg_sum <- fit_ecoreg %>%
  group_by(ecoreg, split) %>% 
  mutate(rmse_model = round( 
    rmse(observed, predicted), 
    digits = 2) 
    ) %>% 
  mutate(rmse_ecoreg = round( 
    rmse(observed, mean_vals), 
    digits = 2) 
    ) %>% 
  mutate(mae_model = round( 
    mae(observed, predicted), 
    digits = 2) 
    ) %>% 
  mutate(mae_ecoreg = round( 
    mae(observed, mean_vals), 
    digits = 2) 
    ) %>% 
  summarise(rmse_model = mean(rmse_model), 
            rmse_ecoreg = mean(rmse_ecoreg), 
            mae_model = mean(mae_model), 
            mae_ecoreg = mean(mae_ecoreg) 
            ) %>% 
  ungroup() %>% 
  filter(split == "test") 
 
 
 
 
 
fit_ecoreg %>%  
  filter(split == "test") %>%  
ggplot(aes(observed, predicted)) +  
  facet_grid(cols = vars(ecoreg)) +  
    theme_bw() +  
# plot points as a density plot to reduce overplotting  
geom_density2d(  
   color = "black")  + 
#geom_smooth(  
#    method = "lm", 
#    color = "black") +  
geom_point(aes(observed, mean_vals),  
   color = "gray20")  +  
# plot a 45-degree line  
  geom_abline(  
    intercept = 0,  
    slope = 1,  
    size = 0.25) +  
# set limits  
#  scale_x_log10(limits = c(0.1, 1000)) + 
#  scale_y_log10(limits = c(0.1, 1000)) + 
# set theme  
  theme_bw() + 
  theme(legend.position = "none") 
 
fit_ecoreg %>%  
  mutate(Date = ymd(Date)) %>% 
ggplot(aes(Date, observed)) +  
  facet_wrap(nrow = 2, facets = "ecoreg") +  
    theme_bw() +  
geom_point(shape = ".", 
           color = "gray50") + 
geom_smooth(method = "loess") 
  theme(legend.position = "none") 
 
ggsave("figure/dv1-fit.png", width = 7, height = 7, units = "in")   
  
``` 

```{r plot_glm_fdc_dv_dont-run}

# flow duration curve 
fit_model_test %>% 
  ggplot(aes(x = freq, y = fitted)) +  
  facet_grid(. ~ model) + 
  geom_point(shape = ".",  
             color = "gray80") + 
  geom_density2d(
             color = "gray50")  + 
  geom_line(aes(x = freq, y = observed)) + 
  scale_x_continuous(name = 'Percentage of time flow exceeded',
                     trans = 'probit', 
                     limits = c(0.01, 0.99), 
          breaks = c(0.99,  0.9,   0.75,  0.5,   0.25,  0.1,   0.01),
          labels = c('99%', '90%', '75%', '50%', '25%', '10%', '1%')) + 
  theme_bw() 

```  
# building models based on PC1 & PC2 - delete after making sure log dv works 

```{r get_glm_observations_predictions}

# get observations -- training ----------------------------------------------  

obs_train_pc1    <- tibble(observed = train_pc1$.fittedPC1) 
obs_train_pc2    <- tibble(observed = train_pc2$.fittedPC2) 

# get predictions 
preds_train_pc1 <- map_dfc(models_pc1, predict, newdata = train_pc1) 
preds_train_pc2 <- map_dfc(models_pc2, predict, newdata = train_pc2) 

# combine observations & predictions  
fit_train_pc1 <- bind_cols(obs_train_pc1, preds_train_pc1) %>% 
  mutate(split = "train") %>% 
  as_tibble() 

fit_train_pc2 <- bind_cols(obs_train_pc2, preds_train_pc2) %>% 
  mutate(split = "train") %>% 
  as_tibble() 
  
# get observations -- test ---------------------------------------------------  
obs_test_pc1   <- tibble(observed = test_pc1$.fittedPC1) 
obs_test_pc2   <- tibble(observed = test_pc2$.fittedPC2)  

# get predictions 
preds_test_pc1 <- map_dfc(models_pc1, predict, newdata = test_pc1) 
preds_test_pc2 <- map_dfc(models_pc2, predict, newdata = test_pc2)  

# combine observations & predictions 
fit_test_pc1 <- bind_cols(obs_test_pc1, preds_test_pc1) %>% 
  mutate(split = "test") %>% 
  as_tibble() 

fit_test_pc2 <- bind_cols(obs_test_pc2, preds_test_pc2) %>% 
  mutate(split = "test") %>% 
  as_tibble() 

# clean-up global environment 
rm(obs_train_pc1, obs_train_pc2, preds_train_pc1, preds_train_pc2) 
rm(obs_test_pc1, obs_test_pc2, preds_test_pc1, preds_test_pc2)  
rm(fit_train_pc1, fit_test_pc1, fit_train_pc2, fit_test_pc2) 
``` 

```{r plot_glm_observations_predictions}

# model plot test - pc1 --- remove later 
fit_model_pc1_l <- fit_model_pc1 %>% 
  gather(key = "model", value = "fitted", -c(observed, split)) 

ggplot(fit_model_pc1_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) + 
# plot points as a density plot to reduce overplotting 
  geom_density2d( 
    color = "gray")  + 
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
  geom_abline( 
    intercept = 0, 
    slope = 1, 
    size = 0.25) + 
# set limits 
  xlim(-4, 4) + 
  ylim(-4, 4) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") + 
  labs(title = "Observed vs. fitted hydrologic export coefficients") 

ggsave("figure/pc1-fit.png", width = 7, height = 7, units = "in") 

# model plot test - pc2 --- remove later 
fit_model_pc2_l <- fit_model_pc2 %>% 
  gather(key = "model", value = "fitted", -c(observed, split)) 


ggplot(fit_model_pc2_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) + 
# plot points as a density plot to reduce overplotting 
  geom_density2d( 
    color = "gray")  + 
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
  geom_abline( 
    intercept = 0, 
    slope = 1, 
    size = 0.25) + 
# set limits 
 xlim(-0.5, 0.8) + 
  ylim(-0.5, 0.8) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") +   
  labs(title = "Observed vs. fitted q30 - q1 diff") 
 
ggsave("figure/pc2-fit.png", width = 7, height = 7, units = "in")  

```

```{r get_glm_observations_model-fits}

# get train data model statistics --------------------------------------------  
# pc1 model fits 
model_fits_train_pc1 <- fit_model_pc1 %>% 
  filter(split == "train") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3), 
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc1") 

# pc2 model fits 
model_fits_train_pc2 <- fit_model_pc2 %>% 
  filter(split == "train") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3),  
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc2") 

# get test data model statistics ---------------------------------------------  
# pc1 model fits 
model_fits_test_pc1 <- fit_model_pc1 %>% 
  filter(split == "test") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3), 
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc1") 

# pc2 model fits 
model_fits_test_pc2 <- fit_model_pc2 %>% 
  filter(split == "test") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3),  
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc2") 

# bind the pc1 & pc2 model fit data ------------------------------------------ 
model_fits <- bind_rows(model_fits_train_pc1, model_fits_train_pc2, 
                        model_fits_test_pc1, model_fits_test_pc2) %>% 
  mutate(split = as.character(split)) 

# get number of parameters for each regression method ------------------------
num_param <- coefs %>% 
  map_dfc(~sum(. != 0)) %>% 
  select(-coeff) %>%                   # transpose df 
  gather(model, num_param) %>%                 
  separate(model, c("axis", "model")) 

# join number of parameters 
model_fits <- full_join(model_fits, num_param,    
                  by = c("axis", "model")) 

# get number of observations -------------------------------------------------
n_train <- tibble( axis = c("pc1", "pc2"), 
  split = "train", 
  n = nrow(train_pc1) 
  ) 

n_test <- tibble( axis = c("pc1", "pc2"), 
  split = "test", 
  n = nrow(test_pc1) 
  ) 

num_obs <- bind_rows(n_test, n_train) 

# join obs, params & clean up ------------------------------------------------
model_fits <- full_join(model_fits, num_obs, 
                  by = c("axis", "split")) %>% 
  select(model, split, axis, num_param, n)

rm(model_fits_train_pc1, model_fits_train_pc2, 
   model_fits_test_pc1, model_fits_test_pc2, 
   n_test, n_train, num_obs, num_param)  

``` 

```{r ggplot-glm} 

ggplot(obs_fitted_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) +
    geom_text(
    data = num_obs_text,
    mapping = aes(x = -4, 
                  y = -4,  
                  label = paste("n: ", n, sep="")), 
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 2.8   
    ) # + 


  geom_text(
    data = adjR2_text,
    mapping = aes(x = -4, 
                  y = -4, 
                  label = paste("adjR^2: ", R2adj, sep="")),
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 1.7
    ) + 
  geom_text(
    data = mae_text,
    mapping = aes(x = -4, 
                  y = -4, 
                  label = paste("MAE: ", MAE, sep="")),
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 0.5
    ) + 
  geom_point(aes(shape = split)) + 
  geom_smooth(method = "lm") + 
  geom_abline(intercept = 0, slope = 1, size = 0.25) +
  xlim(-4, 4) + 
  ylim(-4, 4) + 
  theme(legend.position = "none") + 
  labs(title = "Observed vs. fitted hydrologic export coefficients") 
```

```{r prepare-glm-models-old-to-delete} 

# set up random seed & parallel processing for glmnet & caret ---------------- 
seed <- 42 

cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE) 
registerDoMC(cores)  

# set up 'caret' training control & lambda -- 100 possible lambda vals [-2, 2]
set.seed(seed) 
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5, 
                          search = "grid", allowParallel = TRUE) 

lambda <- 10^seq(-2, 2, length = 100) 

# Split the data into training and test set ---------------------------------- 

# working -- create a training index based on groups --- 
# need to add groups to gaged  
set.seed(seed) 
# create a dummy dataset 
groups <- sort(sample(letters[1:4], size = 20, replace = TRUE)) %>% 
  as_data_frame()
  table(groups)  

# make 4 folds - one for each group 
#folds <- groupKFold(groups$value, k = length(unique(groups$value)))  

k <- gaged %>% 
  group_by(ecoreg) %>% 
  summarise(count = n()) %>% 
  select() 

test <- full_join(gaged, k)

k <- 5 
folds <- groupKFold(gaged$ecoreg, k)    

listviewer::jsonedit(folds)  
#lapply(folds, function(x, y) table(y[x]), y = groups$value) 

# another approach & why 
# Simple Splitting with Important Groups

# In some cases there is an important qualitative factor in the data 
# that should be considered during (re)sampling. For example:
# in clinical trials, there may be hospital-to-hospital differences
# with longitudinal or repeated measures data, subjects (or general 
# independent experimental unit) may have multiple rows in the data set, etc.

# There may be an interest in making sure that these groups are not contained 
# in the training and testing set since this may bias the test set performance 
# to be more optimistic. Also, when one or more specific groups are held out, 
# the resampling might capture the “ruggedness” of the model. In the example 
# where clinical data is recorded over multiple sites, the resampling 
# performance estimates partly measure how extensible the model is across sites.

#To split the data based on groups, groupKFold can be used:

#set.seed(3527)
#subjects <- sample(1:20, size = 80, replace = TRUE)
#table(subjects)

set.seed(123)
matris=matrix(rnorm(10),1000,20)
case1 <- as.factor(ceiling(runif(1000, 0, 4)))
case2 <- as.factor(ceiling(runif(1000, 0, 50)))

df <- as.data.frame(matris)
df$case1 <- case1
df$case2 <- case2

df %>%
  select(case1, case2) %>%
  group_by(case1, case2) %>%
  group_indices() -> indeces 

test <- as.data.frame(indeces)
## subjects
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  2  3  2  5  3  5  4  5  4  4  2  5  4  2  3  3  6  7  8  3

#folds <- groupKFold(subjects, k = 15) 


train_index <- createDataPartition(as.factor(gaged$ecoreg), p=0.8)[[1]]  


# create training & test sets  
train_pc1 <- gaged[train_index,] %>% 
  select(-c(sta, pc2_sta))  

train_pc2 <- gaged[train_index,] %>% 
  select(-c(sta, pc1_sta))  

test_pc1 <- gaged[-train_index,] %>% 
  select(-c(sta, pc2_sta)) 

test_pc2  <- gaged[-train_index,] %>% 
  select(-c(sta, pc1_sta))  






#train_index <- gaged %>% 
#  arrange(pc2_sta) %>% 
 # arrange(pc1_sta) %>% 
  
createDataPartition(as.factor(gaged$ecoreg) , p = .8, 
                                  list = FALSE, times = 1, 
                                  groups = groups) 

# create a training index 
groups <- min(5, length(gaged$pc1_sta))   # sets num. splits 

set.seed(seed) 
train_index <- createDataPartition(gaged$pc1_sta , p = .8, 
                                  list = FALSE, times = 1, 
                                  groups = groups) 

# create training & test sets  
train_pc1 <- gaged[train_index,] %>% 
  select(-c(sta, pc2_sta))  

train_pc2 <- gaged[train_index,] %>% 
  select(-c(sta, pc1_sta))  

test_pc1 <- gaged[-train_index,] %>% 
  select(-c(sta, pc2_sta)) 

test_pc2  <- gaged[-train_index,] %>% 
  select(-c(sta, pc1_sta))  

rm(train_index, groups) 
``` 

```{r prepare-glm-models-new} 
  
# set up random seed & parallel processing for glmnet & caret ----------------  
seed <- 42 
cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE) 
registerDoMC(cores)  

# set up 'caret' training control & lambda -- 100 possible lambda vals [-2, 2]
set.seed(seed) 
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5, 
                          search = "grid", allowParallel = TRUE) 

lambda <- 10^seq(-2, 2, length = 100) 

# Split the data into training and test set ----------------------------------  
# create a data partition balanced by ecoregion 
set.seed(seed) 
train_index <- createDataPartition(as.factor(gaged$ecoreg), p=0.8)[[1]]  

# Don't run - this is how to set up by increasing PC1
# groups <- min(5, length(gaged$pc1_sta))   # sets num. splits 
# set.seed(seed) 
# train_index_pc1 <- createDataPartition(gaged$pc1_sta , p = .8, 
#                                  list = FALSE, times = 1, 
#                                  groups = groups) 

# create training & test sets -- the 'all' dataset is used below for plotting 
train_all <- gaged[train_index,] 
 
train_pc1 <- train_all %>% 
  select(-c(sta, q1_depth, .fittedPC2))   
#  select(-c(sta, ecoreg, .fittedPC2))    

train_pc2 <- train_all %>% 
  select(-c(sta, q1_depth, .fittedPC1))   
#  select(-c(sta, ecoreg, .fittedPC1))     

test_all <- gaged[-train_index,] 

test_pc1 <- test_all %>% 
    select(-c(sta, .fittedPC2))   
#  select(-c(sta, ecoreg, .fittedPC2))   

test_pc2 <- test_all %>%  
    select(-c(sta, .fittedPC1))   
#  select(-c(sta, ecoreg, .fittedPC1))   

``` 

```{r build-glm-models_pc1} 

# ridge -- alpha = 0 ---------------------------------------------------------   
set.seed(seed)              # need to set a seed each time you call a rand num 
ridge_pc1 <- train( 
  .fittedPC1 ~.,             # x = 'medv', y = the rest of the columns, from   
  data = train_pc1,       #   the dataset 'train.data' 
  method = "glmnet",      #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values 
  ) 

# lasso -- alpha = 1 --------------------------------------------------------- 
set.seed(seed)             
lasso_pc1 <- train( 
  .fittedPC1 ~.,             
  data = train_pc1,     
  method = "glmnet",      
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 1, lambda = lambda) # df with tuning values
  )

# elastic net -- alpha vals [0, 1] by caret ----------------------------------  
set.seed(seed) 
elastic_pc1 <- train(  
  .fittedPC1  ~.,              # x = 'medv', y = the rest of the columns, from 
  data = train_pc1,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl, 
  tuneLength = 10 
  ) 

# make a list of the models --------------------------------------------------
models_pc1 <- list(  
  ridge = ridge_pc1, 
  lasso = lasso_pc1, 
  elastic = elastic_pc1)   
``` 

```{r build-glm-models_pc2} 

# ridge -- alpha = 0 ---------------------------------------------------------  
set.seed(seed)             # need to set a seed each time you call a rand num 
ridge_pc2 <- train( 
  .fittedPC2 ~.,           # x = 'medv', y = the rest of the columns, from   
  data = train_pc2,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values 
  ) 

# lasso -- alpha = 1 ---------------------------------------------------------  
set.seed(seed)              # need to set a seed each time you call a rand num 
lasso_pc2 <- train( 
  .fittedPC2 ~.,            # x = 'medv', y = the rest of the columns, from   
  data = train_pc2,         #   the dataset 'train.data'  
  method = "glmnet",        #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 1, lambda = lambda) # df with tuning values
  )

# elastic net -- alpha vals [0, 1] by caret ---------------------------------- 
set.seed(seed) 
elastic_pc2 <- train( 
  .fittedPC2 ~.,           # x = 'medv', y = the rest of the columns, from 
  data = train_pc2,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl, 
  tuneLength = 10          # tune length 
  ) 

# make a list of the models --------------------------------------------------
models_pc2 <- list(
  ridge = ridge_pc2, 
  lasso = lasso_pc2,  
  elastic = elastic_pc2)  
 
rm(cores, lambda, reg.ctrl)  

``` 

```{r select-glm-models}

# evaluate performance of models -- ridge, lasso and elastic net --  
#   best model is the one that minimizes the prediction error. 

# pick out the mse from the output lists  
rmse_call_pc1 <- resamples(models_pc1) %>%   
  summary(metric = c("RMSE", "MAE"))  

rmse_call_pc2 <- resamples(models_pc2) %>%   
  summary(metric = c("RMSE", "MAE"))  

# change list to tibble 
rmse_vals_pc1 <- as_tibble(   
  as.data.frame( 
    pluck(rmse_call_pc1, 'statistics')  
    ), rownames = "model" 
  ) %>% 
  mutate(axis = "pc1") 

rmse_vals_pc2 <- as_tibble(  
  as.data.frame( 
    pluck(rmse_call_pc2, 'statistics') 
    ), rownames = "model" 
  ) %>% 
  mutate(axis = "pc2") 

# prepare for plotting
rmse_vals_pc1 <- rmse_vals_pc1 %>% 
  gather(key, val, -c(model, axis))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>% 
  separate(key, c("statistic", "position"), extra = "drop") %>% 
  filter(position != "NA") %>% 
 # unite(mod_pos, statistic, position) %>% 
  spread(position, val)

rmse_vals_pc2 <- rmse_vals_pc2 %>% 
  gather(key, val, -c(model, axis))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>% 
  separate(key, c("statistic", "position"), extra = "drop") %>% 
  filter(position != "NA") %>% 
 # unite(mod_pos, statistic, position) %>% 
  spread(position, val)

rmse_vals <- bind_rows(rmse_vals_pc1, rmse_vals_pc2) 

rm(rmse_call_pc1, rmse_call_pc2, rmse_vals_pc1, rmse_vals_pc2)

```   

```{r plot-glm-models}
# plot model RMSE
#test <- rmse_vals %>% 
#  filter(statistic == "MAE") 

ggplot(rmse_vals) + 
  facet_grid( 
    rows = vars(axis), 
    cols = vars(statistic)) +
  geom_boxplot(aes(x = as.factor(model), 
                   group = model, 
      lower = `1st`, 
      upper = `3rd`, 
      middle = `Median`, 
      ymin = `Min`, 
      ymax = `Max`),
    stat = "identity") + 
  ylim(0, 1.5) + 
  theme_bw() + 
  xlab("") 
#  ylab("RMSE") 

# save ggplot above & clean up 
ggsave("figure/super_reg_acc.png", width = 7, height = 3.5, units = "in")  

#listviewer::jsonedit(rmse_call) 
```

```{r tidy-glm-coefs}

# this code chunk tidies model coefficients  

# ridge coeffs --------------------------------------------------------------- 

coef_ridge_pc1 <- coef(  
  ridge_pc1$finalModel, 
  ridge_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_ridge_pc2 <- coef( 
  ridge_pc2$finalModel, 
  ridge_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_ridge <- full_join(coef_ridge_pc1,coef_ridge_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "ridge") 

rm(coef_ridge_pc1, coef_ridge_pc2) 

# lasso coeffs  --------------------------------------------------------------- 

coef_lasso_pc1 <- coef( 
  lasso_pc1$finalModel, 
  lasso_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_lasso_pc2 <- coef( 
  lasso_pc2$finalModel, 
  lasso_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_lasso <- full_join(coef_lasso_pc1,coef_lasso_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "lasso") 

rm(coef_lasso_pc1, coef_lasso_pc2) 

# elastic coeffs  ------------------------------------------------------------  
coef_elastic_pc1 <- coef( 
  elastic_pc1$finalModel, 
  elastic_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_elastic_pc2 <- coef( 
  elastic_pc2$finalModel, 
  elastic_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_elastic <- full_join(coef_elastic_pc1,coef_elastic_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "elastic") 

rm(coef_elastic_pc1, coef_elastic_pc2) 

# join model coeff ----------------------------------------------------------- 
coefs <- bind_rows(coef_ridge, coef_lasso, coef_elastic) %>% 
  select(coeff, type, everything()) 

coefs <- coefs %>% 
  gather(variable, value, -(coeff:type)) %>% 
  unite(temp, variable, type) %>%
  spread(temp, value) %>% 
  select(coeff, starts_with("pc1"), everything()) 

rm(coef_ridge, coef_lasso, coef_elastic) 

# prepare for export table 
coefs <- coefs %>% 
  filter(coeff != "(Intercept)") %>%           # drop the model intercept term   
  arrange(desc(pc2_lasso)) %>% 
  arrange(desc(pc1_lasso)) %>% 
  mutate(pc2_arrange = abs(pc2_lasso)) %>%     # temp terms to arrange coeffs  
  mutate(pc1_arrange = abs(pc1_lasso)) %>% 
  mutate(arrange = case_when(
    pc1_arrange != 0 ~ pc1_arrange, 
    TRUE ~ pc2_arrange)  
    ) %>% 
  arrange(desc(arrange)) %>% 
  select(coeff, 
         pc1_lasso, pc1_elastic, pc1_ridge, 
         pc2_lasso, pc2_elastic, pc2_ridge)  
``` 

```{r export-table_glm-coeffs} 

# set the numeric columns 
col_key_num <- coefs %>% 
  select(-coeff) %>% 
  names() 

# convert tibble to a flextable  
coef_table <- coefs %>%   
  flextable() %>% 
  colformat_num(col_keys = col_key_num, 
                big.mark=",", 
                digits = 2, na_str = "N/A") %>%  
  add_header_row(values = c("", "", "PC1", "", "", "PC2", "")) %>%  
  set_header_labels(  
  coeff  = "Explanatory variable",  
  pc1_lasso  = "Lasso",  
  pc1_elastic = "Elastic net",    
  pc1_ridge = "Ridge",     
  pc2_lasso  = "Lasso", 
  pc2_elastic = "Elastic net",  
  pc2_ridge = "Ridge") %>%     
  autofit() %>%  
  theme_booktabs()  
 

# export a docx of flextables    
coef_table <- read_docx() %>% 
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_coef.docx")  
    
rm(coef_table, col_key_num)    
 
```

```{r deconvolute_pca}

# 1. prep to deconvolute PC eigens into trend, seasonal, & random comps. ---- 

# 1.1. dv plots too busy -- scaling daily flow values into monthly mean depths  

gage_mon <- gage_dv %>%  
  mutate(yr = year(Date)) %>% 
  mutate(mon = month(Date)) %>%  
  mutate(q30_q1_diff = q30_depth - q1_depth) %>%  
  group_by(sta, mon, yr) %>% 
  summarize(waterYear = first(waterYear),   
            q1_mon     = mean(q1_depth), 
            q7_mon     = mean(q7_depth), 
            q30_mon    = mean(q30_depth),           
            PC1_mon    = mean(.fittedPC1), 
            PC2_mon    = mean(.fittedPC2), 
            q30_q1_mon = mean(abs(q30_q1_diff)),  
            ecoreg      = first(ecoreg) 
            ) %>% 
  mutate(day = 15) %>%   
  ungroup() %>% 
  unite("Date", c("yr", "mon", "day")) %>% 
  mutate(Date = ymd(Date)) %>% 
  gather(key = axis, value = PCval, 
         -c(sta, Date, waterYear, ecoreg, q1_mon, 
            q7_mon, q30_mon, q30_q1_mon) 
         )  %>% 
  as_tibble() 


# 2. calculate parameters to remove season & trend from observed vals ----
# 2.1. find the average monthly frequency & enframe 
mon_freq <- gage_mon  %>% 
      select(-c(waterYear,ecoreg)) %>%  
  arrange(Date) %>% 
  unite(sta_axis, c(sta, axis), sep = ".") %>% 
    ungroup() %>%                       # not sure what this is for.... 
    split(.$sta_axis) %>% 
  map_dfc(~ time_frequency( 
    period = "auto", data = .) 
  ) %>% 
  gather(key = sta.axis, value = freq) %>%  
  summarise(freq = mean(freq), 
            max = max(freq),
            min = min(freq), 
            sd = sd(freq)
            ) 


# 2.2. find the average monthly trend by sta 
mon_trend_sta <- gage_mon  %>% 
      select(-c(waterYear,ecoreg)) %>%  
  arrange(Date) %>% 
  unite(sta_axis, c(sta, axis), sep = ".") %>% 
    ungroup() %>%                       # not sure what this is for.... 
    split(.$sta_axis) %>% 
  map_dfc(~ time_trend( 
    period = "auto", data = .) 
  ) %>% 
  gather(key = sta.axis, value = trend) %>%  
  separate(sta.axis, into = c("sta", "sta2", "axis"), extra = "merge") %>% 
  unite(sta, c(sta, sta2)) %>%           
  spread(axis, trend) 

# 2.3 find average monthly trend 
mon_trend <- mon_trend_sta %>% 
  summarise(trend = median(PC1_mon)) # no differnce in trend between PC1 & PC2 

# 2.4 join and tidy 
decomp_input <- bind_cols(mon_freq, mon_trend) 

rm(mon_freq, mon_trend, eco_join)

# 3. make function to decompose into seasonal, trend, & remainder -------------  
decomp_fun <- function(df) { 
  arrange(df, .data$Date) %>% 
  group_by(.data$sta) %>% 
  time_decompose( 
    target  = PCval, 
    data         = ., 
    method       = "stl", 
    frequency    = decomp_input$freq, 
    trend        = decomp_input$trend, 
    merge        = TRUE, 
    message      = TRUE 
    ) %>% 
  ungroup() %>% 
  anomalize(remainder, 
            method = "gesd", 
            alpha = 0.003) 
}

# 3.1 map_dfr causes issues with indexing; using a manual split and combine  
gage_mon1 <- gage_mon %>% 
  filter(axis == "PC1_mon") 

gage_mon2 <- gage_mon %>% 
  filter(axis == "PC2_mon") 

gage_mon1 <- decomp_fun(gage_mon1) 
gage_mon2 <- decomp_fun(gage_mon2)  

# prepare to rejoin the dataframes 
gage_mon0 <- gage_mon1 %>% 
  select(sta:ecoreg) 
  
gage_mon1 <- gage_mon1 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC1") %>% 
  rename(sta = sta_PC1) %>% 
  rename(Date = Date_PC1)
  
gage_mon2 <- gage_mon2 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC2") %>% 
  rename(sta = sta_PC2) %>% 
  rename(Date = Date_PC2)

# join the datatables   
gage_mon1 <- full_join(gage_mon0, gage_mon1, 
                  by = c("sta", "Date")
                  ) 

gage_mon <- full_join(gage_mon1, gage_mon2, 
                  by = c("sta", "Date")
                  ) 

rm(gage_mon0, gage_mon1, gage_mon2, decomp_input, mon_trend_sta, decomp_fun)  


# create station and ecoregion summaries 
gage_mon <- gage_mon %>% 
  group_by(sta) %>%  
  mutate(sta_q1 = mean(q1_mon)) %>% 
  mutate(sta_q7 = mean(q7_mon)) %>%   
  mutate(sta_q30 = mean(q30_mon)) %>% 
  mutate(sta_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(sta_PC1 = mean(mon_PC1)) %>% 
  mutate(sta_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>% 
  group_by(ecoreg) %>%  
  mutate(ecoreg_q1 = mean(q1_mon)) %>% 
  mutate(ecoreg_q7 = mean(q7_mon)) %>%   
  mutate(ecoreg_q30 = mean(q30_mon)) %>% 
  mutate(ecoreg_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(ecoreg_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>%  
  group_by(Date, ecoreg) %>% 
  mutate(ecoreg_mean_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_seas_PC1 = mean(season_PC1)) %>% 
  mutate(ecoreg_trend_PC1 = mean(trend_PC1)) %>%   
  mutate(ecoreg_remain_PC1 = mean(remainder_PC1)) %>%   
  ungroup() %>% 
  arrange(ecoreg_PC1) %>% 
  mutate(ecoreg = fct_reorder( 
    as.factor(ecoreg), ecoreg_PC1)  # sets up the ordering by pc-axis 
    ) %>% 
  arrange(sta_PC1) %>%   
  mutate(sta = fct_reorder( 
    as.factor(sta), sta_PC1)  # sets up the ordering by pc-axis 
    ) 
```

```{r plot pca}

pca_vars <- import("data/pca_vars.csv")    
 
# Plot PC1 vs PC2 plot 
pc1_pc2_plot <- gage_mon %>% 
  ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ ecoreg) + 
  geom_point(size = 0.5, color = "gray",  
          aes(color = as.factor(sta)) 
          ) +  
  geom_jitter(aes(sta_PC1, sta_PC2),  # station averages 
        shape = 3,                   # cross  
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_PC1, ecoreg_PC2),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
# plot pc eigenvectors ###
    geom_segment(data = pca_vars, 
    aes(x = 0, y = 0, xend = PC1, yend = PC2), 
    arrow = arrow(length = unit(0.03, "npc")) 
    ) + 
  geom_text(data = pca_vars, 
               size = 3,  
               nudge_x = 0.5, 
               nudge_y = -0.1, 
                hjust = 'outside', 
               aes(x = PC1, y = PC2, label = labels) 
            ) +   
# set up the axes ### 
  scale_y_continuous(limits = c(-1, 2.5)) + 
  theme_bw() + 
  theme(legend.position="none")  +  
#  labs(title = "Principal components of mean daily streamflow for water years 1980-2017", #     subtitle = "Monthly averages of N = 42 stations") + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") 


pc1_q7_plot <- gage_mon %>% 
  ggplot(aes(y =mon_PC1, x = q7_mon))  + 
  geom_jitter(size = 0.7, 
              color = "gray" 
              ) + 
  geom_smooth(method = "lm", 
              color = "black", 
              size = 0.7) + 
  facet_wrap( ~ ecoreg) + 
  geom_jitter(aes(sta_q7, sta_PC1),  # station averages 
        shape = 3,                   # cross 
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_q7, ecoreg_PC1),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
  theme_bw() + 
  scale_x_log10() + 
  theme(legend.position="none") + 
  xlab("7-day mean daily flow") + 
  ylab("PC1 axis (94.8%)") 

pc2_q_diff_plot <- gage_mon %>% 
  ggplot(aes(y = mon_PC2, x = q30_q1_mon))  + 
  geom_jitter(size = 0.7, 
              color = "gray" 
              ) + 
  geom_smooth(method = "lm", 
              color = "black", 
              size = 0.7) + 
  geom_jitter(aes(sta_q30_q1, sta_PC2),  # station averages 
        shape = 3,                   # cross 
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_q30_q1, ecoreg_PC2),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
  facet_wrap( ~ ecoreg) + 
  theme_bw() + 
  scale_x_log10() + 
  theme(legend.position="none") + 
  xlab("Difference between 30-day and 1-day mean daily flow depths") + 
  ylab("PC2 axis (4.5%)") 

cowplot::plot_grid( 
  pc1_pc2_plot, pc1_q7_plot, pc2_q_diff_plot, 
  ncol = 1, 
  align = "v" 
) 

cowplot::ggsave2("figure/pc_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(pc1_pc2_plot, pc1_q7_plot, pc2_q_diff_plot) 
```

```{r plot_time-series_PC1}

# prepare to order time series plot 
#pca_plot <- pca_plot %>% 
#    arrange(PC1_ecoreg) 
  
# split out the PC1 axis 
#mon_plot_pc1 <- gage_mon %>% 
#    filter(axis == "PC1_mon") %>% 
#  mutate(ecoreg = fct_reorder(      # sets up the ordering by pc-axis 
#    as.factor(ecoreg), pca_plot$PC1_ecoreg)  
#    ) 

# filter anomonies to plot below   
anom_PC1 <- gage_mon %>% 
  filter(anomaly_PC1 == "Yes")

# PC1-observation plot 
pc1_obs <- gage_mon %>% 
  ggplot(aes(Date, observed_PC1)) + 
  facet_wrap(~ ecoreg) +   
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_mean_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("PC1 axis values")
  
# PC1-seasons plot 
pc1_seas  <- gage_mon %>% 
  ggplot(aes(Date, season_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_seas_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Season (12-months)")

# PC1-trend plot 
pc1_trend  <- gage_mon %>% 
  ggplot(aes(Date, trend_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_trend_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Trend (60-months)")

# PC1 - remainder plot 
pc1_remainder <- gage_mon %>%   
  ggplot(aes(Date, remainder_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size =  0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_remain_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  geom_jitter(aes(Date, ecoreg_remain_PC1), 
              data = anom_PC1, 
              shape = 4, 
              size = 0.7) + 
  scale_y_continuous(limits = c(-4.0, 5.8)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Remainder")


# plot the plots above as a grid & save 
cowplot::plot_grid(
  pc1_obs, pc1_seas, pc1_trend, pc1_remainder, 
  ncol = 1, 
  align = "v"
)

cowplot::ggsave2("figure/pc1-deconv.png", 
        units = "in", 
        width = 7, 
        height = 9)


rm(pc1_obs, pc1_seas, pc1_trend, pc1_remainder, anom_PC1) 
```

```{r Mclust}

# make monthly inputs for clustering 
clust_input <- gage_mon %>% 
  select(sta, Date, waterYear, ecoreg, q1_mon, q7_mon, q30_mon) %>% 
  group_by(sta) %>% 
  distinct(Date, .keep_all = TRUE) %>% 
  ungroup() 

# Get rid of categorical data for Mclust 
X <- clust_input %>%           
  select(-c(sta, Date, waterYear, ecoreg))  

# find Bayesian Information Criterion for mlust 
BIC <- mclustBIC(X) 

# run Mclust
mod <- Mclust(X, x = BIC) 

# get Mclust model results 
mod_glance_mon <-glance(mod) 

#  use broom to tidy up the Mclust model 
mod_tidy_mon <- tidy(mod)           

# add Mclust model classes to the orig data & add ecoregions 
mod_tidy_mon <- augment(mod, gage_mon)   

# sample cluster plot 
factoextra::fviz_mclust(mod, 
                          "classification", 
                          geom = "point", 
                          ellipse.type = "t", 
                          ellipse.level = 0.7, 
                          pointsize = 1.5,  
                          palatte = "npg" 
                          ) 
 
rm(clust_input)  

```

```{r Mclust-plot}
 
# prepare for plotting - reorganize flow by centroids 
mod_tidy_sum <- mod_tidy_mon %>% 
  group_by(.class) %>% 
  summarize(q1_flow = mean(q1_mon), 
            q7_flow = mean(q7_mon), 
            q30_flow = mean(q30_mon), 
            PC1_flow = mean(mon_PC1), 
            PC2_flow = mean(mon_PC2)            
            ) %>% 
  ungroup() %>% 
  arrange(q1_flow) %>% 
  mutate(flow_type = c("type1", "type2", "type3", 
                       "type4", "type5", "type6", 
                       "type7", "type8", "type9")
         ) 

# join flow_type to tidy cluster model 
mod_tidy_mon <- full_join(mod_tidy_mon, mod_tidy_sum,  by = ".class") %>% 
  group_by(.class, ecoreg) %>% 
  mutate(q1_flow_eco = mean(q1_mon)) %>% 
  mutate(q7_flow_eco = mean(q7_mon)) %>%  
  mutate(q30_flow_eco = mean(q30_mon))  %>% 
  mutate(PC1_flow_eco = mean(mon_PC1)) %>% 
  mutate(PC2_flow_eco = mean(mon_PC2)) %>%  
  ungroup() %>% 
  mutate(flow_type = as.factor(flow_type)) 

# plot clusters by ecoregion
mclust_eco_plot  <- mod_tidy_mon %>%  
ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ ecoreg) + 
  geom_jitter(
    aes(color = flow_type,
        shape = ecoreg), 
    size = 2, 
    show.legend = FALSE)  + 
  scale_alpha(".uncertainty") + 
 #   scale_colour_brewer(palette = "Greys") +
# map the title of the geom for the legend  
#  scale_shape_discrete(name="Ecoregion") + 
#  scale_color_hue(name="Flow type") + 
# plot centroids 
  geom_point(
    aes(ecoreg_PC1, 
        ecoreg_PC2),
    show.legend = FALSE, 
    color = "black", 
    shape = 3,                   # cross 
    size = 2.5)  + 
# plot pc eigenvectors ###
    geom_segment(data = pca_vars, 
    aes(x = 0, y = 0, xend = PC1, yend = PC2), 
    arrow = arrow(length = unit(0.03, "npc")) 
    ) + 
  geom_text(data = pca_vars, 
               size = 3,  
               nudge_x = 0.5, 
               nudge_y = -0.1, 
                hjust = 'outside', 
               aes(x = PC1, y = PC2, label = labels) 
            ) +   
# set up the axes ### 
  theme_bw() + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") + 
  theme(legend.position="bottom") 

# plot clusters by flow type 
mclust_flow_plot <- mod_tidy_mon %>%  
ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ flow_type) + 
  geom_jitter(
    aes(shape = ecoreg, 
        color = flow_type), 
    show.legend=FALSE, 
    size = 2)  + 
#    scale_colour_brewer(palette = "Greys") + 
  geom_point(
    aes(PC1_flow_eco, 
        PC2_flow_eco, 
        shape = ecoreg), 
   show.legend=FALSE, 
    color = "black", 
              size = 3)  + 
  scale_alpha(".uncertainty") + 
  theme_bw() + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") + 
  theme(legend.position="bottom") 

# plot grid of ggplots 
cowplot::plot_grid( 
  mclust_eco_plot,
  mclust_flow_plot, 
  ncol = 1, 
  align = "v" 
) 

cowplot::ggsave2("figure/mclust_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(mclust_flow_plot, mclust_eco_plot, X, BIC, 
   mod_tidy_mon, mod_tidy_sum, mod_glance_mon, 
   mod)  


```

```{r bootstrap, eval=FALSE}

# Construct bootstrap confidence intervals of PC axes to identify a difference 
#   in means 
#
# Notes on dabest
# dabest for monthly scale (14,027 obs; 40,000 reps) takes ~2 hours.  
# even with increasing memory, I ran into a memory allocation 
#  limit at 50,000 reps

# increase memory allocation for dabest() 
usethis::edit_r_environ("project") 

# remove 'leftover' attributes introduced by stl 
gage_mon <- gage_mon %>% 
  as_tibble() 

# shorten names of ecoregion to fix overplotting 

# calculate 95% CI for PC1 - 'control' is Pierre Shale Plains 
ci_input_pc1 <- gage_mon %>%      
  dabest(ecoreg, observed_PC1,  
         idx = c("Pierre Shale Plains",     # groups to test - first is control
                 "Pine Ridge Escarpment", 
                 "White River Badlands", 
                 "Black Hills Plateau", 
                 "Keya Paha Tablelands", 
                 "Sand Hills"), 
         paired = FALSE, 
         reps = 40000                     # number of bootstraps 
         ) 

# calculate 95% CI for PC2 - 'control' is Sand Hills
ci_input_pc2 <- gage_mon %>%  
  dabest(ecoreg, observed_PC2,  
         idx = c("Pierre Shale Plains",     
                 "Pine Ridge Escarpment", 
                 "White River Badlands", 
                 "Black Hills Plateau", 
                 "Keya Paha Tablelands", 
                 "Sand Hills"
                 ), 
         paired = FALSE, 
         reps = 40000 
         ) 

# export results as Rdata files 
# how???

# this is the output of the runs 
ci_input_pc1 
co_input_pc2
```

```{r bootstrap-swarmplots} 
#ci_input_pc1 <- load("data/dabest_pc1.rdata") 

ci_plot_pc1 <- plot(ci_input_pc1, 
                    rawplot.type = "swarmplot", 
                    rawplot.ylabel = "PC1 Axis", 
                    tick.fontsize = 7, 
                    axes.title.fontsize = 11, 
                    palette = "Greys"  
                    )    

ci_plot_pc2 <- plot(ci_input_pc2, 
                    rawplot.type = "swarmplot", 
                    rawplot.ylabel = "PC2 Axis", 
                    tick.fontsize = 7, 
                    axes.title.fontsize = 11, 
                    palette = "Greys"  
                    )    

cowplot::plot_grid( 
  ci_plot_pc1,
  ci_plot_pc2,  
  ncol = 1, 
  align = "v" 
) 
# save the plot 

cowplot::ggsave2("figure/ci_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(ci_plot_pc1, ci_plot_pc2)
```

```{r ci_result_table, eval=FALSE}  

# pluck results for summary tables & bind
ci_results_pc1 <- ci_input_pc1 %>% 
  pluck("result") %>% 
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots)) 
  
ci_results_pc2 <- ci_input_pc2 %>% 
  pluck("result") %>% 
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots)) 

ci_table <- rbind(ci_results_pc1, ci_results_pc2) 

# prepare results for presentation 
ci_table <- ci_table %>% 
  mutate(variable =  case_when( 
    variable == "observed_PC1" ~ "PC1", 
    variable == "observed_PC2" ~ "PC2"    
    )
    ) %>% 
  mutate(ci = as.integer(ci)) %>% 
  select(variable, everything())

# convert tibble to a flextable after fixing vars for presentation 
ci_table <- ci_table %>% 
  flextable() %>% 
#  colformat_num(col_keys = col_key_num, 
#                big.mark=",", 
#                digits = 1, na_str = "N/A") %>% 
  set_header_labels(control_group = "Control Group", 
                    test_group = "Test Groups", 
                    control_size = "Control Size",
                    test_size = "Test Size", 
                    func = "Test Statistic", 
                    variable = "Variable",
                    difference = "Mean Difference", 
                    ci = "CI", 
                    bca_ci_low = "Lower Limit", 
                    bca_ci_high = "Upper Limit") %>% 
  autofit() %>% 
  align(., part = "all", align = "center") %>% 
  theme_booktabs() 

  
ci_table2 <- read_docx() %>% 
  body_add_flextable(value = ci_table)  

print(ci_table2, target = "output/ci_table.docx") 

rm(ci_results_pc1, ci_results_pc2, ci_table2, ci_table) 

```

```{r pca_decomp_table} 

# summarize observations of the time series into observations for a summary table 
mon_sum_gath <- gage_mon %>% 
  group_by(ecoreg, axis) %>% 
  summarize(value = round(mean(observed), digits = 2), 
            obs_eco    = mean(abs(observed)), 
            seas_eco   = mean(abs(season)), 
            trend_eco  = mean(abs(trend)), 
            remain_eco = mean(abs(remainder)) 
            ) %>%  
  ungroup() %>% 
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc) %>% 
  # readjust to 100% 
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>% 
  mutate(axis = str_remove_all(axis, "_mon")) %>% 
  arrange(axis, value) %>% 
  select(-c(obs_eco:sum)) 

# split & mutate column names by PC  
mon_sum_pc1 <- mon_sum_gath %>% 
  filter(axis == "PC1") %>% 
  select(-axis) %>% 
  rename_all(function(x) paste0("PC1_", x)) 
 
mon_sum_pc2 <- mon_sum_gath %>% 
  filter(axis == "PC2") %>% 
  select(-axis) %>% 
  rename_all(function(x) paste0("PC2_", x)) 

# find averages across all the stations 
mon_sum_pc1_all <- gage_mon %>% 
  filter(axis == "PC1_mon") %>% 
  summarize(value = round(mean(observed), digits = 2), 
            obs_eco    = mean(abs(observed)), 
            seas_eco   = mean(abs(season)), 
            trend_eco  = mean(abs(trend)), 
            remain_eco = mean(abs(remainder)) 
            ) %>%  
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc)  %>% 
  # readjust to 100% 
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>% 
#  mutate(axis = str_remove_all(axis, "_mon")) #%>% 
#  arrange(axis, value) %>% 
  select(-c(obs_eco:sum)) %>% 
  mutate(ecoreg = "All Stations") %>% 
  rename_all(function(x) paste0("PC1_", x)) 
  
  
mon_sum <- bind_rows(mon_sum_pc1_all, mon_sum_pc1) %>% 
  select(ecoreg = PC1_ecoreg, everything()) 

rm(mon_sum_gath) 

``` 

```{r plot_time-series_PC2, eval=FALSE}


# split out the PC1 axis 
mon_plot_pc2 <- gage_mon %>% 
    filter(axis == "PC2_mon") %>% 
  mutate(ecoreg = fct_reorder(
    as.factor(ecoreg), obs_eco_mean)  # sets up the ordering by pc-axis 
    ) 

mon_sum_pc2 <- mon_plot_pc2 %>% 
#  select(sta, ecoreg, observed, season, trend, remainder) %
  group_by(ecoreg) %>% 
  summarize(median_pc2 = round(median(obs_eco), digits = 2), 
            obs_eco    = median(abs(obs_eco)), 
            seas_eco   = median(abs(seas_eco)), 
            trend_eco  = median(abs(trend_eco)), 
            remain_eco = median(abs(remain_eco)) 
            ) %>% 
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc) %>% 
  # readjust to 100% 
  mutate(interann_var   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(multi_yr_var  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>%
  ungroup() %>% 
  select(ecoreg, median_pc2, interann_var, multi_yr_var, remainder)

# PC2-observation plot 
pc2_obs <- mon_plot_pc2 %>% 
  ggplot(aes(Date, observed)) + 
  facet_wrap(~ ecoreg) +   
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, obs_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("")

# PC2-seasons plot 
pc2_seas  <- mon_plot_pc2 %>% 
  ggplot(aes(Date, season)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, seas_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_seas 

# PC2-trend plot 
pc2_trend  <- mon_plot_pc2 %>% 
  ggplot(aes(Date, trend)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, trend_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_trend 

# PC2 - remainder plot 
pc2_remainder <- mon_plot_pc2 %>% 
  ggplot(aes(Date, remainder)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size =  0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, remain_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_remainder 

plot_grid(
  pc2_obs, pc2_seas, pc2_trend, pc2_remainder, 
  ncol = 1, 
  align = "v"
)


ggsave2("figure/pc2-deconv.png", 
        units = "in", 
        width = 7, 
        height = 9)

```

```{r summarize-results-PCA_delete,eval=FALSE}

#  4. Create summaries of the results ----------------------------------------



pca_ann <- gage_dv %>% 
  group_by(sta, waterYear) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),  
            ecoreg      = first(ecoreg)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean) 

pca_sum <- gage_dv %>% 
  group_by(sta) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),  
            ecoreg      = first(ecoreg)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean)  
```

```{r to-delete}
# read in geopackage data of zonal summaries for watersheds 
#gage_summary <- st_read("sp_data/eco-drought.gpkg", 
#                       layer = "gage_summary", 
#                       as_tibble = TRUE) %>% 
#  st_drop_geometry() 

# convert environmental data to tibbles -- delete after fixing plots 
#gage_summary <- gage_summary %>% 
#  as_tibble() %>% 
#  modify_if(., is.factor, as.character) 
```

```{r pc-plot_delete,eval=FALSE} 



p1 <- autoplot(pca_matrix, 
               data = gage_dv, 
               scale = 0, 
               colour = "ecoreg", 
               size = 0.1, 
               alpha = 0.5) 
p1 + theme_bw()


ggplot(pca_mon, aes(PC1_mean, PC2_mean)) + 
  geom_jitter(
    aes(
      shape = ecoreg, 
      colour = ecoreg), 
    size = 1.0
    ) + 
  theme_classic()


```

```{r things}



# prepare data 
#pca_input <- gage_input %>%  
#  select(date, sta, q, q7, q30, contrib_drain_area_va)

# training data: 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
#  01    0.013      0.112      0.265     0.942   0.050   0.942   0.992  
#  02   -0.005      0.070      0.223     0.949   0.047   0.949   0.994    
#  03   -0.102     -0.136     -0.148     0.959   0.035   0.959   0.994   
#  04   -0.008      0.071      0.242     0.947   0.047   0.947   0.994   
#  05   -0.056     -0.113     -0.187     0.953   0.041   0.953   0.994 
#  06   -0.031     -0.070     -0.120     0.960   0.034   0.960   0.994 
#  07   -0.055     -0.067     -0.060     0.966   0.029   0.966   0.995 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# vars:
# run  PC  q1_depth  q7_depth  q30_depth       
#   01  1    0.577      0.589     0.566   
#   01  2    0.584      0.186    -0.790     
#   02  1    0.578      0.588     0.567       
#   02  2    0.567      0.210    -0.796      
#   03  1    0.578      0.584     0.569           
#   03  2    0.545      0.243    -0.803       
#   04  1    0.578      0.588     0.566           
#   04  2    0.571      0.205    -0.795       
#   05  1    0.578      0.586     0.568           
#   05  2    0.555      0.228    -0.800       
#   06  1    0.578      0.585     0.569            
#   06  2    0.553      0.233    -0.800       
#   06  1    0.578      0.583     0.571             
#   06  2    0.542      0.248    -0.803  
#   07
#   07
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#Mclust model:
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# num  model  log.likeli  n  df   BIC     ICL     purpose 
#  01  VEV     116.42    26  25  151.4   151.0   estimate NA 
#  02  VEV      82.65    23  17  112.0   111.6   estimate NA 
#  03  VVV     139.30    29  19  214.6   214.5   estimate NA   
#  04  EEV     161.53    24  65  116.5   116.4   estimate NA    
#  05  EEE     139.33    24  37  161.6   161.0   estimate NA    
#  06  EEV     130.93    27  19  199.2   199.2   estimate NA    
#  07  VVV     151.58    31  19  237.9   237.7   estimate NA    
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#    Cl9#
#  01   14      8     4    na    na    na    na      na      na  
#  02   15      8    na    na    na    na    na      na      na  
#  03   14     15    na    na    na    na    na      na      na  
#  04    3      4     3     2     4     2     1       1       4    
#  05    2      9     3     1     5     2     1       1      na  
#  06   14     13    na    na    na    na    na      na      na  
#  07    7     24    na    na    na    na    na      na      na  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# a final cluster analysis was conducted after missing values were found 
# two runs for final cluster analysis are conducted 

# | Run_num | description | num_obs | 
# |:-------:|:-----------:|:-------:|
# |    1    | all years   | 317,036 |                                     
# |    2    | wet years   | 147,188 |
# |    2    | dry years   | 169,848 |

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the series of station runs are as follows: 

pca_eigen 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
# all   -0.082     -0.086     -0.027     0.967   0.028   0.967   0.995 
# wet   -0.060     -0.078     -0.089     0.963   0.031   0.963   0.994 
# dry   -0.145     -0.185     -0.211     0.963   0.032   0.963   0.995
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

pca_vars 
# run  PC  q1_depth  q7_depth  q30_depth   notes    
#  all  1    0.578      0.583     0.571           
#  wet  1    0.578      0.584     0.570           
#  dry  1   -0.578     -0.584    -0.570   need to inverse
#  all  2    0.540      0.251    -0.803       
#  wet  2    0.546      0.243    -0.802    
#  dry  2   -0.546     -0.243     0.802   need to inverse   

# eigenvecter:
#   96% of covarience is explained by PCA1 & 4% of varience by PCA2
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30; 
#         approximates hydrologic export 
#   PC2 - large opposite loadings of Q1 & Q30  
#         contribution of baseflow vs event-flow
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Preparing to cluster by the central tendancy
gage_pca_sum <- gage_pca_sum %>% 
  arrange(sta)

gage_clust <- gage_pca_sum %>% 
  select(q1_mean:q30_mean)

gage_clust_meta <- gage_pca_sum %>% 
  select(sta)

# apply model-based-clustering, extract results & add metadata
gage_clust_l <- Mclust(gage_clust) 

gage_clust <- as_tibble(gage_clust_l$z) 
gage_clust <- bind_cols(gage_clust_meta, gage_clust) 

# drop low probs - join & rearrange 
gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.5) 

gage_clust <- full_join(gage_pca_sum, gage_clust, 
                             by = "sta")

gage_clust <- gage_clust %>% 
  dplyr::select(group, sta, everything()) %>% 
  arrange(sta) %>%  
  arrange(group)

# join cluster to PCA data & arrange
gage_mod <- full_join(gage_pca, gage_clust, by = "sta") 

gage_mod <- gage_mod %>% 
  select(group, sta, everything()) %>% 
  select(-c(q:q30_tr)) %>% 
  select(-eigen_dist) 

summary(gage_clust_l) # Print a summary 

#clean up environment 
rm(pca_input, pca_matrix, pca_check, pca_eigen, pca_vars) 
rm(gage_clust_meta, gage_pca_sum, gage_pca, gage_input) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Mclust model:

# num  model  log.likeli  n  df   BIC     ICL     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all  VEV     190.89    31  41  241.0   240.8  
# wet  EEE     175.15    31  29  250.7   249.4  
# dry  EEE     165.01    31  37  203.0   202.6   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Clustering table:
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all    7     11     5     3     5    na    na      na     
# wet    3      5     1     4    16     2    na      na    
# dry    4     12     5     1     1     6     1       1     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Model-based clustering selected a model with two-three components 
# (i.e. clusters). The optimal selected model name is VEV model. 
# That is the five components are ellipsoidal with varying volume, 
# and orientation, and equal shape. The summary contains also the 
# clustering table specifying the number of observations in each 
# clusters.

tidy(pca_matrix, matrix = "u")

test <- augment(pca_matrix, pca_input) 

```

# PRCP-SPI -- OLD CODE to delete 
```{r import_prcp, eval=FALSE}

# gather daily values  
#sta_gath <- sta_dv %>%  
#  gather(key = "sta", value = "prcp",  
#         -c(date, year, month, name, sta_id)  
#         )  
# make a table for manual import  
# sta <- as.tibble(select(station_meta, 1:2))  


<!--
## Fixing NA values  
These are the stations that NA values were fixed
NAs are caused in part by different start dates  
OEL - OELRICHS    1893 - 2018  - fixed with Murdo  
COT - COTTONWOOD  1909 - 2018  - fixed with Interior & Murdo  
RAP - RAPID CITY  1948 - 2018  - fixed with Interior    
INT - INTERIOR    1949 - 2018  - fixed with Cottonwood & Murdo    
ORA - ORAL        1971 - 2018  - fixed with Oelrichs & Harrison  
-->

# Results: 
# We can drop Ainsworth, Harrison that are outside of study area &
# Oral would be better than Hot Springs for elev, location & coverage 
#  geom_text(data = sta_meta_orig, aes(label = name), size = 2.5) +

## Old Stations and reason for removal, if removed
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US - removed for short record  
# GHCND:USC00253615 HARRISON, NE US - removed for outside region  
#GHCND:USC00396212 OELRICHS, SD US    
#GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US - too close to Oelrichs  
#GHCND:USC00396304 ORAL, SD US    
#GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US    
##GHCND:USC00394184 INTERIOR 3 NE, SD US    
#GHCND:USC00391972 COTTONWOOD 2 E, SD US   
#GHCND:USC00394983 LONG VALLEY, SD US                  
#GHCND:US1SDJK0006 KADOKA 0.3 N, SD US - removed short period record   
#GHCND:USC00395891 MURDO, SD US - removed for outside region   
#GHCND:USC00395638 MISSION 14 S, SD US    
#GHCND:USC00250050 AINSWORTH, NE US 


# iterate across a list of station ids by purrr::map 
# to get NOAA station meta data using rnoaa::ncdc_stationsn
# the output is a list of 7 x 2 x 9.n
# flatten into a dataframe by purrr::flatten 
# reorder and rename columns by dplyr 


# Also possible to check station id with the mapping tool at:
# https://www.ncdc.noaa.gov/cdo-web/datatools/findstation 
# iterate across a list of station ids by purrr::map 
# to get NOAA station meta data using rnoaa::ncdc_stationsn
# the output is a list of 7 x 2 x 9.n
# flatten into a dataframe by purrr::flatten 
# reorder and rename columns by dplyr 
# save the station df by rion
sta_input <- data.frame(name = c('RAPID CITY RGNL AP',  
                                 'HOT SPRINGS',  
                                'OELRICHS',  
                                'INTERIOR 3 NE',  
                                'COTTONWOOD 2 E', 
                                'LONG VALLEY',
                                'HARRISON',
                                'AINSWORTH, NE',
                                'MURDO, SD US',
                                'MISSION 14 S, SD US',
                                'ORAL, SD US',
                                'KADOKA 0.3 N, SD US'),
                        id = c("GHCND:USW00024090", 
                               "GHCND:US1SDFR0001",
                               "GHCND:USC00396212",
                               "GHCND:USC00394184",
                               "GHCND:USC00391972",
                               "GHCND:USC00394983",
                               "GHCND:USC00253615",
                               "GHCND:USC00250050",
                               "GHCND:USC00395891",
                               "GHCND:USC00395638",
                               "GHCND:USC00396304",
                               "GHCND:US1SDJK0006"),
                        stringsAsFactors = FALSE)

station <- map(sta_input$id, ncdc_stations)
station <- flatten_dfr(station)

station <- station %>%
  rename(lat = latitude) %>%
  rename(lon = longitude) %>%
  select(id, name, lat, lon, everything())

# export(station, "data/sta_meta_orig.csv")  

# id                name                              
# <chr>             <chr>                             
# GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US        
# GHCND:USC00396212 OELRICHS, SD US                   
# GHCND:USC00394184 INTERIOR 3 NE, SD US              
# GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# GHCND:USC00394983 LONG VALLEY, SD US                
# GHCND:USC00253615 HARRISON, NE US                   
# GHCND:USC00250050 AINSWORTH, NE US                  
# GHCND:USC00395891 MURDO, SD US                      
# GHCND:USC00395638 MISSION 14 S, SD US               
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US           
# GHCND:USC00396304 ORAL, SD US                       
# GHCND:US1SDJK0006 KADOKA 0.3 N, SD US    
```

```{r import_site-data, eval=FALSE}
# site
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
site <- import("data/Chemistry-1993-2013_17Mar21.csv")
eco  <- import("data/MacroSummaries.csv")

eco <- eco %>%
  clean_names() %>%
  select(1:2) %>%
  rename(id = station) %>%
  distinct(id, .keep_all = TRUE)
  
site <- site %>%
  clean_names() %>% 
  arrange(sample_sites) %>%
  filter(sample_sites != "Bear in the Lodge USGS1") %>% 
  filter(sample_sites != "Bear in the Lodge USGS2") %>%
  filter(sample_sites != "Bear in the Lodge USGS3") %>%
  filter(sample_sites != "Black Pipe II") %>% 
  filter(sample_sites != "Corn Creek I") %>%
  filter(sample_sites != "Little Corn Creek I") %>% 
  filter(sample_sites != "Medicine Root II") %>% 
  filter(sample_sites != "Porcupine Lagoon downstream") %>%
  filter(sample_sites != "Porcupine Lagoon upstream") %>%
  filter(sample_sites != "Pine Ridge Lift Station Downstream") %>%
  distinct(sample_sites, .keep_all = TRUE) %>%
  select(1:4) %>%
  select(-1) %>%
  rename(name = sample_sites) %>%
  rename(lat = latitude) %>%
  rename(lon = longitude) %>%
  mutate(lon = -1 * lon)

site_id <- data.frame(id = c("AMH1", "BEA1", "BEA2", "BEA3", "BEL1", 
                              "BEL2", "BLP1", "BUZ1", "CHR1", "CHR2",
                              "CRA1", "EAN1", "EAN2", "LWR1", "LWR2", 
                              "LWR3", "LWR4", "LON1", "LOD1", "MER1", 
                              "MER2", "MER3", "MER4", "NFL1", "PAS1", 
                              "PAS2", "PAS3", "POR1", "POR2", "POR3", 
                              "POT1", "RED1", "WCC1", "WCC2", "WCC3", 
                              "WHR1", "WHR2", "WHR3", "WHR4", "WHR5", 
                              "WOL1", "WOK1", "WOK2", "WOK3", "WOK4"),
                       name = c("American Horse I", 
                               "Bear Creek I", 
                               "Bear Creek II", 
                               "Bear Creek III", 
                               "Bear in the Lodge I", 
                               "Bear in the Lodge II", 
                               "Black Pipe I", 
                               "Buzzard Creek I", 
                               "Cheyenne River I", 
                               "Cheyenne River II", 
                               "Craven Creek I", 
                               "Eagle Nest I", 
                               "Eagle Nest II", 
                               "Little White River I",  
                               "Little White River II", 
                               "Little White River III", 
                               "Little White River IV", 
                               "Long Creek I", 
                               "Lost Dog Creek I", 
                               "Medicine Root I", 
                               "Medicine Root II", 
                               "Medicine Root III", 
                               "Medicine Root IV", 
                               "No Flesh Creek I", 
                               "Pass Creek I", 
                               "Pass Creek II", 
                               "Pass Creek III", 
                               "Porcupine Creek I", 
                               "Porcupine Creek II", 
                               "Porcupine Creek III", 
                               "Potato Creek", 
                               "Red Water Creek", 
                               "White Clay Creek I", 
                               "White Clay Creek II", 
                               "White Clay Creek III", 
                               "White River I", 
                               "White River II", 
                               "White River III", 
                               "White River IV", 
                               "White River V", 
                               "Wolf Creek I", 
                               "Wounded Knee I", 
                               "Wounded Knee II", 
                               "Wounded Knee III", 
                               "Wounded Knee IV"),
                      stringsAsFactors = FALSE)

site <- full_join(site, site_id, by = "name")  

site <- site %>%  
  drop_na()  

site <- full_join(site, eco, by="id")  

site <- site %>%  
  replace_na(list(ecoregion = "Tablelands")) %>%  
  filter(id != "CHR1") %>%  
  filter(id != "CHR2") %>%  
  mutate(ecoregion = case_when(  
    id == "WHR1" ~ "Tablelands",  
    id == "WHR2" ~ "Badlands",  
    id == "WHR3" ~ "Badlands",  
    id == "WHR4" ~ "Badlands",  
    id == "WHR5" ~ "Badlands",  
    TRUE ~ as.character(ecoregion)))  
  
rm(site_id, eco)  
# export(site, "data/site_meta.csv")  
```  

```{r download-precip, eval=FALSE}
# DL Global Historical Climatology Network (GHCN) Daily Data   
# Note: to flatten a list use: Reduce(rbind, list)  

# load metadata 
sta_meta <- import("data/sta_meta_orig.csv") 

# make a table for manual import
# sta <- as.tibble(select(station_meta, 1:2))

# id                name                              
# <chr>             <chr>                             
# GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# GHCND:US1SDFR0001 HOT SPRINGS 0.5 SSW, SD US        
# GHCND:USC00396212 OELRICHS, SD US                   
# GHCND:USC00394184 INTERIOR 3 NE, SD US              
# GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# GHCND:USC00394983 LONG VALLEY, SD US                
# GHCND:USC00253615 HARRISON, NE US                   
# GHCND:USC00250050 AINSWORTH, NE US                  
# GHCND:USC00395891 MURDO, SD US                      
# GHCND:USC00395638 MISSION 14 S, SD US               
# GHCND:US1SDCS0027 HERMOSA 10.3 ESE, SD US           
# GHCND:USC00396304 ORAL, SD US                       
# GHCND:US1SDJK0006 KADOKA 0.3 N, SD US    

# download data from NOAA
sta_rap <- meteo_tidy_ghcnd('USW00024090', keep_flags = FALSE, 
                         var = "PRCP")
sta_hot <- meteo_tidy_ghcnd('US1SDFR0001', keep_flags = FALSE, 
                         var = "PRCP")
sta_oel <- meteo_tidy_ghcnd('USC00396212', keep_flags = FALSE, 
                         var = "PRCP")
sta_int <- meteo_tidy_ghcnd('USC00394184', keep_flags = FALSE, 
                         var = "PRCP")
sta_cot <- meteo_tidy_ghcnd('USC00391972', keep_flags = FALSE, 
                         var = "PRCP")
sta_lon <- meteo_tidy_ghcnd('USC00394983', keep_flags = FALSE, 
                         var = "PRCP")
sta_har <- meteo_tidy_ghcnd('USC00253615', keep_flags = FALSE, 
                         var = "PRCP")
sta_ain <- meteo_tidy_ghcnd('USC00250050', keep_flags = FALSE, 
                         var = "PRCP")
sta_mur <- meteo_tidy_ghcnd('USC00395891', keep_flags = FALSE, 
                         var = "PRCP")
sta_mis <- meteo_tidy_ghcnd('USC00395638', keep_flags = FALSE, 
                         var = "PRCP")
sta_her <- meteo_tidy_ghcnd('US1SDCS0027', keep_flags = FALSE, 
                         var = "PRCP")
sta_ora <- meteo_tidy_ghcnd('USC00396304', keep_flags = FALSE, 
                         var = "PRCP")
sta_kad <- meteo_tidy_ghcnd('US1SDJK0006', keep_flags = FALSE, 
                         var = "PRCP")

# save the data
# export(sta_rap, file = "data/sta_rap.csv")
# export(sta_hot, file = "data/sta_hot.csv")
# export(sta_oel, file = "data/sta_oel.csv")
# export(sta_int, file = "data/sta_int.csv")
# export(sta_cot, file = "data/sta_cot.csv")
# export(sta_lon, file = "data/sta_lon.csv")
# export(sta_har, file = "data/sta_har.csv")
# export(sta_ain, file = "data/sta_ain.csv")
# export(sta_mur, file = "data/sta_mur.csv")
# export(sta_mis, file = "data/sta_mis.csv")
# export(sta_her, file = "data/sta_her.csv")
# export(sta_ora, file = "data/sta_ora.csv")
# export(sta_kad, file = "data/sta_kad.csv")
```  

```{r merge-precip-data, eval=FALSE}

# load metadata 
sta_meta <- import("data/sta_meta_fin.csv") 
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv")) 

# these were downloaded from NOAA on 2018-06-01 

# stations dropped in the subsequent analysis
# sta_her <- import(file = "data/sta_her.csv") # n = 304 obs so drop
# sta_hot <- import(file = "data/sta_hot.csv") # dropped by Theissen
# sta_lon <- import(file = "data/sta_lon.csv") # dropped by Theissen
# sta_har <- import(file = "data/sta_har.csv") # dropped by Theissen
# sta_ain <- import(file = "data/sta_ain.csv") # dropped by Theissen
# sta_kad <- import(file = "data/sta_kad.csv") # dropped by Theissen

# import prior saved files
sta_oel <- import(file = "data/sta_oel.csv") # 
sta_cot <- import(file = "data/sta_cot.csv") # 
sta_rap <- import(file = "data/sta_rap.csv") # needs fixed & saved 
sta_int <- import(file = "data/sta_int.csv") # 
sta_mis <- import(file = "data/sta_mis.csv") # 
sta_ora <- import(file = "data/sta_ora.csv") # 
sta_mur <- import(file = "data/sta_mur.csv") # need for NA vals 

# join tables and rename prcp cols - data is in mm 
# going from oldest to youngest; 'date' is chr 
merge_sta1 <- full_join(sta_oel, sta_cot, by = "date") 
merge_sta1 <- merge_sta1 %>% 
  rename(oel = prcp.x) %>% 
  rename(cot = prcp.y) %>% 
  select(-id.x, -id.y) 
rm(sta_oel, sta_cot) 

merge_sta2 <- full_join(merge_sta1, sta_rap, by = "date") 
merge_sta2 <- merge_sta2 %>% 
  rename(rap = prcp) %>% 
  select(-id) 
rm(sta_rap) 

merge_sta3 <- full_join(merge_sta2, sta_int, by = "date") 
merge_sta3 <- merge_sta3 %>% 
  rename(int = prcp) %>% 
  select(-id) 
rm(sta_int) 

merge_sta4 <- full_join(merge_sta3, sta_mis, by = "date") 
merge_sta4 <- merge_sta4 %>% 
  rename(mis = prcp) %>% 
  select(-id) 
rm(sta_mis) 

merge_sta5 <- full_join(merge_sta4, sta_ora, by = "date") 
merge_sta5 <- merge_sta5 %>% 
  rename(ora = prcp) %>% 
  select(-id) 
rm(sta_ora) 

# final join & fix date & add year and month 
sta <- full_join(merge_sta5, sta_mur, by = "date") 
sta <- sta %>% 
  select(-starts_with("id")) %>% 
  rename(mur = prcp)  %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

rm(sta_mur, merge_sta1, merge_sta2, merge_sta3, merge_sta4,
   merge_sta5) 

# export(sta, file = "data/stations.csv") 
```  

```{r munge-precip-data-oral, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Oral  

# Variable naming convention:   
# sta          precipitation station  
# _har         Harrision precip station - used to fill NA vals  
# _meta        metadata  
# _raw         the "mostly" raw dataset  
# _geXX        data greater than or equal to year XX  
# _geXXmYY     data greater than or equal to year XX and month YY   
# _ltXXmYY     data less than year XX and month YY   
# _clean       intermediate df - clean part of NA split ;-}  
# _dirty       intermediate df - NA part of NA split ;-}  
# _transZ       final df - after cleaning  

# NAs are caused in part by different start dates
# OELRICHS    1893 - 2018  -fixed with Murdo
# COTTONWOOD  1909 - 2018  -fixed with Interior & Murdo
# RAPID CITY  1948 - 2018  -fixed with Interior  
# INTERIOR    1949 - 2018  -fixed with Cottonwood & Murdo    
# ORAL        1971 - 2018  -fixed with Oelrichs & Harrison
# LONG VALLEY 1927 - 2012  - Removed from further analysis
# MISSION     1951 - 2018  - Removed from further analysis 

# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv"))
sta_raw   <- import("data/stations.csv") # N = 45,784
sta_har   <- as.tibble(import("data/sta_har.csv"))
  sta_har <- sta_har %>%
    mutate(date = ymd(date)) # harrison used to fill NA

# fix date & add year and month
sta_raw <- sta_raw %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())


# remove NA in 2018 data - old code not listed in naming convention
# chopping off everything after 2018-05-31; N= 45,684
sta_not_2018    <- sta_raw %>% filter(year != 2018)
  sta_2018_filt <- sta_raw %>% filter(year == 2018 & month != 6)
  sta_raw       <- bind_rows(sta_not_2018, sta_2018_filt) %>%
    arrange(desc(date)) 
rm(sta_2018_filt, sta_not_2018)

# check NA in original
intro_raw <- as.tibble(introduce(sta_raw)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_raw

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840


# NAs are caused in part by different start dates
# OELRICHS    1893 - 2018    
# COTTONWOOD  1909 - 2018  
# RAPID CITY  1948 - 2018    
# INTERIOR    1949 - 2018       
# MISSION     1951 - 2018       
# ORAL        1971 - 2018 - in.progress


# Clean Oral & part of Oelrichs 1971-05-01 to present
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1971-05-01
  sta_ge72      <- sta_raw %>% filter(year >= 1972) #
  sta_71m05     <- sta_raw %>% filter(year == 1971 & month >= 5)
sta_ge71m05     <- bind_rows(sta_ge72, sta_71m05) # this is active
  rm(sta_ge72, sta_71m05)

  sta_le71      <- sta_raw %>% filter(year < 1971)
  sta_71m01_m05 <- sta_raw %>% filter(year == 1971 & month < 5)
sta_lt71m05     <- bind_rows(sta_le71, sta_71m01_m05) # this is not
  rm(sta_le71, sta_71m01_m05)

#   2. filter NA vals from Oral
#     Before = 368 NA <- filling with Oelrichs
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(ora)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(ora)) %>%
  mutate(ora = oel) # fix oral with oelrichs

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
  filter(is.na(ora)) # check is :-] NA is 1 obs

#   3. filter NA vals from Oelrichs (oel)
#     Before = 368 NA <- filling with Oral
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = ora) # fix oelrichs with oral

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] down to 1 NA
rm(sta_NA)

#   4. fix Oelrichs & Oral NA with Harrision
#     Before: 1 concurrent NA values 

sta_ge71m05 <- left_join(sta_ge71m05, sta_har, by = "date")
  rm(sta_har)

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = prcp) %>%
  mutate(ora = prcp)

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] zero NA vals
rm(sta_NA) 
  
#   4. Put the pieces back together
sta_ge71m05 <- sta_ge71m05 %>%
  select(-prcp, -id) 
sta_trans <- bind_rows(sta_ge71m05, sta_lt71m05)
rm(sta_ge71m05,sta_lt71m05)

# Check work
intro_raw

#A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# export the work to a file
# export(sta_trans, file = "data/stations_trans1.csv")  
```

```{r munge-precip-data-mission, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Mission 
# Variable naming convention - see munge-precip-data-oral code chunk    
# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv"))
sta_trans <- import("data/stations_trans1.csv")
sta_lon   <- import("data/sta_lon.csv")

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

sta_lon <- sta_lon %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  select(date, everything())


# Clean Mission using Long Valley station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1951-08-01
  sta_ge52      <- sta_trans %>% filter(year >= 1952) 
  sta_51m08     <- sta_trans %>% filter(year == 1951 & month >= 8)
sta_ge51m08     <- bind_rows(sta_ge52, sta_51m08) # this is active
  rm(sta_ge52, sta_51m08)

  sta_le51      <- sta_trans %>% filter(year < 1951)
  sta_51m01_m08 <- sta_trans %>% filter(year == 1951 & month < 8)
sta_lt51m08     <- bind_rows(sta_le51, sta_51m01_m08) # this is not
  rm(sta_le51, sta_51m01_m08)

#   2. Attach Long Valley to df
sta_ge51m08 <- left_join(sta_ge51m08, sta_lon, by = "date")
  rm(sta_lon)
  
#   3. filter NA vals from Mission with Long Valley
#     Before = 858 NA <- filling with Long Valley
#     After: 14 NA values 
sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = prcp) # fix Mission with Long Valley

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge51m08 %>%
  filter(is.na(mis)) # check is :-] # down to 6 vals
rm(sta_NA)

#   4. Use the Interior vals to fill missing vals
#     Before = 6 NA <- filling with Interior
#     After: XX  NA values 

sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = int) # fix Mission with Interior

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge51m08 %>%
   filter(is.na(mis)) # check is :-] down to 2 NA vals
 rm(sta_NA)
 
#   5. Use the Oelrich vals to fill missing vals
#     Before = 2 NA <- filling with Interior
#     After: zero NA values 

sta_clean <- sta_ge51m08 %>%
  filter(!is.na(mis)) # this is not active

sta_dirty <- sta_ge51m08 %>%
  filter(is.na(mis)) %>%
  mutate(mis = oel) # fix Mission with Oelrichs

sta_ge51m08 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge51m08 %>%
   filter(is.na(mis)) # check is :-] down to 2 NA vals
 rm(sta_NA)
 
#   6. Put the pieces back together
sta_ge51m08 <- sta_ge51m08 %>%
  select(-prcp, -id) 
sta_trans2 <- bind_rows(sta_ge51m08, sta_lt51m08)
rm(sta_ge51m08,sta_lt51m08)

# Check work
# A tibble: 1 x 4 (RAW)
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans
# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

intro_trans2 <- as.tibble(introduce(sta_trans2)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans2
# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# export the work to a file
# export(sta_trans2, file = "data/stations_trans2.csv")  
```

```{r munge-precip-data-int, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Interior
# Variable naming convention - see munge-precip-data-oral code chunk  


# load metadata & data 
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_trans <- import("data/stations_trans2.csv") 
sta_mur   <- import("data/sta_mur.csv") 

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

sta_mur <- sta_mur %>% 
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Interior using Cottonwood station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1949-11-01
  sta_ge50      <- sta_trans %>% filter(year >= 1950) 
  sta_49m11     <- sta_trans %>% filter(year == 1949 & month >= 11)
sta_ge49m11     <- bind_rows(sta_ge50, sta_49m11) # this is active
  rm(sta_ge50, sta_49m11)

  sta_le50      <- sta_trans %>% filter(year < 1949)
  sta_49m01_m11 <- sta_trans %>% filter(year == 1949 & month < 11)
sta_lt49m11     <- bind_rows(sta_le50, sta_49m01_m11) # this is not
  rm(sta_le50, sta_49m01_m11)

#   2. filter NA vals from Interior with Cottonwood
#     Before = 2,578 NA <- filling with Cottonwood
#     After: 11 NA values 
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(int)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(int)) %>%
  mutate(int = cot) # fix Interior with Cottonwood

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge49m11 %>%
  filter(is.na(int)) # check is :-] # down to 11 vals
  rm(sta_NA)

#   3. Fill NA with Rapid
#     Before: 11 NA <- filling with Murdo
#     After:  zero NA values 
  
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(int)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(int)) %>%
  mutate(int = rap) # fix Interior with Interior

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge49m11 %>%
   filter(is.na(int)) # check is :-] down to zero vals
 rm(sta_NA)

#   4. filter NA vals from Cottonwood with Interior 
#     Before: 850 NA <- filling with Interior
#     After: 11 NA values 
sta_clean <- sta_ge49m11 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge49m11 %>%
  filter(is.na(cot)) %>%
  mutate(cot = int) # fix Cottonwood with Interior

sta_ge49m11 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge49m11 %>%
  filter(is.na(cot)) # check is :-] # down to zero vals
  rm(sta_NA)
  
#   3. Put the pieces back together
sta_trans3 <- bind_rows(sta_ge49m11, sta_lt49m11)
rm(sta_ge49m11, sta_lt49m11)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

intro_trans3 <- as.tibble(introduce(sta_trans3)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans3

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# export the work to a file
# export(sta_trans3, file = "data/stations_trans3.csv")  
```

```{r munge-precip-data-rap, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data
# Specific purpose - clean Rapid City NA with Interior & Cottonwood
# Rationale - this is the closest to the project area;
#  remember the point estimates are for the project area.

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta  <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_trans <- import("data/stations_trans3.csv")

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Rapid City using Interior & Cottonwood station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1948-05-01
  sta_ge49      <- sta_trans %>% filter(year >= 1949) # yr above
  sta_48m05     <- sta_trans %>% filter(year == 1948 & month >= 5)
sta_ge48m05     <- bind_rows(sta_ge49, sta_48m05) # this is active
  rm(sta_ge49, sta_48m05)

  sta_lt48     <- sta_trans %>% filter(year < 1948)
  sta_48m01_m05 <- sta_trans %>% filter(year == 1948 & month < 5)
sta_lt09m06     <- bind_rows(sta_lt48, sta_48m01_m05) # this is not
  rm(sta_lt48, sta_48m01_m05)

#   2. filter NA vals from Rapid City and fill with Interior
#     Before: 6 NA <- filling with Interior
#     After: zero NA values 
sta_clean <- sta_ge48m05 %>%
  filter(!is.na(rap)) # this is not active

sta_dirty <- sta_ge48m05 %>%
  filter(is.na(rap)) %>%
  mutate(rap = int) # fix Rapid City with Interior

sta_ge48m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge48m05 %>%
  filter(is.na(rap)) # check is :-] # down to zero vals
  rm(sta_NA)

#   4. Put the pieces back together
#sta_ge48m05 <- sta_ge48m05 %>%
#  select(-prcp, -id) 
sta_trans4 <- bind_rows(sta_ge48m05, sta_lt09m06)
rm(sta_ge48m05, sta_lt48m05)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

intro_trans4 <- as.tibble(introduce(sta_trans4)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans4

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# export the work to a file
# export(sta_trans4, file = "data/stations_trans4.csv")  
```

```{r munge-precip-data-cot, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Cottonwood with Rapid City
# Rationale - Refactored - RC has similarity to Cottonwood

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta      <- as.tibble(import("data/sta_meta_fin.csv"))
sta_trans     <- import("data/stations_trans4.csv")
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv"))

# fix date & add year and month
sta_trans <- sta_trans %>%
  mutate(date = ymd(date)) %>%
  arrange(desc(date)) %>%
  mutate(year = year(date)) %>%
  mutate(month = month(date)) %>%
  select(date, year, month, everything())

# Clean Cottonwood precip NA using Murdo station data
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1909-06-01
  sta_ge09      <- sta_trans %>% filter(year >= 1910) # yr above
  sta_09m06     <- sta_trans %>% filter(year == 1909 & month >= 6)
sta_ge09m06     <- bind_rows( sta_ge09, sta_09m06) # this is active
  rm(sta_ge09, sta_09m06)

  sta_lt09     <- sta_trans %>% filter(year < 1909)
  sta_09m01_m06 <- sta_trans %>% filter(year == 1909 & month < 6)
sta_lt09m06     <- bind_rows(sta_lt09, sta_09m01_m06) # this is not
  rm(sta_lt09, sta_09m01_m06)

#Check on split :-)
count_check <- bind_rows(sta_ge09m06, sta_lt09m06) 
rm(count_check)
  
#   2. filter NA vals from Cottonwood & fill with Murdo
#     Before: 96 NA <- filling with Murdo
#     After: 12 NA values 
sta_clean <- sta_ge09m06 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge09m06 %>%
  filter(is.na(cot)) %>%
  mutate(cot = mur) # fix Cottonwood with Murdo

sta_ge09m06 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge09m06 %>%
  filter(is.na(cot)) # check is :-] # down to 12 vals
  rm(sta_NA)

# ~~~~~~~~~~~~~~~~~~~~~~~~
#   3. filter NA vals from Cottonwood & fill with Oelrichs
#     Before: 12 NA <- filling with Murdo
#     After: zero NA values 
sta_clean <- sta_ge09m06 %>%
  filter(!is.na(cot)) # this is not active

sta_dirty <- sta_ge09m06 %>%
  filter(is.na(cot)) %>%
  mutate(cot = oel) # fix Cottonwood with Murdo

sta_ge09m06 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge09m06 %>%
  filter(is.na(cot)) # check is :-] # down to zero vals
  rm(sta_NA)
  
#   4. Put the pieces back together
sta_trans5 <- bind_rows(sta_ge09m06, sta_lt09m06)
rm(sta_ge09m06, sta_lt09m06)

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

intro_trans5 <- as.tibble(introduce(sta_trans5)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans5

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

# export the work to a file
# export(sta_trans5, file = "data/stations_trans5.csv")  
```

```{r munge-precip-data-oel, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - clean Oelrichs with Murdo &  
#   define oldest date with continuous data 

# Variable naming convention - see munge-precip-data-oral code chunk   

# load metadata & data 
sta_meta      <- as.tibble(import("data/sta_meta_fin.csv")) 
sta_meta_orig <- as.tibble(import("data/sta_meta_orig.csv")) 
sta_trans     <- import("data/stations_trans5.csv") 

# fix date & add year and month 
sta_trans <- sta_trans %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# Clean Oelrichs precip NA using Murdo station data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# Using nearest year as split-point  
#   In this case it is the filler, Murdo, rather than the recipient 
#   1. Split the raw data into two parts at 1907-12-01 
  sta_ge08      <- sta_trans %>% filter(year >= 1908) # yr above 
  sta_07m12     <- sta_trans %>% filter(year == 1907 & month >= 12) 
sta_ge07m12     <- bind_rows( sta_ge08, sta_07m12) # this is active 
  rm(sta_ge08, sta_07m12) 

  sta_lt07     <- sta_trans %>% filter(year < 1907) 
  sta_07m1_m12 <- sta_trans %>% filter(year == 1907 & month < 12) 
sta_lt07m12     <- bind_rows(sta_lt07, sta_07m1_m12) # this is not 
  rm(sta_lt07, sta_07m1_m12) 

#Check on split :-) 
count_check <- bind_rows(sta_ge07m12, sta_lt07m12)  
rm(count_check) 
  
#   2. filter NA vals from Oelrichs & fill with Murdo 
#     Before: 1478 NA <- filling with Murdo 
#     After: 186 NA values  
sta_clean <- sta_ge07m12 %>% 
  filter(!is.na(oel)) # this is not active 

sta_dirty <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  mutate(oel = mur) # oelrichs with Murdo 

sta_ge07m12 <- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

  #Check on split :-) 
count_check <- bind_rows(sta_ge07m12, sta_lt07m12)  
rm(count_check) 
  
sta_NA <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  arrange() # check is :-] # down to 186 vals; fill with Cottonwood 
rm(sta_NA) 

#   3. filter NA vals from Oelrichs & fill with Cottonwood 
#     Before: 186 NA <- filling with Cottonwood 
#     After: 22 NA values  
sta_clean <- sta_ge07m12 %>% 
  filter(!is.na(oel)) # this is not active 

sta_dirty <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  mutate(oel = cot) # oelrichs with Cottonwood 

sta_ge07m12 <- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

sta_NA <- sta_ge07m12 %>% 
  filter(is.na(oel)) %>% 
  arrange() # check is :-] # down to 22 vals; 
# Oldest is 1909-05-28, so this is the end of the record 
# The end of the complete record is 1909-06-01 
  
#   4. Put the pieces back together & cut to end of complete record 
sta_trans6 <- bind_rows(sta_ge07m12, sta_lt07m12) 
rm(sta_ge07m12, sta_lt07m12, sta_NA) 

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

intro_trans6 <- as.tibble(introduce(sta_trans6)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans6

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               104652             456840
   
# export the work to a file
# export(sta_trans6, file = "data/stations_trans6.csv") 
```

```{r munge-precip-data-lon, eval=FALSE}
# General Purpose: fill NA prior to upscaling to monthly data 
# Specific purpose - Append Long Valley with Mission  
# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data 
# Note: the metadata was changed in this step.  A once only change. 
#sta_meta      <- as.tibble(import("data/sta_meta_fin.csv")) 
#sta_meta_orig <- as.tibble(import("data/Archived/station_meta2.csv") 
sta_trans     <- as.tibble(import("data/stations_trans6.csv")) 
sta_lon       <- as.tibble(import("data/sta_lon.csv")) 

# add Long Valley & remove Mission metadata 
#sta_meta_lon <- sta_meta_orig %>% 
#  filter(name == "LONG VALLEY, SD US") 
#sta_meta2    <- bind_rows(sta_meta, sta_meta_lon)  
#sta_meta2    <- sta_meta2 %>%  
#  filter(name != "MISSION 14 S, SD US") 
#export(sta_meta2, file = "data/sta_meta_fin2.csv") 
sta_meta      <- as.tibble(import("data/sta_meta_fin2.csv")) 


# fix date, add year and month, and join Long Valley 
sta_trans <- sta_trans %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

sta_lon <- sta_lon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, everything()) 

sta_trans <- left_join(sta_trans, sta_lon) 
rm(sta_lon) 

sta_trans <- sta_trans %>% 
  select(-id) %>% 
  rename(lon = prcp) 

# Clean Long Valley precip NA using Mission station data 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  
# Using nearest year as split-point 
#   1. Split the raw data into two parts at 1927-07-01 
  sta_ge28      <- sta_trans %>% filter(year >= 1928) # yr above 
  sta_27m07    <- sta_trans %>% filter(year == 1927 & month >= 7) 
sta_ge27m07    <- bind_rows( sta_ge28, sta_27m07) # this is active 
  rm(sta_ge28, sta_27m07) 

  sta_lt27     <- sta_trans %>% filter(year < 1927) 
  sta_27m1_m7 <- sta_trans %>% filter(year == 1927 & month < 7) 
sta_lt27m07     <- bind_rows(sta_lt27, sta_27m1_m7) # this is not 
  rm(sta_lt27, sta_27m1_m7) 

#Check on split :-) 
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
#   2. filter NA vals from Long Valley & fill with Mission 
#     Before: 2904 NA <- filling with Mission 
#     After: 423 NA values  
sta_clean <- sta_ge27m07%>% 
  filter(!is.na(lon)) # this is not active 

sta_dirty <- sta_ge27m07%>% 
  filter(is.na(lon)) %>% 
  mutate(lon = mis) # Long Valley with Mission 

sta_ge27m07<- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

# Check on split :-) 
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
sta_NA <- sta_ge27m07 %>% 
  filter(is.na(lon)) %>% 
  arrange() # check is :-] # down to 423 vals; fill with Cottonwood 
rm(sta_NA) 

#   3. filter NA vals from Long Valley & fill with Cottonwood 
#     Before: 423 NA <- filling with Mission 
#     After: zero NA values  
sta_clean <- sta_ge27m07%>% 
  filter(!is.na(lon)) # this is not active 

sta_dirty <- sta_ge27m07%>% 
  filter(is.na(lon)) %>% 
  mutate(lon = cot) # Long Valley with Mission 

sta_ge27m07<- bind_rows(sta_clean, sta_dirty) 
  rm(sta_clean, sta_dirty) 

  #Check on split :-)
count_check <- bind_rows(sta_ge27m07, sta_lt27m07)  
rm(count_check) 
  
sta_NA <- sta_ge27m07 %>% 
  filter(is.na(lon)) %>% 
  arrange() # check is :-] # down to zero vals 

#   4. Put the pieces back together & cut to end of complete record 
sta_trans7 <- bind_rows(sta_ge27m07, sta_lt27m07) 
rm(sta_ge27m07, sta_lt27m07, sta_NA) 

# Check work
# A tibble: 1 x 4 - Raw
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

# A tibble: 1 x 4 - Trans1
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# A tibble: 1 x 4 - Trans2
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               109638             456840

# A tibble: 1 x 4 - Trans3
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106210             456840

# A tibble: 1 x 4 - Trans4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106204             456840

# A tibble: 1 x 4 - Trans5
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               106108             456840

# A tibble: 1 x 4 - Trans6
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               104652             456840

intro_trans7 <- as.tibble(introduce(sta_trans7)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans7
   
# A tibble: 1 x 4 - Trans7 - note the extra column
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      11               117128             502524

# export the work to a file
# export(sta_trans7, file = "data/stations_trans7.csv") 

# This is to cut back to complete part of Oelrichs
  sta_ge10      <- sta_trans7 %>% filter(year >= 1910) # yr above 
  sta_09m06     <- sta_trans7 %>% filter(year == 1909 & month >= 6) 
sta_fin     <- bind_rows( sta_ge10, sta_09m06) # this is active 
  rm(sta_ge10, sta_09m06) 

intro_sta_fin <- as.tibble(introduce(sta_fin)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_sta_fin

# A tibble: 1 x 4 - Final
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 39812      11                76046             437932

# export(sta_fin, file = "data/stations_final.csv") 
```

```{r eval-precip-data-lon, message=FALSE}
# Deciding whether to keep Long Valley
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# General Purpose: fill NA prior to upscaling to monthly data
# Specific purpose - Check Mission & Long Valley covariance
# Variable naming convention - see munge-precip-data-oral code chunk   

# load metadata & data 
sta_meta      <- as.tibble(import("data/sta_meta_fin2.csv")) 
sta_fin     <- as.tibble(import("data/stations_final.csv")) 

# Split data to the end of Mission - the purpose here is  
#   to look at a double mass plot with Mission & Long Valley

sta_test <- sta_fin %>%
  filter(year > 1951) %>%
  filter(year < 2012) 

# gather values & create groups
sta_gath <- gather(sta_test, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

sta_group <- sta_gath %>%
  group_by(year, month, station)

# sum daily precip over a month
sta_gath_mon <- sta_group %>%
  summarize(prcp_tenths = sum(prcp)) %>%
  mutate(prcp_mm = prcp_tenths/10) %>%
  select(-prcp_tenths)

# spread result - now in months 
sta_mon <- sta_gath_mon %>%
  spread(station, prcp_mm) %>%
  mutate(day = 1) %>%
  mutate(date = make_date(year = year, month = month, day = day)) %>%
  select(date, year, month, everything()) %>%
  select(-day) %>%
  ungroup()
rm(sta_gath, sta_group, sta_gath_mon)

# filter Mission, Long Valley, Interior
sta_gath2 <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, -lon, factor_key = TRUE) %>%
  filter(station == "int" | 
           station == "mis")

# plot the graphs of Interior and Mission
# see which precip is more similiar.

ggplot(sta_gath2, aes(prcp, lon)) +
  geom_point() +
  geom_smooth(method = "lm", aes(color = "red")) +
  facet_grid(.~station) +
   scale_x_sqrt() +
  scale_y_sqrt() +
  geom_smooth() + 
  ggtitle("Long Valley similarity to nearest stations")
```  

```{r check_precip_data2}

sta_dv <- import(file = "data/sta_dv.csv") 
sta_meta_orig <- import(file = "data/sta_meta_orig.csv") 

# fix missing data -----------------------------------------------------------  

# appended station data to fix missing years 
# Region   Station       Add-Station   Year 
#  NW    RC Regional         NA         NA
#  NC    Cottonwood      Interior 3    1996   
#  NE      Onida          Kennebec     1995 
#  SW      Oral           Oelrichs     2005 
#  SC     Gordon         Valentine     2012 
#  SE     Mission           NA          NA  


# calculate percentages  
length <- sta_dv %>%    
  group_by(name) %>%  
  summarize(length = n()) 
  
sta_check_sum <- sta_check %>% 
  group_by(name, mindate, maxdate) %>%  
  summarize(count = n())  


# Clean Oral & part of Oelrichs 1971-05-01 to present
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Using nearest year as split-point
#   1. Split the raw data into two parts at 1971-05-01
  sta_ge72      <- sta_raw %>% filter(year >= 1972) #
  sta_71m05     <- sta_raw %>% filter(year == 1971 & month >= 5)
sta_ge71m05     <- bind_rows(sta_ge72, sta_71m05) # this is active
  rm(sta_ge72, sta_71m05)

  sta_le71      <- sta_raw %>% filter(year < 1971)
  sta_71m01_m05 <- sta_raw %>% filter(year == 1971 & month < 5)
sta_lt71m05     <- bind_rows(sta_le71, sta_71m01_m05) # this is not
  rm(sta_le71, sta_71m01_m05)

#   2. filter NA vals from Oral
#     Before = 368 NA <- filling with Oelrichs
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(ora)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(ora)) %>%
  mutate(ora = oel) # fix oral with oelrichs

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
  filter(is.na(ora)) # check is :-] NA is 1 obs

#   3. filter NA vals from Oelrichs (oel)
#     Before = 368 NA <- filling with Oral
#     After: 10 concurrent NA values 

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = ora) # fix oelrichs with oral

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)
  
sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] down to 1 NA
rm(sta_NA)

#   4. fix Oelrichs & Oral NA with Harrision
#     Before: 1 concurrent NA values 

sta_ge71m05 <- left_join(sta_ge71m05, sta_har, by = "date")
  rm(sta_har)

sta_clean <- sta_ge71m05 %>%
  filter(!is.na(oel)) # this is not active

sta_dirty <- sta_ge71m05 %>%
  filter(is.na(oel)) %>%
  mutate(oel = prcp) %>%
  mutate(ora = prcp)

sta_ge71m05 <- bind_rows(sta_clean, sta_dirty)
  rm(sta_clean, sta_dirty)

sta_NA <- sta_ge71m05 %>%
   filter(is.na(ora)) # check is :-] zero NA vals
rm(sta_NA) 
  
#   4. Put the pieces back together
sta_ge71m05 <- sta_ge71m05 %>%
  select(-prcp, -id) 
sta_trans <- bind_rows(sta_ge71m05, sta_lt71m05)
rm(sta_ge71m05,sta_lt71m05)

# Check work
intro_raw

#A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               111229             456840

intro_trans <- as.tibble(introduce(sta_trans)) %>%
  select(-(3:5)) %>%
  select(-5)
intro_trans

# A tibble: 1 x 4
#   rows columns total_missing_values total_observations
#  <int>   <int>                <int>              <int>
# 45684      10               110496             456840

# export the work to a file
# export(sta_trans, file = "data/stations_trans1.csv")  
```  

```{r modeling-results-1}
# continued from above 
lm_both <- lm(data = sta_mon, sqrt(lon) ~ sqrt(mis) + sqrt(int)) 
lm_both_tidy <- tidy(lm_both)
lm_both_glance <- glance(lm_both)

lm_int <- lm(data = sta_mon, sqrt(lon) ~ sqrt(int))
lm_int_tidy <- tidy(lm_int)
lm_int_glance <- glance(lm_int)

lm_mis <- lm(data = sta_mon, sqrt(lon) ~ sqrt(mis))
lm_mis_tidy <- tidy(lm_mis)
lm_mis_glance <- glance(lm_mis)

# combine models
lm_glance <- bind_rows(lm_both_glance, lm_int_glance)
lm_glance <- bind_rows(lm_glance, lm_mis_glance)

lm_glance <- as.tibble(lm_glance) %>%
  mutate(name = c("both", "int", "mis")) %>%
  select(name, everything())
lm_glance
```

```{r modeling-results-2}
# continued from above 
# result: drop the long valley data and use Interior for southeast 
# remove not-needed stations from metadata 
sta_meta <- sta_meta %>% 
  filter(name != "LONG VALLEY, SD US") 

# export(sta_meta, file = "data/sta_meta_fin3.csv")
```  

```{r voroni-diagram-inter}
# import site location data and filter out:
#   hermosa, ainsworth, harrison, hot springs

sta_meta <- import("data/sta_meta_orig.csv")
sta_meta <- sta_meta %>%
  filter(id != "GHCND:US1SDCS0027") %>% # very short length - HER
  filter(id != "GHCND:USC00250050") %>% # outside region - AIN
  filter(id != "GHCND:USC00253615") %>% # outside region - HAR
  filter(id != "GHCND:US1SDFR0001") %>% # close to oral - HOT
  filter(id != "GHCND:USC00395891") %>% # outside range - MUR
  filter(id != "GHCND:USC00394983") %>% # love - but stopped 2012 - LON
  filter(id != "GHCND:US1SDJK0006") 

# import gage location data
gage_meta <- import("data/gage_meta.csv")
  
# define the study area using data from the 'maps' package
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# import polygon data - counties
counties <- map_data("county") 
counties <- subset(counties, region %in% 
   c("south dakota", "nebraska"))
prr <- subset(counties, subregion %in% 
   c("shannon", "jackson", "bennett"))

# create voroni line segments
voronoi <- deldir(sta_meta$lon, sta_meta$lat)

#Plot the points, voronoi lines, and annotate
p_site <- ggplot(data = sta_meta, aes(x = lon, y = lat)) +
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333") +   
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 2,
    data = voronoi$dirsgs,
    linetype = 1,
    color = "#FFB958") +
  geom_polygon(data = prr, aes(x = long, y = lat, group = group), 
              color = "black", linetype = "dashed", fill = "NA") +
  theme_bw() +
  geom_text(data = sta_meta, aes(label = name), size = 2.5) +
  ggtitle("Intermediate Theissen Polygon Map")
  
p_site +
  geom_point(data = gage_meta, aes(x = dec_long_va, 
                                   y = dec_lat_va),
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    fill = rgb(70, 130, 180, 255, maxColorValue = 255)) 
  

#sta_meta <- as.tibble(sta_meta)
# final list of stations
# id                name                              
#  <chr>             <chr>                             
# 1 GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# 2 GHCND:USC00396212 OELRICHS, SD US                   
# 3 GHCND:USC00394184 INTERIOR 3 NE, SD US              
# 4 GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# 5 GHCND:USC00395638 MISSION 14 S, SD US               
# 6 GHCND:USC00396304 ORAL, SD US   

ggplot2::ggsave(path = "figure/", filename = "theissen_inter.png", 
                width = 6, height = 6, units = "in")
# export(sta_meta, file = "index/data/sta_meta_fin.csv")

### Note: Mission is kept on the map to extend the INT - COT line 
```

```{r voroni-diagram-final, eval=FALSE}
# import site location data and filter out:
#   hermosa, ainsworth, harrison, hot springs
sta_meta <- import("data/sta_meta_fin3.csv")

# import gage location data
gage_meta <- import("data/gage_meta.csv")
  
# define the study area using data from the 'maps' package
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# import polygon data - counties
counties <- map_data("county") 
counties <- subset(counties, region %in% 
   c("south dakota", "nebraska"))
prr <- subset(counties, subregion %in% 
   c("shannon", "jackson", "bennett"))

# create voroni line segments
voronoi <- deldir(sta_meta$lon, sta_meta$lat)

#Plot the points, voronoi lines, and annotate
p_site <- ggplot(data = sta_meta, aes(x = lon, y = lat)) +
  geom_point(
    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
    pch = 21,
    size = 4,
    color = "#333333") +   
  geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 2,
    data = voronoi$dirsgs,
    linetype = 1,
    color = "#FFB958") +
  geom_polygon(data = prr, aes(x = long, y = lat, group = group), 
              color = "black", linetype = "dashed", fill = "NA") +
  theme_bw() +
  geom_text(data = sta_meta, aes(label = name), size = 2.5) +
  ggtitle("Final Theissen Polygon Map")

# this adds all of the weather stations as points
#p_site +
#  geom_point(data = gage_meta, aes(x = dec_long_va, 
#                                   y = dec_lat_va),
#    fill = rgb(70, 130, 180, 255, maxColorValue = 255),
#    fill = rgb(70, 130, 180, 255, maxColorValue = 255)) 
  

#sta_meta <- as.tibble(sta_meta)
# final list of stations
# id                name                              
#  <chr>             <chr>                             
# 1 GHCND:USW00024090 RAPID CITY REGIONAL AIRPORT, SD US
# 2 GHCND:USC00396212 OELRICHS, SD US                   
# 3 GHCND:USC00394184 INTERIOR 3 NE, SD US              
# 4 GHCND:USC00391972 COTTONWOOD 2 E, SD US             
# 5 GHCND:USC00395638 MISSION 14 S, SD US               
# 6 GHCND:USC00396304 ORAL, SD US   

ggplot2::ggsave(path = "figure/", filename = "theissen_fin.png", 
               width = 6, height = 6, units = "in")
# export(sta_meta, file = "index/data/sta_meta_fin.csv")
```  

```{r daily2monthly-precip}

# fix date & add year and month 
sta_day <- sta_day %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  mutate(year = year(date)) %>% 
  mutate(month = month(date)) %>% 
  select(date, year, month, everything()) 

# gather daily values 
sta_gath <- gather(sta_day, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE)

# create groups 
sta_group <- sta_gath %>% 
  group_by(year, month, station) 

# sum daily precip over a month 
sta_gath_mon <- sta_group %>% 
  summarize(prcp_tenths = sum(prcp)) %>% 
  mutate(prcp_mm = prcp_tenths/10) %>% 
  select(-prcp_tenths) 

# spread result - now in months  
#   ...and take a bow, because this is MAGIC!  Thnx Tidyverse. 
sta_mon <- sta_gath_mon %>% 
  spread(station, prcp_mm) %>% 
  mutate(day = 1) %>% 
  mutate(date = make_date(year = year, month = month, day = day)) %>% 
  select(date, year, month, everything()) %>% 
  select(-day) %>% 
  ungroup()

rm(sta_day, sta_gath, sta_gath_mon, sta_group) 
# export(sta_mon, file = "data/stations_monthly.csv") 
```  

```{r eval-precip-data-ora}
# General Purpose: check a short-term record: Oral 

# Variable naming convention - see munge-precip-data-oral code chunk  

# load metadata & data
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))
sta_mon     <- as.tibble(import("data/stations_monthly.csv"))

# Split data to the end of Oral - the purpose here is 
#   to look at a double mass plot with Oelrichs

sta_test <- sta_mon %>%
  filter(year > 1971) 

# check a linear model
lm <- lm(data = sta_mon, sqrt(ora) ~ sqrt(oel))
lm.tidy <- tidy(lm)
lm.glance <- glance(lm)

# plot the graphs of Oral & Oelrichs

ggplot(sta_test, aes(ora, oel)) +
  geom_point() +
  geom_smooth(method = "lm", aes(color = "red")) +
   scale_x_sqrt() +
  scale_y_sqrt() +
  geom_smooth() + 
  theme_bw() +
  ggtitle("Double mass plots")
# it's ok, keep Oral...
```  

```{r monthly2yearly-precip}
# General Purpose: prepare data for drought index   
# Specific purpose: convert monthly precip to yearly prcp  
 
# load metadata & data 
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month 
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything())  

# gather monthly values  
sta_gath <- gather(sta_mon, key = "station", value = "prcp", -date, 
                   -year, -month, factor_key = TRUE) 

# create groups 
sta_group <- sta_gath %>% 
  group_by(year,  station) 

# sum monthly precip over a year 
sta_gath_yr <- sta_group %>% 
  summarize(prcp = sum(prcp))  

# spread result - now in years 
sta_yr <- sta_gath_yr %>% 
  spread(station, prcp) %>% 
  filter(year != 1909) %>% 
  filter(year != 2018) %>% 
  ungroup()

rm(sta_mon, sta_gath, sta_gath_yr, sta_group)
# export(sta_yr, file = "data/stations_yearly.csv") 
```  

```{r summaries}

# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_mon   <- as.tibble(import("data/stations_monthly.csv")) 

# fix dates
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date)) %>% 
  select(date, year, month, everything()) 

# gather and summarize monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE)

sta_summary_mon <- as.tibble(sta_gath_mon) %>%
  group_by(station, month) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(month) %>%
  arrange(station)

#sta_summary_mon 
# export(sta_summary_mon, file = "data/sta_summary_mon.csv") 
```  

```{r yearly_summaries}
# General Purpose: prepare data for drought index  
# Specific purpose: create summaries of data 
sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr   <- as.tibble(import("data/stations_yearly.csv")) 

# gather and summarize yearly values 
# Next step - do by water year???
sta_gath_yr <- gather(sta_yr, key = "station", value = "prcp", 
                   -year, factor_key = TRUE)

sta_summary_yr <- as.tibble(sta_gath_yr) %>%
  group_by(station) %>%
  summarise(mean = mean(prcp, na.rm = TRUE), 
            med = median(prcp, na.rm = TRUE),
            IQR = IQR(prcp, na.rm = TRUE), 
            min = min(prcp, na.rm = TRUE), 
            max = max(prcp, na.rm = TRUE)) %>%
  arrange(desc(med))

#sta_summary_yr
# export(sta_summary_yr, file = "data/sta_summary_yr.csv") 
```  

```{r ggplot_yearly}
# General Purpose: prepare data for drought index 
# Specific purpose: graphical EDA - yearly 

# NEED TO FIX - screwed up variables 

sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv")) 
sta_yr     <- as.tibble(import("data/stations_yearly.csv"))

# gather monthly values 
sta_gath <- gather(sta_yr, key = "station", value = "prcp",  
                   -year, factor_key = TRUE) 

# plot
ggplot(sta_gath, aes(year, prcp)) +
  geom_line() +
  facet_grid(station ~ .) +
  theme_classic() + 
  labs(title = "Annual precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

ggplot2::ggsave(path = "figure/", filename = "precip_yr.png", 
                width = 6, height = 6, units = "in") 
```

```{r ggplot_monthly_boxplots}
# General Purpose: prepare data for drought index  
# Specific purpose: graphical EDA  
sta_meta    <- as.tibble(import("data/sta_meta_fin3.csv"))  
sta_mon     <- as.tibble(import("data/stations_monthly.csv")) 

# fix date & add year and month  
sta_mon <- sta_mon %>% 
  mutate(date = ymd(date)) %>% 
  arrange(desc(date))  

# gather monthly values 
sta_gath_mon <- gather(sta_mon, key = "station", value = "prcp", 
                       -date, -year, -month, factor_key = TRUE) 

ggplot(sta_gath_mon, aes(month, prcp, group = month)) +
  geom_boxplot() +
  facet_wrap(~station) +
  theme_classic() + 
  labs(title = "Monthly precipitation depths",
       subtitle = "Pine Ridge Reservation, SD for 1910-2017") +
       xlab("Date") +
       ylab("Depth, mm")

ggplot2::ggsave(path = "figure/", filename = "precip_boxpl_mon.png",
                width = 6, height = 6, units = "in")
```  

```{r correlation}
# General Purpose: prepare data for drought index
# Specific purpose: graphical EDA - correlation plot

# rewrite code from below
corr_ann   <- as.tibble(import("data/stations_yearly.csv")) %>% 
  gather(key = "station", value = "prcp",  -year, na.rm = TRUE) %>% 
  filter(year > 1972) %>% 
  spread(station, prcp) %>% 
  select(-year) %>% 
  cor()

# save the corrplot - see below -
# By default, RStudio enables inline output (notebook mode) on all R 
# Markdown documents, so you can interact with any R Markdown document # as though it were a notebook. If you have a document with which you 
# prefer to use the traditional console method of interaction, you can # disable notebook mode by clicking the gear in the editor toolbar and # choosing Chunk Output in Console.
corrplot.mixed(corr_ann, order = "hclust", 
                 addrect = 2, upper = "ellipse", 
                 lower = "number")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# this is the old code - saving to make sure it works in the future
# sta_meta  <- as.tibble(import("data/sta_meta_fin3.csv"))
# sta_yr   <- as.tibble(import("data/stations_yearly.csv"))

# fix date & add year and month
# sta_yr <- sta_yr %>%
#   arrange(year) 

# need to have a correlation matrix without any NA vals
# gather yearly values 
# sta_gath <- gather(sta_yr, key = "station", value = "prcp", 
#                    -year, factor_key = TRUE)

# filter NAs
# sta_gath_72 <- sta_gath %>%
#   filter(year > 1972) 

# spread remaining matrix & arrange from west to east
# sta_72 <- sta_gath_72 %>%
#   spread(station, prcp) %>%
#   select(oel, ora, rap, int, cot)

# create a correlation matrix and plot it
# sta_M <- cor(sta_72)
# corrplot.mixed(sta_M,  order = "hclust", addrect = 2, upper =  "ellipse", 
# lower = "number", title = "Precipitation station correlation")  
```  

















