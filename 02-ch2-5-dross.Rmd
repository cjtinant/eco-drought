---
title: "02-ch2-5-junk"
author: "CJ Tinant"
date: "10/7/2019"
output: html_document
---

<!--
This R markdown file includes the junk that should be deleted but has not been...

-->

```{r export-table_ecoreg_mae} 
  
# set the numeric columns  
col_key_num <- mae_ecoreg %>% 
  select(-c(model, ecoreg)) %>%  
  names() 
  
# convert tibble to a flextable   
coef_table <- mae_ecoreg %>%    
  flextable() %>%  
  colformat_num(col_keys = col_key_num,  
                big.mark=",",   
                digits = 2, na_str = "N/A") %>%   
  set_header_labels(   
    ecoreg  = "Ecoregion",   
    model  = "Model",   
    MAE = "MAE") %>%    
  autofit() %>%   
  theme_booktabs()  
  
# export a docx of flextables    
coef_table <- read_docx() %>%  
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_mae_ecoreg.docx")  
  
rm(coef_table, col_key_num)     
  
```  

```{r delete-this}

# model plot test - ecoreg --- remove later   
#try_plot <- try %>%  
#  mutate(obs_antilog = 10^observed) %>% 
#  mutate(fit_antilog = 10^value)    

# observations vs predictions plot

obs_pred_plot <- fit_model_test %>% 
ggplot(., aes(observed, fitted)) +  
  facet_wrap(vars(model), ncol = 2) +  
# plot points as a density plot to reduce overplotting 
geom_point( 
  shape = ".", 
  color = "gray80")  + 
 # plot points as a density plot to reduce overplotting  
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
#  geom_abline( 
#    intercept = 0,  
#    slope = 1,  
#    size = 0.25) +  
# set limits  
#  scale_x_continuous(limits = c(-1, 4)) + 
#  scale_x_reverse(limits = c(-1, 4)) + 
  scale_y_continuous(limits = c(-1, 4)) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") 

obs_pred_plot 

# residuals plot 
resid_plot <- fit_model_test %>% 
ggplot(., aes(observed, residual)) +  
  facet_wrap(vars(model), ncol = 2) +  
# plot points as a density plot to reduce overplotting 
geom_point( 
  shape = ".", 
  color = "gray80")  + 
 # plot points as a density plot to reduce overplotting  
geom_density2d( 
   color = "gray50")  + 
#  geom_smooth( 
#    method = "lm", 
#    color = "black") + 
# plot a horizontal line 
  geom_abline( 
    intercept = 0,  
    slope = 0,  
    size = 0.25) +  
# set limits  
  scale_x_continuous(limits = c(4, -1)) + 
  scale_y_continuous(limits = c(4, -1)) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") 

resid_plot 

cowplot::plot_grid( 
  freq_plot, obs_pred_plot, resid_plot, 
  ncol = 1, align = "v")   

cowplot::ggsave2("figure/dv_obs_pred_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

# find mean values for any date ---------------------------------------------- 
# -- this is the null model to test if there is a better fit by lasso coefs  
fit_train_ecoreg <- fit_train_ecoreg %>% 
  group_by(ecoreg) %>% 
  mutate(mean_vals = mean(observed)) %>% 
  ungroup() 
  
fit_test_ecoreg <- fit_test_ecoreg %>% 
  group_by(ecoreg) %>% 
  mutate(mean_vals = mean(observed)) %>% 
  ungroup() 

fit_ecoreg_sum <- fit_ecoreg %>%
  group_by(ecoreg, split) %>% 
  mutate(rmse_model = round( 
    rmse(observed, predicted), 
    digits = 2) 
    ) %>% 
  mutate(rmse_ecoreg = round( 
    rmse(observed, mean_vals), 
    digits = 2) 
    ) %>% 
  mutate(mae_model = round( 
    mae(observed, predicted), 
    digits = 2) 
    ) %>% 
  mutate(mae_ecoreg = round( 
    mae(observed, mean_vals), 
    digits = 2) 
    ) %>% 
  summarise(rmse_model = mean(rmse_model), 
            rmse_ecoreg = mean(rmse_ecoreg), 
            mae_model = mean(mae_model), 
            mae_ecoreg = mean(mae_ecoreg) 
            ) %>% 
  ungroup() %>% 
  filter(split == "test") 
 
 
 
 
 
fit_ecoreg %>%  
  filter(split == "test") %>%  
ggplot(aes(observed, predicted)) +  
  facet_grid(cols = vars(ecoreg)) +  
    theme_bw() +  
# plot points as a density plot to reduce overplotting  
geom_density2d(  
   color = "black")  + 
#geom_smooth(  
#    method = "lm", 
#    color = "black") +  
geom_point(aes(observed, mean_vals),  
   color = "gray20")  +  
# plot a 45-degree line  
  geom_abline(  
    intercept = 0,  
    slope = 1,  
    size = 0.25) +  
# set limits  
#  scale_x_log10(limits = c(0.1, 1000)) + 
#  scale_y_log10(limits = c(0.1, 1000)) + 
# set theme  
  theme_bw() + 
  theme(legend.position = "none") 
 
fit_ecoreg %>%  
  mutate(Date = ymd(Date)) %>% 
ggplot(aes(Date, observed)) +  
  facet_wrap(nrow = 2, facets = "ecoreg") +  
    theme_bw() +  
geom_point(shape = ".", 
           color = "gray50") + 
geom_smooth(method = "loess") 
  theme(legend.position = "none") 
 
ggsave("figure/dv1-fit.png", width = 7, height = 7, units = "in")   
  
``` 

```{r plot_glm_fdc_dv_dont-run}

# flow duration curve 
fit_model_test %>% 
  ggplot(aes(x = freq, y = fitted)) +  
  facet_grid(. ~ model) + 
  geom_point(shape = ".",  
             color = "gray80") + 
  geom_density2d(
             color = "gray50")  + 
  geom_line(aes(x = freq, y = observed)) + 
  scale_x_continuous(name = 'Percentage of time flow exceeded',
                     trans = 'probit', 
                     limits = c(0.01, 0.99), 
          breaks = c(0.99,  0.9,   0.75,  0.5,   0.25,  0.1,   0.01),
          labels = c('99%', '90%', '75%', '50%', '25%', '10%', '1%')) + 
  theme_bw() 

```  
# building models based on PC1 & PC2 - delete after making sure log dv works 

```{r get_glm_observations_predictions}

# get observations -- training ----------------------------------------------  

obs_train_pc1    <- tibble(observed = train_pc1$.fittedPC1) 
obs_train_pc2    <- tibble(observed = train_pc2$.fittedPC2) 

# get predictions 
preds_train_pc1 <- map_dfc(models_pc1, predict, newdata = train_pc1) 
preds_train_pc2 <- map_dfc(models_pc2, predict, newdata = train_pc2) 

# combine observations & predictions  
fit_train_pc1 <- bind_cols(obs_train_pc1, preds_train_pc1) %>% 
  mutate(split = "train") %>% 
  as_tibble() 

fit_train_pc2 <- bind_cols(obs_train_pc2, preds_train_pc2) %>% 
  mutate(split = "train") %>% 
  as_tibble() 
  
# get observations -- test ---------------------------------------------------  
obs_test_pc1   <- tibble(observed = test_pc1$.fittedPC1) 
obs_test_pc2   <- tibble(observed = test_pc2$.fittedPC2)  

# get predictions 
preds_test_pc1 <- map_dfc(models_pc1, predict, newdata = test_pc1) 
preds_test_pc2 <- map_dfc(models_pc2, predict, newdata = test_pc2)  

# combine observations & predictions 
fit_test_pc1 <- bind_cols(obs_test_pc1, preds_test_pc1) %>% 
  mutate(split = "test") %>% 
  as_tibble() 

fit_test_pc2 <- bind_cols(obs_test_pc2, preds_test_pc2) %>% 
  mutate(split = "test") %>% 
  as_tibble() 

# clean-up global environment 
rm(obs_train_pc1, obs_train_pc2, preds_train_pc1, preds_train_pc2) 
rm(obs_test_pc1, obs_test_pc2, preds_test_pc1, preds_test_pc2)  
rm(fit_train_pc1, fit_test_pc1, fit_train_pc2, fit_test_pc2) 
``` 

```{r plot_glm_observations_predictions}

# model plot test - pc1 --- remove later 
fit_model_pc1_l <- fit_model_pc1 %>% 
  gather(key = "model", value = "fitted", -c(observed, split)) 

ggplot(fit_model_pc1_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) + 
# plot points as a density plot to reduce overplotting 
  geom_density2d( 
    color = "gray")  + 
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
  geom_abline( 
    intercept = 0, 
    slope = 1, 
    size = 0.25) + 
# set limits 
  xlim(-4, 4) + 
  ylim(-4, 4) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") + 
  labs(title = "Observed vs. fitted hydrologic export coefficients") 

ggsave("figure/pc1-fit.png", width = 7, height = 7, units = "in") 

# model plot test - pc2 --- remove later 
fit_model_pc2_l <- fit_model_pc2 %>% 
  gather(key = "model", value = "fitted", -c(observed, split)) 


ggplot(fit_model_pc2_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) + 
# plot points as a density plot to reduce overplotting 
  geom_density2d( 
    color = "gray")  + 
  geom_smooth( 
    method = "lm", 
    color = "black") + 
# plot a 45-degree line 
  geom_abline( 
    intercept = 0, 
    slope = 1, 
    size = 0.25) + 
# set limits 
 xlim(-0.5, 0.8) + 
  ylim(-0.5, 0.8) + 
# set theme 
  theme_bw() + 
  theme(legend.position = "none") +   
  labs(title = "Observed vs. fitted q30 - q1 diff") 
 
ggsave("figure/pc2-fit.png", width = 7, height = 7, units = "in")  

```

```{r get_glm_observations_model-fits}

# get train data model statistics --------------------------------------------  
# pc1 model fits 
model_fits_train_pc1 <- fit_model_pc1 %>% 
  filter(split == "train") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3), 
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc1") 

# pc2 model fits 
model_fits_train_pc2 <- fit_model_pc2 %>% 
  filter(split == "train") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3),  
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc2") 

# get test data model statistics ---------------------------------------------  
# pc1 model fits 
model_fits_test_pc1 <- fit_model_pc1 %>% 
  filter(split == "test") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3), 
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc1") 

# pc2 model fits 
model_fits_test_pc2 <- fit_model_pc2 %>% 
  filter(split == "test") %>%    
  gather(model, value, -c(observed, split)) %>% 
  group_by(model) %>% 
  mutate(rmse = rmse(.$observed, .$value)) %>% 
  mutate(mae = mae(.$observed, .$value)) %>% 
  mutate(rsquare = cor(observed, value)^2) %>% 
  summarise(rsquare = first(rsquare) %>% 
              round(., digits = 3), 
            rmse    = first(rmse) %>% 
              round(., digits = 3), 
            mae     = first(mae)  %>% 
              round(., digits = 3),  
            split   = first(split) 
            ) %>% 
    ungroup() %>% 
  mutate(axis = "pc2") 

# bind the pc1 & pc2 model fit data ------------------------------------------ 
model_fits <- bind_rows(model_fits_train_pc1, model_fits_train_pc2, 
                        model_fits_test_pc1, model_fits_test_pc2) %>% 
  mutate(split = as.character(split)) 

# get number of parameters for each regression method ------------------------
num_param <- coefs %>% 
  map_dfc(~sum(. != 0)) %>% 
  select(-coeff) %>%                   # transpose df 
  gather(model, num_param) %>%                 
  separate(model, c("axis", "model")) 

# join number of parameters 
model_fits <- full_join(model_fits, num_param,    
                  by = c("axis", "model")) 

# get number of observations -------------------------------------------------
n_train <- tibble( axis = c("pc1", "pc2"), 
  split = "train", 
  n = nrow(train_pc1) 
  ) 

n_test <- tibble( axis = c("pc1", "pc2"), 
  split = "test", 
  n = nrow(test_pc1) 
  ) 

num_obs <- bind_rows(n_test, n_train) 

# join obs, params & clean up ------------------------------------------------
model_fits <- full_join(model_fits, num_obs, 
                  by = c("axis", "split")) %>% 
  select(model, split, axis, num_param, n)

rm(model_fits_train_pc1, model_fits_train_pc2, 
   model_fits_test_pc1, model_fits_test_pc2, 
   n_test, n_train, num_obs, num_param)  

``` 

```{r ggplot-glm} 

ggplot(obs_fitted_l, aes(observed, fitted)) + 
  facet_grid(model ~ split) +
    geom_text(
    data = num_obs_text,
    mapping = aes(x = -4, 
                  y = -4,  
                  label = paste("n: ", n, sep="")), 
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 2.8   
    ) # + 


  geom_text(
    data = adjR2_text,
    mapping = aes(x = -4, 
                  y = -4, 
                  label = paste("adjR^2: ", R2adj, sep="")),
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 1.7
    ) + 
  geom_text(
    data = mae_text,
    mapping = aes(x = -4, 
                  y = -4, 
                  label = paste("MAE: ", MAE, sep="")),
    parse = TRUE, 
    size = 2.5,  
    nudge_x = 7,
    nudge_y = 0.5
    ) + 
  geom_point(aes(shape = split)) + 
  geom_smooth(method = "lm") + 
  geom_abline(intercept = 0, slope = 1, size = 0.25) +
  xlim(-4, 4) + 
  ylim(-4, 4) + 
  theme(legend.position = "none") + 
  labs(title = "Observed vs. fitted hydrologic export coefficients") 
```

```{r prepare-glm-models-old-to-delete} 

# set up random seed & parallel processing for glmnet & caret ---------------- 
seed <- 42 

cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE) 
registerDoMC(cores)  

# set up 'caret' training control & lambda -- 100 possible lambda vals [-2, 2]
set.seed(seed) 
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5, 
                          search = "grid", allowParallel = TRUE) 

lambda <- 10^seq(-2, 2, length = 100) 

# Split the data into training and test set ---------------------------------- 

# working -- create a training index based on groups --- 
# need to add groups to gaged  
set.seed(seed) 
# create a dummy dataset 
groups <- sort(sample(letters[1:4], size = 20, replace = TRUE)) %>% 
  as_data_frame()
  table(groups)  

# make 4 folds - one for each group 
#folds <- groupKFold(groups$value, k = length(unique(groups$value)))  

k <- gaged %>% 
  group_by(ecoreg) %>% 
  summarise(count = n()) %>% 
  select() 

test <- full_join(gaged, k)

k <- 5 
folds <- groupKFold(gaged$ecoreg, k)    

listviewer::jsonedit(folds)  
#lapply(folds, function(x, y) table(y[x]), y = groups$value) 

# another approach & why 
# Simple Splitting with Important Groups

# In some cases there is an important qualitative factor in the data 
# that should be considered during (re)sampling. For example:
# in clinical trials, there may be hospital-to-hospital differences
# with longitudinal or repeated measures data, subjects (or general 
# independent experimental unit) may have multiple rows in the data set, etc.

# There may be an interest in making sure that these groups are not contained 
# in the training and testing set since this may bias the test set performance 
# to be more optimistic. Also, when one or more specific groups are held out, 
# the resampling might capture the “ruggedness” of the model. In the example 
# where clinical data is recorded over multiple sites, the resampling 
# performance estimates partly measure how extensible the model is across sites.

#To split the data based on groups, groupKFold can be used:

#set.seed(3527)
#subjects <- sample(1:20, size = 80, replace = TRUE)
#table(subjects)

set.seed(123)
matris=matrix(rnorm(10),1000,20)
case1 <- as.factor(ceiling(runif(1000, 0, 4)))
case2 <- as.factor(ceiling(runif(1000, 0, 50)))

df <- as.data.frame(matris)
df$case1 <- case1
df$case2 <- case2

df %>%
  select(case1, case2) %>%
  group_by(case1, case2) %>%
  group_indices() -> indeces 

test <- as.data.frame(indeces)
## subjects
##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 
##  2  3  2  5  3  5  4  5  4  4  2  5  4  2  3  3  6  7  8  3

#folds <- groupKFold(subjects, k = 15) 


train_index <- createDataPartition(as.factor(gaged$ecoreg), p=0.8)[[1]]  


# create training & test sets  
train_pc1 <- gaged[train_index,] %>% 
  select(-c(sta, pc2_sta))  

train_pc2 <- gaged[train_index,] %>% 
  select(-c(sta, pc1_sta))  

test_pc1 <- gaged[-train_index,] %>% 
  select(-c(sta, pc2_sta)) 

test_pc2  <- gaged[-train_index,] %>% 
  select(-c(sta, pc1_sta))  






#train_index <- gaged %>% 
#  arrange(pc2_sta) %>% 
 # arrange(pc1_sta) %>% 
  
createDataPartition(as.factor(gaged$ecoreg) , p = .8, 
                                  list = FALSE, times = 1, 
                                  groups = groups) 

# create a training index 
groups <- min(5, length(gaged$pc1_sta))   # sets num. splits 

set.seed(seed) 
train_index <- createDataPartition(gaged$pc1_sta , p = .8, 
                                  list = FALSE, times = 1, 
                                  groups = groups) 

# create training & test sets  
train_pc1 <- gaged[train_index,] %>% 
  select(-c(sta, pc2_sta))  

train_pc2 <- gaged[train_index,] %>% 
  select(-c(sta, pc1_sta))  

test_pc1 <- gaged[-train_index,] %>% 
  select(-c(sta, pc2_sta)) 

test_pc2  <- gaged[-train_index,] %>% 
  select(-c(sta, pc1_sta))  

rm(train_index, groups) 
``` 

```{r prepare-glm-models-new} 
  
# set up random seed & parallel processing for glmnet & caret ----------------  
seed <- 42 
cores <- parallel::detectCores(all.tests = FALSE, logical = TRUE) 
registerDoMC(cores)  

# set up 'caret' training control & lambda -- 100 possible lambda vals [-2, 2]
set.seed(seed) 
reg.ctrl <-  trainControl(method = "repeatedcv", number = 5, repeats = 5, 
                          search = "grid", allowParallel = TRUE) 

lambda <- 10^seq(-2, 2, length = 100) 

# Split the data into training and test set ----------------------------------  
# create a data partition balanced by ecoregion 
set.seed(seed) 
train_index <- createDataPartition(as.factor(gaged$ecoreg), p=0.8)[[1]]  

# Don't run - this is how to set up by increasing PC1
# groups <- min(5, length(gaged$pc1_sta))   # sets num. splits 
# set.seed(seed) 
# train_index_pc1 <- createDataPartition(gaged$pc1_sta , p = .8, 
#                                  list = FALSE, times = 1, 
#                                  groups = groups) 

# create training & test sets -- the 'all' dataset is used below for plotting 
train_all <- gaged[train_index,] 
 
train_pc1 <- train_all %>% 
  select(-c(sta, q1_depth, .fittedPC2))   
#  select(-c(sta, ecoreg, .fittedPC2))    

train_pc2 <- train_all %>% 
  select(-c(sta, q1_depth, .fittedPC1))   
#  select(-c(sta, ecoreg, .fittedPC1))     

test_all <- gaged[-train_index,] 

test_pc1 <- test_all %>% 
    select(-c(sta, .fittedPC2))   
#  select(-c(sta, ecoreg, .fittedPC2))   

test_pc2 <- test_all %>%  
    select(-c(sta, .fittedPC1))   
#  select(-c(sta, ecoreg, .fittedPC1))   

``` 

```{r build-glm-models_pc1} 

# ridge -- alpha = 0 ---------------------------------------------------------   
set.seed(seed)              # need to set a seed each time you call a rand num 
ridge_pc1 <- train( 
  .fittedPC1 ~.,             # x = 'medv', y = the rest of the columns, from   
  data = train_pc1,       #   the dataset 'train.data' 
  method = "glmnet",      #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values 
  ) 

# lasso -- alpha = 1 --------------------------------------------------------- 
set.seed(seed)             
lasso_pc1 <- train( 
  .fittedPC1 ~.,             
  data = train_pc1,     
  method = "glmnet",      
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 1, lambda = lambda) # df with tuning values
  )

# elastic net -- alpha vals [0, 1] by caret ----------------------------------  
set.seed(seed) 
elastic_pc1 <- train(  
  .fittedPC1  ~.,              # x = 'medv', y = the rest of the columns, from 
  data = train_pc1,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl, 
  tuneLength = 10 
  ) 

# make a list of the models --------------------------------------------------
models_pc1 <- list(  
  ridge = ridge_pc1, 
  lasso = lasso_pc1, 
  elastic = elastic_pc1)   
``` 

```{r build-glm-models_pc2} 

# ridge -- alpha = 0 ---------------------------------------------------------  
set.seed(seed)             # need to set a seed each time you call a rand num 
ridge_pc2 <- train( 
  .fittedPC2 ~.,           # x = 'medv', y = the rest of the columns, from   
  data = train_pc2,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 0, lambda = lambda) # df with tuning values 
  ) 

# lasso -- alpha = 1 ---------------------------------------------------------  
set.seed(seed)              # need to set a seed each time you call a rand num 
lasso_pc2 <- train( 
  .fittedPC2 ~.,            # x = 'medv', y = the rest of the columns, from   
  data = train_pc2,         #   the dataset 'train.data'  
  method = "glmnet",        #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl,  
  tuneGrid = expand.grid(alpha = 1, lambda = lambda) # df with tuning values
  )

# elastic net -- alpha vals [0, 1] by caret ---------------------------------- 
set.seed(seed) 
elastic_pc2 <- train( 
  .fittedPC2 ~.,           # x = 'medv', y = the rest of the columns, from 
  data = train_pc2,        #   the dataset 'train.data' 
  method = "glmnet",       #   using method "glmnet" 
  preProcess = c("center", "scale"), 
  trControl = reg.ctrl, 
  tuneLength = 10          # tune length 
  ) 

# make a list of the models --------------------------------------------------
models_pc2 <- list(
  ridge = ridge_pc2, 
  lasso = lasso_pc2,  
  elastic = elastic_pc2)  
 
rm(cores, lambda, reg.ctrl)  

``` 

```{r select-glm-models}

# evaluate performance of models -- ridge, lasso and elastic net --  
#   best model is the one that minimizes the prediction error. 

# pick out the mse from the output lists  
rmse_call_pc1 <- resamples(models_pc1) %>%   
  summary(metric = c("RMSE", "MAE"))  

rmse_call_pc2 <- resamples(models_pc2) %>%   
  summary(metric = c("RMSE", "MAE"))  

# change list to tibble 
rmse_vals_pc1 <- as_tibble(   
  as.data.frame( 
    pluck(rmse_call_pc1, 'statistics')  
    ), rownames = "model" 
  ) %>% 
  mutate(axis = "pc1") 

rmse_vals_pc2 <- as_tibble(  
  as.data.frame( 
    pluck(rmse_call_pc2, 'statistics') 
    ), rownames = "model" 
  ) %>% 
  mutate(axis = "pc2") 

# prepare for plotting
rmse_vals_pc1 <- rmse_vals_pc1 %>% 
  gather(key, val, -c(model, axis))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>% 
  separate(key, c("statistic", "position"), extra = "drop") %>% 
  filter(position != "NA") %>% 
 # unite(mod_pos, statistic, position) %>% 
  spread(position, val)

rmse_vals_pc2 <- rmse_vals_pc2 %>% 
  gather(key, val, -c(model, axis))  %>%  
  mutate(key = str_replace(key, "[.]", "_")) %>% 
  separate(key, c("statistic", "position"), extra = "drop") %>% 
  filter(position != "NA") %>% 
 # unite(mod_pos, statistic, position) %>% 
  spread(position, val)

rmse_vals <- bind_rows(rmse_vals_pc1, rmse_vals_pc2) 

rm(rmse_call_pc1, rmse_call_pc2, rmse_vals_pc1, rmse_vals_pc2)

```   

```{r plot-glm-models}
# plot model RMSE
#test <- rmse_vals %>% 
#  filter(statistic == "MAE") 

ggplot(rmse_vals) + 
  facet_grid( 
    rows = vars(axis), 
    cols = vars(statistic)) +
  geom_boxplot(aes(x = as.factor(model), 
                   group = model, 
      lower = `1st`, 
      upper = `3rd`, 
      middle = `Median`, 
      ymin = `Min`, 
      ymax = `Max`),
    stat = "identity") + 
  ylim(0, 1.5) + 
  theme_bw() + 
  xlab("") 
#  ylab("RMSE") 

# save ggplot above & clean up 
ggsave("figure/super_reg_acc.png", width = 7, height = 3.5, units = "in")  

#listviewer::jsonedit(rmse_call) 
```

```{r tidy-glm-coefs}

# this code chunk tidies model coefficients  

# ridge coeffs --------------------------------------------------------------- 

coef_ridge_pc1 <- coef(  
  ridge_pc1$finalModel, 
  ridge_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_ridge_pc2 <- coef( 
  ridge_pc2$finalModel, 
  ridge_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_ridge <- full_join(coef_ridge_pc1,coef_ridge_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "ridge") 

rm(coef_ridge_pc1, coef_ridge_pc2) 

# lasso coeffs  --------------------------------------------------------------- 

coef_lasso_pc1 <- coef( 
  lasso_pc1$finalModel, 
  lasso_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_lasso_pc2 <- coef( 
  lasso_pc2$finalModel, 
  lasso_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_lasso <- full_join(coef_lasso_pc1,coef_lasso_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "lasso") 

rm(coef_lasso_pc1, coef_lasso_pc2) 

# elastic coeffs  ------------------------------------------------------------  
coef_elastic_pc1 <- coef( 
  elastic_pc1$finalModel, 
  elastic_pc1$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_elastic_pc2 <- coef( 
  elastic_pc2$finalModel, 
  elastic_pc2$bestTune$lambda) %>% 
  as.matrix() %>%    
  as_tibble(., rownames = "coeff") 

coef_elastic <- full_join(coef_elastic_pc1,coef_elastic_pc2, 
                        by = "coeff") %>% 
  rename(pc1 = 2) %>% 
  rename(pc2 = 3) %>% 
  mutate(type = "elastic") 

rm(coef_elastic_pc1, coef_elastic_pc2) 

# join model coeff ----------------------------------------------------------- 
coefs <- bind_rows(coef_ridge, coef_lasso, coef_elastic) %>% 
  select(coeff, type, everything()) 

coefs <- coefs %>% 
  gather(variable, value, -(coeff:type)) %>% 
  unite(temp, variable, type) %>%
  spread(temp, value) %>% 
  select(coeff, starts_with("pc1"), everything()) 

rm(coef_ridge, coef_lasso, coef_elastic) 

# prepare for export table 
coefs <- coefs %>% 
  filter(coeff != "(Intercept)") %>%           # drop the model intercept term   
  arrange(desc(pc2_lasso)) %>% 
  arrange(desc(pc1_lasso)) %>% 
  mutate(pc2_arrange = abs(pc2_lasso)) %>%     # temp terms to arrange coeffs  
  mutate(pc1_arrange = abs(pc1_lasso)) %>% 
  mutate(arrange = case_when(
    pc1_arrange != 0 ~ pc1_arrange, 
    TRUE ~ pc2_arrange)  
    ) %>% 
  arrange(desc(arrange)) %>% 
  select(coeff, 
         pc1_lasso, pc1_elastic, pc1_ridge, 
         pc2_lasso, pc2_elastic, pc2_ridge)  
``` 

```{r export-table_glm-coeffs} 

# set the numeric columns 
col_key_num <- coefs %>% 
  select(-coeff) %>% 
  names() 

# convert tibble to a flextable  
coef_table <- coefs %>%   
  flextable() %>% 
  colformat_num(col_keys = col_key_num, 
                big.mark=",", 
                digits = 2, na_str = "N/A") %>%  
  add_header_row(values = c("", "", "PC1", "", "", "PC2", "")) %>%  
  set_header_labels(  
  coeff  = "Explanatory variable",  
  pc1_lasso  = "Lasso",  
  pc1_elastic = "Elastic net",    
  pc1_ridge = "Ridge",     
  pc2_lasso  = "Lasso", 
  pc2_elastic = "Elastic net",  
  pc2_ridge = "Ridge") %>%     
  autofit() %>%  
  theme_booktabs()  
 

# export a docx of flextables    
coef_table <- read_docx() %>% 
  body_add_flextable(value = coef_table) 
print(coef_table, target = "output/table_coef.docx")  
    
rm(coef_table, col_key_num)    
 
```


```{r deconvolute_pca}

# 1. prep to deconvolute PC eigens into trend, seasonal, & random comps. ---- 

# 1.1. dv plots too busy -- scaling daily flow values into monthly mean depths  

gage_mon <- gage_dv %>%  
  mutate(yr = year(Date)) %>% 
  mutate(mon = month(Date)) %>%  
  mutate(q30_q1_diff = q30_depth - q1_depth) %>%  
  group_by(sta, mon, yr) %>% 
  summarize(waterYear = first(waterYear),   
            q1_mon     = mean(q1_depth), 
            q7_mon     = mean(q7_depth), 
            q30_mon    = mean(q30_depth),           
            PC1_mon    = mean(.fittedPC1), 
            PC2_mon    = mean(.fittedPC2), 
            q30_q1_mon = mean(abs(q30_q1_diff)),  
            ecoreg      = first(ecoreg) 
            ) %>% 
  mutate(day = 15) %>%   
  ungroup() %>% 
  unite("Date", c("yr", "mon", "day")) %>% 
  mutate(Date = ymd(Date)) %>% 
  gather(key = axis, value = PCval, 
         -c(sta, Date, waterYear, ecoreg, q1_mon, 
            q7_mon, q30_mon, q30_q1_mon) 
         )  %>% 
  as_tibble() 


# 2. calculate parameters to remove season & trend from observed vals ----
# 2.1. find the average monthly frequency & enframe 
mon_freq <- gage_mon  %>% 
      select(-c(waterYear,ecoreg)) %>%  
  arrange(Date) %>% 
  unite(sta_axis, c(sta, axis), sep = ".") %>% 
    ungroup() %>%                       # not sure what this is for.... 
    split(.$sta_axis) %>% 
  map_dfc(~ time_frequency( 
    period = "auto", data = .) 
  ) %>% 
  gather(key = sta.axis, value = freq) %>%  
  summarise(freq = mean(freq), 
            max = max(freq),
            min = min(freq), 
            sd = sd(freq)
            ) 


# 2.2. find the average monthly trend by sta 
mon_trend_sta <- gage_mon  %>% 
      select(-c(waterYear,ecoreg)) %>%  
  arrange(Date) %>% 
  unite(sta_axis, c(sta, axis), sep = ".") %>% 
    ungroup() %>%                       # not sure what this is for.... 
    split(.$sta_axis) %>% 
  map_dfc(~ time_trend( 
    period = "auto", data = .) 
  ) %>% 
  gather(key = sta.axis, value = trend) %>%  
  separate(sta.axis, into = c("sta", "sta2", "axis"), extra = "merge") %>% 
  unite(sta, c(sta, sta2)) %>%           
  spread(axis, trend) 

# 2.3 find average monthly trend 
mon_trend <- mon_trend_sta %>% 
  summarise(trend = median(PC1_mon)) # no differnce in trend between PC1 & PC2 

# 2.4 join and tidy 
decomp_input <- bind_cols(mon_freq, mon_trend) 

rm(mon_freq, mon_trend, eco_join)

# 3. make function to decompose into seasonal, trend, & remainder -------------  
decomp_fun <- function(df) { 
  arrange(df, .data$Date) %>% 
  group_by(.data$sta) %>% 
  time_decompose( 
    target  = PCval, 
    data         = ., 
    method       = "stl", 
    frequency    = decomp_input$freq, 
    trend        = decomp_input$trend, 
    merge        = TRUE, 
    message      = TRUE 
    ) %>% 
  ungroup() %>% 
  anomalize(remainder, 
            method = "gesd", 
            alpha = 0.003) 
}

# 3.1 map_dfr causes issues with indexing; using a manual split and combine  
gage_mon1 <- gage_mon %>% 
  filter(axis == "PC1_mon") 

gage_mon2 <- gage_mon %>% 
  filter(axis == "PC2_mon") 

gage_mon1 <- decomp_fun(gage_mon1) 
gage_mon2 <- decomp_fun(gage_mon2)  

# prepare to rejoin the dataframes 
gage_mon0 <- gage_mon1 %>% 
  select(sta:ecoreg) 
  
gage_mon1 <- gage_mon1 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC1") %>% 
  rename(sta = sta_PC1) %>% 
  rename(Date = Date_PC1)
  
gage_mon2 <- gage_mon2 %>% 
  select(sta, Date, PCval:anomaly) %>% 
  rename(mon = PCval) %>% 
  rename_all(paste0, "_PC2") %>% 
  rename(sta = sta_PC2) %>% 
  rename(Date = Date_PC2)

# join the datatables   
gage_mon1 <- full_join(gage_mon0, gage_mon1, 
                  by = c("sta", "Date")
                  ) 

gage_mon <- full_join(gage_mon1, gage_mon2, 
                  by = c("sta", "Date")
                  ) 

rm(gage_mon0, gage_mon1, gage_mon2, decomp_input, mon_trend_sta, decomp_fun)  


# create station and ecoregion summaries 
gage_mon <- gage_mon %>% 
  group_by(sta) %>%  
  mutate(sta_q1 = mean(q1_mon)) %>% 
  mutate(sta_q7 = mean(q7_mon)) %>%   
  mutate(sta_q30 = mean(q30_mon)) %>% 
  mutate(sta_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(sta_PC1 = mean(mon_PC1)) %>% 
  mutate(sta_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>% 
  group_by(ecoreg) %>%  
  mutate(ecoreg_q1 = mean(q1_mon)) %>% 
  mutate(ecoreg_q7 = mean(q7_mon)) %>%   
  mutate(ecoreg_q30 = mean(q30_mon)) %>% 
  mutate(ecoreg_q30_q1 = mean(q30_q1_mon)) %>% 
  mutate(ecoreg_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_PC2 = mean(mon_PC2)) %>% 
  ungroup() %>%  
  group_by(Date, ecoreg) %>% 
  mutate(ecoreg_mean_PC1 = mean(mon_PC1)) %>% 
  mutate(ecoreg_seas_PC1 = mean(season_PC1)) %>% 
  mutate(ecoreg_trend_PC1 = mean(trend_PC1)) %>%   
  mutate(ecoreg_remain_PC1 = mean(remainder_PC1)) %>%   
  ungroup() %>% 
  arrange(ecoreg_PC1) %>% 
  mutate(ecoreg = fct_reorder( 
    as.factor(ecoreg), ecoreg_PC1)  # sets up the ordering by pc-axis 
    ) %>% 
  arrange(sta_PC1) %>%   
  mutate(sta = fct_reorder( 
    as.factor(sta), sta_PC1)  # sets up the ordering by pc-axis 
    ) 
```

```{r plot pca}

pca_vars <- import("data/pca_vars.csv")    
 
# Plot PC1 vs PC2 plot 
pc1_pc2_plot <- gage_mon %>% 
  ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ ecoreg) + 
  geom_point(size = 0.5, color = "gray",  
          aes(color = as.factor(sta)) 
          ) +  
  geom_jitter(aes(sta_PC1, sta_PC2),  # station averages 
        shape = 3,                   # cross  
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_PC1, ecoreg_PC2),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
# plot pc eigenvectors ###
    geom_segment(data = pca_vars, 
    aes(x = 0, y = 0, xend = PC1, yend = PC2), 
    arrow = arrow(length = unit(0.03, "npc")) 
    ) + 
  geom_text(data = pca_vars, 
               size = 3,  
               nudge_x = 0.5, 
               nudge_y = -0.1, 
                hjust = 'outside', 
               aes(x = PC1, y = PC2, label = labels) 
            ) +   
# set up the axes ### 
  scale_y_continuous(limits = c(-1, 2.5)) + 
  theme_bw() + 
  theme(legend.position="none")  +  
#  labs(title = "Principal components of mean daily streamflow for water years 1980-2017", #     subtitle = "Monthly averages of N = 42 stations") + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") 


pc1_q7_plot <- gage_mon %>% 
  ggplot(aes(y =mon_PC1, x = q7_mon))  + 
  geom_jitter(size = 0.7, 
              color = "gray" 
              ) + 
  geom_smooth(method = "lm", 
              color = "black", 
              size = 0.7) + 
  facet_wrap( ~ ecoreg) + 
  geom_jitter(aes(sta_q7, sta_PC1),  # station averages 
        shape = 3,                   # cross 
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_q7, ecoreg_PC1),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
  theme_bw() + 
  scale_x_log10() + 
  theme(legend.position="none") + 
  xlab("7-day mean daily flow") + 
  ylab("PC1 axis (94.8%)") 

pc2_q_diff_plot <- gage_mon %>% 
  ggplot(aes(y = mon_PC2, x = q30_q1_mon))  + 
  geom_jitter(size = 0.7, 
              color = "gray" 
              ) + 
  geom_smooth(method = "lm", 
              color = "black", 
              size = 0.7) + 
  geom_jitter(aes(sta_q30_q1, sta_PC2),  # station averages 
        shape = 3,                   # cross 
        size = 1.5, 
    colour = "black") +  
  geom_point(aes(ecoreg_q30_q1, ecoreg_PC2),  # ecoregion averages 
             shape = 15,                   # filled square 
             size = 2, 
    colour = "black") +  
  facet_wrap( ~ ecoreg) + 
  theme_bw() + 
  scale_x_log10() + 
  theme(legend.position="none") + 
  xlab("Difference between 30-day and 1-day mean daily flow depths") + 
  ylab("PC2 axis (4.5%)") 

cowplot::plot_grid( 
  pc1_pc2_plot, pc1_q7_plot, pc2_q_diff_plot, 
  ncol = 1, 
  align = "v" 
) 

cowplot::ggsave2("figure/pc_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(pc1_pc2_plot, pc1_q7_plot, pc2_q_diff_plot) 
```

```{r plot_time-series_PC1}

# prepare to order time series plot 
#pca_plot <- pca_plot %>% 
#    arrange(PC1_ecoreg) 
  
# split out the PC1 axis 
#mon_plot_pc1 <- gage_mon %>% 
#    filter(axis == "PC1_mon") %>% 
#  mutate(ecoreg = fct_reorder(      # sets up the ordering by pc-axis 
#    as.factor(ecoreg), pca_plot$PC1_ecoreg)  
#    ) 

# filter anomonies to plot below   
anom_PC1 <- gage_mon %>% 
  filter(anomaly_PC1 == "Yes")

# PC1-observation plot 
pc1_obs <- gage_mon %>% 
  ggplot(aes(Date, observed_PC1)) + 
  facet_wrap(~ ecoreg) +   
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_mean_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("PC1 axis values")
  
# PC1-seasons plot 
pc1_seas  <- gage_mon %>% 
  ggplot(aes(Date, season_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_seas_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Season (12-months)")

# PC1-trend plot 
pc1_trend  <- gage_mon %>% 
  ggplot(aes(Date, trend_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_trend_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-4.6, 5.1)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Trend (60-months)")

# PC1 - remainder plot 
pc1_remainder <- gage_mon %>%   
  ggplot(aes(Date, remainder_PC1)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size =  0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, ecoreg_remain_PC1), 
            size = 0.3, 
            colour = "black"
            ) + 
  geom_jitter(aes(Date, ecoreg_remain_PC1), 
              data = anom_PC1, 
              shape = 4, 
              size = 0.7) + 
  scale_y_continuous(limits = c(-4.0, 5.8)) + 
  theme_bw() + 
  xlab("") + 
  ylab("Remainder")


# plot the plots above as a grid & save 
cowplot::plot_grid(
  pc1_obs, pc1_seas, pc1_trend, pc1_remainder, 
  ncol = 1, 
  align = "v"
)

cowplot::ggsave2("figure/pc1-deconv.png", 
        units = "in", 
        width = 7, 
        height = 9)


rm(pc1_obs, pc1_seas, pc1_trend, pc1_remainder, anom_PC1) 
```

```{r Mclust}

# make monthly inputs for clustering 
clust_input <- gage_mon %>% 
  select(sta, Date, waterYear, ecoreg, q1_mon, q7_mon, q30_mon) %>% 
  group_by(sta) %>% 
  distinct(Date, .keep_all = TRUE) %>% 
  ungroup() 

# Get rid of categorical data for Mclust 
X <- clust_input %>%           
  select(-c(sta, Date, waterYear, ecoreg))  

# find Bayesian Information Criterion for mlust 
BIC <- mclustBIC(X) 

# run Mclust
mod <- Mclust(X, x = BIC) 

# get Mclust model results 
mod_glance_mon <-glance(mod) 

#  use broom to tidy up the Mclust model 
mod_tidy_mon <- tidy(mod)           

# add Mclust model classes to the orig data & add ecoregions 
mod_tidy_mon <- augment(mod, gage_mon)   

# sample cluster plot 
factoextra::fviz_mclust(mod, 
                          "classification", 
                          geom = "point", 
                          ellipse.type = "t", 
                          ellipse.level = 0.7, 
                          pointsize = 1.5,  
                          palatte = "npg" 
                          ) 
 
rm(clust_input)  

```

```{r Mclust-plot}
 
# prepare for plotting - reorganize flow by centroids 
mod_tidy_sum <- mod_tidy_mon %>% 
  group_by(.class) %>% 
  summarize(q1_flow = mean(q1_mon), 
            q7_flow = mean(q7_mon), 
            q30_flow = mean(q30_mon), 
            PC1_flow = mean(mon_PC1), 
            PC2_flow = mean(mon_PC2)            
            ) %>% 
  ungroup() %>% 
  arrange(q1_flow) %>% 
  mutate(flow_type = c("type1", "type2", "type3", 
                       "type4", "type5", "type6", 
                       "type7", "type8", "type9")
         ) 

# join flow_type to tidy cluster model 
mod_tidy_mon <- full_join(mod_tidy_mon, mod_tidy_sum,  by = ".class") %>% 
  group_by(.class, ecoreg) %>% 
  mutate(q1_flow_eco = mean(q1_mon)) %>% 
  mutate(q7_flow_eco = mean(q7_mon)) %>%  
  mutate(q30_flow_eco = mean(q30_mon))  %>% 
  mutate(PC1_flow_eco = mean(mon_PC1)) %>% 
  mutate(PC2_flow_eco = mean(mon_PC2)) %>%  
  ungroup() %>% 
  mutate(flow_type = as.factor(flow_type)) 

# plot clusters by ecoregion
mclust_eco_plot  <- mod_tidy_mon %>%  
ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ ecoreg) + 
  geom_jitter(
    aes(color = flow_type,
        shape = ecoreg), 
    size = 2, 
    show.legend = FALSE)  + 
  scale_alpha(".uncertainty") + 
 #   scale_colour_brewer(palette = "Greys") +
# map the title of the geom for the legend  
#  scale_shape_discrete(name="Ecoregion") + 
#  scale_color_hue(name="Flow type") + 
# plot centroids 
  geom_point(
    aes(ecoreg_PC1, 
        ecoreg_PC2),
    show.legend = FALSE, 
    color = "black", 
    shape = 3,                   # cross 
    size = 2.5)  + 
# plot pc eigenvectors ###
    geom_segment(data = pca_vars, 
    aes(x = 0, y = 0, xend = PC1, yend = PC2), 
    arrow = arrow(length = unit(0.03, "npc")) 
    ) + 
  geom_text(data = pca_vars, 
               size = 3,  
               nudge_x = 0.5, 
               nudge_y = -0.1, 
                hjust = 'outside', 
               aes(x = PC1, y = PC2, label = labels) 
            ) +   
# set up the axes ### 
  theme_bw() + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") + 
  theme(legend.position="bottom") 

# plot clusters by flow type 
mclust_flow_plot <- mod_tidy_mon %>%  
ggplot(aes(mon_PC1, mon_PC2)) + 
  facet_wrap(~ flow_type) + 
  geom_jitter(
    aes(shape = ecoreg, 
        color = flow_type), 
    show.legend=FALSE, 
    size = 2)  + 
#    scale_colour_brewer(palette = "Greys") + 
  geom_point(
    aes(PC1_flow_eco, 
        PC2_flow_eco, 
        shape = ecoreg), 
   show.legend=FALSE, 
    color = "black", 
              size = 3)  + 
  scale_alpha(".uncertainty") + 
  theme_bw() + 
  xlab("PC1 axis (94.8%)") + 
  ylab("PC2 axis (4.5%)") + 
  theme(legend.position="bottom") 

# plot grid of ggplots 
cowplot::plot_grid( 
  mclust_eco_plot,
  mclust_flow_plot, 
  ncol = 1, 
  align = "v" 
) 

cowplot::ggsave2("figure/mclust_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(mclust_flow_plot, mclust_eco_plot, X, BIC, 
   mod_tidy_mon, mod_tidy_sum, mod_glance_mon, 
   mod)  


```

```{r bootstrap, eval=FALSE}

# Construct bootstrap confidence intervals of PC axes to identify a difference 
#   in means 
#
# Notes on dabest
# dabest for monthly scale (14,027 obs; 40,000 reps) takes ~2 hours.  
# even with increasing memory, I ran into a memory allocation 
#  limit at 50,000 reps

# increase memory allocation for dabest() 
usethis::edit_r_environ("project") 

# remove 'leftover' attributes introduced by stl 
gage_mon <- gage_mon %>% 
  as_tibble() 

# shorten names of ecoregion to fix overplotting 

# calculate 95% CI for PC1 - 'control' is Pierre Shale Plains 
ci_input_pc1 <- gage_mon %>%      
  dabest(ecoreg, observed_PC1,  
         idx = c("Pierre Shale Plains",     # groups to test - first is control
                 "Pine Ridge Escarpment", 
                 "White River Badlands", 
                 "Black Hills Plateau", 
                 "Keya Paha Tablelands", 
                 "Sand Hills"), 
         paired = FALSE, 
         reps = 40000                     # number of bootstraps 
         ) 

# calculate 95% CI for PC2 - 'control' is Sand Hills
ci_input_pc2 <- gage_mon %>%  
  dabest(ecoreg, observed_PC2,  
         idx = c("Pierre Shale Plains",     
                 "Pine Ridge Escarpment", 
                 "White River Badlands", 
                 "Black Hills Plateau", 
                 "Keya Paha Tablelands", 
                 "Sand Hills"
                 ), 
         paired = FALSE, 
         reps = 40000 
         ) 

# export results as Rdata files 
# how???

# this is the output of the runs 
ci_input_pc1 
co_input_pc2
```

```{r bootstrap-swarmplots} 
#ci_input_pc1 <- load("data/dabest_pc1.rdata") 

ci_plot_pc1 <- plot(ci_input_pc1, 
                    rawplot.type = "swarmplot", 
                    rawplot.ylabel = "PC1 Axis", 
                    tick.fontsize = 7, 
                    axes.title.fontsize = 11, 
                    palette = "Greys"  
                    )    

ci_plot_pc2 <- plot(ci_input_pc2, 
                    rawplot.type = "swarmplot", 
                    rawplot.ylabel = "PC2 Axis", 
                    tick.fontsize = 7, 
                    axes.title.fontsize = 11, 
                    palette = "Greys"  
                    )    

cowplot::plot_grid( 
  ci_plot_pc1,
  ci_plot_pc2,  
  ncol = 1, 
  align = "v" 
) 
# save the plot 

cowplot::ggsave2("figure/ci_plot.png", 
        units = "in",  
        width = 7, 
        height = 9) 

rm(ci_plot_pc1, ci_plot_pc2)
```

```{r ci_result_table, eval=FALSE}  

# pluck results for summary tables & bind
ci_results_pc1 <- ci_input_pc1 %>% 
  pluck("result") %>% 
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots)) 
  
ci_results_pc2 <- ci_input_pc2 %>% 
  pluck("result") %>% 
  select(-c(paired, pct_ci_low, pct_ci_high, bootstraps, nboots)) 

ci_table <- rbind(ci_results_pc1, ci_results_pc2) 

# prepare results for presentation 
ci_table <- ci_table %>% 
  mutate(variable =  case_when( 
    variable == "observed_PC1" ~ "PC1", 
    variable == "observed_PC2" ~ "PC2"    
    )
    ) %>% 
  mutate(ci = as.integer(ci)) %>% 
  select(variable, everything())

# convert tibble to a flextable after fixing vars for presentation 
ci_table <- ci_table %>% 
  flextable() %>% 
#  colformat_num(col_keys = col_key_num, 
#                big.mark=",", 
#                digits = 1, na_str = "N/A") %>% 
  set_header_labels(control_group = "Control Group", 
                    test_group = "Test Groups", 
                    control_size = "Control Size",
                    test_size = "Test Size", 
                    func = "Test Statistic", 
                    variable = "Variable",
                    difference = "Mean Difference", 
                    ci = "CI", 
                    bca_ci_low = "Lower Limit", 
                    bca_ci_high = "Upper Limit") %>% 
  autofit() %>% 
  align(., part = "all", align = "center") %>% 
  theme_booktabs() 

  
ci_table2 <- read_docx() %>% 
  body_add_flextable(value = ci_table)  

print(ci_table2, target = "output/ci_table.docx") 

rm(ci_results_pc1, ci_results_pc2, ci_table2, ci_table) 

```

```{r pca_decomp_table} 

# summarize observations of the time series into observations for a summary table 
mon_sum_gath <- gage_mon %>% 
  group_by(ecoreg, axis) %>% 
  summarize(value = round(mean(observed), digits = 2), 
            obs_eco    = mean(abs(observed)), 
            seas_eco   = mean(abs(season)), 
            trend_eco  = mean(abs(trend)), 
            remain_eco = mean(abs(remainder)) 
            ) %>%  
  ungroup() %>% 
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc) %>% 
  # readjust to 100% 
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>% 
  mutate(axis = str_remove_all(axis, "_mon")) %>% 
  arrange(axis, value) %>% 
  select(-c(obs_eco:sum)) 

# split & mutate column names by PC  
mon_sum_pc1 <- mon_sum_gath %>% 
  filter(axis == "PC1") %>% 
  select(-axis) %>% 
  rename_all(function(x) paste0("PC1_", x)) 
 
mon_sum_pc2 <- mon_sum_gath %>% 
  filter(axis == "PC2") %>% 
  select(-axis) %>% 
  rename_all(function(x) paste0("PC2_", x)) 

# find averages across all the stations 
mon_sum_pc1_all <- gage_mon %>% 
  filter(axis == "PC1_mon") %>% 
  summarize(value = round(mean(observed), digits = 2), 
            obs_eco    = mean(abs(observed)), 
            seas_eco   = mean(abs(season)), 
            trend_eco  = mean(abs(trend)), 
            remain_eco = mean(abs(remainder)) 
            ) %>%  
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc)  %>% 
  # readjust to 100% 
  mutate(ann_explained   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(trend_explained  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>% 
#  mutate(axis = str_remove_all(axis, "_mon")) #%>% 
#  arrange(axis, value) %>% 
  select(-c(obs_eco:sum)) %>% 
  mutate(ecoreg = "All Stations") %>% 
  rename_all(function(x) paste0("PC1_", x)) 
  
  
mon_sum <- bind_rows(mon_sum_pc1_all, mon_sum_pc1) %>% 
  select(ecoreg = PC1_ecoreg, everything()) 

rm(mon_sum_gath) 

``` 

```{r plot_time-series_PC2, eval=FALSE}


# split out the PC1 axis 
mon_plot_pc2 <- gage_mon %>% 
    filter(axis == "PC2_mon") %>% 
  mutate(ecoreg = fct_reorder(
    as.factor(ecoreg), obs_eco_mean)  # sets up the ordering by pc-axis 
    ) 

mon_sum_pc2 <- mon_plot_pc2 %>% 
#  select(sta, ecoreg, observed, season, trend, remainder) %
  group_by(ecoreg) %>% 
  summarize(median_pc2 = round(median(obs_eco), digits = 2), 
            obs_eco    = median(abs(obs_eco)), 
            seas_eco   = median(abs(seas_eco)), 
            trend_eco  = median(abs(trend_eco)), 
            remain_eco = median(abs(remain_eco)) 
            ) %>% 
  mutate(seas_perc   = seas_eco / obs_eco) %>% 
  mutate(trend_perc  = trend_eco / obs_eco) %>% 
  mutate(remain_perc = remain_eco / obs_eco) %>% 
  mutate(sum = seas_perc + trend_perc + remain_perc) %>% 
  # readjust to 100% 
  mutate(interann_var   = round(seas_perc / sum, digits = 2)) %>% 
  mutate(multi_yr_var  = round(trend_perc / sum, digits = 2)) %>% 
  mutate(remainder = round(remain_perc / sum, digits = 2)) %>%
  ungroup() %>% 
  select(ecoreg, median_pc2, interann_var, multi_yr_var, remainder)

# PC2-observation plot 
pc2_obs <- mon_plot_pc2 %>% 
  ggplot(aes(Date, observed)) + 
  facet_wrap(~ ecoreg) +   
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, obs_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("")

# PC2-seasons plot 
pc2_seas  <- mon_plot_pc2 %>% 
  ggplot(aes(Date, season)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, seas_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_seas 

# PC2-trend plot 
pc2_trend  <- mon_plot_pc2 %>% 
  ggplot(aes(Date, trend)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size = 0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, trend_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_trend 

# PC2 - remainder plot 
pc2_remainder <- mon_plot_pc2 %>% 
  ggplot(aes(Date, remainder)) + 
  facet_wrap(~ ecoreg) + 
  geom_line(size =  0.5, 
            colour = "grey"
            ) + 
  geom_line(aes(Date, remain_eco), 
            size = 0.3, 
            colour = "black"
            ) + 
  scale_y_continuous(limits = c(-0.5, 2.5)) + 
  theme_bw() + 
  xlab("") 

pc2_remainder 

plot_grid(
  pc2_obs, pc2_seas, pc2_trend, pc2_remainder, 
  ncol = 1, 
  align = "v"
)


ggsave2("figure/pc2-deconv.png", 
        units = "in", 
        width = 7, 
        height = 9)

```


```{r summarize-results-PCA_delete,eval=FALSE}

#  4. Create summaries of the results ----------------------------------------



pca_ann <- gage_dv %>% 
  group_by(sta, waterYear) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),  
            ecoreg      = first(ecoreg)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean) 

pca_sum <- gage_dv %>% 
  group_by(sta) %>% 
  summarize(PC1_mean    = mean(.fittedPC1),
            PC2_mean    = mean(.fittedPC2), 
            q1_mean     = mean(q1_depth), 
            q7_mean     = mean(q7_depth),
            q30_mean    = mean(q30_depth), 
            q1_q30_mean = mean(q1_q30_diff),  
            ecoreg      = first(ecoreg)
            ) %>% 
  ungroup() %>% 
  arrange(PC2_mean) %>% 
  arrange(PC1_mean)  
```


```{r to-delete}
# read in geopackage data of zonal summaries for watersheds 
#gage_summary <- st_read("sp_data/eco-drought.gpkg", 
#                       layer = "gage_summary", 
#                       as_tibble = TRUE) %>% 
#  st_drop_geometry() 

# convert environmental data to tibbles -- delete after fixing plots 
#gage_summary <- gage_summary %>% 
#  as_tibble() %>% 
#  modify_if(., is.factor, as.character) 
```

```{r pc-plot_delete,eval=FALSE} 



p1 <- autoplot(pca_matrix, 
               data = gage_dv, 
               scale = 0, 
               colour = "ecoreg", 
               size = 0.1, 
               alpha = 0.5) 
p1 + theme_bw()


ggplot(pca_mon, aes(PC1_mean, PC2_mean)) + 
  geom_jitter(
    aes(
      shape = ecoreg, 
      colour = ecoreg), 
    size = 1.0
    ) + 
  theme_classic()


```

```{r things}



# prepare data 
#pca_input <- gage_input %>%  
#  select(date, sta, q, q7, q30, contrib_drain_area_va)

# training data: 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
#  01    0.013      0.112      0.265     0.942   0.050   0.942   0.992  
#  02   -0.005      0.070      0.223     0.949   0.047   0.949   0.994    
#  03   -0.102     -0.136     -0.148     0.959   0.035   0.959   0.994   
#  04   -0.008      0.071      0.242     0.947   0.047   0.947   0.994   
#  05   -0.056     -0.113     -0.187     0.953   0.041   0.953   0.994 
#  06   -0.031     -0.070     -0.120     0.960   0.034   0.960   0.994 
#  07   -0.055     -0.067     -0.060     0.966   0.029   0.966   0.995 
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# vars:
# run  PC  q1_depth  q7_depth  q30_depth       
#   01  1    0.577      0.589     0.566   
#   01  2    0.584      0.186    -0.790     
#   02  1    0.578      0.588     0.567       
#   02  2    0.567      0.210    -0.796      
#   03  1    0.578      0.584     0.569           
#   03  2    0.545      0.243    -0.803       
#   04  1    0.578      0.588     0.566           
#   04  2    0.571      0.205    -0.795       
#   05  1    0.578      0.586     0.568           
#   05  2    0.555      0.228    -0.800       
#   06  1    0.578      0.585     0.569            
#   06  2    0.553      0.233    -0.800       
#   06  1    0.578      0.583     0.571             
#   06  2    0.542      0.248    -0.803  
#   07
#   07
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

#Mclust model:
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# num  model  log.likeli  n  df   BIC     ICL     purpose 
#  01  VEV     116.42    26  25  151.4   151.0   estimate NA 
#  02  VEV      82.65    23  17  112.0   111.6   estimate NA 
#  03  VVV     139.30    29  19  214.6   214.5   estimate NA   
#  04  EEV     161.53    24  65  116.5   116.4   estimate NA    
#  05  EEE     139.33    24  37  161.6   161.0   estimate NA    
#  06  EEV     130.93    27  19  199.2   199.2   estimate NA    
#  07  VVV     151.58    31  19  237.9   237.7   estimate NA    
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#    Cl9#
#  01   14      8     4    na    na    na    na      na      na  
#  02   15      8    na    na    na    na    na      na      na  
#  03   14     15    na    na    na    na    na      na      na  
#  04    3      4     3     2     4     2     1       1       4    
#  05    2      9     3     1     5     2     1       1      na  
#  06   14     13    na    na    na    na    na      na      na  
#  07    7     24    na    na    na    na    na      na      na  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# a final cluster analysis was conducted after missing values were found 
# two runs for final cluster analysis are conducted 

# | Run_num | description | num_obs | 
# |:-------:|:-----------:|:-------:|
# |    1    | all years   | 317,036 |                                     
# |    2    | wet years   | 147,188 |
# |    2    | dry years   | 169,848 |

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the series of station runs are as follows: 

pca_eigen 
# num  lambda_q1  lambda_q7  lambda_q30 PC1perc PC2perc PC1cum  PC2cum 
# all   -0.082     -0.086     -0.027     0.967   0.028   0.967   0.995 
# wet   -0.060     -0.078     -0.089     0.963   0.031   0.963   0.994 
# dry   -0.145     -0.185     -0.211     0.963   0.032   0.963   0.995
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

pca_vars 
# run  PC  q1_depth  q7_depth  q30_depth   notes    
#  all  1    0.578      0.583     0.571           
#  wet  1    0.578      0.584     0.570           
#  dry  1   -0.578     -0.584    -0.570   need to inverse
#  all  2    0.540      0.251    -0.803       
#  wet  2    0.546      0.243    -0.802    
#  dry  2   -0.546     -0.243     0.802   need to inverse   

# eigenvecter:
#   96% of covarience is explained by PCA1 & 4% of varience by PCA2
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
# variables:
#   PC1 - approximately equal loadings of Q1, Q7, Q30; 
#         approximates hydrologic export 
#   PC2 - large opposite loadings of Q1 & Q30  
#         contribution of baseflow vs event-flow
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Preparing to cluster by the central tendancy
gage_pca_sum <- gage_pca_sum %>% 
  arrange(sta)

gage_clust <- gage_pca_sum %>% 
  select(q1_mean:q30_mean)

gage_clust_meta <- gage_pca_sum %>% 
  select(sta)

# apply model-based-clustering, extract results & add metadata
gage_clust_l <- Mclust(gage_clust) 

gage_clust <- as_tibble(gage_clust_l$z) 
gage_clust <- bind_cols(gage_clust_meta, gage_clust) 

# drop low probs - join & rearrange 
gage_clust <- gage_clust %>% 
  gather(key = group, value = prob, -sta) %>% 
  filter(prob > 0.5) 

gage_clust <- full_join(gage_pca_sum, gage_clust, 
                             by = "sta")

gage_clust <- gage_clust %>% 
  dplyr::select(group, sta, everything()) %>% 
  arrange(sta) %>%  
  arrange(group)

# join cluster to PCA data & arrange
gage_mod <- full_join(gage_pca, gage_clust, by = "sta") 

gage_mod <- gage_mod %>% 
  select(group, sta, everything()) %>% 
  select(-c(q:q30_tr)) %>% 
  select(-eigen_dist) 

summary(gage_clust_l) # Print a summary 

#clean up environment 
rm(pca_input, pca_matrix, pca_check, pca_eigen, pca_vars) 
rm(gage_clust_meta, gage_pca_sum, gage_pca, gage_input) 

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Mclust model:

# num  model  log.likeli  n  df   BIC     ICL     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all  VEV     190.89    31  41  241.0   240.8  
# wet  EEE     175.15    31  29  250.7   249.4  
# dry  EEE     165.01    31  37  203.0   202.6   
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Clustering table:
# run#  Cl1#  Cl2#  Cl3#  Cl4#  Cl5#  Cl6#  Cl7#    Cl8#  
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# all    7     11     5     3     5    na    na      na     
# wet    3      5     1     4    16     2    na      na    
# dry    4     12     5     1     1     6     1       1     
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Model-based clustering selected a model with two-three components 
# (i.e. clusters). The optimal selected model name is VEV model. 
# That is the five components are ellipsoidal with varying volume, 
# and orientation, and equal shape. The summary contains also the 
# clustering table specifying the number of observations in each 
# clusters.

tidy(pca_matrix, matrix = "u")

test <- augment(pca_matrix, pca_input) 

```
```